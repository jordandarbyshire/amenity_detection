{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Capstone Project: Using Object Detection to Classify Amenities in Vacation Rental Listings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Jordan Darbyshire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Project Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this project to identify objects from images of homes uploaded onto vacation rental sites. Potential customers browsing vacation rental sites are looking for specific amenities in their vacation rental. For example, they want an oven to cook in, a pool to swim in, a pool table for entertainment... the list goes on. When a property owner creates an ad on a vacation rental website, they would usually manually type in their amenities or select from a large list. Inaccurate amenities can result in lost revenue due to bad reviews or last-minute cancellations.\n",
    "\n",
    "Users of the website will search for properties based on their desired amenities, and will be shown a list of properties based on their preferences. What if there was an easier way to add amenities to the listing, simply by uploading the property images without the manual labor? This is where Object Detection come in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspiration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inspiration for this project comes from an [article](https://medium.com/airbnb-engineering/amenity-detection-and-beyond-new-frontiers-of-computer-vision-at-airbnb-144a4441b72e) I came across that highlighted an Airbnb computer vision project where they aim to detect room type by the types of objects identified in their listing photos.\n",
    "\n",
    "I was interested in this project because of my interest in the Computer Vision field, which is still rapidly evolving. I also have a love for travelling, and can draw on my experience in using vacation rental sites for this project. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any hotel or vacation rental site such as Vrbo, Hotels.com, TripAdvisor, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset I used for this Capstone project is Open Images V4 (OIDv4), which contains 1.99 million images spanning 600 classes. The dataset was produced by Google. It is the largest existing dataset with object location annotations, which are bounding boxes drawn by humans to ensure accuracy and consistency [Source](https://storage.googleapis.com/openimages/web/factsfigures_v4.html). The images often show complex scenes with several objects (8 annotated objects per image on average) [Source](https://arxiv.org/abs/1811.00982). There are annual competitions hosted by Kaggle based on this dataset, for the purpose of advancing the Computer Vision field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose this dataset due to the number of classes applicable to my business problem. The inclusion of the bounding box \"labels\" was also a bonus. Without these labels, I would have had to create these manually. To begin this project, I decided that I would choose the most relevant 20 classes, based on amenities that would be desirable to the consumer. The image classes I chose are indoor objects that belong to these room types: Bedroom, Bathroom, Living Room, Kitchen, and Outdoor (Swimming pool)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "import urllib\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I quickly realized that the biggest challenge with starting a project for Object Detection is putting together a custom dataset to run models. Since the Open Images dataset is so large, it would be a monumental task to download the full dataset (split into 513GB for training, 12GB for validation, and 36GB for testing) and sort through it to get what I need. The Open Images website does not provide an option to download select classes of images. Fortunately, I discovered that there are a few open source solutions to this problem. I attempted a few different \"toolkits\" that promised to solve this problem by downloading only the desired classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [OIDv4 ToolKit](https://github.com/EscVM/OIDv4_ToolKit) finally allowed me to obtain only the classes I was interested in from the Open Images dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Toolkit:\n",
    "#!git clone https://github.com/EscVM/OIDv4_ToolKit.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download train set images for desired classes\n",
    "# The script below allowed me to copy and paste the commands into Anaconda prompt without manually editing each line:\n",
    "#for name in classes:   \n",
    "    #print(f\"python main.py downloader --classes {name} --type_csv train --limit 750\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I capped the number of images at 750 after observing some classes had tens of thousands of images, while half of my desired classes had less than 750 for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at count of .jpg files in the dataset folder\n",
    "import pathlib\n",
    "\n",
    "data_dir = (\"data/Dataset_20\")\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "\n",
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "image_count "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completion of the downloading process, I ended up with 11,907 images (3 GB in size)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every downloaded image is accompanied by a text file that contains the class name and the bounding box coordinates in Pascal VOC format (more on this later, refer to data dictionary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Image Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OpenCV to read in an image, open a window to view the image\n",
    "# OpenCV converts the image to a numpy array\n",
    "example_image = cv2.imread(\"data/Dataset_20/Bathtub/00a60d051d75144f.jpg\", cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "# Opens a window to display the image\n",
    "cv2.imshow(\"image\",example_image)\n",
    "# Wait until user presses a key\n",
    "cv2.waitKey(0)\n",
    "# Closes window based on waitforkey parameter\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dimensions of an image using OpenCV\n",
    "# This code is from https://www.tutorialkart.com/opencv/python/opencv-python-get-image-size/\n",
    "\n",
    "# Read an image\n",
    "image = cv2.imread(\"data/Dataset_20/Bathtub/00a60d051d75144f.jpg\", cv2.IMREAD_UNCHANGED)\n",
    " \n",
    "# Get dimensions of image using shape\n",
    "dimensions = image.shape\n",
    " \n",
    "# height, width, number of channels in image\n",
    "height = image.shape[0]\n",
    "width = image.shape[1]\n",
    "channels = image.shape[2]\n",
    "pixel_num = image.size\n",
    " \n",
    "print(\"Image Dimension    : \",dimensions)\n",
    "print(\"Image Height       : \",height)\n",
    "print(\"Image Width        : \",width)\n",
    "print(\"Number of Channels : \",channels)\n",
    "print(\"Number of pixels   : \",pixel_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the image array\n",
    "# print(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE A DOCSTRING FOR THIS FUNCTION\n",
    "\n",
    "# This function reads through Dataset folders, returns a DataFrame with number of images per class and a visualization\n",
    "# It was created with help from https://stackoverflow.com/questions/62114998/python-make-dictionary-with-folder-names-as-key-and-file-names-as-values\n",
    "# and https://thispointer.com/python-pandas-how-to-convert-lists-to-a-dataframe/\n",
    "import fnmatch\n",
    "from collections import defaultdict\n",
    "\n",
    "def visualize_dataset(path):\n",
    "    \n",
    "    data_dict=defaultdict(set)\n",
    "    global data_df\n",
    "    \n",
    "    # This code will read the images from the Dataset folder, put them into a dictionary\n",
    "    for path,dirs,files in os.walk(path):\n",
    "        for f in fnmatch.filter(files,'*.jpg'):\n",
    "            data_dict[os.path.basename(path)].add(f)\n",
    "        \n",
    "    # Get number of pictures per class, append values to lists (intermediary step to a DataFrame)\n",
    "    key_list = []\n",
    "    count_list = []\n",
    "\n",
    "    # Get number of photos per class(folder)\n",
    "    for k,v in data_dict.items():\n",
    "        value = len(list(filter(None, v)))\n",
    "        key_list.append(k)\n",
    "        count_list.append(value)\n",
    "\n",
    "    # Create a zipped list of tuples from above lists\n",
    "    zippedList =  list(zip(key_list, count_list))\n",
    "\n",
    "    data_df = pd.DataFrame(zippedList, columns = [\"Class\" , \"Image_Count\"])\n",
    "\n",
    "    data_df = data_df.sort_values(by=\"Image_Count\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    print(data_df)\n",
    "        \n",
    "    # Make a plot of number of images per class\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.bar(data_df[\"Class\"], data_df[\"Image_Count\"])\n",
    "    plt.title(\"Number of images per class\", size=26)\n",
    "    plt.ylabel(\"Image count\", size=20)\n",
    "    plt.xticks(rotation=90, size=16)\n",
    "    plt.yticks(size=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_dataset(\"data/Dataset_20/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class imbalance problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above graph that there is a serious issue with class imbalance in this dataset. However due to time constrains, I am going to drop 5 classes that are not well represented in the data. This will also going to help reduce training time by reducing the size of the dataset. A solution to this first problem is to employ image augmentation, where copies of images in the unbalanced classes are made by flipping, scaling, or skewing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop from 20 to 15 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove classes from Dataset folder\n",
    "# import shutil\n",
    "\n",
    "# to_delete = [\"Oven\", \"Coffeemaker\", \"Microwave oven\", \"Washing machine\", \"Ceiling fan\"]\n",
    "\n",
    "# for folder in to_delete:\n",
    "\n",
    "    # Removes entire folder and contents\n",
    "    # shutil.rmtree(f\"Dataset_20/{folder}\")\n",
    "    # print(f\"The {folder} folder has been removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_dataset(\"data/Dataset_15/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at count of .jpg files in the dataset folder\n",
    "\n",
    "data_dir = (\"data/Dataset_15\")\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "\n",
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "image_count "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Pre-processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_retinanet import models\n",
    "from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\n",
    "from keras_retinanet.utils.visualization import draw_box, draw_caption\n",
    "from keras_retinanet.utils.colors import label_color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/set split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the file structure of my images and annotations, I used a script called \"partition_dataset.py\" from GitHub to split my image and XML data into 80-20 Train-test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorda\\Documents\\Capstone\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\Users\\jorda\\Documents\\Capstone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/armaanpriyadarshan/Training-a-Custom-TensorFlow-2.X-Object-Detector.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The partition was successful\n"
     ]
    }
   ],
   "source": [
    "# Run this command to split the data\n",
    "\n",
    "!python partition_dataset.py -i data/Dataset_15/Toilet -o data/Dataset_15_copy_after_split -r 0.2 -x\n",
    "print(f\"The partition was successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert annotations to XML, CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download tool to convert txt annotations to PASCAL VOC\n",
    "#!git clone https://github.com/thehetpandya/OIDv4_annotation_tool.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that annotations are in XML (Pascal), need to get them into a single csv file for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Pascal to CSV program\n",
    "# Source Code: https://gist.github.com/rotemtam/88d9a4efae243fc77ed4a0f9917c8f6c\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def xml_to_csv(path):\n",
    "    xml_list = []\n",
    "    for xml_file in glob.glob(path + '/*.xml'):\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        for member in root.findall('object'):\n",
    "            bbx = member.find('bndbox')\n",
    "            xmin = int(bbx.find('xmin').text)\n",
    "            ymin = int(bbx.find('ymin').text)\n",
    "            xmax = int(bbx.find('xmax').text)\n",
    "            ymax = int(bbx.find('ymax').text)\n",
    "            label = member.find('name').text\n",
    "\n",
    "            value = (root.find('filename').text,\n",
    "                     #int(root.find('size')[0].text),\n",
    "                     #int(root.find('size')[1].text),\n",
    "                     xmin,\n",
    "                     ymin,\n",
    "                     xmax,\n",
    "                     ymax,\n",
    "                     label\n",
    "                     )\n",
    "            xml_list.append(value)\n",
    "    column_name = ['image_name', #'width', 'height',\n",
    "                   'x_min', 'y_min', 'x_max', 'y_max', 'class_name']\n",
    "    xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
    "    return xml_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the train dataset\n",
    "train_df = xml_to_csv(\"data/Dataset_15_split/train\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train data set has 8,127 images, with 12,989 annotations (bounding boxes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the test dataset\n",
    "test_df = xml_to_csv(\"data/Dataset_15_split/test\")\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test data set has 2,032 images with 3,291 annotations (bounding boxes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The annotations are now in a format required for TensorFlow modelling. Column descriptions:\n",
    "- image_name = name of the image\n",
    "- x_min, y_min = coordinates of the top-left corner of the bounding box\n",
    "- x_max, y_max = coordinates of bottom-right corner of the bounding box\n",
    "- class_name = class that the image and bounding box belong to \\\n",
    "\n",
    "**Important note:** The bounding box coordinates are in **Pascal VOC** format. The coordinates refer to the pixel values of the image (see image below for example). Other commmon annotation formats are COCO and YOLO [Bounding box guide](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://albumentations.ai/docs/images/getting_started/augmenting_bboxes/bbox_formats.jpg>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrames to a CSV\n",
    "train_df.to_csv(\"data/labels_train.csv\")\n",
    "test_df.to_csv(\"data/labels_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the bounding boxes on the images to complete the pre-processing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source code: https://www.curiousily.com/posts/object-detection-on-custom-dataset-with-tensorflow-2-and-keras-using-python/\n",
    "def show_image_objects(image_row):\n",
    "    \n",
    "    data_folder = Path(\"data/Dataset_15_split/train\")\n",
    "    \n",
    "    img_path = data_folder / image_row.image_name \n",
    "    box = [\n",
    "        image_row.x_min, image_row.y_min, image_row.x_max, image_row.y_max\n",
    "    ]\n",
    "\n",
    "    image = read_image_bgr(img_path)\n",
    "\n",
    "    draw = image.copy()\n",
    "    draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    draw_box(draw, box, color=(255, 255, 0))\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.imshow(draw)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_objects(train_df.iloc[19])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move ahead and do some modelling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare inputs for TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions taken from the [Tensorflow Object Detection API](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/using_your_own_dataset.md):\n",
    "1. Create Label Maps\n",
    "2. Convert dataset to TFRecord file format\n",
    "\n",
    "For every example in dataset, we need:\\\n",
    "    - An RGB image for the dataset encoded as jpeg or png.\\\n",
    "    - A list of bounding boxes for the image. Each bounding box should contain:\\\n",
    "        i. A bounding box coordinates (with origin in top left corner) defined by 4 floating point numbers - ymin, xmin, ymax, xmax. Note that we need to store the normalized coordinates (x / width, y / height) in the TFRecord dataset.\\\n",
    "        ii. The class of the object in the bounding box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the TensorFlow models repository\n",
    "# !git clone https://github.com/tensorflow/models.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Create Label Maps** Now that the annotations are in CSV format, we also need to create a label map which is required to make TFRecords out of dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the classes to class list in order to make the label map\n",
    "s = pd.Series(data_df[\"Class\"])\n",
    "class_list = s.values.tolist()\n",
    "print(class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the list of classes to label_map.pbtxt\n",
    "\n",
    "from object_detection.protos.string_int_label_map_pb2 import StringIntLabelMap, StringIntLabelMapItem\n",
    "from google.protobuf import text_format\n",
    "\n",
    "\n",
    "def convert_classes(classes, start=1):\n",
    "    msg = StringIntLabelMap()\n",
    "    for id, name in enumerate(classes, start=start):\n",
    "        msg.item.append(StringIntLabelMapItem(id=id, name=name))\n",
    "\n",
    "    text = str(text_format.MessageToBytes(msg, as_utf8=True), 'utf-8')\n",
    "    return text\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    txt = convert_classes(class_list)\n",
    "    print(txt)\n",
    "    with open('label_map.pbtxt', 'w') as f:\n",
    "        f.write(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Convert dataset to TFRecord file format**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Label Map, can generate a tf.Example proto for images using a script based on the TensorFlow Object Detector API. I cloned the following repository, and used the \"generate_tfrecord.py\" script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/armaanpriyadarshan/Training-a-Custom-TensorFlow-2.X-Object-Detector.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to preprocessing folder in Tensorflow\n",
    "%cd Tensorflow/scripts/preprocessing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python generate_tfrecord.py -x C:\\Users\\jorda\\Documents\\Capstone\\Tensorflow\\workspace\\capstone\\images\\train -l C:\\Users\\jorda\\Documents\\Capstone\\Tensorflow\\workspace\\capstone\\annotations\\label_map.pbtxt -o C:\\Users\\jorda\\Documents\\Capstone\\Tensorflow\\workspace\\capstone\\annotations\\train.record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generate_tfrecord.py -x C:\\Users\\jorda\\Documents\\Capstone\\Tensorflow\\workspace\\capstone\\images\\test -l C:\\Users\\jorda\\Documents\\Capstone\\Tensorflow\\workspace\\capstone\\annotations\\label_map.pbtxt -o C:\\Users\\jorda\\Documents\\Capstone\\Tensorflow\\workspace\\capstone\\annotations\\test.record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation of the train and test TFRecords were successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Training - TensorFlow**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project I decided to make use of pre-trained models for object detection, rather than building a model from scratch. The benefit to this is that we are leveraging knowledge (features, weights) from previously trained models to apply to related datasets. In the case of computer vision, low-level features such as edges, shapes, corners and intensity can be shared among tasks (transfer learning). By usng pre-trained models we are saving time and computer resources which is what is needed for this large dataset and the task at hand (object detection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset with pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "System Setup:\n",
    "- Windows 10\n",
    "- Ryzen 7 AMD CPU\n",
    "- NVIDIA GeForce GTX 1660-Ti / 6GB memory\n",
    "- 16 GB RAM\n",
    "- Tensorflow-GPU Version 2.3.0\n",
    "- CUDA Toolkit 10.1\n",
    "- cuDNN 7.6.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to make use of the GPU on my laptop to run the pre-trained models. I selected the following pre-trained models to train my amenity dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Pre-trained model |   Feature extractor  |                        Pros                        |                      Cons                     |\n",
    "|:-----------------:|:--------------------:|:--------------------------------------------------:|:---------------------------------------------:|\n",
    "|  SSD MobileNet V2 |      VGG16 (FPN)     | Works well on large objects                        | Doesn't fare well on small objects            |\n",
    "|     RetinaNet     |    ResNet50 (FPN)    | Works well on multiple scales | None                                          |\n",
    "|       YOLOv3      | Darknet53 (FPN-like) | Very fast, works well on small objects             | Doesn't fare well on medium and large objects |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I selected these models based on training speeds. YOLOv3 is the fastest, while RetinaNet is fast while being highly accurate. SSD MobileNet is inbetween in terms of speed and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the TensorFlow version\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn on memory growth for GPU by running the following code prior to allocating any tensors or executing any ops [Source: TensorFlow - Using a GPU](https://www.tensorflow.org/guide/gpu). Attempts to allocate only as much GPU memory as needed for the runtime allocations (Experienced errors previously)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following code before training\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained model no. 1: SSD MobileNet V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MobileNet V2 is a Single Shot Detector (SSD) model that goes straight from image pixels to bounding box coordinates and class probabilities in a single forward pass. The model is able to detect multiple objects within the image. SSD is much faster compared with two-shot RPN based approaches. [Source](https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11). SSD uses the VGG-16 model pre-trained on ImageNet as its base model for extracting useful image features. On top of VGG16, SSD adds several conv feature layers of decreasing sizes. They can be seen as a pyramid representation of images at different scales. The detection happens in every pyramidal layer, targeting at objects of various sizes [Source](https://lilianweng.github.io/lil-log/2018/12/27/object-detection-part-4.html#yolov2--yolo9000)\n",
    "\n",
    "The loss function is a dynamically scaled cross-entropy loss, where the scaling factor decays to zero as confidence in the correct class increases. [Source](https://medium.com/analytics-vidhya/how-focal-loss-fixes-the-class-imbalance-problem-in-object-detection-3d2e1c4da8d7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://lilianweng.github.io/lil-log/assets/images/SSD-architecture.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd C:\\Users\\jorda\\Documents\\Capstone\\Tensorflow\\workspace\\capstone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python model_main_tf2.py \\\n",
    "    --model_dir=models/my_ssd_mobilenet_v2_fpnlite \\\n",
    "    --pipeline_config_path=models/my_ssd_mobilenet_v2_fpnlite/pipeline.config\n",
    "    --alsologtostderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure GPU is being used\n",
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Tensorboard for monitoring training, navigate to TensorFlow/workspace/capstone\n",
    "# !tensorboard --logdir=models\\my_ssd_mobilenet_v2_fpnlite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training started Sep 9 @ 1:55AM. Training ended 11:04AM. Training ran for 50,000 steps (50 epochs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Inference Graph from finished model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to TensorFlow/workspace/capstone\n",
    "!python exporter_main_v2.py \\\n",
    "    --input_type image_tensor \\\n",
    "    --pipeline_config_path models\\my_ssd_mobilenet_v2_fpnlite\\pipeline.config \\\n",
    "    --trained_checkpoint_dir models\\my_ssd_mobilenet_v2_fpnlite \\\n",
    "    --output_directory exported-models\\my_mobilenet_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above script creates a file called saved_'model.pb' which is the inference graph. Copy the 'label_map.pbtxt' file into this directory to make things a bit easier for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out finished model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize test image with OpenCV\n",
    "!python TF-image-od.py \\\n",
    "    --image images/test/4ad2b99ef5fb838c.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python model_main_tf2.py \\\n",
    "    --pipeline_config_path=exported-models\\my_mobilenet_model\\pipeline.config \\\n",
    "    --model_dir=exported-models\\my_mobilenet_model\\saved_model \\\n",
    "    --checkpoint_dir=exported-models\\my_mobilenet_model\\checkpoint \\\n",
    "    --alsologtostderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does not work due to eager execution (old script that does not work with newer version of Tensorflow)\n",
    "# Tried to use older version of Tensorflow, did not work because my system is not compatible\n",
    "!python eval.py \\\n",
    "    --logtostderr \\\n",
    "    --checkpoint_dir=exported-models/my_mobilenet_model/checkpoint/model.ckpt \\\n",
    "    --eval_dir=exported-models/my_mobilenet_model/saved_model/eval \\\n",
    "    --pipeline_config_path=exported-models/my_mobilenet_model/pipeline.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View evaluation results in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start TensorBoard to monitor training\n",
    "!tensorboard --logdir=exported-models\\my_mobilenet_model\\saved_model\\eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained model no. 2: RetinaNet + Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RetinaNet solved the problem of the extreme foreground-background class imbalance problem in one-stage detectors by introducing Focal Loss. \n",
    "\n",
    "In RetinaNet, a one-stage detector, by using focal loss, lower loss is contributed by “easy” negative samples so that the loss is focusing on “hard” samples, which improves the prediction accuracy. With ResNet+FPN as backbone for feature extraction, plus two task-specific subnetworks for classification and bounding box regression, forming the RetinaNet, which achieves state-of-the-art performance, outperforms Faster R-CNN, the well-known two-stage detectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RetinaNet is built on top of two crucial concepts - Focal Loss and Featurized Image Pyramid:\n",
    "\n",
    "1. **Focal Loss** is designed to mitigate the issue of extreme imbalance between background and foreground with objects of interest. It assigns more weight on hard, easily misclassified examples and small weight to easier ones.\n",
    "\n",
    "2. **The Featurized Image Pyramid** is the vision component of RetinaNet. It allows for object detection at different scales by stacking multiple convolutional layers [Source](https://www.curiousily.com/posts/object-detection-on-custom-dataset-with-tensorflow-2-and-keras-using-python/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://developers.arcgis.com/assets/img/python-graphics/retinanet.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used this [guide](https://www.curiousily.com/posts/object-detection-on-custom-dataset-with-tensorflow-2-and-keras-using-python/) to train a RetinaNet object detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/fizyr/keras-retinanet.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install keras-retinanet\n",
    "# %cd keras-retinanet/\n",
    "#! pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data in format required for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras requires two input files\n",
    "\n",
    "ANNOTATIONS_FILE = \"annotations.csv\"\n",
    "CLASSES_FILE = \"classes.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at class list created previously\n",
    "class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create numbers for the classes (indexing starts at 0)\n",
    "class_number = list(np.arange(0, 20))\n",
    "class_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the two lists together into a DataFrame\n",
    "class_tuples = list(zip(class_list,class_number))\n",
    "classes_df = pd.DataFrame(class_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers need to be removed\n",
    "classes_df.to_csv(CLASSES_FILE, index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers need to be removed\n",
    "train_df.to_csv(ANNOTATIONS_FILE, index=False, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download pre-trained model from RetinaNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " cd C:\\Users\\jorda\\Documents\\Capstone\\keras-retinanet\\keras_retinanet\\bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pre-trained model\n",
    "PRETRAINED_MODEL = './snapshots/_pretrained_model.h5'\n",
    "\n",
    "URL_MODEL = 'https://github.com/fizyr/keras-retinanet/releases/download/0.5.1/resnet50_coco_best_v2.1.0.h5'\n",
    "\n",
    "urllib.request.urlretrieve(URL_MODEL, PRETRAINED_MODEL)\n",
    "\n",
    "print('Downloaded pretrained model to ' + PRETRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start TensorBoard to monitor training\n",
    "#!tensorboard --logdir=keras_retinanet\\bin\\logs\\train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure GPU is being used\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate training for 50 epochs\n",
    "!python train.py \\\n",
    "    --tensorboard-dir logs \\\n",
    "    --freeze-backbone \\\n",
    "    --random-transform \\\n",
    "    --weights {PRETRAINED_MODEL} \\\n",
    "    --batch-size 5 \\\n",
    "    --steps 500 \\\n",
    "    --epochs 50 \\\n",
    "    --gpu 0\\\n",
    "    --multi-gpu 1 \\\n",
    "    csv annotations.csv classes.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training started Sep 8 @ 2:15AM. Training ended 9:36AM. Training ran for 50 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in last checkpoint\n",
    "model_path = os.path.join(\"snapshots\", sorted(os.listdir(\"snapshots\"), reverse=True)[0])\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the type of feature extraction (backbone) used for the object detection\n",
    "model = models.load_model(model_path, backbone_name=\"resnet50\")\n",
    "# Convert model for Keras\n",
    "model = models.convert_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the label classes\n",
    "labels_to_names = pd.read_csv(CLASSES_FILE, header=None).T.loc[0].to_dict()\n",
    "labels_to_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function #1: Get predictions from model\n",
    "def predict(image):\n",
    "\n",
    "    image = preprocess_image(image.copy())\n",
    "    # Re-size and preprocess the image\n",
    "    image, scale = resize_image(image)\n",
    "    # Add additional dimension to image tensor\n",
    "    boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))\n",
    "    # Rescale detected boxes based on the resized iamge scale\n",
    "    boxes /= scale\n",
    "    # Return all predictions\n",
    "    return boxes, scores, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function #2: Draw detected boxes on the image\n",
    "# Set minimum threshold score\n",
    "THRES_SCORE = 0.3\n",
    "\n",
    "def draw_detections(image, boxes, scores, labels):\n",
    "\n",
    "    for box, score, label in zip(boxes[0], scores[0], labels[0]):\n",
    "        if score < THRES_SCORE:\n",
    "            break\n",
    "\n",
    "    color = label_color(label)\n",
    "\n",
    "    b = box.astype(int)\n",
    "    draw_box(image, b, color=color)\n",
    "\n",
    "    caption = \"{} {:.3f}\".format(labels_to_names[label], score)\n",
    "    draw_caption(image, b, caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function #3: Show detected objects along with predictions\n",
    "def show_detected_objects(image_row):\n",
    "\n",
    "    img_path = image_row.image_name\n",
    "    image = read_image_bgr(img_path)\n",
    "    boxes, scores, labels = predict(image)\n",
    "    draw = image.copy()\n",
    "    draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n",
    "    true_box = [image_row.x_min, image_row.y_min, image_row.x_max, image_row.y_max]\n",
    "\n",
    "    draw_box(draw, true_box, color=(255, 255, 0))\n",
    "    draw_detections(draw, boxes, scores, labels)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.imshow(draw)\n",
    "    plt.imsave('predicted_image.jpg', draw)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will be running detections on the test images\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd keras-retinanet\\keras_retinanet\\bin\\test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yellow boxes are the ground-truth boxes (the true hand labeled bounding box)\n",
    "# Other colors are the predicted boxes with confidence score above 0.3\n",
    "show_detected_objects(test_df.iloc[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By browsing through a few of the test images, a little less than 50% are classified correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained model no. 3: YOLOv3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to try YOLOv3 due to speed (100x faster than Fast R-CNN). YOLO (You Only Look Once) looks at the whole image at test time so it's predictions are informed by global context in the image. It makes predictions with a single network evaluation unlike systems like R-CNN which require thousands for a single image [Source](https://pjreddie.com/darknet/yolo/). YOLO3 uses Darknet-53 as its backbone feature extractor, which has 53 convolutional layers trained on ImageNet. For the task of detection, 53 more layers are stacked onto it, giving a 106 layer fully convolutional underlying architecture for YOLOv3 [Source](https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b). YOLO3 is better (in terms of Average Precision) than Faster R-CNN for small objects, but behind RetinaNet. It is not as accurate as other models for medium and large objects.\n",
    "\n",
    "YOLOv3 makes detections on 3 different scales.  YOLO is a fully convolutional network and its eventual output is generated by applying a 1 x 1 kernel on a feature map. In YOLO v3, the detection is done by applying 1 x 1 detection kernels on feature maps of three different sizes at three different places in the network [Source](https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://miro.medium.com/max/1000/1*d4Eg17IVJ0L41e7CTWLLSg.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data in format required for YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the annotations, YOLO required that they bounding boxes are in normalized coordinates (numbers between 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the windows version of Darknet:\n",
    "# !git clone https://github.com/AlexeyAB/darknet.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Images into Numpy Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for Images to go into a TensorFlow model, they to be read and decoded into integer tensors, then converted to floating point and normalized to small values (between 0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below function converts all of the images into numpy arrays\n",
    "from PIL import Image\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "# We need to get all of the images into arrays, which will be used to combine with the bounding boxes & labels\n",
    "for image_path in TEST_IMAGE_PATHS:\n",
    "  image = Image.open(image_path)\n",
    "  # the array based representation of the image will be used later in order to prepare the\n",
    "  # result image with boxes and labels on it.\n",
    "  image_np = load_image_into_numpy_array(image)\n",
    "plt.figure(figsize=IMAGE_SIZE)\n",
    "plt.imshow(image_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense,GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained model no. 1: SSD MobileNet V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "# Import the pretrained MobileNet V2 model and do not include the final layer:\n",
    "print('loading model...')\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False)\n",
    "print('model loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final layers:\n",
    "# Assign the output of this base_model to a variable:\n",
    "base_model_out = base_model.output\n",
    "\n",
    "# Add a pooling layer:\n",
    "base_model_out = GlobalAveragePooling2D()(base_model_out)\n",
    "\n",
    "# Add 3 dense layers so that the model can learn aspects of our new dataset \n",
    "# and classify for better results.\n",
    "base_model_out = Dense(243, activation='relu')(base_model_out) \n",
    "base_model_out = Dense(243, activation='relu')(base_model_out)\n",
    "base_model_out = Dense(81, activation='relu')(base_model_out)\n",
    "\n",
    "# Add a final layer with 3 neurons, one for each class in our dataset \n",
    "# using a softmabase_model_out activation function:\n",
    "preds = Dense(15, activation='softmax')(base_model_out)\n",
    "\n",
    "# Instantiate our final model, where we specify what are the inputs and \n",
    "# the outputs will look like\n",
    "model = Model(inputs = base_model.input, \n",
    "              outputs = preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_pad (ZeroPadding2D)       (None, None, None, 3 0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv2D)                  (None, None, None, 3 864         Conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_Conv1 (BatchNormalization)   (None, None, None, 3 128         Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_relu (ReLU)               (None, None, None, 3 0           bn_Conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise (Depthw (None, None, None, 3 288         Conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_BN (Bat (None, None, None, 3 128         expanded_conv_depthwise[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_relu (R (None, None, None, 3 0           expanded_conv_depthwise_BN[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project (Conv2D)  (None, None, None, 1 512         expanded_conv_depthwise_relu[0][0\n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project_BN (Batch (None, None, None, 1 64          expanded_conv_project[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand (Conv2D)         (None, None, None, 9 1536        expanded_conv_project_BN[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_BN (BatchNormali (None, None, None, 9 384         block_1_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_relu (ReLU)      (None, None, None, 9 0           block_1_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_pad (ZeroPadding2D)     (None, None, None, 9 0           block_1_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise (DepthwiseCon (None, None, None, 9 864         block_1_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_BN (BatchNorm (None, None, None, 9 384         block_1_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_relu (ReLU)   (None, None, None, 9 0           block_1_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project (Conv2D)        (None, None, None, 2 2304        block_1_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project_BN (BatchNormal (None, None, None, 2 96          block_1_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand (Conv2D)         (None, None, None, 1 3456        block_1_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_BN (BatchNormali (None, None, None, 1 576         block_2_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_relu (ReLU)      (None, None, None, 1 0           block_2_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise (DepthwiseCon (None, None, None, 1 1296        block_2_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_BN (BatchNorm (None, None, None, 1 576         block_2_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_relu (ReLU)   (None, None, None, 1 0           block_2_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project (Conv2D)        (None, None, None, 2 3456        block_2_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project_BN (BatchNormal (None, None, None, 2 96          block_2_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_add (Add)               (None, None, None, 2 0           block_1_project_BN[0][0]         \n",
      "                                                                 block_2_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand (Conv2D)         (None, None, None, 1 3456        block_2_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_BN (BatchNormali (None, None, None, 1 576         block_3_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_relu (ReLU)      (None, None, None, 1 0           block_3_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_pad (ZeroPadding2D)     (None, None, None, 1 0           block_3_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise (DepthwiseCon (None, None, None, 1 1296        block_3_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_BN (BatchNorm (None, None, None, 1 576         block_3_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_relu (ReLU)   (None, None, None, 1 0           block_3_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project (Conv2D)        (None, None, None, 3 4608        block_3_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project_BN (BatchNormal (None, None, None, 3 128         block_3_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand (Conv2D)         (None, None, None, 1 6144        block_3_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_BN (BatchNormali (None, None, None, 1 768         block_4_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_relu (ReLU)      (None, None, None, 1 0           block_4_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise (DepthwiseCon (None, None, None, 1 1728        block_4_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_BN (BatchNorm (None, None, None, 1 768         block_4_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_relu (ReLU)   (None, None, None, 1 0           block_4_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project (Conv2D)        (None, None, None, 3 6144        block_4_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project_BN (BatchNormal (None, None, None, 3 128         block_4_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_add (Add)               (None, None, None, 3 0           block_3_project_BN[0][0]         \n",
      "                                                                 block_4_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand (Conv2D)         (None, None, None, 1 6144        block_4_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_BN (BatchNormali (None, None, None, 1 768         block_5_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_relu (ReLU)      (None, None, None, 1 0           block_5_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise (DepthwiseCon (None, None, None, 1 1728        block_5_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_BN (BatchNorm (None, None, None, 1 768         block_5_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_relu (ReLU)   (None, None, None, 1 0           block_5_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project (Conv2D)        (None, None, None, 3 6144        block_5_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project_BN (BatchNormal (None, None, None, 3 128         block_5_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_5_add (Add)               (None, None, None, 3 0           block_4_add[0][0]                \n",
      "                                                                 block_5_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand (Conv2D)         (None, None, None, 1 6144        block_5_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_BN (BatchNormali (None, None, None, 1 768         block_6_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_relu (ReLU)      (None, None, None, 1 0           block_6_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_pad (ZeroPadding2D)     (None, None, None, 1 0           block_6_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise (DepthwiseCon (None, None, None, 1 1728        block_6_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_BN (BatchNorm (None, None, None, 1 768         block_6_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_relu (ReLU)   (None, None, None, 1 0           block_6_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project (Conv2D)        (None, None, None, 6 12288       block_6_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project_BN (BatchNormal (None, None, None, 6 256         block_6_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand (Conv2D)         (None, None, None, 3 24576       block_6_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_BN (BatchNormali (None, None, None, 3 1536        block_7_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_relu (ReLU)      (None, None, None, 3 0           block_7_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise (DepthwiseCon (None, None, None, 3 3456        block_7_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_BN (BatchNorm (None, None, None, 3 1536        block_7_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_relu (ReLU)   (None, None, None, 3 0           block_7_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project (Conv2D)        (None, None, None, 6 24576       block_7_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project_BN (BatchNormal (None, None, None, 6 256         block_7_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_add (Add)               (None, None, None, 6 0           block_6_project_BN[0][0]         \n",
      "                                                                 block_7_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand (Conv2D)         (None, None, None, 3 24576       block_7_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_BN (BatchNormali (None, None, None, 3 1536        block_8_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_relu (ReLU)      (None, None, None, 3 0           block_8_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise (DepthwiseCon (None, None, None, 3 3456        block_8_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_BN (BatchNorm (None, None, None, 3 1536        block_8_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_relu (ReLU)   (None, None, None, 3 0           block_8_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project (Conv2D)        (None, None, None, 6 24576       block_8_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project_BN (BatchNormal (None, None, None, 6 256         block_8_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_8_add (Add)               (None, None, None, 6 0           block_7_add[0][0]                \n",
      "                                                                 block_8_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand (Conv2D)         (None, None, None, 3 24576       block_8_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_BN (BatchNormali (None, None, None, 3 1536        block_9_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_relu (ReLU)      (None, None, None, 3 0           block_9_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise (DepthwiseCon (None, None, None, 3 3456        block_9_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_BN (BatchNorm (None, None, None, 3 1536        block_9_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_relu (ReLU)   (None, None, None, 3 0           block_9_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project (Conv2D)        (None, None, None, 6 24576       block_9_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project_BN (BatchNormal (None, None, None, 6 256         block_9_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_9_add (Add)               (None, None, None, 6 0           block_8_add[0][0]                \n",
      "                                                                 block_9_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand (Conv2D)        (None, None, None, 3 24576       block_9_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_BN (BatchNormal (None, None, None, 3 1536        block_10_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_relu (ReLU)     (None, None, None, 3 0           block_10_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise (DepthwiseCo (None, None, None, 3 3456        block_10_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_BN (BatchNor (None, None, None, 3 1536        block_10_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_relu (ReLU)  (None, None, None, 3 0           block_10_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project (Conv2D)       (None, None, None, 9 36864       block_10_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project_BN (BatchNorma (None, None, None, 9 384         block_10_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand (Conv2D)        (None, None, None, 5 55296       block_10_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_BN (BatchNormal (None, None, None, 5 2304        block_11_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_relu (ReLU)     (None, None, None, 5 0           block_11_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise (DepthwiseCo (None, None, None, 5 5184        block_11_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_BN (BatchNor (None, None, None, 5 2304        block_11_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_relu (ReLU)  (None, None, None, 5 0           block_11_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project (Conv2D)       (None, None, None, 9 55296       block_11_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project_BN (BatchNorma (None, None, None, 9 384         block_11_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_add (Add)              (None, None, None, 9 0           block_10_project_BN[0][0]        \n",
      "                                                                 block_11_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand (Conv2D)        (None, None, None, 5 55296       block_11_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_BN (BatchNormal (None, None, None, 5 2304        block_12_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_relu (ReLU)     (None, None, None, 5 0           block_12_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise (DepthwiseCo (None, None, None, 5 5184        block_12_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_BN (BatchNor (None, None, None, 5 2304        block_12_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_relu (ReLU)  (None, None, None, 5 0           block_12_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project (Conv2D)       (None, None, None, 9 55296       block_12_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project_BN (BatchNorma (None, None, None, 9 384         block_12_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_12_add (Add)              (None, None, None, 9 0           block_11_add[0][0]               \n",
      "                                                                 block_12_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand (Conv2D)        (None, None, None, 5 55296       block_12_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_BN (BatchNormal (None, None, None, 5 2304        block_13_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_relu (ReLU)     (None, None, None, 5 0           block_13_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_pad (ZeroPadding2D)    (None, None, None, 5 0           block_13_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise (DepthwiseCo (None, None, None, 5 5184        block_13_pad[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_BN (BatchNor (None, None, None, 5 2304        block_13_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_relu (ReLU)  (None, None, None, 5 0           block_13_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project (Conv2D)       (None, None, None, 1 92160       block_13_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project_BN (BatchNorma (None, None, None, 1 640         block_13_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand (Conv2D)        (None, None, None, 9 153600      block_13_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_BN (BatchNormal (None, None, None, 9 3840        block_14_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_relu (ReLU)     (None, None, None, 9 0           block_14_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise (DepthwiseCo (None, None, None, 9 8640        block_14_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_BN (BatchNor (None, None, None, 9 3840        block_14_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_relu (ReLU)  (None, None, None, 9 0           block_14_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project (Conv2D)       (None, None, None, 1 153600      block_14_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project_BN (BatchNorma (None, None, None, 1 640         block_14_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_add (Add)              (None, None, None, 1 0           block_13_project_BN[0][0]        \n",
      "                                                                 block_14_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand (Conv2D)        (None, None, None, 9 153600      block_14_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_BN (BatchNormal (None, None, None, 9 3840        block_15_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_relu (ReLU)     (None, None, None, 9 0           block_15_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise (DepthwiseCo (None, None, None, 9 8640        block_15_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_BN (BatchNor (None, None, None, 9 3840        block_15_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_relu (ReLU)  (None, None, None, 9 0           block_15_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project (Conv2D)       (None, None, None, 1 153600      block_15_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project_BN (BatchNorma (None, None, None, 1 640         block_15_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_15_add (Add)              (None, None, None, 1 0           block_14_add[0][0]               \n",
      "                                                                 block_15_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand (Conv2D)        (None, None, None, 9 153600      block_15_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_BN (BatchNormal (None, None, None, 9 3840        block_16_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_relu (ReLU)     (None, None, None, 9 0           block_16_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise (DepthwiseCo (None, None, None, 9 8640        block_16_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_BN (BatchNor (None, None, None, 9 3840        block_16_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_relu (ReLU)  (None, None, None, 9 0           block_16_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project (Conv2D)       (None, None, None, 3 307200      block_16_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project_BN (BatchNorma (None, None, None, 3 1280        block_16_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1 (Conv2D)                 (None, None, None, 1 409600      block_16_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1_bn (BatchNormalization)  (None, None, None, 1 5120        Conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_relu (ReLU)                 (None, None, None, 1 0           Conv_1_bn[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 1280)         0           out_relu[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 243)          311283      global_average_pooling2d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 243)          59292       dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 81)           19764       dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 15)           1230        dense_10[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,649,553\n",
      "Trainable params: 2,615,441\n",
      "Non-trainable params: 34,112\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block_9_depthwise_relu\n",
      "block_9_project\n",
      "block_9_project_BN\n",
      "block_9_add\n",
      "block_10_expand\n",
      "block_10_expand_BN\n",
      "block_10_expand_relu\n",
      "block_10_depthwise\n",
      "block_10_depthwise_BN\n",
      "block_10_depthwise_relu\n",
      "block_10_project\n",
      "block_10_project_BN\n",
      "block_11_expand\n",
      "block_11_expand_BN\n",
      "block_11_expand_relu\n",
      "block_11_depthwise\n",
      "block_11_depthwise_BN\n",
      "block_11_depthwise_relu\n",
      "block_11_project\n",
      "block_11_project_BN\n",
      "block_11_add\n",
      "block_12_expand\n",
      "block_12_expand_BN\n",
      "block_12_expand_relu\n",
      "block_12_depthwise\n",
      "block_12_depthwise_BN\n",
      "block_12_depthwise_relu\n",
      "block_12_project\n",
      "block_12_project_BN\n",
      "block_12_add\n",
      "block_13_expand\n",
      "block_13_expand_BN\n",
      "block_13_expand_relu\n",
      "block_13_pad\n",
      "block_13_depthwise\n",
      "block_13_depthwise_BN\n",
      "block_13_depthwise_relu\n",
      "block_13_project\n",
      "block_13_project_BN\n",
      "block_14_expand\n",
      "block_14_expand_BN\n",
      "block_14_expand_relu\n",
      "block_14_depthwise\n",
      "block_14_depthwise_BN\n",
      "block_14_depthwise_relu\n",
      "block_14_project\n",
      "block_14_project_BN\n",
      "block_14_add\n",
      "block_15_expand\n",
      "block_15_expand_BN\n",
      "block_15_expand_relu\n",
      "block_15_depthwise\n",
      "block_15_depthwise_BN\n",
      "block_15_depthwise_relu\n",
      "block_15_project\n",
      "block_15_project_BN\n",
      "block_15_add\n",
      "block_16_expand\n",
      "block_16_expand_BN\n",
      "block_16_expand_relu\n",
      "block_16_depthwise\n",
      "block_16_depthwise_BN\n",
      "block_16_depthwise_relu\n",
      "block_16_project\n",
      "block_16_project_BN\n",
      "Conv_1\n",
      "Conv_1_bn\n",
      "out_relu\n",
      "global_average_pooling2d_2\n",
      "dense_8\n",
      "dense_9\n",
      "dense_10\n",
      "dense_11\n"
     ]
    }
   ],
   "source": [
    "# Lock weights in pre-trained model\n",
    "for layer in model.layers[:87]:\n",
    "    layer.trainable=False\n",
    "    \n",
    "for layer in model.layers[87:]:\n",
    "    print(layer.name)\n",
    "    layer.trainable=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorda\\Documents\\Capstone\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\Users\\jorda\\Documents\\Capstone\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8119 images belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory('data/Dataset_15_after_split/train', # path to the training data folder\n",
    "                                                 target_size=(224,224),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='categorical',\n",
    "                                                 shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bathtub': 0,\n",
       " 'Bed': 1,\n",
       " 'Billiard table': 2,\n",
       " 'Chair': 3,\n",
       " 'Couch': 4,\n",
       " 'Fireplace': 5,\n",
       " 'Gas stove': 6,\n",
       " 'Kitchen & dining room table': 7,\n",
       " 'Lamp': 8,\n",
       " 'Refrigerator': 9,\n",
       " 'Sink': 10,\n",
       " 'Stairs': 11,\n",
       " 'Swimming pool': 12,\n",
       " 'Television': 13,\n",
       " 'Toilet': 14}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = train_generator.class_indices\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Instantiating the encoder:\n",
    "my_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the encoder with the labels from 'label_map':\n",
    "my_encoder.fit(list(label_map.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Bathtub\n",
      "1: Bed\n",
      "2: Billiard table\n",
      "3: Chair\n",
      "4: Couch\n",
      "5: Fireplace\n",
      "6: Gas stove\n",
      "7: Kitchen & dining room table\n",
      "8: Lamp\n",
      "9: Refrigerator\n",
      "10: Sink\n",
      "11: Stairs\n",
      "12: Swimming pool\n",
      "13: Television\n",
      "14: Toilet\n"
     ]
    }
   ],
   "source": [
    "# Verifying the encoding:\n",
    "for i, j in enumerate(my_encoder.classes_):\n",
    "    print(f'{i}: {j}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size_train = train_generator.n//train_generator.batch_size + 1\n",
    "\n",
    "# 'train_generator.n' = 182 (number of datapoints; i.e. images)\n",
    "# 'train_generator.batch_size' = 32 (size of batches previously passed in)\n",
    "# 'step_size_train' = 254 (step size; needs to be int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_size_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "254/254 [==============================] - 135s 533ms/step - loss: 1.3262 - accuracy: 0.5942\n",
      "Epoch 2/20\n",
      "254/254 [==============================] - 96s 378ms/step - loss: 0.9036 - accuracy: 0.7226\n",
      "Epoch 3/20\n",
      "254/254 [==============================] - 95s 373ms/step - loss: 0.7452 - accuracy: 0.7651\n",
      "Epoch 4/20\n",
      "254/254 [==============================] - 96s 378ms/step - loss: 0.6270 - accuracy: 0.8022\n",
      "Epoch 5/20\n",
      "254/254 [==============================] - 95s 374ms/step - loss: 0.5326 - accuracy: 0.8315\n",
      "Epoch 6/20\n",
      "254/254 [==============================] - 95s 373ms/step - loss: 0.4691 - accuracy: 0.8508\n",
      "Epoch 7/20\n",
      "254/254 [==============================] - 95s 376ms/step - loss: 0.4090 - accuracy: 0.8650\n",
      "Epoch 8/20\n",
      "254/254 [==============================] - 96s 377ms/step - loss: 0.3447 - accuracy: 0.8893\n",
      "Epoch 9/20\n",
      "254/254 [==============================] - 96s 378ms/step - loss: 0.3039 - accuracy: 0.9044\n",
      "Epoch 10/20\n",
      "254/254 [==============================] - 96s 377ms/step - loss: 0.2841 - accuracy: 0.9085\n",
      "Epoch 11/20\n",
      "254/254 [==============================] - 92s 362ms/step - loss: 0.2719 - accuracy: 0.9143\n",
      "Epoch 12/20\n",
      "254/254 [==============================] - 91s 359ms/step - loss: 0.2642 - accuracy: 0.9145\n",
      "Epoch 13/20\n",
      "254/254 [==============================] - 91s 359ms/step - loss: 0.2205 - accuracy: 0.9329\n",
      "Epoch 14/20\n",
      "254/254 [==============================] - 91s 359ms/step - loss: 0.1979 - accuracy: 0.9403\n",
      "Epoch 15/20\n",
      "254/254 [==============================] - 91s 357ms/step - loss: 0.1987 - accuracy: 0.9400\n",
      "Epoch 16/20\n",
      "254/254 [==============================] - 90s 356ms/step - loss: 0.1600 - accuracy: 0.9497\n",
      "Epoch 17/20\n",
      "254/254 [==============================] - 91s 359ms/step - loss: 0.1791 - accuracy: 0.9414\n",
      "Epoch 18/20\n",
      "254/254 [==============================] - 90s 355ms/step - loss: 0.1457 - accuracy: 0.9574\n",
      "Epoch 19/20\n",
      "254/254 [==============================] - 91s 357ms/step - loss: 0.1603 - accuracy: 0.9534\n",
      "Epoch 20/20\n",
      "254/254 [==============================] - 91s 357ms/step - loss: 0.1614 - accuracy: 0.9525\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(generator = train_generator, \n",
    "                    steps_per_epoch = step_size_train, \n",
    "                    epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Accuracy Vs Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(history.history['loss'], marker='o', label='loss')\n",
    "plt.plot(history.history['accuracy'], marker='o', label='accuracy')\n",
    "plt.title('Training Accuracy and Loss')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy / Loss')\n",
    "plt.xlim(0, 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2040 images belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "desired_batch_size=15\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory('data/Dataset_15_after_split/test', # path to the training data folder\n",
    "                                                 target_size=(224,224),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=desired_batch_size,\n",
    "                                                 class_mode='categorical',\n",
    "                                                 shuffle=True)\n",
    "\n",
    "filenames = test_generator.filenames\n",
    "nb_samples = len(filenames)\n",
    "\n",
    "predict = model.predict_generator(test_generator,steps = \n",
    "                                   np.ceil(nb_samples/desired_batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Bathtub', 'Bed', 'Billiard table', 'Chair', 'Couch', 'Fireplace',\n",
       "       'Gas stove', 'Kitchen & dining room table', 'Lamp', 'Refrigerator',\n",
       "       'Sink', 'Stairs', 'Swimming pool', 'Television', 'Toilet'],\n",
       "      dtype='<U27')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(label_map.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2040"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return an array of probabilities\n",
    "prediction = model.predict_generator(test_generator, steps = np.ceil(nb_samples/desired_batch_size))\n",
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_true = test_generator.classes\n",
    "test_pred = prediction.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>t3</th>\n",
       "      <th>t4</th>\n",
       "      <th>t5</th>\n",
       "      <th>t6</th>\n",
       "      <th>t7</th>\n",
       "      <th>t8</th>\n",
       "      <th>t9</th>\n",
       "      <th>t10</th>\n",
       "      <th>t11</th>\n",
       "      <th>t12</th>\n",
       "      <th>t13</th>\n",
       "      <th>t14</th>\n",
       "      <th>t15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>p1</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>119</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>105</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p5</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p6</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p7</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p8</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>107</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p9</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p10</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p11</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p12</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>118</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p13</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p14</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>110</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p15</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     t1  t2   t3  t4  t5  t6  t7  t8  t9  t10  t11  t12  t13  t14  t15\n",
       "p1    4   2   87   1   2   6   7   1   0    5    1    2    2    0    1\n",
       "p2    2   2  119   2   1   6   3   0   1    3    0    5    1    1    4\n",
       "p3    1   0   98   3   1   8   1   5   0    5    0    3    3    2    4\n",
       "p4    5   0  105   2   0  14   5   4   0    1    1    3    5    0    4\n",
       "p5    3   1   97   2   1  12   3   4   1    3    2    2    5    1    9\n",
       "p6    5   0   93   1   1   8   5   1   0    0    2    4    6    2    4\n",
       "p7    7   0   75   0   0   9   2   0   0    2    4    3    1    1    4\n",
       "p8    5   3  107   1   0   7   5   1   0    1    5    1    6    2    2\n",
       "p9    6   2  105   0   1   7   3   2   0    3    1    5    3    5    4\n",
       "p10   6   2   81   0   0   7   2   0   0    1    1    3    4    0    3\n",
       "p11   4   0   91   1   1   5   1   1   0    2    2    8    3    1    4\n",
       "p12   3   1  118   3   1  11   5   1   0    1    1    2    3    0    0\n",
       "p13   5   5  105   1   0  14   6   0   0    2    1    1    6    3    1\n",
       "p14   3   4  110   2   2   8   3   1   0    0    1    1    5    1    5\n",
       "p15   6   0   91   0   0   8   0   1   0    5    6    4    0    3    3"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "pd.DataFrame(confusion_matrix(test_true,test_pred),\n",
    "    index = ['p1','p2','p3', 'p4','p5','p6','p7','p8','p9','p10','p11','p12','p13','p14','p15'],\n",
    "    columns =  ['t1','t2','t3', 't4','t5','t6','t7','t8','t9','t10','t11','t12','t13','t14','t15'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06519607843137255"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_true,test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
