{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Capstone Project: Using Object Detection to Classify Amenities in Vacation Rental Listings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Jordan Darbyshire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Project Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this project to identify objects from images of homes uploaded onto vacation rental sites. Potential customers browsing vacation rental sites are looking for specific amenities in their vacation rental. For example, they want an oven to cook in, a pool to swim in, a pool table for entertainment... the list goes on. When a property owner creates an ad on a vacation rental website, they would usually manually type in their amenities or select from a large list. Inaccurate amenities can result in lost revenue due to bad reviews or last-minute cancellations.\n",
    "\n",
    "Users of the website will search for properties based on their desired amenities, and will be shown a list of properties based on their preferences. What if there was an easier way to add amenities to the listing, simply by uploading the property images without the manual labor? This is where Object Detection come in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspiration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inspiration for this project comes from an [article](https://medium.com/airbnb-engineering/amenity-detection-and-beyond-new-frontiers-of-computer-vision-at-airbnb-144a4441b72e) I came across that highlighted an Airbnb computer vision project where they aim to detect room type by the types of objects identified in their listing photos.\n",
    "\n",
    "I was also interested in this project because of my interest in the Computer Vision field, which is still rapidly evolving. I have a love for travelling, and I have had a lot of experience using these rental sites. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any hotel or Vacation rental site such as Vrbo, Hotels.com, TripAdvisor, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset I used for this Capstone project is Open Images V4, which contains 1.99 million images spanning 600 classes. The dataset is open source, put together by Google. It is \"the largest existing dataset with object location annotations\", it includes bounding boxes manually drawn by humans to ensure accuracy and consistency [Source](https://storage.googleapis.com/openimages/web/factsfigures_v4.html). The images often show complex scenes with several objects (8 annotated objects per image on average) [Source](https://arxiv.org/abs/1811.00982). There are yearly competitions based on this dataset for the purpose of advancing the Computer Vision field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose this dataset due to the large number and variety of the image classes available, and the inclusion of the bounding boxes. If the images did not come with these bounding box \"labels\", I would have had to create these manually using open source software. To begin this project, I decided that I would choose 20 classes most applicable to this problem. To go about this, I chose the classes most relevant to each room type: Bedroom, Bathroom, Living Room, Kitchen, and Outdoor. On the Open Images website, you are able to download a csv that lists all of the classes contained in this dataset. I filtered the dataset to 30 classes based on what amenities would be the most desirable to the consumer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "import urllib\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I quickly realized that the biggest challenge with starting a project for Object Detection is putting together a dataset to run models on. Since the Open Images dataset is so large, it would be a monumental task to download the full dataset (split into 513GB for training, 12GB for validation and 36GB for testing), and sort through it to get what I need. The Open Images website does not provide an option to download select classes of images. Fortunately, I discovered that there are a few open source solutions to this problem. I attempted a few different \"toolkits\" that promised to solve this problem by downloading only desired classes. I ended up going with the [OIDv4 ToolKit](https://github.com/EscVM/OIDv4_ToolKit) to complete the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Toolkit:\n",
    "#!git clone https://github.com/EscVM/OIDv4_ToolKit.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OIDv4 ToolKit finally allowed me to download only the classes I was interested in from the Open Images dataset. Running the OID4 script initiated downloads of the images accompanied with an annotation text file (labels). The annotations have the class of each image and the bounding box coordinates which are used for Object Recognition purposes. Examples of the command to initiate the image downloader are found in the below code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download train set images for desired classes\n",
    "# The script below allowed me to copy and paste the commands into Anaconda prompt without manually editing each line:\n",
    "#for name in classes:   \n",
    "    #print(f\"python main.py downloader --classes {name} --type_csv train --limit 750\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I capped the number of images at 750 after observing some classes had tens of thousands of images, while half of my desired classes had less than 750 for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11907"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at count of .jpg files in the dataset folder\n",
    "\n",
    "data_dir = (\"data/Dataset_20\")\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "\n",
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "image_count "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completion of the downloading process, I ended up with 11,907 images (3 GB in size)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is accompanied by a text file that has the class name, and bounding box coordinates. These coordinates consist of four values that correspond to the actual number of pixels of the related image, and define the top left corner and bottom right corner of the bounding box (see below image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://raw.githubusercontent.com/EscVM/OIDv4_ToolKit/master/images/rectangle.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Image Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OpenCV to read in an image, open a window to view the image\n",
    "# OpenCV converts the image to a numpy array\n",
    "example_image = cv2.imread(\"data/Dataset_20/Bathtub/00a60d051d75144f.jpg\", cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "# Opens a window to display the image\n",
    "cv2.imshow(\"image\",example_image)\n",
    "# Wait until user presses a key\n",
    "cv2.waitKey(0)\n",
    "# Closes window based on waitforkey parameter\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Dimension    :  (1024, 683, 3)\n",
      "Image Height       :  1024\n",
      "Image Width        :  683\n",
      "Number of Channels :  3\n",
      "Number of pixels   :  2098176\n"
     ]
    }
   ],
   "source": [
    "# Get dimensions of an image using OpenCV\n",
    "# This code is from https://www.tutorialkart.com/opencv/python/opencv-python-get-image-size/\n",
    "\n",
    "# Read an image\n",
    "image = cv2.imread(\"data/Dataset_20/Bathtub/00a60d051d75144f.jpg\", cv2.IMREAD_UNCHANGED)\n",
    " \n",
    "# Get dimensions of image using shape\n",
    "dimensions = image.shape\n",
    " \n",
    "# height, width, number of channels in image\n",
    "height = image.shape[0]\n",
    "width = image.shape[1]\n",
    "channels = image.shape[2]\n",
    "pixel_num = image.size\n",
    " \n",
    "print(\"Image Dimension    : \",dimensions)\n",
    "print(\"Image Height       : \",height)\n",
    "print(\"Image Width        : \",width)\n",
    "print(\"Number of Channels : \",channels)\n",
    "print(\"Number of pixels   : \",pixel_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the image array\n",
    "# print(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE A DOCSTRING FOR THIS FUNCTION\n",
    "\n",
    "# This function reads through Dataset folders, returns a DataFrame with number of images per class and a visualization\n",
    "# It was created with help from https://stackoverflow.com/questions/62114998/python-make-dictionary-with-folder-names-as-key-and-file-names-as-values\n",
    "# and https://thispointer.com/python-pandas-how-to-convert-lists-to-a-dataframe/\n",
    "import fnmatch\n",
    "from collections import defaultdict\n",
    "\n",
    "def visualize_dataset(path):\n",
    "    \n",
    "    data_dict=defaultdict(set)\n",
    "    global data_df\n",
    "    \n",
    "    # This code will read the images from the Dataset folder, put them into a dictionary\n",
    "    for path,dirs,files in os.walk(path):\n",
    "        for f in fnmatch.filter(files,'*.jpg'):\n",
    "            data_dict[os.path.basename(path)].add(f)\n",
    "        \n",
    "    # Get number of pictures per class, append values to lists (intermediary step to a DataFrame)\n",
    "    key_list = []\n",
    "    count_list = []\n",
    "\n",
    "    # Get number of photos per class(folder)\n",
    "    for k,v in data_dict.items():\n",
    "        value = len(list(filter(None, v)))\n",
    "        key_list.append(k)\n",
    "        count_list.append(value)\n",
    "\n",
    "    # Create a zipped list of tuples from above lists\n",
    "    zippedList =  list(zip(key_list, count_list))\n",
    "\n",
    "    data_df = pd.DataFrame(zippedList, columns = [\"Class\" , \"Image_Count\"])\n",
    "\n",
    "    data_df = data_df.sort_values(by=\"Image_Count\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    print(data_df)\n",
    "        \n",
    "    # Make a plot of number of images per class\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.bar(data_df[\"Class\"], data_df[\"Image_Count\"])\n",
    "    plt.title(\"Number of images per class\", size=26)\n",
    "    plt.ylabel(\"Image count\", size=20)\n",
    "    plt.xticks(rotation=90, size=16)\n",
    "    plt.yticks(size=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Class  Image_Count\n",
      "0                           Bed          749\n",
      "1                 Swimming pool          748\n",
      "2                        Stairs          747\n",
      "3                         Chair          745\n",
      "4                          Lamp          731\n",
      "5                    Television          727\n",
      "6                         Couch          727\n",
      "7   Kitchen & dining room table          726\n",
      "8                Billiard table          666\n",
      "9                     Fireplace          657\n",
      "10                       Toilet          632\n",
      "11                         Sink          617\n",
      "12                      Bathtub          605\n",
      "13                 Refrigerator          547\n",
      "14                    Gas stove          536\n",
      "15                  Ceiling fan          463\n",
      "16              Washing machine          425\n",
      "17                    Microwave          376\n",
      "18                  Coffeemaker          295\n",
      "19                         Oven          188\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_dataset(\"data/Dataset_20/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Exploratory Data Analysis, there are two main concerns with this dataset which could affect training accuracy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Class imbalance: \\\n",
    "A solution to the first problem is to employ image augmentation, where copies of images in the unbalanced classes are made by flipping, scaling, or skewing. However due to time constrains, I am going to drop 5 classes that are not well represented in the data. This is also going to help reduce training time by reducing the size of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Large image dimensions / not consistent:\n",
    "The images dimenstions differ slightly, however one side is always 1024 pixels. This may or may not be a problem, but the large size of the images will also slow down training. These images may need to be rescaled before being passed into a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop from 20 to 15 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\jorda\\\\Documents\\\\Capstone'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Microwave oven folder has been removed\n",
      "The Washing machine folder has been removed\n",
      "The Ceiling fan folder has been removed\n"
     ]
    }
   ],
   "source": [
    "# Remove classes from Dataset folder\n",
    "# import shutil\n",
    "\n",
    "# to_delete = [\"Oven\", \"Coffeemaker\", \"Microwave oven\", \"Washing machine\", \"Ceiling fan\"]\n",
    "\n",
    "# for folder in to_delete:\n",
    "\n",
    "    # Removes entire folder and contents\n",
    "    # shutil.rmtree(f\"Dataset_20/{folder}\")\n",
    "    # print(f\"The {folder} folder has been removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Class  Image_Count\n",
      "0                           Bed          749\n",
      "1                 Swimming pool          748\n",
      "2                        Stairs          747\n",
      "3                         Chair          745\n",
      "4                          Lamp          731\n",
      "5                         Couch          727\n",
      "6                    Television          727\n",
      "7   Kitchen & dining room table          726\n",
      "8                Billiard table          666\n",
      "9                     Fireplace          657\n",
      "10                       Toilet          632\n",
      "11                         Sink          617\n",
      "12                      Bathtub          605\n",
      "13                 Refrigerator          547\n",
      "14                    Gas stove          536\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_dataset(\"data/Dataset_15/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10160"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at count of .jpg files in the dataset folder\n",
    "\n",
    "data_dir = (\"data/Dataset_15\")\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "\n",
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "image_count "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Pre-processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_retinanet import models\n",
    "from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\n",
    "from keras_retinanet.utils.visualization import draw_box, draw_caption\n",
    "from keras_retinanet.utils.colors import label_color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/set split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the file structure of my images and annotations, I used a script called \"partition_dataset.py\" from GitHub to split my image and XML data into 80-20 Train-test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\jorda\\\\Documents\\\\Capstone'"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/armaanpriyadarshan/Training-a-Custom-TensorFlow-2.X-Object-Detector.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The partition was successful\n"
     ]
    }
   ],
   "source": [
    "# Run this command to split the data\n",
    "\n",
    "#!python partition_dataset.py -i data/Dataset_15_copy -o data/Dataset_15_split -r 0.2 -x\n",
    "#print(f\"The partition was successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert annotations to XML, CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download tool to convert txt annotations to PASCAL VOC\n",
    "#!git clone https://github.com/thehetpandya/OIDv4_annotation_tool.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that annotations are in XML (Pascal), need to get them into a single csv file for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Pascal to CSV program\n",
    "# Source Code: https://gist.github.com/rotemtam/88d9a4efae243fc77ed4a0f9917c8f6c\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def xml_to_csv(path):\n",
    "    xml_list = []\n",
    "    for xml_file in glob.glob(path + '/*.xml'):\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        for member in root.findall('object'):\n",
    "            bbx = member.find('bndbox')\n",
    "            xmin = int(bbx.find('xmin').text)\n",
    "            ymin = int(bbx.find('ymin').text)\n",
    "            xmax = int(bbx.find('xmax').text)\n",
    "            ymax = int(bbx.find('ymax').text)\n",
    "            label = member.find('name').text\n",
    "\n",
    "            value = (root.find('filename').text,\n",
    "                     #int(root.find('size')[0].text),\n",
    "                     #int(root.find('size')[1].text),\n",
    "                     xmin,\n",
    "                     ymin,\n",
    "                     xmax,\n",
    "                     ymax,\n",
    "                     label\n",
    "                     )\n",
    "            xml_list.append(value)\n",
    "    column_name = ['image_name', #'width', 'height',\n",
    "                   'x_min', 'y_min', 'x_max', 'y_max', 'class_name']\n",
    "    xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
    "    return xml_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>x_min</th>\n",
       "      <th>y_min</th>\n",
       "      <th>x_max</th>\n",
       "      <th>y_max</th>\n",
       "      <th>class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00011aec5d7324f4.jpg</td>\n",
       "      <td>651</td>\n",
       "      <td>342</td>\n",
       "      <td>840</td>\n",
       "      <td>426</td>\n",
       "      <td>Gas stove</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001cb734adac2ee.jpg</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1020</td>\n",
       "      <td>765</td>\n",
       "      <td>Refrigerator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00045a4fa4352fc4.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>357</td>\n",
       "      <td>952</td>\n",
       "      <td>680</td>\n",
       "      <td>Swimming pool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00046bdb3022f7aa.jpg</td>\n",
       "      <td>267</td>\n",
       "      <td>312</td>\n",
       "      <td>330</td>\n",
       "      <td>349</td>\n",
       "      <td>Lamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00056223ec2b5aa1.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>473</td>\n",
       "      <td>167</td>\n",
       "      <td>767</td>\n",
       "      <td>Chair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12984</th>\n",
       "      <td>ffd7e79d4cb18e98.jpg</td>\n",
       "      <td>484</td>\n",
       "      <td>849</td>\n",
       "      <td>553</td>\n",
       "      <td>880</td>\n",
       "      <td>Lamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12985</th>\n",
       "      <td>ffd7e79d4cb18e98.jpg</td>\n",
       "      <td>512</td>\n",
       "      <td>880</td>\n",
       "      <td>555</td>\n",
       "      <td>913</td>\n",
       "      <td>Lamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12986</th>\n",
       "      <td>ffdae9dfe9d00503.jpg</td>\n",
       "      <td>242</td>\n",
       "      <td>341</td>\n",
       "      <td>554</td>\n",
       "      <td>652</td>\n",
       "      <td>Fireplace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12987</th>\n",
       "      <td>ffe4978e60297279.jpg</td>\n",
       "      <td>440</td>\n",
       "      <td>431</td>\n",
       "      <td>958</td>\n",
       "      <td>744</td>\n",
       "      <td>Chair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12988</th>\n",
       "      <td>ffe680faac5536de.jpg</td>\n",
       "      <td>76</td>\n",
       "      <td>705</td>\n",
       "      <td>375</td>\n",
       "      <td>1015</td>\n",
       "      <td>Toilet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12989 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 image_name  x_min  y_min  x_max  y_max     class_name\n",
       "0      00011aec5d7324f4.jpg    651    342    840    426      Gas stove\n",
       "1      0001cb734adac2ee.jpg      5      5   1020    765   Refrigerator\n",
       "2      00045a4fa4352fc4.jpg      0    357    952    680  Swimming pool\n",
       "3      00046bdb3022f7aa.jpg    267    312    330    349           Lamp\n",
       "4      00056223ec2b5aa1.jpg      0    473    167    767          Chair\n",
       "...                     ...    ...    ...    ...    ...            ...\n",
       "12984  ffd7e79d4cb18e98.jpg    484    849    553    880           Lamp\n",
       "12985  ffd7e79d4cb18e98.jpg    512    880    555    913           Lamp\n",
       "12986  ffdae9dfe9d00503.jpg    242    341    554    652      Fireplace\n",
       "12987  ffe4978e60297279.jpg    440    431    958    744          Chair\n",
       "12988  ffe680faac5536de.jpg     76    705    375   1015         Toilet\n",
       "\n",
       "[12989 rows x 6 columns]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the train dataset\n",
    "train_df = xml_to_csv(\"data/Dataset_15_split/train\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train data set has 8,127 images, with 12,989 annotations (bounding boxes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>x_min</th>\n",
       "      <th>y_min</th>\n",
       "      <th>x_max</th>\n",
       "      <th>y_max</th>\n",
       "      <th>class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000325b47e09c6aa.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>469</td>\n",
       "      <td>128</td>\n",
       "      <td>741</td>\n",
       "      <td>Chair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000325b47e09c6aa.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>546</td>\n",
       "      <td>183</td>\n",
       "      <td>960</td>\n",
       "      <td>Chair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00061ea456165593.jpg</td>\n",
       "      <td>196</td>\n",
       "      <td>530</td>\n",
       "      <td>481</td>\n",
       "      <td>761</td>\n",
       "      <td>Sink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00061ea456165593.jpg</td>\n",
       "      <td>604</td>\n",
       "      <td>536</td>\n",
       "      <td>875</td>\n",
       "      <td>700</td>\n",
       "      <td>Sink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000698b6a00772ac.jpg</td>\n",
       "      <td>463</td>\n",
       "      <td>393</td>\n",
       "      <td>1023</td>\n",
       "      <td>584</td>\n",
       "      <td>Bathtub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3286</th>\n",
       "      <td>ff4d029d6b2c461d.jpg</td>\n",
       "      <td>567</td>\n",
       "      <td>577</td>\n",
       "      <td>768</td>\n",
       "      <td>958</td>\n",
       "      <td>Toilet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3287</th>\n",
       "      <td>ff5d89c0517480ad.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1022</td>\n",
       "      <td>575</td>\n",
       "      <td>Television</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3288</th>\n",
       "      <td>ff9b80a751bc3ecc.jpg</td>\n",
       "      <td>51</td>\n",
       "      <td>249</td>\n",
       "      <td>894</td>\n",
       "      <td>803</td>\n",
       "      <td>Television</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3289</th>\n",
       "      <td>ffd3a541c54324a3.jpg</td>\n",
       "      <td>167</td>\n",
       "      <td>170</td>\n",
       "      <td>291</td>\n",
       "      <td>245</td>\n",
       "      <td>Sink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3290</th>\n",
       "      <td>ffd3a541c54324a3.jpg</td>\n",
       "      <td>265</td>\n",
       "      <td>163</td>\n",
       "      <td>401</td>\n",
       "      <td>233</td>\n",
       "      <td>Sink</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3291 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                image_name  x_min  y_min  x_max  y_max  class_name\n",
       "0     000325b47e09c6aa.jpg      0    469    128    741       Chair\n",
       "1     000325b47e09c6aa.jpg      0    546    183    960       Chair\n",
       "2     00061ea456165593.jpg    196    530    481    761        Sink\n",
       "3     00061ea456165593.jpg    604    536    875    700        Sink\n",
       "4     000698b6a00772ac.jpg    463    393   1023    584     Bathtub\n",
       "...                    ...    ...    ...    ...    ...         ...\n",
       "3286  ff4d029d6b2c461d.jpg    567    577    768    958      Toilet\n",
       "3287  ff5d89c0517480ad.jpg      0      0   1022    575  Television\n",
       "3288  ff9b80a751bc3ecc.jpg     51    249    894    803  Television\n",
       "3289  ffd3a541c54324a3.jpg    167    170    291    245        Sink\n",
       "3290  ffd3a541c54324a3.jpg    265    163    401    233        Sink\n",
       "\n",
       "[3291 rows x 6 columns]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the test dataset\n",
    "test_df = xml_to_csv(\"data/Dataset_15_split/test\")\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test data set has 2,032 images with 3,291 annotations (bounding boxes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrames to a CSV\n",
    "train_df.to_csv(\"data/labels_train.csv\")\n",
    "test_df.to_csv(\"data/labels_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the bounding boxes on the images to complete the pre-processing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source code: https://www.curiousily.com/posts/object-detection-on-custom-dataset-with-tensorflow-2-and-keras-using-python/\n",
    "def show_image_objects(image_row):\n",
    "    \n",
    "    data_folder = Path(\"data/Dataset_15_split/train\")\n",
    "    \n",
    "    img_path = data_folder / image_row.image_name \n",
    "    box = [\n",
    "        image_row.x_min, image_row.y_min, image_row.x_max, image_row.y_max\n",
    "    ]\n",
    "\n",
    "    image = read_image_bgr(img_path)\n",
    "\n",
    "    draw = image.copy()\n",
    "    draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    draw_box(draw, box, color=(255, 255, 0))\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.imshow(draw)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image_objects(train_df.iloc[18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move ahead and do some modelling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare inputs for TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions taken from the [Tensorflow Object Detection API](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/using_your_own_dataset.md):\n",
    "1. Create Label Maps\n",
    "2. Convert dataset to TFRecord file format\n",
    "\n",
    "For every example in dataset, we need:\\\n",
    "    - An RGB image for the dataset encoded as jpeg or png.\\\n",
    "    - A list of bounding boxes for the image. Each bounding box should contain:\\\n",
    "        i. A bounding box coordinates (with origin in top left corner) defined by 4 floating point numbers - ymin, xmin, ymax, xmax. Note that we need to store the normalized coordinates (x / width, y / height) in the TFRecord dataset.\\\n",
    "        ii. The class of the object in the bounding box.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Create Label Maps** Now that the annotations are in CSV format, we also need to create a label map which is required to make TFRecords out of dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bed', 'Swimming pool', 'Stairs', 'Chair', 'Lamp', 'Couch', 'Television', 'Kitchen & dining room table', 'Billiard table', 'Fireplace', 'Toilet', 'Sink', 'Bathtub', 'Refrigerator', 'Gas stove']\n"
     ]
    }
   ],
   "source": [
    "s = pd.Series(data_df[\"Class\"])\n",
    "class_list = s.values.tolist()\n",
    "print(class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item {\n",
      "  name: \"Bed\"\n",
      "  id: 1\n",
      "}\n",
      "item {\n",
      "  name: \"Swimming pool\"\n",
      "  id: 2\n",
      "}\n",
      "item {\n",
      "  name: \"Stairs\"\n",
      "  id: 3\n",
      "}\n",
      "item {\n",
      "  name: \"Chair\"\n",
      "  id: 4\n",
      "}\n",
      "item {\n",
      "  name: \"Lamp\"\n",
      "  id: 5\n",
      "}\n",
      "item {\n",
      "  name: \"Couch\"\n",
      "  id: 6\n",
      "}\n",
      "item {\n",
      "  name: \"Television\"\n",
      "  id: 7\n",
      "}\n",
      "item {\n",
      "  name: \"Kitchen & dining room table\"\n",
      "  id: 8\n",
      "}\n",
      "item {\n",
      "  name: \"Billiard table\"\n",
      "  id: 9\n",
      "}\n",
      "item {\n",
      "  name: \"Fireplace\"\n",
      "  id: 10\n",
      "}\n",
      "item {\n",
      "  name: \"Toilet\"\n",
      "  id: 11\n",
      "}\n",
      "item {\n",
      "  name: \"Sink\"\n",
      "  id: 12\n",
      "}\n",
      "item {\n",
      "  name: \"Bathtub\"\n",
      "  id: 13\n",
      "}\n",
      "item {\n",
      "  name: \"Refrigerator\"\n",
      "  id: 14\n",
      "}\n",
      "item {\n",
      "  name: \"Gas stove\"\n",
      "  id: 15\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Converting the list of classes to label_map.pbtxt\n",
    "\n",
    "from object_detection.protos.string_int_label_map_pb2 import StringIntLabelMap, StringIntLabelMapItem\n",
    "from google.protobuf import text_format\n",
    "\n",
    "\n",
    "def convert_classes(classes, start=1):\n",
    "    msg = StringIntLabelMap()\n",
    "    for id, name in enumerate(classes, start=start):\n",
    "        msg.item.append(StringIntLabelMapItem(id=id, name=name))\n",
    "\n",
    "    text = str(text_format.MessageToBytes(msg, as_utf8=True), 'utf-8')\n",
    "    return text\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    txt = convert_classes(class_list)\n",
    "    print(txt)\n",
    "    with open('label_map.pbtxt', 'w') as f:\n",
    "        f.write(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Convert dataset to TFRecord file format**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Label Map, can generate a tf.Example proto for images using a script based on the TensorFlow Object Detector API. I cloned the following repository, and used the \"generate_tfrecord.py\" script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/armaanpriyadarshan/Training-a-Custom-TensorFlow-2.X-Object-Detector.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python generate_tfrecord.py -x Tensorflow/workspace/capstone/images/train -l workspace/capstone/annotations/label_map.pbtxt -o workspace/capstone/annotations/train.record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python generate_tfrecord.py -x Tensorflow/workspace/capstone/images/test -l workspace/capstone/annotations/label_map.pbtxt -o workspace/capstone/annotations/test.record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation of the TFRecords was successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Training - TensorFlow**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project I decided to make use of pre-trained models for object detection, rather than building a model from scratch. The benefit to this is that we are leveraging knowledge (features, weights) from previously trained models to apply to related datasets. In the case of computer vision, low-level features such as edges, shapes, corners and intensity can be shared among tasks (transfer learning). By usng pre-trained models we are saving time and computer resources which is what is needed for this large dataset and the task at hand (object detection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset with pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to make use of the CUDA-enabled GPU on my laptop to run the pre-trained models (GeForce GTX 1660-Ti with 6GB of GDDR6 memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "# Show the TensorFlow version\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn on memory growth for GPU by running the following code prior to allocating any tensors or executing any ops [Source: TensorFlow - Using a GPU](https://www.tensorflow.org/guide/gpu). Attempts to allocate only as much GPU memory as needed for the runtime allocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# Run the following code before executing anything\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained model no. 1: ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Released in 2019, this model is a Single Shot Detector (SSD) model that goes straight from image pixels to bounding box coordinates and class probabilities. By using SSD, we only need to take one single shot to detect multiple objects within the image. Compare this to R-CNN that needs two shots, one for generating region proposals, one for detecting the object of each proposal. Thus, SSD is much faster compared with two-shot RPN based approaches. [Source](https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11). The model architecture is based on inverted residual structure where the input and output of the residual block are thin bottleneck layers as opposed to traditional residual models. Moreover, nonlinearities are removed from intermediate layers and lightweight depthwise convolution is used. This model is part of the Tensorflow object detection API. [Source](https://resources.wolframcloud.com/NeuralNetRepository/resources/SSD-MobileNet-V2-Trained-on-MS-COCO-Data)\n",
    "\n",
    "The SSD Mobilenet V2 is an Object detection model with FPN-lite feature extractor, shared box predictor and focal loss, trained on COCO 2017 dataset with training images scaled to 640x640 [Source](https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_640x640/1). Focal loss is very useful for dealing with class imbalance, especially in object detection tasks. The loss function is a dynamically scaled cross-entropy loss, where the scaling factor decays to zero as confidence in the correct class increases. [Source](https://medium.com/analytics-vidhya/how-focal-loss-fixes-the-class-imbalance-problem-in-object-detection-3d2e1c4da8d7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://cdn-images-1.medium.com/max/1000/1*GmJiirxTSuSVrh-r7gtJdA.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained model no. 2: RetinaNet + Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used this [guide](https://www.curiousily.com/posts/object-detection-on-custom-dataset-with-tensorflow-2-and-keras-using-python/) to train a RetinaNet object detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/fizyr/keras-retinanet.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install keras-retinanet\n",
    "# %cd keras-retinanet/\n",
    "#! pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data in format required for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras requires two input files\n",
    "\n",
    "ANNOTATIONS_FILE = \"annotations.csv\"\n",
    "CLASSES_FILE = \"classes.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bed',\n",
       " 'Swimming pool',\n",
       " 'Stairs',\n",
       " 'Chair',\n",
       " 'Lamp',\n",
       " 'Couch',\n",
       " 'Television',\n",
       " 'Kitchen & dining room table',\n",
       " 'Billiard table',\n",
       " 'Fireplace',\n",
       " 'Toilet',\n",
       " 'Sink',\n",
       " 'Bathtub',\n",
       " 'Refrigerator',\n",
       " 'Gas stove']"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at class list created previously\n",
    "class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create numbers for the classes (indexing starts at 0)\n",
    "class_number = list(np.arange(0, 20))\n",
    "class_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the two lists together into a DataFrame\n",
    "class_tuples = list(zip(class_list,class_number))\n",
    "classes_df = pd.DataFrame(class_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers need to be removed\n",
    "classes_df.to_csv(CLASSES_FILE, index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers need to be removed\n",
    "train_df.to_csv(ANNOTATIONS_FILE, index=False, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download pre-trained model from RetinaNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\jorda\\\\Documents\\\\Capstone\\\\keras-retinanet\\\\keras_retinanet\\\\bin'"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded pretrained model to ./snapshots/_pretrained_model.h5\n"
     ]
    }
   ],
   "source": [
    "PRETRAINED_MODEL = './snapshots/_pretrained_model.h5'\n",
    "\n",
    "URL_MODEL = 'https://github.com/fizyr/keras-retinanet/releases/download/0.5.1/resnet50_coco_best_v2.1.0.h5'\n",
    "\n",
    "urllib.request.urlretrieve(URL_MODEL, PRETRAINED_MODEL)\n",
    "\n",
    "print('Downloaded pretrained model to ' + PRETRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start TensorBoard to monitor training\n",
    "#!tensorboard --logdir=keras_retinanet\\bin\\logs\\train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep 08 12:20:25 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 452.06       Driver Version: 452.06       CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 166... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   48C    P8     5W /  N/A |   5318MiB /  6144MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     15724      C   ...nvs\\tensorflow\\python.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Ensure GPU is being used\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "Creating model, this may take a second...\n",
      "Model: \"retinanet\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-08 02:16:21.457972: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\n",
      "2020-09-08 02:16:23.506723: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\n",
      "2020-09-08 02:16:23.526821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: GeForce GTX 1660 Ti with Max-Q Design computeCapability: 7.5\n",
      "coreClock: 1.335GHz coreCount: 24 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 268.26GiB/s\n",
      "2020-09-08 02:16:23.526842: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\n",
      "2020-09-08 02:16:23.529137: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\n",
      "2020-09-08 02:16:23.531180: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\n",
      "2020-09-08 02:16:23.531952: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\n",
      "2020-09-08 02:16:23.534197: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\n",
      "2020-09-08 02:16:23.535632: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\n",
      "2020-09-08 02:16:23.539813: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\n",
      "2020-09-08 02:16:23.539905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2020-09-08 02:16:23.540355: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-09-08 02:16:23.548820: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2cd47279040 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-09-08 02:16:23.548843: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-09-08 02:16:23.548975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: GeForce GTX 1660 Ti with Max-Q Design computeCapability: 7.5\n",
      "coreClock: 1.335GHz coreCount: 24 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 268.26GiB/s\n",
      "2020-09-08 02:16:23.548994: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\n",
      "2020-09-08 02:16:23.549007: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\n",
      "2020-09-08 02:16:23.549014: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\n",
      "2020-09-08 02:16:23.549019: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\n",
      "2020-09-08 02:16:23.549025: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\n",
      "2020-09-08 02:16:23.549030: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\n",
      "2020-09-08 02:16:23.549035: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\n",
      "2020-09-08 02:16:23.549078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2020-09-08 02:16:24.030316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, None, None, 6 9408        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, None, None, 6 256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, None, None, 6 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, None, None, 6 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, None, None, 6 4096        pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, None, None, 6 256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a_relu (Activation (None, None, None, 6 0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "padding2a_branch2b (ZeroPadding (None, None, None, 6 0           res2a_branch2a_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, None, None, 6 36864       padding2a_branch2b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, None, None, 6 256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b_relu (Activation (None, None, None, 6 0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, None, None, 2 16384       res2a_branch2b_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, None, None, 2 16384       pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, None, None, 2 1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, None, None, 2 1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a (Add)                     (None, None, None, 2 0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res2a_relu (Activation)         (None, None, None, 2 0           res2a[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, None, None, 6 16384       res2a_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, None, None, 6 256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a_relu (Activation (None, None, None, 6 0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "padding2b_branch2b (ZeroPadding (None, None, None, 6 0           res2b_branch2a_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, None, None, 6 36864       padding2b_branch2b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, None, None, 6 256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b_relu (Activation (None, None, None, 6 0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, None, None, 2 16384       res2b_branch2b_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, None, None, 2 1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2b (Add)                     (None, None, None, 2 0           bn2b_branch2c[0][0]              \n",
      "                                                                 res2a_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res2b_relu (Activation)         (None, None, None, 2 0           res2b[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, None, None, 6 16384       res2b_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, None, None, 6 256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a_relu (Activation (None, None, None, 6 0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "padding2c_branch2b (ZeroPadding (None, None, None, 6 0           res2c_branch2a_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, None, None, 6 36864       padding2c_branch2b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, None, None, 6 256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b_relu (Activation (None, None, None, 6 0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, None, None, 2 16384       res2c_branch2b_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, None, None, 2 1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2c (Add)                     (None, None, None, 2 0           bn2c_branch2c[0][0]              \n",
      "                                                                 res2b_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res2c_relu (Activation)         (None, None, None, 2 0           res2c[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, None, None, 1 32768       res2c_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, None, None, 1 512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a_relu (Activation (None, None, None, 1 0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "padding3a_branch2b (ZeroPadding (None, None, None, 1 0           res3a_branch2a_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, None, None, 1 147456      padding3a_branch2b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, None, None, 1 512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b_relu (Activation (None, None, None, 1 0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, None, None, 5 65536       res3a_branch2b_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, None, None, 5 131072      res2c_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, None, None, 5 2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, None, None, 5 2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a (Add)                     (None, None, None, 5 0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res3a_relu (Activation)         (None, None, None, 5 0           res3a[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, None, None, 1 65536       res3a_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, None, None, 1 512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a_relu (Activation (None, None, None, 1 0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "padding3b_branch2b (ZeroPadding (None, None, None, 1 0           res3b_branch2a_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, None, None, 1 147456      padding3b_branch2b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, None, None, 1 512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b_relu (Activation (None, None, None, 1 0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, None, None, 5 65536       res3b_branch2b_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, None, None, 5 2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3b (Add)                     (None, None, None, 5 0           bn3b_branch2c[0][0]              \n",
      "                                                                 res3a_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res3b_relu (Activation)         (None, None, None, 5 0           res3b[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, None, None, 1 65536       res3b_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, None, None, 1 512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a_relu (Activation (None, None, None, 1 0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "padding3c_branch2b (ZeroPadding (None, None, None, 1 0           res3c_branch2a_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, None, None, 1 147456      padding3c_branch2b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, None, None, 1 512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b_relu (Activation (None, None, None, 1 0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, None, None, 5 65536       res3c_branch2b_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, None, None, 5 2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3c (Add)                     (None, None, None, 5 0           bn3c_branch2c[0][0]              \n",
      "                                                                 res3b_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res3c_relu (Activation)         (None, None, None, 5 0           res3c[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, None, None, 1 65536       res3c_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, None, None, 1 512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a_relu (Activation (None, None, None, 1 0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "padding3d_branch2b (ZeroPadding (None, None, None, 1 0           res3d_branch2a_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, None, None, 1 147456      padding3d_branch2b[0][0]         \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-09-08 02:16:24.030336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
      "2020-09-08 02:16:24.030341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
      "2020-09-08 02:16:24.030474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4743 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1660 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5)\n",
      "2020-09-08 02:16:24.033683: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2cd70e5d240 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2020-09-08 02:16:24.033702: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1660 Ti with Max-Q Design, Compute Capability 7.5\n",
      "WARNING:tensorflow:Skipping loading of weights for layer classification_submodel due to mismatch in shape ((3, 3, 256, 135) vs (3, 3, 256, 720)).\n",
      "WARNING:tensorflow:Skipping loading of weights for layer classification_submodel due to mismatch in shape ((135,) vs (720,)).\n",
      "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n",
      "2020-09-08 02:17:00.407124: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.\n",
      "2020-09-08 02:17:00.408239: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1391] Profiler found 1 GPUs\n",
      "2020-09-08 02:17:00.413942: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cupti64_101.dll'; dlerror: cupti64_101.dll not found\n",
      "2020-09-08 02:17:00.414737: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cupti.dll'; dlerror: cupti.dll not found\n",
      "2020-09-08 02:17:00.414755: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "WARNING:tensorflow:From train.py:541: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "2020-09-08 02:17:09.090954: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\n",
      "2020-09-08 02:17:11.597535: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2020-09-08 02:17:11.791544: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\n",
      "2020-09-08 02:17:13.592454: W tensorflow/core/common_runtime/bfc_allocator.cc:312] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.\n",
      "2020-09-08 02:17:13.644276: I tensorflow/stream_executor/cuda/cuda_driver.cc:775] failed to allocate 3.63G (3900769536 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2020-09-08 02:17:15.550209: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.12GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2020-09-08 02:17:16.848757: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.\n",
      "2020-09-08 02:17:16.848812: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "WARNING:tensorflow:From C:\\Users\\jorda\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "2020-09-08 02:17:20.859553: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events. \n",
      "2020-09-08 02:17:20.881712: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs\\train\\plugins\\profile\\2020_09_08_08_17_20\n",
      "2020-09-08 02:17:20.884524: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs\\train\\plugins\\profile\\2020_09_08_08_17_20\\LAPTOP-8QLI5GU4.trace.json.gz\n",
      "2020-09-08 02:17:20.935744: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs\\train\\plugins\\profile\\2020_09_08_08_17_20\n",
      "2020-09-08 02:17:20.941920: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs\\train\\plugins\\profile\\2020_09_08_08_17_20\\LAPTOP-8QLI5GU4.memory_profile.json.gz\n",
      "2020-09-08 02:17:20.948455: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs\\train\\plugins\\profile\\2020_09_08_08_17_20Dumped tool data for xplane.pb to logs\\train\\plugins\\profile\\2020_09_08_08_17_20\\LAPTOP-8QLI5GU4.xplane.pb\n",
      "Dumped tool data for overview_page.pb to logs\\train\\plugins\\profile\\2020_09_08_08_17_20\\LAPTOP-8QLI5GU4.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs\\train\\plugins\\profile\\2020_09_08_08_17_20\\LAPTOP-8QLI5GU4.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs\\train\\plugins\\profile\\2020_09_08_08_17_20\\LAPTOP-8QLI5GU4.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs\\train\\plugins\\profile\\2020_09_08_08_17_20\\LAPTOP-8QLI5GU4.kernel_stats.pb\n",
      "\n",
      "2020-09-08 02:17:28.899572: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.12GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2020-09-08 02:17:28.923186: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.08GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2020-09-08 02:18:01.635673: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.12GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2020-09-08 02:18:01.659174: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.08GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2020-09-08 02:18:06.238561: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.12GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2020-09-08 02:18:06.263067: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.08GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2020-09-08 02:18:11.944798: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.12GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2020-09-08 02:18:48.055199: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.12GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2020-09-08 02:18:48.079274: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.08GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bn3d_branch2b (BatchNormalizati (None, None, None, 1 512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b_relu (Activation (None, None, None, 1 0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, None, None, 5 65536       res3d_branch2b_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, None, None, 5 2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3d (Add)                     (None, None, None, 5 0           bn3d_branch2c[0][0]              \n",
      "                                                                 res3c_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res3d_relu (Activation)         (None, None, None, 5 0           res3d[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, None, None, 2 131072      res3d_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, None, None, 2 1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a_relu (Activation (None, None, None, 2 0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "padding4a_branch2b (ZeroPadding (None, None, None, 2 0           res4a_branch2a_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, None, None, 2 589824      padding4a_branch2b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, None, None, 2 1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b_relu (Activation (None, None, None, 2 0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, None, None, 1 262144      res4a_branch2b_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, None, None, 1 524288      res3d_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, None, None, 1 4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, None, None, 1 4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a (Add)                     (None, None, None, 1 0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res4a_relu (Activation)         (None, None, None, 1 0           res4a[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, None, None, 2 262144      res4a_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, None, None, 2 1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a_relu (Activation (None, None, None, 2 0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "padding4b_branch2b (ZeroPadding (None, None, None, 2 0           res4b_branch2a_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, None, None, 2 589824      padding4b_branch2b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, None, None, 2 1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b_relu (Activation (None, None, None, 2 0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, None, None, 1 262144      res4b_branch2b_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, None, None, 1 4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4b (Add)                     (None, None, None, 1 0           bn4b_branch2c[0][0]              \n",
      "                                                                 res4a_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res4b_relu (Activation)         (None, None, None, 1 0           res4b[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, None, None, 2 262144      res4b_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, None, None, 2 1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a_relu (Activation (None, None, None, 2 0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "padding4c_branch2b (ZeroPadding (None, None, None, 2 0           res4c_branch2a_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, None, None, 2 589824      padding4c_branch2b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, None, None, 2 1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b_relu (Activation (None, None, None, 2 0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, None, None, 1 262144      res4c_branch2b_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, None, None, 1 4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4c (Add)                     (None, None, None, 1 0           bn4c_branch2c[0][0]              \n",
      "                                                                 res4b_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res4c_relu (Activation)         (None, None, None, 1 0           res4c[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, None, None, 2 262144      res4c_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, None, None, 2 1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a_relu (Activation (None, None, None, 2 0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "padding4d_branch2b (ZeroPadding (None, None, None, 2 0           res4d_branch2a_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, None, None, 2 589824      padding4d_branch2b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, None, None, 2 1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b_relu (Activation (None, None, None, 2 0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, None, None, 1 262144      res4d_branch2b_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, None, None, 1 4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4d (Add)                     (None, None, None, 1 0           bn4d_branch2c[0][0]              \n",
      "                                                                 res4c_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res4d_relu (Activation)         (None, None, None, 1 0           res4d[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, None, None, 2 262144      res4d_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, None, None, 2 1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a_relu (Activation (None, None, None, 2 0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "padding4e_branch2b (ZeroPadding (None, None, None, 2 0           res4e_branch2a_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, None, None, 2 589824      padding4e_branch2b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, None, None, 2 1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b_relu (Activation (None, None, None, 2 0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, None, None, 1 262144      res4e_branch2b_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, None, None, 1 4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4e (Add)                     (None, None, None, 1 0           bn4e_branch2c[0][0]              \n",
      "                                                                 res4d_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res4e_relu (Activation)         (None, None, None, 1 0           res4e[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, None, None, 2 262144      res4e_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, None, None, 2 1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a_relu (Activation (None, None, None, 2 0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "padding4f_branch2b (ZeroPadding (None, None, None, 2 0           res4f_branch2a_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, None, None, 2 589824      padding4f_branch2b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, None, None, 2 1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b_relu (Activation (None, None, None, 2 0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, None, None, 1 262144      res4f_branch2b_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, None, None, 1 4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4f (Add)                     (None, None, None, 1 0           bn4f_branch2c[0][0]              \n",
      "                                                                 res4e_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res4f_relu (Activation)         (None, None, None, 1 0           res4f[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, None, None, 5 524288      res4f_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, None, None, 5 2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a_relu (Activation (None, None, None, 5 0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "padding5a_branch2b (ZeroPadding (None, None, None, 5 0           res5a_branch2a_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, None, None, 5 2359296     padding5a_branch2b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, None, None, 5 2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b_relu (Activation (None, None, None, 5 0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, None, None, 2 1048576     res5a_branch2b_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, None, None, 2 2097152     res4f_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, None, None, 2 8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, None, None, 2 8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a (Add)                     (None, None, None, 2 0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res5a_relu (Activation)         (None, None, None, 2 0           res5a[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, None, None, 5 1048576     res5a_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, None, None, 5 2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a_relu (Activation (None, None, None, 5 0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "padding5b_branch2b (ZeroPadding (None, None, None, 5 0           res5b_branch2a_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, None, None, 5 2359296     padding5b_branch2b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, None, None, 5 2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b_relu (Activation (None, None, None, 5 0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, None, None, 2 1048576     res5b_branch2b_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, None, None, 2 8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5b (Add)                     (None, None, None, 2 0           bn5b_branch2c[0][0]              \n",
      "                                                                 res5a_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res5b_relu (Activation)         (None, None, None, 2 0           res5b[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, None, None, 5 1048576     res5b_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, None, None, 5 2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a_relu (Activation (None, None, None, 5 0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "padding5c_branch2b (ZeroPadding (None, None, None, 5 0           res5c_branch2a_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, None, None, 5 2359296     padding5c_branch2b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, None, None, 5 2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b_relu (Activation (None, None, None, 5 0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, None, None, 2 1048576     res5c_branch2b_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, None, None, 2 8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5c (Add)                     (None, None, None, 2 0           bn5c_branch2c[0][0]              \n",
      "                                                                 res5b_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res5c_relu (Activation)         (None, None, None, 2 0           res5c[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "C5_reduced (Conv2D)             (None, None, None, 2 524544      res5c_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "P5_upsampled (UpsampleLike)     (None, None, None, 2 0           C5_reduced[0][0]                 \n",
      "                                                                 res4f_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "C4_reduced (Conv2D)             (None, None, None, 2 262400      res4f_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "P4_merged (Add)                 (None, None, None, 2 0           P5_upsampled[0][0]               \n",
      "                                                                 C4_reduced[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "P4_upsampled (UpsampleLike)     (None, None, None, 2 0           P4_merged[0][0]                  \n",
      "                                                                 res3d_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "C3_reduced (Conv2D)             (None, None, None, 2 131328      res3d_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "P6 (Conv2D)                     (None, None, None, 2 4718848     res5c_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "P3_merged (Add)                 (None, None, None, 2 0           P4_upsampled[0][0]               \n",
      "                                                                 C3_reduced[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "C6_relu (Activation)            (None, None, None, 2 0           P6[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "P3 (Conv2D)                     (None, None, None, 2 590080      P3_merged[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "P4 (Conv2D)                     (None, None, None, 2 590080      P4_merged[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "P5 (Conv2D)                     (None, None, None, 2 590080      C5_reduced[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "P7 (Conv2D)                     (None, None, None, 2 590080      C6_relu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "regression_submodel (Functional (None, None, 4)      2443300     P3[0][0]                         \n",
      "                                                                 P4[0][0]                         \n",
      "                                                                 P5[0][0]                         \n",
      "                                                                 P6[0][0]                         \n",
      "                                                                 P7[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "classification_submodel (Functi (None, None, 15)     2671495     P3[0][0]                         \n",
      "                                                                 P4[0][0]                         \n",
      "                                                                 P5[0][0]                         \n",
      "                                                                 P6[0][0]                         \n",
      "                                                                 P7[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "regression (Concatenate)        (None, None, 4)      0           regression_submodel[0][0]        \n",
      "                                                                 regression_submodel[1][0]        \n",
      "                                                                 regression_submodel[2][0]        \n",
      "                                                                 regression_submodel[3][0]        \n",
      "                                                                 regression_submodel[4][0]        \n",
      "__________________________________________________________________________________________________\n",
      "classification (Concatenate)    (None, None, 15)     0           classification_submodel[0][0]    \n",
      "                                                                 classification_submodel[1][0]    \n",
      "                                                                 classification_submodel[2][0]    \n",
      "                                                                 classification_submodel[3][0]    \n",
      "                                                                 classification_submodel[4][0]    \n",
      "==================================================================================================\n",
      "Total params: 36,673,387\n",
      "Trainable params: 13,112,235\n",
      "Non-trainable params: 23,561,152\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 3.3515 - regression_loss: 2.1774 - classification_loss: 1.1742\n",
      "  2/500 [..............................] - ETA: 17:01 - loss: 3.2503 - regression_loss: 2.0589 - classification_loss: 1.1914\n",
      "  3/500 [..............................] - ETA: 20:49 - loss: 3.0788 - regression_loss: 1.8698 - classification_loss: 1.2090\n",
      "  4/500 [..............................] - ETA: 18:33 - loss: 3.2367 - regression_loss: 2.0381 - classification_loss: 1.1987\n",
      "  5/500 [..............................] - ETA: 22:02 - loss: 3.1812 - regression_loss: 1.9744 - classification_loss: 1.2069\n",
      "  6/500 [..............................] - ETA: 23:55 - loss: 3.0683 - regression_loss: 1.8577 - classification_loss: 1.2106\n",
      "  7/500 [..............................] - ETA: 24:44 - loss: 3.0765 - regression_loss: 1.8592 - classification_loss: 1.2173\n",
      "  8/500 [..............................] - ETA: 22:35 - loss: 2.9844 - regression_loss: 1.7746 - classification_loss: 1.2098\n",
      "  9/500 [..............................] - ETA: 20:55 - loss: 2.9469 - regression_loss: 1.7327 - classification_loss: 1.2142\n",
      " 10/500 [..............................] - ETA: 19:58 - loss: 2.9366 - regression_loss: 1.7217 - classification_loss: 1.2149\n",
      " 11/500 [..............................] - ETA: 18:54 - loss: 2.9755 - regression_loss: 1.7620 - classification_loss: 1.2135\n",
      " 12/500 [..............................] - ETA: 20:04 - loss: 3.0484 - regression_loss: 1.8321 - classification_loss: 1.2163\n",
      " 13/500 [..............................] - ETA: 19:15 - loss: 3.0520 - regression_loss: 1.8412 - classification_loss: 1.2108\n",
      " 14/500 [..............................] - ETA: 20:12 - loss: 3.0317 - regression_loss: 1.8181 - classification_loss: 1.2136\n",
      " 15/500 [..............................] - ETA: 19:36 - loss: 3.0272 - regression_loss: 1.8140 - classification_loss: 1.2132\n",
      " 16/500 [..............................] - ETA: 18:50 - loss: 2.9864 - regression_loss: 1.7733 - classification_loss: 1.2130\n",
      " 17/500 [>.............................] - ETA: 18:19 - loss: 3.0241 - regression_loss: 1.8156 - classification_loss: 1.2085\n",
      " 18/500 [>.............................] - ETA: 17:42 - loss: 3.0432 - regression_loss: 1.8367 - classification_loss: 1.2065\n",
      " 19/500 [>.............................] - ETA: 17:09 - loss: 3.0455 - regression_loss: 1.8349 - classification_loss: 1.2105\n",
      " 20/500 [>.............................] - ETA: 16:39 - loss: 3.0034 - regression_loss: 1.7993 - classification_loss: 1.2042\n",
      " 21/500 [>.............................] - ETA: 17:32 - loss: 2.9802 - regression_loss: 1.7768 - classification_loss: 1.2034\n",
      " 22/500 [>.............................] - ETA: 18:23 - loss: 2.9967 - regression_loss: 1.7938 - classification_loss: 1.2029\n",
      " 23/500 [>.............................] - ETA: 17:58 - loss: 3.0125 - regression_loss: 1.8075 - classification_loss: 1.2049\n",
      " 24/500 [>.............................] - ETA: 18:38 - loss: 3.0173 - regression_loss: 1.8129 - classification_loss: 1.2044\n",
      " 25/500 [>.............................] - ETA: 18:10 - loss: 3.0149 - regression_loss: 1.8106 - classification_loss: 1.2043\n",
      " 26/500 [>.............................] - ETA: 17:46 - loss: 3.0358 - regression_loss: 1.8297 - classification_loss: 1.2061\n",
      " 27/500 [>.............................] - ETA: 17:23 - loss: 3.0417 - regression_loss: 1.8356 - classification_loss: 1.2061\n",
      " 28/500 [>.............................] - ETA: 17:02 - loss: 3.0539 - regression_loss: 1.8489 - classification_loss: 1.2050\n",
      " 29/500 [>.............................] - ETA: 16:41 - loss: 3.0496 - regression_loss: 1.8479 - classification_loss: 1.2016\n",
      " 30/500 [>.............................] - ETA: 16:57 - loss: 3.0332 - regression_loss: 1.8324 - classification_loss: 1.2008\n",
      " 31/500 [>.............................] - ETA: 16:37 - loss: 3.0392 - regression_loss: 1.8383 - classification_loss: 1.2009\n",
      " 32/500 [>.............................] - ETA: 16:19 - loss: 3.0315 - regression_loss: 1.8310 - classification_loss: 1.2005\n",
      " 33/500 [>.............................] - ETA: 16:37 - loss: 3.0443 - regression_loss: 1.8435 - classification_loss: 1.2008\n",
      " 34/500 [=>............................] - ETA: 16:21 - loss: 3.0347 - regression_loss: 1.8363 - classification_loss: 1.1984\n",
      " 35/500 [=>............................] - ETA: 16:05 - loss: 3.0428 - regression_loss: 1.8465 - classification_loss: 1.1963\n",
      " 36/500 [=>............................] - ETA: 15:50 - loss: 3.0434 - regression_loss: 1.8486 - classification_loss: 1.1948\n",
      " 37/500 [=>............................] - ETA: 15:35 - loss: 3.0386 - regression_loss: 1.8445 - classification_loss: 1.1941\n",
      " 38/500 [=>............................] - ETA: 15:24 - loss: 3.0474 - regression_loss: 1.8554 - classification_loss: 1.1920\n",
      " 39/500 [=>............................] - ETA: 15:14 - loss: 3.0636 - regression_loss: 1.8722 - classification_loss: 1.1914\n",
      " 40/500 [=>............................] - ETA: 15:01 - loss: 3.0625 - regression_loss: 1.8721 - classification_loss: 1.1904\n",
      " 41/500 [=>............................] - ETA: 14:49 - loss: 3.0687 - regression_loss: 1.8797 - classification_loss: 1.1890\n",
      " 42/500 [=>............................] - ETA: 14:35 - loss: 3.0691 - regression_loss: 1.8806 - classification_loss: 1.1885\n",
      " 43/500 [=>............................] - ETA: 14:25 - loss: 3.0865 - regression_loss: 1.8986 - classification_loss: 1.1879\n",
      " 44/500 [=>............................] - ETA: 14:15 - loss: 3.0914 - regression_loss: 1.9038 - classification_loss: 1.1877\n",
      " 45/500 [=>............................] - ETA: 14:25 - loss: 3.0913 - regression_loss: 1.9049 - classification_loss: 1.1864\n",
      " 46/500 [=>............................] - ETA: 14:15 - loss: 3.0756 - regression_loss: 1.8905 - classification_loss: 1.1851\n",
      " 47/500 [=>............................] - ETA: 14:07 - loss: 3.0703 - regression_loss: 1.8847 - classification_loss: 1.1856\n",
      " 48/500 [=>............................] - ETA: 14:32 - loss: 3.0655 - regression_loss: 1.8794 - classification_loss: 1.1861\n",
      " 49/500 [=>............................] - ETA: 14:21 - loss: 3.0546 - regression_loss: 1.8707 - classification_loss: 1.1838\n",
      " 50/500 [==>...........................] - ETA: 14:12 - loss: 3.0338 - regression_loss: 1.8518 - classification_loss: 1.1821\n",
      " 51/500 [==>...........................] - ETA: 14:03 - loss: 3.0443 - regression_loss: 1.8618 - classification_loss: 1.1825\n",
      " 52/500 [==>...........................] - ETA: 13:53 - loss: 3.0469 - regression_loss: 1.8660 - classification_loss: 1.1809\n",
      " 53/500 [==>...........................] - ETA: 13:44 - loss: 3.0546 - regression_loss: 1.8739 - classification_loss: 1.1807\n",
      " 54/500 [==>...........................] - ETA: 13:36 - loss: 3.0466 - regression_loss: 1.8650 - classification_loss: 1.1816\n",
      " 55/500 [==>...........................] - ETA: 13:43 - loss: 3.0436 - regression_loss: 1.8621 - classification_loss: 1.1815\n",
      " 56/500 [==>...........................] - ETA: 13:35 - loss: 3.0437 - regression_loss: 1.8623 - classification_loss: 1.1814\n",
      " 57/500 [==>...........................] - ETA: 13:27 - loss: 3.0490 - regression_loss: 1.8677 - classification_loss: 1.1813\n",
      " 58/500 [==>...........................] - ETA: 13:20 - loss: 3.0415 - regression_loss: 1.8602 - classification_loss: 1.1813\n",
      " 59/500 [==>...........................] - ETA: 13:13 - loss: 3.0591 - regression_loss: 1.8776 - classification_loss: 1.1815\n",
      " 60/500 [==>...........................] - ETA: 13:24 - loss: 3.0545 - regression_loss: 1.8739 - classification_loss: 1.1807\n",
      " 61/500 [==>...........................] - ETA: 13:17 - loss: 3.0444 - regression_loss: 1.8638 - classification_loss: 1.1805\n",
      " 62/500 [==>...........................] - ETA: 13:10 - loss: 3.0443 - regression_loss: 1.8642 - classification_loss: 1.1801\n",
      " 63/500 [==>...........................] - ETA: 13:03 - loss: 3.0527 - regression_loss: 1.8726 - classification_loss: 1.1801\n",
      " 64/500 [==>...........................] - ETA: 12:55 - loss: 3.0481 - regression_loss: 1.8699 - classification_loss: 1.1782\n",
      " 65/500 [==>...........................] - ETA: 12:48 - loss: 3.0469 - regression_loss: 1.8694 - classification_loss: 1.1775\n",
      " 66/500 [==>...........................] - ETA: 12:42 - loss: 3.0450 - regression_loss: 1.8684 - classification_loss: 1.1766\n",
      " 67/500 [===>..........................] - ETA: 12:35 - loss: 3.0387 - regression_loss: 1.8632 - classification_loss: 1.1755\n",
      " 68/500 [===>..........................] - ETA: 12:29 - loss: 3.0320 - regression_loss: 1.8558 - classification_loss: 1.1762\n",
      " 69/500 [===>..........................] - ETA: 12:22 - loss: 3.0357 - regression_loss: 1.8595 - classification_loss: 1.1762\n",
      " 70/500 [===>..........................] - ETA: 12:16 - loss: 3.0312 - regression_loss: 1.8549 - classification_loss: 1.1763\n",
      " 71/500 [===>..........................] - ETA: 12:22 - loss: 3.0291 - regression_loss: 1.8520 - classification_loss: 1.1771\n",
      " 72/500 [===>..........................] - ETA: 12:16 - loss: 3.0270 - regression_loss: 1.8502 - classification_loss: 1.1769\n",
      " 73/500 [===>..........................] - ETA: 12:11 - loss: 3.0181 - regression_loss: 1.8415 - classification_loss: 1.1766\n",
      " 74/500 [===>..........................] - ETA: 12:26 - loss: 3.0206 - regression_loss: 1.8443 - classification_loss: 1.1763\n",
      " 75/500 [===>..........................] - ETA: 12:20 - loss: 3.0127 - regression_loss: 1.8375 - classification_loss: 1.1753\n",
      " 76/500 [===>..........................] - ETA: 12:14 - loss: 3.0091 - regression_loss: 1.8349 - classification_loss: 1.1743\n",
      " 77/500 [===>..........................] - ETA: 12:22 - loss: 3.0174 - regression_loss: 1.8427 - classification_loss: 1.1747\n",
      " 78/500 [===>..........................] - ETA: 12:18 - loss: 3.0199 - regression_loss: 1.8455 - classification_loss: 1.1745\n",
      " 79/500 [===>..........................] - ETA: 12:12 - loss: 3.0106 - regression_loss: 1.8365 - classification_loss: 1.1741\n",
      " 80/500 [===>..........................] - ETA: 12:07 - loss: 3.0002 - regression_loss: 1.8266 - classification_loss: 1.1736\n",
      " 81/500 [===>..........................] - ETA: 12:02 - loss: 2.9996 - regression_loss: 1.8272 - classification_loss: 1.1725\n",
      " 82/500 [===>..........................] - ETA: 11:57 - loss: 2.9943 - regression_loss: 1.8220 - classification_loss: 1.1723\n",
      " 83/500 [===>..........................] - ETA: 11:52 - loss: 3.0019 - regression_loss: 1.8304 - classification_loss: 1.1715\n",
      " 84/500 [====>.........................] - ETA: 11:47 - loss: 3.0020 - regression_loss: 1.8304 - classification_loss: 1.1716\n",
      " 85/500 [====>.........................] - ETA: 11:42 - loss: 3.0074 - regression_loss: 1.8368 - classification_loss: 1.1707\n",
      " 86/500 [====>.........................] - ETA: 11:37 - loss: 3.0062 - regression_loss: 1.8360 - classification_loss: 1.1702\n",
      " 87/500 [====>.........................] - ETA: 11:32 - loss: 3.0100 - regression_loss: 1.8397 - classification_loss: 1.1702\n",
      " 88/500 [====>.........................] - ETA: 11:28 - loss: 3.0071 - regression_loss: 1.8368 - classification_loss: 1.1703\n",
      " 89/500 [====>.........................] - ETA: 11:23 - loss: 3.0163 - regression_loss: 1.8471 - classification_loss: 1.1693\n",
      " 90/500 [====>.........................] - ETA: 11:19 - loss: 3.0196 - regression_loss: 1.8508 - classification_loss: 1.1688\n",
      " 91/500 [====>.........................] - ETA: 11:26 - loss: 3.0124 - regression_loss: 1.8433 - classification_loss: 1.1691\n",
      " 92/500 [====>.........................] - ETA: 11:22 - loss: 3.0126 - regression_loss: 1.8438 - classification_loss: 1.1688\n",
      " 93/500 [====>.........................] - ETA: 11:18 - loss: 3.0056 - regression_loss: 1.8372 - classification_loss: 1.1684\n",
      " 94/500 [====>.........................] - ETA: 11:13 - loss: 3.0102 - regression_loss: 1.8417 - classification_loss: 1.1685\n",
      " 95/500 [====>.........................] - ETA: 11:09 - loss: 3.0076 - regression_loss: 1.8401 - classification_loss: 1.1676\n",
      " 96/500 [====>.........................] - ETA: 11:05 - loss: 3.0032 - regression_loss: 1.8356 - classification_loss: 1.1676\n",
      " 97/500 [====>.........................] - ETA: 11:13 - loss: 2.9995 - regression_loss: 1.8323 - classification_loss: 1.1672\n",
      " 98/500 [====>.........................] - ETA: 11:08 - loss: 2.9934 - regression_loss: 1.8271 - classification_loss: 1.1663\n",
      " 99/500 [====>.........................] - ETA: 11:04 - loss: 2.9986 - regression_loss: 1.8322 - classification_loss: 1.1664\n",
      "100/500 [=====>........................] - ETA: 11:09 - loss: 2.9978 - regression_loss: 1.8314 - classification_loss: 1.1664\n",
      "101/500 [=====>........................] - ETA: 11:17 - loss: 2.9931 - regression_loss: 1.8274 - classification_loss: 1.1656\n",
      "102/500 [=====>........................] - ETA: 11:13 - loss: 2.9975 - regression_loss: 1.8323 - classification_loss: 1.1652\n",
      "103/500 [=====>........................] - ETA: 11:21 - loss: 2.9969 - regression_loss: 1.8315 - classification_loss: 1.1653\n",
      "104/500 [=====>........................] - ETA: 11:17 - loss: 2.9954 - regression_loss: 1.8300 - classification_loss: 1.1654\n",
      "105/500 [=====>........................] - ETA: 11:22 - loss: 2.9916 - regression_loss: 1.8263 - classification_loss: 1.1653\n",
      "106/500 [=====>........................] - ETA: 11:18 - loss: 2.9924 - regression_loss: 1.8280 - classification_loss: 1.1644\n",
      "107/500 [=====>........................] - ETA: 11:14 - loss: 2.9945 - regression_loss: 1.8300 - classification_loss: 1.1646\n",
      "108/500 [=====>........................] - ETA: 11:09 - loss: 2.9881 - regression_loss: 1.8248 - classification_loss: 1.1633\n",
      "109/500 [=====>........................] - ETA: 11:05 - loss: 2.9880 - regression_loss: 1.8250 - classification_loss: 1.1631\n",
      "110/500 [=====>........................] - ETA: 11:01 - loss: 2.9880 - regression_loss: 1.8256 - classification_loss: 1.1624\n",
      "111/500 [=====>........................] - ETA: 10:57 - loss: 2.9860 - regression_loss: 1.8244 - classification_loss: 1.1616\n",
      "112/500 [=====>........................] - ETA: 10:54 - loss: 2.9900 - regression_loss: 1.8279 - classification_loss: 1.1621\n",
      "113/500 [=====>........................] - ETA: 10:49 - loss: 2.9832 - regression_loss: 1.8214 - classification_loss: 1.1617\n",
      "114/500 [=====>........................] - ETA: 10:46 - loss: 2.9763 - regression_loss: 1.8150 - classification_loss: 1.1612\n",
      "115/500 [=====>........................] - ETA: 10:42 - loss: 2.9780 - regression_loss: 1.8172 - classification_loss: 1.1609\n",
      "116/500 [=====>........................] - ETA: 10:49 - loss: 2.9745 - regression_loss: 1.8133 - classification_loss: 1.1612\n",
      "117/500 [======>.......................] - ETA: 10:45 - loss: 2.9746 - regression_loss: 1.8138 - classification_loss: 1.1608\n",
      "118/500 [======>.......................] - ETA: 10:42 - loss: 2.9798 - regression_loss: 1.8195 - classification_loss: 1.1603\n",
      "119/500 [======>.......................] - ETA: 10:38 - loss: 2.9790 - regression_loss: 1.8194 - classification_loss: 1.1595\n",
      "120/500 [======>.......................] - ETA: 10:34 - loss: 2.9753 - regression_loss: 1.8163 - classification_loss: 1.1590\n",
      "121/500 [======>.......................] - ETA: 10:31 - loss: 2.9763 - regression_loss: 1.8181 - classification_loss: 1.1582\n",
      "122/500 [======>.......................] - ETA: 10:36 - loss: 2.9748 - regression_loss: 1.8164 - classification_loss: 1.1584\n",
      "123/500 [======>.......................] - ETA: 10:32 - loss: 2.9675 - regression_loss: 1.8094 - classification_loss: 1.1581\n",
      "124/500 [======>.......................] - ETA: 10:28 - loss: 2.9613 - regression_loss: 1.8041 - classification_loss: 1.1573\n",
      "125/500 [======>.......................] - ETA: 10:24 - loss: 2.9584 - regression_loss: 1.8011 - classification_loss: 1.1573\n",
      "126/500 [======>.......................] - ETA: 10:21 - loss: 2.9577 - regression_loss: 1.8002 - classification_loss: 1.1575\n",
      "127/500 [======>.......................] - ETA: 10:17 - loss: 2.9583 - regression_loss: 1.8010 - classification_loss: 1.1573\n",
      "128/500 [======>.......................] - ETA: 10:14 - loss: 2.9547 - regression_loss: 1.7980 - classification_loss: 1.1567\n",
      "129/500 [======>.......................] - ETA: 10:10 - loss: 2.9573 - regression_loss: 1.8014 - classification_loss: 1.1559\n",
      "130/500 [======>.......................] - ETA: 10:07 - loss: 2.9634 - regression_loss: 1.8076 - classification_loss: 1.1558\n",
      "131/500 [======>.......................] - ETA: 10:03 - loss: 2.9655 - regression_loss: 1.8097 - classification_loss: 1.1558\n",
      "132/500 [======>.......................] - ETA: 10:00 - loss: 2.9635 - regression_loss: 1.8084 - classification_loss: 1.1551\n",
      "133/500 [======>.......................] - ETA: 9:57 - loss: 2.9681 - regression_loss: 1.8132 - classification_loss: 1.1549 \n",
      "134/500 [=======>......................] - ETA: 9:54 - loss: 2.9727 - regression_loss: 1.8176 - classification_loss: 1.1550\n",
      "135/500 [=======>......................] - ETA: 9:50 - loss: 2.9656 - regression_loss: 1.8113 - classification_loss: 1.1543\n",
      "136/500 [=======>......................] - ETA: 9:47 - loss: 2.9674 - regression_loss: 1.8140 - classification_loss: 1.1534\n",
      "137/500 [=======>......................] - ETA: 9:44 - loss: 2.9652 - regression_loss: 1.8122 - classification_loss: 1.1531\n",
      "138/500 [=======>......................] - ETA: 9:41 - loss: 2.9617 - regression_loss: 1.8094 - classification_loss: 1.1523\n",
      "139/500 [=======>......................] - ETA: 9:38 - loss: 2.9593 - regression_loss: 1.8080 - classification_loss: 1.1513\n",
      "140/500 [=======>......................] - ETA: 9:35 - loss: 2.9595 - regression_loss: 1.8093 - classification_loss: 1.1502\n",
      "141/500 [=======>......................] - ETA: 9:31 - loss: 2.9603 - regression_loss: 1.8105 - classification_loss: 1.1498\n",
      "142/500 [=======>......................] - ETA: 9:29 - loss: 2.9612 - regression_loss: 1.8119 - classification_loss: 1.1493\n",
      "143/500 [=======>......................] - ETA: 9:33 - loss: 2.9618 - regression_loss: 1.8133 - classification_loss: 1.1485\n",
      "144/500 [=======>......................] - ETA: 9:30 - loss: 2.9618 - regression_loss: 1.8139 - classification_loss: 1.1479\n",
      "145/500 [=======>......................] - ETA: 9:28 - loss: 2.9612 - regression_loss: 1.8141 - classification_loss: 1.1470\n",
      "146/500 [=======>......................] - ETA: 9:25 - loss: 2.9610 - regression_loss: 1.8139 - classification_loss: 1.1470\n",
      "147/500 [=======>......................] - ETA: 9:21 - loss: 2.9608 - regression_loss: 1.8147 - classification_loss: 1.1462\n",
      "148/500 [=======>......................] - ETA: 9:19 - loss: 2.9581 - regression_loss: 1.8137 - classification_loss: 1.1444\n",
      "149/500 [=======>......................] - ETA: 9:16 - loss: 2.9494 - regression_loss: 1.8059 - classification_loss: 1.1435\n",
      "150/500 [========>.....................] - ETA: 9:13 - loss: 2.9521 - regression_loss: 1.8087 - classification_loss: 1.1434\n",
      "151/500 [========>.....................] - ETA: 9:10 - loss: 2.9465 - regression_loss: 1.8037 - classification_loss: 1.1428\n",
      "152/500 [========>.....................] - ETA: 9:14 - loss: 2.9459 - regression_loss: 1.8042 - classification_loss: 1.1417\n",
      "153/500 [========>.....................] - ETA: 9:11 - loss: 2.9437 - regression_loss: 1.8032 - classification_loss: 1.1404\n",
      "154/500 [========>.....................] - ETA: 9:09 - loss: 2.9434 - regression_loss: 1.8033 - classification_loss: 1.1400\n",
      "155/500 [========>.....................] - ETA: 9:06 - loss: 2.9373 - regression_loss: 1.7979 - classification_loss: 1.1394\n",
      "156/500 [========>.....................] - ETA: 9:07 - loss: 2.9348 - regression_loss: 1.7956 - classification_loss: 1.1392\n",
      "157/500 [========>.....................] - ETA: 9:04 - loss: 2.9342 - regression_loss: 1.7958 - classification_loss: 1.1384\n",
      "158/500 [========>.....................] - ETA: 9:06 - loss: 2.9319 - regression_loss: 1.7936 - classification_loss: 1.1383\n",
      "159/500 [========>.....................] - ETA: 9:03 - loss: 2.9311 - regression_loss: 1.7938 - classification_loss: 1.1373\n",
      "160/500 [========>.....................] - ETA: 8:59 - loss: 2.9308 - regression_loss: 1.7938 - classification_loss: 1.1370\n",
      "161/500 [========>.....................] - ETA: 8:57 - loss: 2.9280 - regression_loss: 1.7914 - classification_loss: 1.1366\n",
      "162/500 [========>.....................] - ETA: 8:54 - loss: 2.9230 - regression_loss: 1.7873 - classification_loss: 1.1357\n",
      "163/500 [========>.....................] - ETA: 8:52 - loss: 2.9184 - regression_loss: 1.7830 - classification_loss: 1.1354\n",
      "164/500 [========>.....................] - ETA: 8:49 - loss: 2.9189 - regression_loss: 1.7835 - classification_loss: 1.1354\n",
      "165/500 [========>.....................] - ETA: 8:46 - loss: 2.9208 - regression_loss: 1.7856 - classification_loss: 1.1353\n",
      "166/500 [========>.....................] - ETA: 8:44 - loss: 2.9217 - regression_loss: 1.7868 - classification_loss: 1.1348\n",
      "167/500 [=========>....................] - ETA: 8:41 - loss: 2.9208 - regression_loss: 1.7864 - classification_loss: 1.1345\n",
      "168/500 [=========>....................] - ETA: 8:39 - loss: 2.9229 - regression_loss: 1.7890 - classification_loss: 1.1339\n",
      "169/500 [=========>....................] - ETA: 8:36 - loss: 2.9208 - regression_loss: 1.7876 - classification_loss: 1.1332\n",
      "170/500 [=========>....................] - ETA: 8:34 - loss: 2.9180 - regression_loss: 1.7847 - classification_loss: 1.1333\n",
      "171/500 [=========>....................] - ETA: 8:31 - loss: 2.9169 - regression_loss: 1.7839 - classification_loss: 1.1330\n",
      "172/500 [=========>....................] - ETA: 8:28 - loss: 2.9151 - regression_loss: 1.7829 - classification_loss: 1.1322\n",
      "173/500 [=========>....................] - ETA: 8:26 - loss: 2.9155 - regression_loss: 1.7839 - classification_loss: 1.1316\n",
      "174/500 [=========>....................] - ETA: 8:23 - loss: 2.9130 - regression_loss: 1.7819 - classification_loss: 1.1311\n",
      "175/500 [=========>....................] - ETA: 8:20 - loss: 2.9133 - regression_loss: 1.7824 - classification_loss: 1.1309\n",
      "176/500 [=========>....................] - ETA: 8:22 - loss: 2.9128 - regression_loss: 1.7820 - classification_loss: 1.1308\n",
      "177/500 [=========>....................] - ETA: 8:19 - loss: 2.9171 - regression_loss: 1.7857 - classification_loss: 1.1314\n",
      "178/500 [=========>....................] - ETA: 8:21 - loss: 2.9176 - regression_loss: 1.7871 - classification_loss: 1.1305\n",
      "179/500 [=========>....................] - ETA: 8:18 - loss: 2.9174 - regression_loss: 1.7874 - classification_loss: 1.1301\n",
      "180/500 [=========>....................] - ETA: 8:16 - loss: 2.9135 - regression_loss: 1.7842 - classification_loss: 1.1293\n",
      "181/500 [=========>....................] - ETA: 8:13 - loss: 2.9133 - regression_loss: 1.7841 - classification_loss: 1.1292\n",
      "182/500 [=========>....................] - ETA: 8:11 - loss: 2.9148 - regression_loss: 1.7861 - classification_loss: 1.1287\n",
      "183/500 [=========>....................] - ETA: 8:08 - loss: 2.9146 - regression_loss: 1.7864 - classification_loss: 1.1282\n",
      "184/500 [==========>...................] - ETA: 8:06 - loss: 2.9142 - regression_loss: 1.7871 - classification_loss: 1.1272\n",
      "185/500 [==========>...................] - ETA: 8:03 - loss: 2.9136 - regression_loss: 1.7867 - classification_loss: 1.1269\n",
      "186/500 [==========>...................] - ETA: 8:01 - loss: 2.9134 - regression_loss: 1.7869 - classification_loss: 1.1265\n",
      "187/500 [==========>...................] - ETA: 7:59 - loss: 2.9145 - regression_loss: 1.7884 - classification_loss: 1.1261\n",
      "188/500 [==========>...................] - ETA: 7:57 - loss: 2.9147 - regression_loss: 1.7884 - classification_loss: 1.1262\n",
      "189/500 [==========>...................] - ETA: 7:55 - loss: 2.9120 - regression_loss: 1.7860 - classification_loss: 1.1260\n",
      "190/500 [==========>...................] - ETA: 7:52 - loss: 2.9133 - regression_loss: 1.7878 - classification_loss: 1.1255\n",
      "191/500 [==========>...................] - ETA: 7:55 - loss: 2.9104 - regression_loss: 1.7855 - classification_loss: 1.1249\n",
      "192/500 [==========>...................] - ETA: 7:53 - loss: 2.9058 - regression_loss: 1.7816 - classification_loss: 1.1242\n",
      "193/500 [==========>...................] - ETA: 7:50 - loss: 2.9023 - regression_loss: 1.7784 - classification_loss: 1.1239\n",
      "194/500 [==========>...................] - ETA: 7:52 - loss: 2.9030 - regression_loss: 1.7793 - classification_loss: 1.1237\n",
      "195/500 [==========>...................] - ETA: 7:50 - loss: 2.9013 - regression_loss: 1.7785 - classification_loss: 1.1228\n",
      "196/500 [==========>...................] - ETA: 7:48 - loss: 2.9008 - regression_loss: 1.7788 - classification_loss: 1.1219\n",
      "197/500 [==========>...................] - ETA: 7:50 - loss: 2.9004 - regression_loss: 1.7798 - classification_loss: 1.1207\n",
      "198/500 [==========>...................] - ETA: 7:47 - loss: 2.8981 - regression_loss: 1.7781 - classification_loss: 1.1200\n",
      "199/500 [==========>...................] - ETA: 7:45 - loss: 2.8968 - regression_loss: 1.7776 - classification_loss: 1.1192\n",
      "200/500 [===========>..................] - ETA: 7:43 - loss: 2.8946 - regression_loss: 1.7765 - classification_loss: 1.1181\n",
      "201/500 [===========>..................] - ETA: 7:40 - loss: 2.8933 - regression_loss: 1.7754 - classification_loss: 1.1178\n",
      "202/500 [===========>..................] - ETA: 7:38 - loss: 2.8929 - regression_loss: 1.7754 - classification_loss: 1.1175\n",
      "203/500 [===========>..................] - ETA: 7:36 - loss: 2.8899 - regression_loss: 1.7735 - classification_loss: 1.1164\n",
      "204/500 [===========>..................] - ETA: 7:33 - loss: 2.8877 - regression_loss: 1.7723 - classification_loss: 1.1154\n",
      "205/500 [===========>..................] - ETA: 7:31 - loss: 2.8870 - regression_loss: 1.7726 - classification_loss: 1.1145\n",
      "206/500 [===========>..................] - ETA: 7:29 - loss: 2.8834 - regression_loss: 1.7697 - classification_loss: 1.1137\n",
      "207/500 [===========>..................] - ETA: 7:27 - loss: 2.8795 - regression_loss: 1.7671 - classification_loss: 1.1124\n",
      "208/500 [===========>..................] - ETA: 7:25 - loss: 2.8805 - regression_loss: 1.7688 - classification_loss: 1.1117\n",
      "209/500 [===========>..................] - ETA: 7:23 - loss: 2.8810 - regression_loss: 1.7709 - classification_loss: 1.1101\n",
      "210/500 [===========>..................] - ETA: 7:20 - loss: 2.8792 - regression_loss: 1.7692 - classification_loss: 1.1100\n",
      "211/500 [===========>..................] - ETA: 7:18 - loss: 2.8796 - regression_loss: 1.7700 - classification_loss: 1.1096\n",
      "212/500 [===========>..................] - ETA: 7:16 - loss: 2.8793 - regression_loss: 1.7702 - classification_loss: 1.1091\n",
      "213/500 [===========>..................] - ETA: 7:14 - loss: 2.8796 - regression_loss: 1.7698 - classification_loss: 1.1098\n",
      "214/500 [===========>..................] - ETA: 7:12 - loss: 2.8784 - regression_loss: 1.7689 - classification_loss: 1.1095\n",
      "215/500 [===========>..................] - ETA: 7:10 - loss: 2.8756 - regression_loss: 1.7668 - classification_loss: 1.1088\n",
      "216/500 [===========>..................] - ETA: 7:08 - loss: 2.8752 - regression_loss: 1.7669 - classification_loss: 1.1083\n",
      "217/500 [============>.................] - ETA: 7:06 - loss: 2.8752 - regression_loss: 1.7673 - classification_loss: 1.1079\n",
      "218/500 [============>.................] - ETA: 7:04 - loss: 2.8739 - regression_loss: 1.7670 - classification_loss: 1.1069\n",
      "219/500 [============>.................] - ETA: 7:02 - loss: 2.8727 - regression_loss: 1.7661 - classification_loss: 1.1066\n",
      "220/500 [============>.................] - ETA: 6:59 - loss: 2.8694 - regression_loss: 1.7640 - classification_loss: 1.1053\n",
      "221/500 [============>.................] - ETA: 6:57 - loss: 2.8665 - regression_loss: 1.7616 - classification_loss: 1.1048\n",
      "222/500 [============>.................] - ETA: 6:55 - loss: 2.8633 - regression_loss: 1.7589 - classification_loss: 1.1043\n",
      "223/500 [============>.................] - ETA: 6:54 - loss: 2.8619 - regression_loss: 1.7582 - classification_loss: 1.1037\n",
      "224/500 [============>.................] - ETA: 6:51 - loss: 2.8571 - regression_loss: 1.7549 - classification_loss: 1.1022\n",
      "225/500 [============>.................] - ETA: 6:50 - loss: 2.8554 - regression_loss: 1.7541 - classification_loss: 1.1013\n",
      "226/500 [============>.................] - ETA: 6:50 - loss: 2.8576 - regression_loss: 1.7567 - classification_loss: 1.1010\n",
      "227/500 [============>.................] - ETA: 6:53 - loss: 2.8574 - regression_loss: 1.7578 - classification_loss: 1.0996\n",
      "228/500 [============>.................] - ETA: 6:51 - loss: 2.8537 - regression_loss: 1.7550 - classification_loss: 1.0987\n",
      "229/500 [============>.................] - ETA: 6:48 - loss: 2.8549 - regression_loss: 1.7568 - classification_loss: 1.0981\n",
      "230/500 [============>.................] - ETA: 6:46 - loss: 2.8528 - regression_loss: 1.7557 - classification_loss: 1.0971\n",
      "231/500 [============>.................] - ETA: 6:48 - loss: 2.8503 - regression_loss: 1.7538 - classification_loss: 1.0966\n",
      "232/500 [============>.................] - ETA: 6:46 - loss: 2.8473 - regression_loss: 1.7514 - classification_loss: 1.0959\n",
      "233/500 [============>.................] - ETA: 6:43 - loss: 2.8451 - regression_loss: 1.7494 - classification_loss: 1.0957\n",
      "234/500 [=============>................] - ETA: 6:41 - loss: 2.8418 - regression_loss: 1.7474 - classification_loss: 1.0944\n",
      "235/500 [=============>................] - ETA: 6:39 - loss: 2.8419 - regression_loss: 1.7483 - classification_loss: 1.0936\n",
      "236/500 [=============>................] - ETA: 6:40 - loss: 2.8382 - regression_loss: 1.7455 - classification_loss: 1.0928\n",
      "237/500 [=============>................] - ETA: 6:38 - loss: 2.8358 - regression_loss: 1.7437 - classification_loss: 1.0921\n",
      "238/500 [=============>................] - ETA: 6:36 - loss: 2.8340 - regression_loss: 1.7431 - classification_loss: 1.0909\n",
      "239/500 [=============>................] - ETA: 6:34 - loss: 2.8342 - regression_loss: 1.7440 - classification_loss: 1.0902\n",
      "240/500 [=============>................] - ETA: 6:32 - loss: 2.8329 - regression_loss: 1.7431 - classification_loss: 1.0898\n",
      "241/500 [=============>................] - ETA: 6:32 - loss: 2.8299 - regression_loss: 1.7415 - classification_loss: 1.0885\n",
      "242/500 [=============>................] - ETA: 6:30 - loss: 2.8295 - regression_loss: 1.7420 - classification_loss: 1.0875\n",
      "243/500 [=============>................] - ETA: 6:28 - loss: 2.8294 - regression_loss: 1.7433 - classification_loss: 1.0861\n",
      "244/500 [=============>................] - ETA: 6:26 - loss: 2.8304 - regression_loss: 1.7446 - classification_loss: 1.0858\n",
      "245/500 [=============>................] - ETA: 6:24 - loss: 2.8303 - regression_loss: 1.7446 - classification_loss: 1.0857\n",
      "246/500 [=============>................] - ETA: 6:26 - loss: 2.8321 - regression_loss: 1.7463 - classification_loss: 1.0858\n",
      "247/500 [=============>................] - ETA: 6:24 - loss: 2.8302 - regression_loss: 1.7451 - classification_loss: 1.0851\n",
      "248/500 [=============>................] - ETA: 6:22 - loss: 2.8325 - regression_loss: 1.7473 - classification_loss: 1.0851\n",
      "249/500 [=============>................] - ETA: 6:20 - loss: 2.8321 - regression_loss: 1.7473 - classification_loss: 1.0848\n",
      "250/500 [==============>...............] - ETA: 6:18 - loss: 2.8303 - regression_loss: 1.7462 - classification_loss: 1.0841\n",
      "251/500 [==============>...............] - ETA: 6:16 - loss: 2.8284 - regression_loss: 1.7451 - classification_loss: 1.0833\n",
      "252/500 [==============>...............] - ETA: 6:14 - loss: 2.8284 - regression_loss: 1.7453 - classification_loss: 1.0831\n",
      "253/500 [==============>...............] - ETA: 6:12 - loss: 2.8279 - regression_loss: 1.7453 - classification_loss: 1.0825\n",
      "254/500 [==============>...............] - ETA: 6:10 - loss: 2.8263 - regression_loss: 1.7449 - classification_loss: 1.0814\n",
      "255/500 [==============>...............] - ETA: 6:08 - loss: 2.8221 - regression_loss: 1.7416 - classification_loss: 1.0804\n",
      "256/500 [==============>...............] - ETA: 6:06 - loss: 2.8237 - regression_loss: 1.7438 - classification_loss: 1.0799\n",
      "257/500 [==============>...............] - ETA: 6:04 - loss: 2.8220 - regression_loss: 1.7428 - classification_loss: 1.0792\n",
      "258/500 [==============>...............] - ETA: 6:02 - loss: 2.8222 - regression_loss: 1.7434 - classification_loss: 1.0788\n",
      "259/500 [==============>...............] - ETA: 6:00 - loss: 2.8221 - regression_loss: 1.7440 - classification_loss: 1.0781\n",
      "260/500 [==============>...............] - ETA: 5:58 - loss: 2.8238 - regression_loss: 1.7460 - classification_loss: 1.0778\n",
      "261/500 [==============>...............] - ETA: 5:56 - loss: 2.8262 - regression_loss: 1.7483 - classification_loss: 1.0779\n",
      "262/500 [==============>...............] - ETA: 5:54 - loss: 2.8269 - regression_loss: 1.7498 - classification_loss: 1.0771\n",
      "263/500 [==============>...............] - ETA: 5:52 - loss: 2.8272 - regression_loss: 1.7509 - classification_loss: 1.0763\n",
      "264/500 [==============>...............] - ETA: 5:53 - loss: 2.8256 - regression_loss: 1.7501 - classification_loss: 1.0755\n",
      "265/500 [==============>...............] - ETA: 5:51 - loss: 2.8233 - regression_loss: 1.7490 - classification_loss: 1.0743\n",
      "266/500 [==============>...............] - ETA: 5:49 - loss: 2.8228 - regression_loss: 1.7496 - classification_loss: 1.0733\n",
      "267/500 [===============>..............] - ETA: 5:47 - loss: 2.8231 - regression_loss: 1.7501 - classification_loss: 1.0731\n",
      "268/500 [===============>..............] - ETA: 5:46 - loss: 2.8218 - regression_loss: 1.7499 - classification_loss: 1.0719\n",
      "269/500 [===============>..............] - ETA: 5:44 - loss: 2.8215 - regression_loss: 1.7496 - classification_loss: 1.0719\n",
      "270/500 [===============>..............] - ETA: 5:42 - loss: 2.8208 - regression_loss: 1.7493 - classification_loss: 1.0716\n",
      "271/500 [===============>..............] - ETA: 5:40 - loss: 2.8221 - regression_loss: 1.7514 - classification_loss: 1.0707\n",
      "272/500 [===============>..............] - ETA: 5:40 - loss: 2.8201 - regression_loss: 1.7506 - classification_loss: 1.0695\n",
      "273/500 [===============>..............] - ETA: 5:38 - loss: 2.8219 - regression_loss: 1.7528 - classification_loss: 1.0691\n",
      "274/500 [===============>..............] - ETA: 5:36 - loss: 2.8185 - regression_loss: 1.7503 - classification_loss: 1.0682\n",
      "275/500 [===============>..............] - ETA: 5:34 - loss: 2.8169 - regression_loss: 1.7495 - classification_loss: 1.0674\n",
      "276/500 [===============>..............] - ETA: 5:32 - loss: 2.8172 - regression_loss: 1.7503 - classification_loss: 1.0669\n",
      "277/500 [===============>..............] - ETA: 5:30 - loss: 2.8156 - regression_loss: 1.7493 - classification_loss: 1.0663\n",
      "278/500 [===============>..............] - ETA: 5:29 - loss: 2.8158 - regression_loss: 1.7503 - classification_loss: 1.0655\n",
      "279/500 [===============>..............] - ETA: 5:27 - loss: 2.8162 - regression_loss: 1.7515 - classification_loss: 1.0647\n",
      "280/500 [===============>..............] - ETA: 5:25 - loss: 2.8164 - regression_loss: 1.7524 - classification_loss: 1.0639\n",
      "281/500 [===============>..............] - ETA: 5:23 - loss: 2.8162 - regression_loss: 1.7534 - classification_loss: 1.0628\n",
      "282/500 [===============>..............] - ETA: 5:21 - loss: 2.8140 - regression_loss: 1.7519 - classification_loss: 1.0621\n",
      "283/500 [===============>..............] - ETA: 5:19 - loss: 2.8116 - regression_loss: 1.7505 - classification_loss: 1.0611\n",
      "284/500 [================>.............] - ETA: 5:17 - loss: 2.8094 - regression_loss: 1.7488 - classification_loss: 1.0606\n",
      "285/500 [================>.............] - ETA: 5:16 - loss: 2.8096 - regression_loss: 1.7500 - classification_loss: 1.0597\n",
      "286/500 [================>.............] - ETA: 5:14 - loss: 2.8093 - regression_loss: 1.7503 - classification_loss: 1.0590\n",
      "287/500 [================>.............] - ETA: 5:12 - loss: 2.8066 - regression_loss: 1.7481 - classification_loss: 1.0586\n",
      "288/500 [================>.............] - ETA: 5:10 - loss: 2.8051 - regression_loss: 1.7473 - classification_loss: 1.0578\n",
      "289/500 [================>.............] - ETA: 5:09 - loss: 2.8039 - regression_loss: 1.7466 - classification_loss: 1.0573\n",
      "290/500 [================>.............] - ETA: 5:07 - loss: 2.8008 - regression_loss: 1.7447 - classification_loss: 1.0561\n",
      "291/500 [================>.............] - ETA: 5:05 - loss: 2.7999 - regression_loss: 1.7445 - classification_loss: 1.0554\n",
      "292/500 [================>.............] - ETA: 5:04 - loss: 2.7982 - regression_loss: 1.7438 - classification_loss: 1.0544\n",
      "293/500 [================>.............] - ETA: 5:02 - loss: 2.7978 - regression_loss: 1.7437 - classification_loss: 1.0541\n",
      "294/500 [================>.............] - ETA: 5:00 - loss: 2.7971 - regression_loss: 1.7443 - classification_loss: 1.0528\n",
      "295/500 [================>.............] - ETA: 4:58 - loss: 2.8002 - regression_loss: 1.7475 - classification_loss: 1.0527\n",
      "296/500 [================>.............] - ETA: 4:56 - loss: 2.7992 - regression_loss: 1.7467 - classification_loss: 1.0525\n",
      "297/500 [================>.............] - ETA: 4:56 - loss: 2.7976 - regression_loss: 1.7465 - classification_loss: 1.0511\n",
      "298/500 [================>.............] - ETA: 4:54 - loss: 2.7955 - regression_loss: 1.7448 - classification_loss: 1.0508\n",
      "299/500 [================>.............] - ETA: 4:52 - loss: 2.7936 - regression_loss: 1.7433 - classification_loss: 1.0503\n",
      "300/500 [=================>............] - ETA: 4:51 - loss: 2.7933 - regression_loss: 1.7432 - classification_loss: 1.0501\n",
      "301/500 [=================>............] - ETA: 4:49 - loss: 2.7941 - regression_loss: 1.7452 - classification_loss: 1.0489\n",
      "302/500 [=================>............] - ETA: 4:47 - loss: 2.7935 - regression_loss: 1.7450 - classification_loss: 1.0485\n",
      "303/500 [=================>............] - ETA: 4:47 - loss: 2.7952 - regression_loss: 1.7468 - classification_loss: 1.0485\n",
      "304/500 [=================>............] - ETA: 4:48 - loss: 2.7956 - regression_loss: 1.7476 - classification_loss: 1.0480\n",
      "305/500 [=================>............] - ETA: 4:46 - loss: 2.7945 - regression_loss: 1.7471 - classification_loss: 1.0474\n",
      "306/500 [=================>............] - ETA: 4:44 - loss: 2.7929 - regression_loss: 1.7463 - classification_loss: 1.0465\n",
      "307/500 [=================>............] - ETA: 4:43 - loss: 2.7904 - regression_loss: 1.7449 - classification_loss: 1.0455\n",
      "308/500 [=================>............] - ETA: 4:41 - loss: 2.7879 - regression_loss: 1.7432 - classification_loss: 1.0448\n",
      "309/500 [=================>............] - ETA: 4:39 - loss: 2.7857 - regression_loss: 1.7414 - classification_loss: 1.0443\n",
      "310/500 [=================>............] - ETA: 4:37 - loss: 2.7856 - regression_loss: 1.7419 - classification_loss: 1.0437\n",
      "311/500 [=================>............] - ETA: 4:36 - loss: 2.7846 - regression_loss: 1.7415 - classification_loss: 1.0431\n",
      "312/500 [=================>............] - ETA: 4:34 - loss: 2.7856 - regression_loss: 1.7433 - classification_loss: 1.0423\n",
      "313/500 [=================>............] - ETA: 4:32 - loss: 2.7846 - regression_loss: 1.7432 - classification_loss: 1.0414\n",
      "314/500 [=================>............] - ETA: 4:30 - loss: 2.7836 - regression_loss: 1.7432 - classification_loss: 1.0404\n",
      "315/500 [=================>............] - ETA: 4:30 - loss: 2.7825 - regression_loss: 1.7425 - classification_loss: 1.0400\n",
      "316/500 [=================>............] - ETA: 4:30 - loss: 2.7803 - regression_loss: 1.7408 - classification_loss: 1.0395\n",
      "317/500 [==================>...........] - ETA: 4:28 - loss: 2.7810 - regression_loss: 1.7418 - classification_loss: 1.0393\n",
      "318/500 [==================>...........] - ETA: 4:27 - loss: 2.7806 - regression_loss: 1.7420 - classification_loss: 1.0386\n",
      "319/500 [==================>...........] - ETA: 4:25 - loss: 2.7795 - regression_loss: 1.7410 - classification_loss: 1.0386\n",
      "320/500 [==================>...........] - ETA: 4:23 - loss: 2.7771 - regression_loss: 1.7395 - classification_loss: 1.0376\n",
      "321/500 [==================>...........] - ETA: 4:23 - loss: 2.7770 - regression_loss: 1.7398 - classification_loss: 1.0372\n",
      "322/500 [==================>...........] - ETA: 4:21 - loss: 2.7759 - regression_loss: 1.7391 - classification_loss: 1.0368\n",
      "323/500 [==================>...........] - ETA: 4:20 - loss: 2.7775 - regression_loss: 1.7412 - classification_loss: 1.0363\n",
      "324/500 [==================>...........] - ETA: 4:18 - loss: 2.7776 - regression_loss: 1.7417 - classification_loss: 1.0359\n",
      "325/500 [==================>...........] - ETA: 4:18 - loss: 2.7773 - regression_loss: 1.7417 - classification_loss: 1.0356\n",
      "326/500 [==================>...........] - ETA: 4:16 - loss: 2.7771 - regression_loss: 1.7421 - classification_loss: 1.0350\n",
      "327/500 [==================>...........] - ETA: 4:14 - loss: 2.7740 - regression_loss: 1.7398 - classification_loss: 1.0342\n",
      "328/500 [==================>...........] - ETA: 4:12 - loss: 2.7740 - regression_loss: 1.7404 - classification_loss: 1.0337\n",
      "329/500 [==================>...........] - ETA: 4:11 - loss: 2.7737 - regression_loss: 1.7399 - classification_loss: 1.0338\n",
      "330/500 [==================>...........] - ETA: 4:09 - loss: 2.7713 - regression_loss: 1.7385 - classification_loss: 1.0329\n",
      "331/500 [==================>...........] - ETA: 4:07 - loss: 2.7718 - regression_loss: 1.7393 - classification_loss: 1.0325\n",
      "332/500 [==================>...........] - ETA: 4:06 - loss: 2.7722 - regression_loss: 1.7402 - classification_loss: 1.0320\n",
      "333/500 [==================>...........] - ETA: 4:04 - loss: 2.7720 - regression_loss: 1.7407 - classification_loss: 1.0313\n",
      "334/500 [===================>..........] - ETA: 4:02 - loss: 2.7712 - regression_loss: 1.7410 - classification_loss: 1.0303\n",
      "335/500 [===================>..........] - ETA: 4:01 - loss: 2.7728 - regression_loss: 1.7424 - classification_loss: 1.0303\n",
      "336/500 [===================>..........] - ETA: 3:59 - loss: 2.7707 - regression_loss: 1.7415 - classification_loss: 1.0292\n",
      "337/500 [===================>..........] - ETA: 3:57 - loss: 2.7690 - regression_loss: 1.7402 - classification_loss: 1.0288\n",
      "338/500 [===================>..........] - ETA: 3:57 - loss: 2.7678 - regression_loss: 1.7397 - classification_loss: 1.0281\n",
      "339/500 [===================>..........] - ETA: 3:55 - loss: 2.7690 - regression_loss: 1.7412 - classification_loss: 1.0278\n",
      "340/500 [===================>..........] - ETA: 3:53 - loss: 2.7692 - regression_loss: 1.7413 - classification_loss: 1.0280\n",
      "341/500 [===================>..........] - ETA: 3:52 - loss: 2.7687 - regression_loss: 1.7411 - classification_loss: 1.0276\n",
      "342/500 [===================>..........] - ETA: 3:51 - loss: 2.7697 - regression_loss: 1.7429 - classification_loss: 1.0269\n",
      "343/500 [===================>..........] - ETA: 3:49 - loss: 2.7688 - regression_loss: 1.7424 - classification_loss: 1.0264\n",
      "344/500 [===================>..........] - ETA: 3:48 - loss: 2.7667 - regression_loss: 1.7409 - classification_loss: 1.0258\n",
      "345/500 [===================>..........] - ETA: 3:46 - loss: 2.7672 - regression_loss: 1.7421 - classification_loss: 1.0251\n",
      "346/500 [===================>..........] - ETA: 3:46 - loss: 2.7679 - regression_loss: 1.7429 - classification_loss: 1.0250\n",
      "347/500 [===================>..........] - ETA: 3:44 - loss: 2.7661 - regression_loss: 1.7421 - classification_loss: 1.0241\n",
      "348/500 [===================>..........] - ETA: 3:42 - loss: 2.7663 - regression_loss: 1.7435 - classification_loss: 1.0228\n",
      "349/500 [===================>..........] - ETA: 3:41 - loss: 2.7673 - regression_loss: 1.7449 - classification_loss: 1.0224\n",
      "350/500 [====================>.........] - ETA: 3:40 - loss: 2.7678 - regression_loss: 1.7461 - classification_loss: 1.0217\n",
      "351/500 [====================>.........] - ETA: 3:38 - loss: 2.7658 - regression_loss: 1.7449 - classification_loss: 1.0209\n",
      "352/500 [====================>.........] - ETA: 3:36 - loss: 2.7654 - regression_loss: 1.7449 - classification_loss: 1.0206\n",
      "353/500 [====================>.........] - ETA: 3:35 - loss: 2.7637 - regression_loss: 1.7439 - classification_loss: 1.0198\n",
      "354/500 [====================>.........] - ETA: 3:34 - loss: 2.7625 - regression_loss: 1.7436 - classification_loss: 1.0189\n",
      "355/500 [====================>.........] - ETA: 3:34 - loss: 2.7630 - regression_loss: 1.7444 - classification_loss: 1.0186\n",
      "356/500 [====================>.........] - ETA: 3:32 - loss: 2.7641 - regression_loss: 1.7460 - classification_loss: 1.0181\n",
      "357/500 [====================>.........] - ETA: 3:30 - loss: 2.7632 - regression_loss: 1.7459 - classification_loss: 1.0173\n",
      "358/500 [====================>.........] - ETA: 3:29 - loss: 2.7614 - regression_loss: 1.7450 - classification_loss: 1.0164\n",
      "359/500 [====================>.........] - ETA: 3:27 - loss: 2.7587 - regression_loss: 1.7433 - classification_loss: 1.0155\n",
      "360/500 [====================>.........] - ETA: 3:26 - loss: 2.7555 - regression_loss: 1.7411 - classification_loss: 1.0144\n",
      "361/500 [====================>.........] - ETA: 3:25 - loss: 2.7550 - regression_loss: 1.7414 - classification_loss: 1.0136\n",
      "362/500 [====================>.........] - ETA: 3:23 - loss: 2.7541 - regression_loss: 1.7411 - classification_loss: 1.0130\n",
      "363/500 [====================>.........] - ETA: 3:21 - loss: 2.7550 - regression_loss: 1.7424 - classification_loss: 1.0126\n",
      "364/500 [====================>.........] - ETA: 3:20 - loss: 2.7541 - regression_loss: 1.7422 - classification_loss: 1.0119\n",
      "365/500 [====================>.........] - ETA: 3:18 - loss: 2.7553 - regression_loss: 1.7438 - classification_loss: 1.0115\n",
      "366/500 [====================>.........] - ETA: 3:16 - loss: 2.7551 - regression_loss: 1.7436 - classification_loss: 1.0115\n",
      "367/500 [=====================>........] - ETA: 3:15 - loss: 2.7541 - regression_loss: 1.7427 - classification_loss: 1.0114\n",
      "368/500 [=====================>........] - ETA: 3:13 - loss: 2.7519 - regression_loss: 1.7413 - classification_loss: 1.0107\n",
      "369/500 [=====================>........] - ETA: 3:12 - loss: 2.7519 - regression_loss: 1.7417 - classification_loss: 1.0102\n",
      "370/500 [=====================>........] - ETA: 3:10 - loss: 2.7506 - regression_loss: 1.7412 - classification_loss: 1.0094\n",
      "371/500 [=====================>........] - ETA: 3:08 - loss: 2.7503 - regression_loss: 1.7415 - classification_loss: 1.0088\n",
      "372/500 [=====================>........] - ETA: 3:07 - loss: 2.7501 - regression_loss: 1.7418 - classification_loss: 1.0084\n",
      "373/500 [=====================>........] - ETA: 3:05 - loss: 2.7504 - regression_loss: 1.7428 - classification_loss: 1.0076\n",
      "374/500 [=====================>........] - ETA: 3:03 - loss: 2.7510 - regression_loss: 1.7434 - classification_loss: 1.0076\n",
      "375/500 [=====================>........] - ETA: 3:02 - loss: 2.7506 - regression_loss: 1.7430 - classification_loss: 1.0076\n",
      "376/500 [=====================>........] - ETA: 3:00 - loss: 2.7497 - regression_loss: 1.7431 - classification_loss: 1.0066\n",
      "377/500 [=====================>........] - ETA: 2:59 - loss: 2.7495 - regression_loss: 1.7431 - classification_loss: 1.0064\n",
      "378/500 [=====================>........] - ETA: 2:57 - loss: 2.7502 - regression_loss: 1.7443 - classification_loss: 1.0059\n",
      "379/500 [=====================>........] - ETA: 2:55 - loss: 2.7520 - regression_loss: 1.7460 - classification_loss: 1.0060\n",
      "380/500 [=====================>........] - ETA: 2:54 - loss: 2.7511 - regression_loss: 1.7460 - classification_loss: 1.0051\n",
      "381/500 [=====================>........] - ETA: 2:52 - loss: 2.7491 - regression_loss: 1.7445 - classification_loss: 1.0046\n",
      "382/500 [=====================>........] - ETA: 2:51 - loss: 2.7478 - regression_loss: 1.7438 - classification_loss: 1.0040\n",
      "383/500 [=====================>........] - ETA: 2:50 - loss: 2.7464 - regression_loss: 1.7431 - classification_loss: 1.0033\n",
      "384/500 [======================>.......] - ETA: 2:48 - loss: 2.7466 - regression_loss: 1.7439 - classification_loss: 1.0027\n",
      "385/500 [======================>.......] - ETA: 2:47 - loss: 2.7447 - regression_loss: 1.7430 - classification_loss: 1.0018\n",
      "386/500 [======================>.......] - ETA: 2:45 - loss: 2.7447 - regression_loss: 1.7435 - classification_loss: 1.0012\n",
      "387/500 [======================>.......] - ETA: 2:44 - loss: 2.7436 - regression_loss: 1.7425 - classification_loss: 1.0011\n",
      "388/500 [======================>.......] - ETA: 2:42 - loss: 2.7423 - regression_loss: 1.7419 - classification_loss: 1.0004\n",
      "389/500 [======================>.......] - ETA: 2:41 - loss: 2.7426 - regression_loss: 1.7423 - classification_loss: 1.0003\n",
      "390/500 [======================>.......] - ETA: 2:39 - loss: 2.7427 - regression_loss: 1.7428 - classification_loss: 0.9998\n",
      "391/500 [======================>.......] - ETA: 2:38 - loss: 2.7409 - regression_loss: 1.7418 - classification_loss: 0.9991\n",
      "392/500 [======================>.......] - ETA: 2:37 - loss: 2.7392 - regression_loss: 1.7407 - classification_loss: 0.9985\n",
      "393/500 [======================>.......] - ETA: 2:35 - loss: 2.7379 - regression_loss: 1.7400 - classification_loss: 0.9979\n",
      "394/500 [======================>.......] - ETA: 2:34 - loss: 2.7368 - regression_loss: 1.7394 - classification_loss: 0.9974\n",
      "395/500 [======================>.......] - ETA: 2:32 - loss: 2.7363 - regression_loss: 1.7396 - classification_loss: 0.9966\n",
      "396/500 [======================>.......] - ETA: 2:30 - loss: 2.7364 - regression_loss: 1.7400 - classification_loss: 0.9964\n",
      "397/500 [======================>.......] - ETA: 2:29 - loss: 2.7338 - regression_loss: 1.7385 - classification_loss: 0.9953\n",
      "398/500 [======================>.......] - ETA: 2:27 - loss: 2.7324 - regression_loss: 1.7380 - classification_loss: 0.9943\n",
      "399/500 [======================>.......] - ETA: 2:26 - loss: 2.7313 - regression_loss: 1.7376 - classification_loss: 0.9937\n",
      "400/500 [=======================>......] - ETA: 2:24 - loss: 2.7283 - regression_loss: 1.7353 - classification_loss: 0.9930\n",
      "401/500 [=======================>......] - ETA: 2:23 - loss: 2.7296 - regression_loss: 1.7368 - classification_loss: 0.9929\n",
      "402/500 [=======================>......] - ETA: 2:22 - loss: 2.7309 - regression_loss: 1.7380 - classification_loss: 0.9929\n",
      "403/500 [=======================>......] - ETA: 2:20 - loss: 2.7297 - regression_loss: 1.7376 - classification_loss: 0.9920\n",
      "404/500 [=======================>......] - ETA: 2:19 - loss: 2.7287 - regression_loss: 1.7374 - classification_loss: 0.9913\n",
      "405/500 [=======================>......] - ETA: 2:17 - loss: 2.7289 - regression_loss: 1.7386 - classification_loss: 0.9903\n",
      "406/500 [=======================>......] - ETA: 2:16 - loss: 2.7276 - regression_loss: 1.7379 - classification_loss: 0.9898\n",
      "407/500 [=======================>......] - ETA: 2:15 - loss: 2.7251 - regression_loss: 1.7358 - classification_loss: 0.9892\n",
      "408/500 [=======================>......] - ETA: 2:13 - loss: 2.7233 - regression_loss: 1.7350 - classification_loss: 0.9883\n",
      "409/500 [=======================>......] - ETA: 2:12 - loss: 2.7242 - regression_loss: 1.7360 - classification_loss: 0.9883\n",
      "410/500 [=======================>......] - ETA: 2:10 - loss: 2.7240 - regression_loss: 1.7365 - classification_loss: 0.9875\n",
      "411/500 [=======================>......] - ETA: 2:09 - loss: 2.7242 - regression_loss: 1.7370 - classification_loss: 0.9872\n",
      "412/500 [=======================>......] - ETA: 2:07 - loss: 2.7239 - regression_loss: 1.7375 - classification_loss: 0.9864\n",
      "413/500 [=======================>......] - ETA: 2:06 - loss: 2.7223 - regression_loss: 1.7369 - classification_loss: 0.9854\n",
      "414/500 [=======================>......] - ETA: 2:04 - loss: 2.7220 - regression_loss: 1.7372 - classification_loss: 0.9848\n",
      "415/500 [=======================>......] - ETA: 2:02 - loss: 2.7201 - regression_loss: 1.7358 - classification_loss: 0.9843\n",
      "416/500 [=======================>......] - ETA: 2:01 - loss: 2.7195 - regression_loss: 1.7356 - classification_loss: 0.9839\n",
      "417/500 [========================>.....] - ETA: 1:59 - loss: 2.7188 - regression_loss: 1.7352 - classification_loss: 0.9835\n",
      "418/500 [========================>.....] - ETA: 1:58 - loss: 2.7191 - regression_loss: 1.7360 - classification_loss: 0.9831\n",
      "419/500 [========================>.....] - ETA: 1:56 - loss: 2.7175 - regression_loss: 1.7353 - classification_loss: 0.9822\n",
      "420/500 [========================>.....] - ETA: 1:55 - loss: 2.7153 - regression_loss: 1.7340 - classification_loss: 0.9812\n",
      "421/500 [========================>.....] - ETA: 1:53 - loss: 2.7123 - regression_loss: 1.7322 - classification_loss: 0.9801\n",
      "422/500 [========================>.....] - ETA: 1:52 - loss: 2.7111 - regression_loss: 1.7312 - classification_loss: 0.9798\n",
      "423/500 [========================>.....] - ETA: 1:51 - loss: 2.7106 - regression_loss: 1.7313 - classification_loss: 0.9793\n",
      "424/500 [========================>.....] - ETA: 1:50 - loss: 2.7087 - regression_loss: 1.7300 - classification_loss: 0.9787\n",
      "425/500 [========================>.....] - ETA: 1:48 - loss: 2.7099 - regression_loss: 1.7314 - classification_loss: 0.9786\n",
      "426/500 [========================>.....] - ETA: 1:47 - loss: 2.7083 - regression_loss: 1.7300 - classification_loss: 0.9783\n",
      "427/500 [========================>.....] - ETA: 1:45 - loss: 2.7089 - regression_loss: 1.7311 - classification_loss: 0.9778\n",
      "428/500 [========================>.....] - ETA: 1:44 - loss: 2.7092 - regression_loss: 1.7311 - classification_loss: 0.9781\n",
      "429/500 [========================>.....] - ETA: 1:42 - loss: 2.7084 - regression_loss: 1.7307 - classification_loss: 0.9777\n",
      "430/500 [========================>.....] - ETA: 1:41 - loss: 2.7080 - regression_loss: 1.7312 - classification_loss: 0.9768\n",
      "431/500 [========================>.....] - ETA: 1:39 - loss: 2.7075 - regression_loss: 1.7314 - classification_loss: 0.9762\n",
      "432/500 [========================>.....] - ETA: 1:38 - loss: 2.7066 - regression_loss: 1.7313 - classification_loss: 0.9754\n",
      "433/500 [========================>.....] - ETA: 1:36 - loss: 2.7070 - regression_loss: 1.7318 - classification_loss: 0.9752\n",
      "434/500 [=========================>....] - ETA: 1:35 - loss: 2.7053 - regression_loss: 1.7308 - classification_loss: 0.9745\n",
      "435/500 [=========================>....] - ETA: 1:33 - loss: 2.7041 - regression_loss: 1.7304 - classification_loss: 0.9738\n",
      "436/500 [=========================>....] - ETA: 1:32 - loss: 2.7015 - regression_loss: 1.7287 - classification_loss: 0.9729\n",
      "437/500 [=========================>....] - ETA: 1:30 - loss: 2.7028 - regression_loss: 1.7302 - classification_loss: 0.9725\n",
      "438/500 [=========================>....] - ETA: 1:29 - loss: 2.7017 - regression_loss: 1.7297 - classification_loss: 0.9720\n",
      "439/500 [=========================>....] - ETA: 1:27 - loss: 2.6993 - regression_loss: 1.7282 - classification_loss: 0.9711\n",
      "440/500 [=========================>....] - ETA: 1:26 - loss: 2.6975 - regression_loss: 1.7269 - classification_loss: 0.9706\n",
      "441/500 [=========================>....] - ETA: 1:24 - loss: 2.6961 - regression_loss: 1.7262 - classification_loss: 0.9699\n",
      "442/500 [=========================>....] - ETA: 1:23 - loss: 2.6963 - regression_loss: 1.7268 - classification_loss: 0.9695\n",
      "443/500 [=========================>....] - ETA: 1:21 - loss: 2.6954 - regression_loss: 1.7265 - classification_loss: 0.9688\n",
      "444/500 [=========================>....] - ETA: 1:20 - loss: 2.6963 - regression_loss: 1.7275 - classification_loss: 0.9688\n",
      "445/500 [=========================>....] - ETA: 1:18 - loss: 2.6960 - regression_loss: 1.7278 - classification_loss: 0.9682\n",
      "446/500 [=========================>....] - ETA: 1:17 - loss: 2.6953 - regression_loss: 1.7273 - classification_loss: 0.9680\n",
      "447/500 [=========================>....] - ETA: 1:15 - loss: 2.6950 - regression_loss: 1.7277 - classification_loss: 0.9674\n",
      "448/500 [=========================>....] - ETA: 1:14 - loss: 2.6941 - regression_loss: 1.7273 - classification_loss: 0.9668\n",
      "449/500 [=========================>....] - ETA: 1:13 - loss: 2.6929 - regression_loss: 1.7268 - classification_loss: 0.9661\n",
      "450/500 [==========================>...] - ETA: 1:11 - loss: 2.6915 - regression_loss: 1.7262 - classification_loss: 0.9652\n",
      "451/500 [==========================>...] - ETA: 1:10 - loss: 2.6919 - regression_loss: 1.7269 - classification_loss: 0.9650\n",
      "452/500 [==========================>...] - ETA: 1:08 - loss: 2.6919 - regression_loss: 1.7272 - classification_loss: 0.9646\n",
      "453/500 [==========================>...] - ETA: 1:07 - loss: 2.6929 - regression_loss: 1.7282 - classification_loss: 0.9647\n",
      "454/500 [==========================>...] - ETA: 1:06 - loss: 2.6928 - regression_loss: 1.7286 - classification_loss: 0.9642\n",
      "455/500 [==========================>...] - ETA: 1:04 - loss: 2.6930 - regression_loss: 1.7292 - classification_loss: 0.9638\n",
      "456/500 [==========================>...] - ETA: 1:03 - loss: 2.6919 - regression_loss: 1.7286 - classification_loss: 0.9633\n",
      "457/500 [==========================>...] - ETA: 1:01 - loss: 2.6915 - regression_loss: 1.7284 - classification_loss: 0.9631\n",
      "458/500 [==========================>...] - ETA: 1:00 - loss: 2.6892 - regression_loss: 1.7272 - classification_loss: 0.9620\n",
      "459/500 [==========================>...] - ETA: 58s - loss: 2.6894 - regression_loss: 1.7281 - classification_loss: 0.9613 \n",
      "460/500 [==========================>...] - ETA: 57s - loss: 2.6890 - regression_loss: 1.7285 - classification_loss: 0.9605\n",
      "461/500 [==========================>...] - ETA: 55s - loss: 2.6875 - regression_loss: 1.7276 - classification_loss: 0.9599\n",
      "462/500 [==========================>...] - ETA: 54s - loss: 2.6887 - regression_loss: 1.7287 - classification_loss: 0.9599\n",
      "463/500 [==========================>...] - ETA: 53s - loss: 2.6881 - regression_loss: 1.7286 - classification_loss: 0.9594\n",
      "464/500 [==========================>...] - ETA: 51s - loss: 2.6868 - regression_loss: 1.7280 - classification_loss: 0.9588\n",
      "465/500 [==========================>...] - ETA: 50s - loss: 2.6850 - regression_loss: 1.7270 - classification_loss: 0.9580\n",
      "466/500 [==========================>...] - ETA: 48s - loss: 2.6867 - regression_loss: 1.7290 - classification_loss: 0.9577\n",
      "467/500 [===========================>..] - ETA: 47s - loss: 2.6867 - regression_loss: 1.7294 - classification_loss: 0.9573\n",
      "468/500 [===========================>..] - ETA: 45s - loss: 2.6854 - regression_loss: 1.7290 - classification_loss: 0.9564\n",
      "469/500 [===========================>..] - ETA: 44s - loss: 2.6839 - regression_loss: 1.7282 - classification_loss: 0.9557\n",
      "470/500 [===========================>..] - ETA: 42s - loss: 2.6823 - regression_loss: 1.7269 - classification_loss: 0.9554\n",
      "471/500 [===========================>..] - ETA: 41s - loss: 2.6823 - regression_loss: 1.7271 - classification_loss: 0.9552\n",
      "472/500 [===========================>..] - ETA: 40s - loss: 2.6810 - regression_loss: 1.7264 - classification_loss: 0.9546\n",
      "473/500 [===========================>..] - ETA: 38s - loss: 2.6802 - regression_loss: 1.7263 - classification_loss: 0.9539\n",
      "474/500 [===========================>..] - ETA: 37s - loss: 2.6814 - regression_loss: 1.7276 - classification_loss: 0.9538\n",
      "475/500 [===========================>..] - ETA: 35s - loss: 2.6805 - regression_loss: 1.7274 - classification_loss: 0.9531\n",
      "476/500 [===========================>..] - ETA: 34s - loss: 2.6791 - regression_loss: 1.7267 - classification_loss: 0.9524\n",
      "477/500 [===========================>..] - ETA: 32s - loss: 2.6772 - regression_loss: 1.7257 - classification_loss: 0.9515\n",
      "478/500 [===========================>..] - ETA: 31s - loss: 2.6749 - regression_loss: 1.7241 - classification_loss: 0.9508\n",
      "479/500 [===========================>..] - ETA: 29s - loss: 2.6738 - regression_loss: 1.7233 - classification_loss: 0.9505\n",
      "480/500 [===========================>..] - ETA: 28s - loss: 2.6736 - regression_loss: 1.7239 - classification_loss: 0.9497\n",
      "481/500 [===========================>..] - ETA: 27s - loss: 2.6739 - regression_loss: 1.7245 - classification_loss: 0.9494\n",
      "482/500 [===========================>..] - ETA: 25s - loss: 2.6723 - regression_loss: 1.7237 - classification_loss: 0.9486\n",
      "483/500 [===========================>..] - ETA: 24s - loss: 2.6710 - regression_loss: 1.7229 - classification_loss: 0.9482\n",
      "484/500 [============================>.] - ETA: 22s - loss: 2.6720 - regression_loss: 1.7235 - classification_loss: 0.9485\n",
      "485/500 [============================>.] - ETA: 21s - loss: 2.6707 - regression_loss: 1.7226 - classification_loss: 0.9481\n",
      "486/500 [============================>.] - ETA: 19s - loss: 2.6710 - regression_loss: 1.7234 - classification_loss: 0.9477\n",
      "487/500 [============================>.] - ETA: 18s - loss: 2.6701 - regression_loss: 1.7229 - classification_loss: 0.9471\n",
      "488/500 [============================>.] - ETA: 17s - loss: 2.6691 - regression_loss: 1.7227 - classification_loss: 0.9464\n",
      "489/500 [============================>.] - ETA: 15s - loss: 2.6682 - regression_loss: 1.7222 - classification_loss: 0.9460\n",
      "490/500 [============================>.] - ETA: 14s - loss: 2.6688 - regression_loss: 1.7228 - classification_loss: 0.9460\n",
      "491/500 [============================>.] - ETA: 12s - loss: 2.6678 - regression_loss: 1.7224 - classification_loss: 0.9454\n",
      "492/500 [============================>.] - ETA: 11s - loss: 2.6682 - regression_loss: 1.7227 - classification_loss: 0.9455\n",
      "493/500 [============================>.] - ETA: 9s - loss: 2.6678 - regression_loss: 1.7226 - classification_loss: 0.9452 \n",
      "494/500 [============================>.] - ETA: 8s - loss: 2.6661 - regression_loss: 1.7219 - classification_loss: 0.9442\n",
      "495/500 [============================>.] - ETA: 7s - loss: 2.6663 - regression_loss: 1.7222 - classification_loss: 0.9441\n",
      "496/500 [============================>.] - ETA: 5s - loss: 2.6649 - regression_loss: 1.7215 - classification_loss: 0.9434\n",
      "497/500 [============================>.] - ETA: 4s - loss: 2.6640 - regression_loss: 1.7209 - classification_loss: 0.9431\n",
      "498/500 [============================>.] - ETA: 2s - loss: 2.6609 - regression_loss: 1.7188 - classification_loss: 0.9421\n",
      "499/500 [============================>.] - ETA: 1s - loss: 2.6598 - regression_loss: 1.7186 - classification_loss: 0.9413\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.6591 - regression_loss: 1.7184 - classification_loss: 0.9407\n",
      "Epoch 00001: saving model to ./snapshots\\resnet50_csv_01.h5\n",
      "\n",
      "500/500 [==============================] - 706s 1s/step - loss: 2.6591 - regression_loss: 1.7184 - classification_loss: 0.9407\n",
      "Epoch 2/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 2.7762 - regression_loss: 1.8899 - classification_loss: 0.8862\n",
      "  2/500 [..............................] - ETA: 4:25 - loss: 2.6138 - regression_loss: 1.7786 - classification_loss: 0.8351\n",
      "  3/500 [..............................] - ETA: 5:40 - loss: 2.5868 - regression_loss: 1.7554 - classification_loss: 0.8314\n",
      "  4/500 [..............................] - ETA: 6:32 - loss: 2.7112 - regression_loss: 1.8453 - classification_loss: 0.8659\n",
      "  5/500 [..............................] - ETA: 6:49 - loss: 2.4470 - regression_loss: 1.6466 - classification_loss: 0.8004\n",
      "  6/500 [..............................] - ETA: 7:09 - loss: 2.4046 - regression_loss: 1.6289 - classification_loss: 0.7758\n",
      "  7/500 [..............................] - ETA: 7:24 - loss: 2.3138 - regression_loss: 1.5874 - classification_loss: 0.7264\n",
      "  8/500 [..............................] - ETA: 7:33 - loss: 2.3534 - regression_loss: 1.6336 - classification_loss: 0.7198\n",
      "  9/500 [..............................] - ETA: 7:41 - loss: 2.4422 - regression_loss: 1.7027 - classification_loss: 0.7395\n",
      " 10/500 [..............................] - ETA: 7:42 - loss: 2.4966 - regression_loss: 1.7483 - classification_loss: 0.7483\n",
      " 11/500 [..............................] - ETA: 7:43 - loss: 2.4628 - regression_loss: 1.7216 - classification_loss: 0.7412\n",
      " 12/500 [..............................] - ETA: 7:44 - loss: 2.3717 - regression_loss: 1.6482 - classification_loss: 0.7235\n",
      " 13/500 [..............................] - ETA: 8:46 - loss: 2.4220 - regression_loss: 1.6822 - classification_loss: 0.7398\n",
      " 14/500 [..............................] - ETA: 10:02 - loss: 2.4253 - regression_loss: 1.6934 - classification_loss: 0.7320\n",
      " 15/500 [..............................] - ETA: 9:53 - loss: 2.3580 - regression_loss: 1.6341 - classification_loss: 0.7240 \n",
      " 16/500 [..............................] - ETA: 9:41 - loss: 2.3303 - regression_loss: 1.6121 - classification_loss: 0.7182\n",
      " 17/500 [>.............................] - ETA: 9:38 - loss: 2.3697 - regression_loss: 1.6491 - classification_loss: 0.7207\n",
      " 18/500 [>.............................] - ETA: 9:25 - loss: 2.3178 - regression_loss: 1.6044 - classification_loss: 0.7134\n",
      " 19/500 [>.............................] - ETA: 9:25 - loss: 2.3283 - regression_loss: 1.6138 - classification_loss: 0.7145\n",
      " 20/500 [>.............................] - ETA: 9:19 - loss: 2.3011 - regression_loss: 1.5921 - classification_loss: 0.7089\n",
      " 21/500 [>.............................] - ETA: 10:05 - loss: 2.3091 - regression_loss: 1.6025 - classification_loss: 0.7067\n",
      " 22/500 [>.............................] - ETA: 10:00 - loss: 2.3401 - regression_loss: 1.6322 - classification_loss: 0.7079\n",
      " 23/500 [>.............................] - ETA: 9:53 - loss: 2.3203 - regression_loss: 1.6189 - classification_loss: 0.7013 \n",
      " 24/500 [>.............................] - ETA: 9:48 - loss: 2.3330 - regression_loss: 1.6353 - classification_loss: 0.6976\n",
      " 25/500 [>.............................] - ETA: 9:38 - loss: 2.3294 - regression_loss: 1.6315 - classification_loss: 0.6979\n",
      " 26/500 [>.............................] - ETA: 9:37 - loss: 2.3224 - regression_loss: 1.6237 - classification_loss: 0.6986\n",
      " 27/500 [>.............................] - ETA: 10:38 - loss: 2.3364 - regression_loss: 1.6281 - classification_loss: 0.7083\n",
      " 28/500 [>.............................] - ETA: 10:31 - loss: 2.3238 - regression_loss: 1.6216 - classification_loss: 0.7022\n",
      " 29/500 [>.............................] - ETA: 10:28 - loss: 2.3260 - regression_loss: 1.6261 - classification_loss: 0.6999\n",
      " 30/500 [>.............................] - ETA: 10:21 - loss: 2.3408 - regression_loss: 1.6361 - classification_loss: 0.7047\n",
      " 31/500 [>.............................] - ETA: 10:14 - loss: 2.3261 - regression_loss: 1.6268 - classification_loss: 0.6993\n",
      " 32/500 [>.............................] - ETA: 10:08 - loss: 2.3355 - regression_loss: 1.6359 - classification_loss: 0.6996\n",
      " 33/500 [>.............................] - ETA: 10:00 - loss: 2.3214 - regression_loss: 1.6195 - classification_loss: 0.7019\n",
      " 34/500 [=>............................] - ETA: 9:56 - loss: 2.3317 - regression_loss: 1.6309 - classification_loss: 0.7007 \n",
      " 35/500 [=>............................] - ETA: 9:53 - loss: 2.3346 - regression_loss: 1.6366 - classification_loss: 0.6980\n",
      " 36/500 [=>............................] - ETA: 9:49 - loss: 2.3366 - regression_loss: 1.6411 - classification_loss: 0.6955\n",
      " 37/500 [=>............................] - ETA: 9:44 - loss: 2.3346 - regression_loss: 1.6415 - classification_loss: 0.6931\n",
      " 38/500 [=>............................] - ETA: 9:41 - loss: 2.3147 - regression_loss: 1.6228 - classification_loss: 0.6919\n",
      " 39/500 [=>............................] - ETA: 9:34 - loss: 2.3122 - regression_loss: 1.6213 - classification_loss: 0.6909\n",
      " 40/500 [=>............................] - ETA: 9:34 - loss: 2.3282 - regression_loss: 1.6315 - classification_loss: 0.6967\n",
      " 41/500 [=>............................] - ETA: 9:30 - loss: 2.3400 - regression_loss: 1.6415 - classification_loss: 0.6985\n",
      " 42/500 [=>............................] - ETA: 9:26 - loss: 2.3398 - regression_loss: 1.6418 - classification_loss: 0.6980\n",
      " 43/500 [=>............................] - ETA: 9:22 - loss: 2.3585 - regression_loss: 1.6562 - classification_loss: 0.7023\n",
      " 44/500 [=>............................] - ETA: 9:18 - loss: 2.3621 - regression_loss: 1.6542 - classification_loss: 0.7078\n",
      " 45/500 [=>............................] - ETA: 9:15 - loss: 2.3513 - regression_loss: 1.6445 - classification_loss: 0.7069\n",
      " 46/500 [=>............................] - ETA: 9:09 - loss: 2.3341 - regression_loss: 1.6312 - classification_loss: 0.7029\n",
      " 47/500 [=>............................] - ETA: 9:08 - loss: 2.3284 - regression_loss: 1.6284 - classification_loss: 0.7000\n",
      " 48/500 [=>............................] - ETA: 9:06 - loss: 2.3305 - regression_loss: 1.6294 - classification_loss: 0.7012\n",
      " 49/500 [=>............................] - ETA: 9:03 - loss: 2.3371 - regression_loss: 1.6339 - classification_loss: 0.7032\n",
      " 50/500 [==>...........................] - ETA: 9:05 - loss: 2.3285 - regression_loss: 1.6284 - classification_loss: 0.7001\n",
      " 51/500 [==>...........................] - ETA: 9:02 - loss: 2.3305 - regression_loss: 1.6278 - classification_loss: 0.7027\n",
      " 52/500 [==>...........................] - ETA: 9:00 - loss: 2.3260 - regression_loss: 1.6262 - classification_loss: 0.6998\n",
      " 53/500 [==>...........................] - ETA: 8:58 - loss: 2.3304 - regression_loss: 1.6334 - classification_loss: 0.6971\n",
      " 54/500 [==>...........................] - ETA: 8:56 - loss: 2.3217 - regression_loss: 1.6283 - classification_loss: 0.6933\n",
      " 55/500 [==>...........................] - ETA: 9:03 - loss: 2.3243 - regression_loss: 1.6296 - classification_loss: 0.6947\n",
      " 56/500 [==>...........................] - ETA: 9:01 - loss: 2.3350 - regression_loss: 1.6387 - classification_loss: 0.6963\n",
      " 57/500 [==>...........................] - ETA: 8:58 - loss: 2.3525 - regression_loss: 1.6549 - classification_loss: 0.6976\n",
      " 58/500 [==>...........................] - ETA: 8:55 - loss: 2.3594 - regression_loss: 1.6590 - classification_loss: 0.7004\n",
      " 59/500 [==>...........................] - ETA: 8:54 - loss: 2.3692 - regression_loss: 1.6683 - classification_loss: 0.7009\n",
      " 60/500 [==>...........................] - ETA: 8:52 - loss: 2.3706 - regression_loss: 1.6716 - classification_loss: 0.6989\n",
      " 61/500 [==>...........................] - ETA: 8:50 - loss: 2.3489 - regression_loss: 1.6557 - classification_loss: 0.6932\n",
      " 62/500 [==>...........................] - ETA: 8:48 - loss: 2.3460 - regression_loss: 1.6552 - classification_loss: 0.6909\n",
      " 63/500 [==>...........................] - ETA: 8:45 - loss: 2.3619 - regression_loss: 1.6666 - classification_loss: 0.6953\n",
      " 64/500 [==>...........................] - ETA: 8:42 - loss: 2.3633 - regression_loss: 1.6679 - classification_loss: 0.6954\n",
      " 65/500 [==>...........................] - ETA: 8:40 - loss: 2.3814 - regression_loss: 1.6801 - classification_loss: 0.7013\n",
      " 66/500 [==>...........................] - ETA: 8:38 - loss: 2.3794 - regression_loss: 1.6785 - classification_loss: 0.7009\n",
      " 67/500 [===>..........................] - ETA: 8:35 - loss: 2.3798 - regression_loss: 1.6798 - classification_loss: 0.7000\n",
      " 68/500 [===>..........................] - ETA: 8:33 - loss: 2.3717 - regression_loss: 1.6743 - classification_loss: 0.6975\n",
      " 69/500 [===>..........................] - ETA: 8:30 - loss: 2.3681 - regression_loss: 1.6731 - classification_loss: 0.6949\n",
      " 70/500 [===>..........................] - ETA: 8:28 - loss: 2.3653 - regression_loss: 1.6698 - classification_loss: 0.6954\n",
      " 71/500 [===>..........................] - ETA: 8:25 - loss: 2.3663 - regression_loss: 1.6722 - classification_loss: 0.6941\n",
      " 72/500 [===>..........................] - ETA: 8:22 - loss: 2.3734 - regression_loss: 1.6782 - classification_loss: 0.6952\n",
      " 73/500 [===>..........................] - ETA: 8:21 - loss: 2.3793 - regression_loss: 1.6839 - classification_loss: 0.6954\n",
      " 74/500 [===>..........................] - ETA: 8:19 - loss: 2.3835 - regression_loss: 1.6870 - classification_loss: 0.6964\n",
      " 75/500 [===>..........................] - ETA: 8:17 - loss: 2.3817 - regression_loss: 1.6863 - classification_loss: 0.6955\n",
      " 76/500 [===>..........................] - ETA: 8:15 - loss: 2.3764 - regression_loss: 1.6841 - classification_loss: 0.6923\n",
      " 77/500 [===>..........................] - ETA: 8:14 - loss: 2.3883 - regression_loss: 1.6928 - classification_loss: 0.6955\n",
      " 78/500 [===>..........................] - ETA: 8:13 - loss: 2.3843 - regression_loss: 1.6876 - classification_loss: 0.6967\n",
      " 79/500 [===>..........................] - ETA: 8:10 - loss: 2.3805 - regression_loss: 1.6834 - classification_loss: 0.6971\n",
      " 80/500 [===>..........................] - ETA: 8:10 - loss: 2.3780 - regression_loss: 1.6798 - classification_loss: 0.6983\n",
      " 81/500 [===>..........................] - ETA: 8:08 - loss: 2.3831 - regression_loss: 1.6864 - classification_loss: 0.6968\n",
      " 82/500 [===>..........................] - ETA: 8:06 - loss: 2.3837 - regression_loss: 1.6855 - classification_loss: 0.6981\n",
      " 83/500 [===>..........................] - ETA: 8:04 - loss: 2.3888 - regression_loss: 1.6899 - classification_loss: 0.6988\n",
      " 84/500 [====>.........................] - ETA: 8:03 - loss: 2.3901 - regression_loss: 1.6914 - classification_loss: 0.6987\n",
      " 85/500 [====>.........................] - ETA: 8:01 - loss: 2.3954 - regression_loss: 1.6949 - classification_loss: 0.7005\n",
      " 86/500 [====>.........................] - ETA: 7:59 - loss: 2.4008 - regression_loss: 1.7005 - classification_loss: 0.7003\n",
      " 87/500 [====>.........................] - ETA: 7:58 - loss: 2.3935 - regression_loss: 1.6955 - classification_loss: 0.6980\n",
      " 88/500 [====>.........................] - ETA: 7:57 - loss: 2.3925 - regression_loss: 1.6936 - classification_loss: 0.6988\n",
      " 89/500 [====>.........................] - ETA: 8:07 - loss: 2.4007 - regression_loss: 1.7015 - classification_loss: 0.6993\n",
      " 90/500 [====>.........................] - ETA: 8:06 - loss: 2.4084 - regression_loss: 1.7090 - classification_loss: 0.6993\n",
      " 91/500 [====>.........................] - ETA: 8:04 - loss: 2.4064 - regression_loss: 1.7082 - classification_loss: 0.6982\n",
      " 92/500 [====>.........................] - ETA: 8:02 - loss: 2.4053 - regression_loss: 1.7070 - classification_loss: 0.6983\n",
      " 93/500 [====>.........................] - ETA: 8:01 - loss: 2.4073 - regression_loss: 1.7102 - classification_loss: 0.6971\n",
      " 94/500 [====>.........................] - ETA: 7:59 - loss: 2.4126 - regression_loss: 1.7143 - classification_loss: 0.6983\n",
      " 95/500 [====>.........................] - ETA: 7:58 - loss: 2.4187 - regression_loss: 1.7186 - classification_loss: 0.7001\n",
      " 96/500 [====>.........................] - ETA: 7:57 - loss: 2.4265 - regression_loss: 1.7239 - classification_loss: 0.7026\n",
      " 97/500 [====>.........................] - ETA: 7:55 - loss: 2.4313 - regression_loss: 1.7264 - classification_loss: 0.7049\n",
      " 98/500 [====>.........................] - ETA: 7:52 - loss: 2.4256 - regression_loss: 1.7211 - classification_loss: 0.7045\n",
      " 99/500 [====>.........................] - ETA: 8:00 - loss: 2.4240 - regression_loss: 1.7210 - classification_loss: 0.7030\n",
      "100/500 [=====>........................] - ETA: 7:58 - loss: 2.4160 - regression_loss: 1.7157 - classification_loss: 0.7003\n",
      "101/500 [=====>........................] - ETA: 7:56 - loss: 2.4055 - regression_loss: 1.7080 - classification_loss: 0.6975\n",
      "102/500 [=====>........................] - ETA: 7:55 - loss: 2.4084 - regression_loss: 1.7099 - classification_loss: 0.6986\n",
      "103/500 [=====>........................] - ETA: 7:53 - loss: 2.4059 - regression_loss: 1.7092 - classification_loss: 0.6967\n",
      "104/500 [=====>........................] - ETA: 7:51 - loss: 2.3974 - regression_loss: 1.7032 - classification_loss: 0.6942\n",
      "105/500 [=====>........................] - ETA: 7:49 - loss: 2.3992 - regression_loss: 1.7048 - classification_loss: 0.6944\n",
      "106/500 [=====>........................] - ETA: 7:47 - loss: 2.3974 - regression_loss: 1.7030 - classification_loss: 0.6944\n",
      "107/500 [=====>........................] - ETA: 7:45 - loss: 2.3928 - regression_loss: 1.7001 - classification_loss: 0.6927\n",
      "108/500 [=====>........................] - ETA: 7:44 - loss: 2.3919 - regression_loss: 1.6978 - classification_loss: 0.6940\n",
      "109/500 [=====>........................] - ETA: 7:42 - loss: 2.3968 - regression_loss: 1.7018 - classification_loss: 0.6950\n",
      "110/500 [=====>........................] - ETA: 7:40 - loss: 2.4010 - regression_loss: 1.7055 - classification_loss: 0.6955\n",
      "111/500 [=====>........................] - ETA: 7:38 - loss: 2.4035 - regression_loss: 1.7077 - classification_loss: 0.6958\n",
      "112/500 [=====>........................] - ETA: 7:37 - loss: 2.4061 - regression_loss: 1.7085 - classification_loss: 0.6976\n",
      "113/500 [=====>........................] - ETA: 7:35 - loss: 2.4085 - regression_loss: 1.7115 - classification_loss: 0.6969\n",
      "114/500 [=====>........................] - ETA: 7:34 - loss: 2.4045 - regression_loss: 1.7093 - classification_loss: 0.6951\n",
      "115/500 [=====>........................] - ETA: 7:32 - loss: 2.4041 - regression_loss: 1.7087 - classification_loss: 0.6954\n",
      "116/500 [=====>........................] - ETA: 7:31 - loss: 2.4003 - regression_loss: 1.7058 - classification_loss: 0.6945\n",
      "117/500 [======>.......................] - ETA: 7:29 - loss: 2.4017 - regression_loss: 1.7065 - classification_loss: 0.6953\n",
      "118/500 [======>.......................] - ETA: 7:27 - loss: 2.4045 - regression_loss: 1.7091 - classification_loss: 0.6954\n",
      "119/500 [======>.......................] - ETA: 7:26 - loss: 2.4038 - regression_loss: 1.7088 - classification_loss: 0.6949\n",
      "120/500 [======>.......................] - ETA: 7:24 - loss: 2.4080 - regression_loss: 1.7121 - classification_loss: 0.6960\n",
      "121/500 [======>.......................] - ETA: 7:22 - loss: 2.4089 - regression_loss: 1.7130 - classification_loss: 0.6959\n",
      "122/500 [======>.......................] - ETA: 7:21 - loss: 2.4116 - regression_loss: 1.7155 - classification_loss: 0.6961\n",
      "123/500 [======>.......................] - ETA: 7:19 - loss: 2.4095 - regression_loss: 1.7125 - classification_loss: 0.6970\n",
      "124/500 [======>.......................] - ETA: 7:25 - loss: 2.4066 - regression_loss: 1.7094 - classification_loss: 0.6972\n",
      "125/500 [======>.......................] - ETA: 7:24 - loss: 2.4111 - regression_loss: 1.7141 - classification_loss: 0.6971\n",
      "126/500 [======>.......................] - ETA: 7:22 - loss: 2.4130 - regression_loss: 1.7157 - classification_loss: 0.6972\n",
      "127/500 [======>.......................] - ETA: 7:20 - loss: 2.4157 - regression_loss: 1.7172 - classification_loss: 0.6985\n",
      "128/500 [======>.......................] - ETA: 7:18 - loss: 2.4110 - regression_loss: 1.7140 - classification_loss: 0.6970\n",
      "129/500 [======>.......................] - ETA: 7:17 - loss: 2.4101 - regression_loss: 1.7132 - classification_loss: 0.6969\n",
      "130/500 [======>.......................] - ETA: 7:15 - loss: 2.4120 - regression_loss: 1.7139 - classification_loss: 0.6981\n",
      "131/500 [======>.......................] - ETA: 7:14 - loss: 2.4086 - regression_loss: 1.7117 - classification_loss: 0.6969\n",
      "132/500 [======>.......................] - ETA: 7:12 - loss: 2.4091 - regression_loss: 1.7126 - classification_loss: 0.6965\n",
      "133/500 [======>.......................] - ETA: 7:11 - loss: 2.4094 - regression_loss: 1.7138 - classification_loss: 0.6956\n",
      "134/500 [=======>......................] - ETA: 7:09 - loss: 2.4064 - regression_loss: 1.7126 - classification_loss: 0.6938\n",
      "135/500 [=======>......................] - ETA: 7:07 - loss: 2.3988 - regression_loss: 1.7057 - classification_loss: 0.6931\n",
      "136/500 [=======>......................] - ETA: 7:06 - loss: 2.3961 - regression_loss: 1.7040 - classification_loss: 0.6921\n",
      "137/500 [=======>......................] - ETA: 7:05 - loss: 2.3955 - regression_loss: 1.7031 - classification_loss: 0.6924\n",
      "138/500 [=======>......................] - ETA: 7:04 - loss: 2.3913 - regression_loss: 1.7005 - classification_loss: 0.6908\n",
      "139/500 [=======>......................] - ETA: 7:03 - loss: 2.3966 - regression_loss: 1.7052 - classification_loss: 0.6915\n",
      "140/500 [=======>......................] - ETA: 7:02 - loss: 2.3960 - regression_loss: 1.7039 - classification_loss: 0.6921\n",
      "141/500 [=======>......................] - ETA: 7:00 - loss: 2.3961 - regression_loss: 1.7034 - classification_loss: 0.6927\n",
      "142/500 [=======>......................] - ETA: 6:58 - loss: 2.3970 - regression_loss: 1.7044 - classification_loss: 0.6926\n",
      "143/500 [=======>......................] - ETA: 6:57 - loss: 2.3963 - regression_loss: 1.7037 - classification_loss: 0.6926\n",
      "144/500 [=======>......................] - ETA: 6:55 - loss: 2.3958 - regression_loss: 1.7036 - classification_loss: 0.6922\n",
      "145/500 [=======>......................] - ETA: 6:54 - loss: 2.3972 - regression_loss: 1.7051 - classification_loss: 0.6921\n",
      "146/500 [=======>......................] - ETA: 6:52 - loss: 2.3914 - regression_loss: 1.7009 - classification_loss: 0.6905\n",
      "147/500 [=======>......................] - ETA: 6:51 - loss: 2.3909 - regression_loss: 1.7013 - classification_loss: 0.6896\n",
      "148/500 [=======>......................] - ETA: 6:50 - loss: 2.3905 - regression_loss: 1.7017 - classification_loss: 0.6888\n",
      "149/500 [=======>......................] - ETA: 6:48 - loss: 2.3879 - regression_loss: 1.6996 - classification_loss: 0.6883\n",
      "150/500 [========>.....................] - ETA: 6:46 - loss: 2.3918 - regression_loss: 1.7009 - classification_loss: 0.6908\n",
      "151/500 [========>.....................] - ETA: 6:45 - loss: 2.3918 - regression_loss: 1.7013 - classification_loss: 0.6905\n",
      "152/500 [========>.....................] - ETA: 6:44 - loss: 2.3945 - regression_loss: 1.7031 - classification_loss: 0.6914\n",
      "153/500 [========>.....................] - ETA: 6:42 - loss: 2.3917 - regression_loss: 1.7009 - classification_loss: 0.6908\n",
      "154/500 [========>.....................] - ETA: 6:41 - loss: 2.3889 - regression_loss: 1.6985 - classification_loss: 0.6904\n",
      "155/500 [========>.....................] - ETA: 6:39 - loss: 2.3865 - regression_loss: 1.6961 - classification_loss: 0.6903\n",
      "156/500 [========>.....................] - ETA: 6:38 - loss: 2.3840 - regression_loss: 1.6939 - classification_loss: 0.6901\n",
      "157/500 [========>.....................] - ETA: 6:36 - loss: 2.3876 - regression_loss: 1.6970 - classification_loss: 0.6906\n",
      "158/500 [========>.....................] - ETA: 6:35 - loss: 2.3859 - regression_loss: 1.6951 - classification_loss: 0.6908\n",
      "159/500 [========>.....................] - ETA: 6:33 - loss: 2.3857 - regression_loss: 1.6951 - classification_loss: 0.6906\n",
      "160/500 [========>.....................] - ETA: 6:32 - loss: 2.3896 - regression_loss: 1.6979 - classification_loss: 0.6918\n",
      "161/500 [========>.....................] - ETA: 6:31 - loss: 2.3934 - regression_loss: 1.7003 - classification_loss: 0.6931\n",
      "162/500 [========>.....................] - ETA: 6:30 - loss: 2.3949 - regression_loss: 1.7016 - classification_loss: 0.6933\n",
      "163/500 [========>.....................] - ETA: 6:33 - loss: 2.3927 - regression_loss: 1.7004 - classification_loss: 0.6923\n",
      "164/500 [========>.....................] - ETA: 6:32 - loss: 2.3899 - regression_loss: 1.6988 - classification_loss: 0.6911\n",
      "165/500 [========>.....................] - ETA: 6:31 - loss: 2.3858 - regression_loss: 1.6959 - classification_loss: 0.6899\n",
      "166/500 [========>.....................] - ETA: 6:29 - loss: 2.3826 - regression_loss: 1.6936 - classification_loss: 0.6891\n",
      "167/500 [=========>....................] - ETA: 6:29 - loss: 2.3841 - regression_loss: 1.6945 - classification_loss: 0.6895\n",
      "168/500 [=========>....................] - ETA: 6:28 - loss: 2.3912 - regression_loss: 1.6999 - classification_loss: 0.6913\n",
      "169/500 [=========>....................] - ETA: 6:26 - loss: 2.3911 - regression_loss: 1.7005 - classification_loss: 0.6906\n",
      "170/500 [=========>....................] - ETA: 6:25 - loss: 2.3875 - regression_loss: 1.6978 - classification_loss: 0.6897\n",
      "171/500 [=========>....................] - ETA: 6:23 - loss: 2.3901 - regression_loss: 1.6996 - classification_loss: 0.6905\n",
      "172/500 [=========>....................] - ETA: 6:22 - loss: 2.3895 - regression_loss: 1.6977 - classification_loss: 0.6918\n",
      "173/500 [=========>....................] - ETA: 6:20 - loss: 2.3880 - regression_loss: 1.6970 - classification_loss: 0.6910\n",
      "174/500 [=========>....................] - ETA: 6:19 - loss: 2.3869 - regression_loss: 1.6969 - classification_loss: 0.6899\n",
      "175/500 [=========>....................] - ETA: 6:17 - loss: 2.3886 - regression_loss: 1.6990 - classification_loss: 0.6896\n",
      "176/500 [=========>....................] - ETA: 6:16 - loss: 2.3924 - regression_loss: 1.7013 - classification_loss: 0.6910\n",
      "177/500 [=========>....................] - ETA: 6:14 - loss: 2.3899 - regression_loss: 1.6993 - classification_loss: 0.6905\n",
      "178/500 [=========>....................] - ETA: 6:16 - loss: 2.3936 - regression_loss: 1.7027 - classification_loss: 0.6909\n",
      "179/500 [=========>....................] - ETA: 6:15 - loss: 2.3876 - regression_loss: 1.6980 - classification_loss: 0.6896\n",
      "180/500 [=========>....................] - ETA: 6:13 - loss: 2.3898 - regression_loss: 1.7003 - classification_loss: 0.6895\n",
      "181/500 [=========>....................] - ETA: 6:12 - loss: 2.3863 - regression_loss: 1.6974 - classification_loss: 0.6889\n",
      "182/500 [=========>....................] - ETA: 6:10 - loss: 2.3843 - regression_loss: 1.6959 - classification_loss: 0.6884\n",
      "183/500 [=========>....................] - ETA: 6:09 - loss: 2.3807 - regression_loss: 1.6925 - classification_loss: 0.6882\n",
      "184/500 [==========>...................] - ETA: 6:08 - loss: 2.3837 - regression_loss: 1.6946 - classification_loss: 0.6891\n",
      "185/500 [==========>...................] - ETA: 6:10 - loss: 2.3830 - regression_loss: 1.6930 - classification_loss: 0.6900\n",
      "186/500 [==========>...................] - ETA: 6:08 - loss: 2.3845 - regression_loss: 1.6947 - classification_loss: 0.6898\n",
      "187/500 [==========>...................] - ETA: 6:10 - loss: 2.3837 - regression_loss: 1.6939 - classification_loss: 0.6898\n",
      "188/500 [==========>...................] - ETA: 6:09 - loss: 2.3816 - regression_loss: 1.6928 - classification_loss: 0.6888\n",
      "189/500 [==========>...................] - ETA: 6:08 - loss: 2.3839 - regression_loss: 1.6948 - classification_loss: 0.6891\n",
      "190/500 [==========>...................] - ETA: 6:06 - loss: 2.3862 - regression_loss: 1.6966 - classification_loss: 0.6895\n",
      "191/500 [==========>...................] - ETA: 6:05 - loss: 2.3857 - regression_loss: 1.6963 - classification_loss: 0.6894\n",
      "192/500 [==========>...................] - ETA: 6:03 - loss: 2.3896 - regression_loss: 1.6991 - classification_loss: 0.6905\n",
      "193/500 [==========>...................] - ETA: 6:02 - loss: 2.3856 - regression_loss: 1.6954 - classification_loss: 0.6901\n",
      "194/500 [==========>...................] - ETA: 6:00 - loss: 2.3885 - regression_loss: 1.6984 - classification_loss: 0.6901\n",
      "195/500 [==========>...................] - ETA: 6:03 - loss: 2.3871 - regression_loss: 1.6979 - classification_loss: 0.6892\n",
      "196/500 [==========>...................] - ETA: 6:04 - loss: 2.3866 - regression_loss: 1.6980 - classification_loss: 0.6887\n",
      "197/500 [==========>...................] - ETA: 6:03 - loss: 2.3875 - regression_loss: 1.6985 - classification_loss: 0.6889\n",
      "198/500 [==========>...................] - ETA: 6:01 - loss: 2.3931 - regression_loss: 1.7023 - classification_loss: 0.6908\n",
      "199/500 [==========>...................] - ETA: 6:00 - loss: 2.3877 - regression_loss: 1.6985 - classification_loss: 0.6892\n",
      "200/500 [===========>..................] - ETA: 5:58 - loss: 2.3908 - regression_loss: 1.7014 - classification_loss: 0.6894\n",
      "201/500 [===========>..................] - ETA: 5:57 - loss: 2.3865 - regression_loss: 1.6984 - classification_loss: 0.6881\n",
      "202/500 [===========>..................] - ETA: 5:55 - loss: 2.3863 - regression_loss: 1.6974 - classification_loss: 0.6889\n",
      "203/500 [===========>..................] - ETA: 5:54 - loss: 2.3840 - regression_loss: 1.6964 - classification_loss: 0.6876\n",
      "204/500 [===========>..................] - ETA: 5:53 - loss: 2.3838 - regression_loss: 1.6957 - classification_loss: 0.6881\n",
      "205/500 [===========>..................] - ETA: 5:51 - loss: 2.3862 - regression_loss: 1.6971 - classification_loss: 0.6891\n",
      "206/500 [===========>..................] - ETA: 5:50 - loss: 2.3853 - regression_loss: 1.6963 - classification_loss: 0.6889\n",
      "207/500 [===========>..................] - ETA: 5:49 - loss: 2.3803 - regression_loss: 1.6926 - classification_loss: 0.6877\n",
      "208/500 [===========>..................] - ETA: 5:47 - loss: 2.3810 - regression_loss: 1.6938 - classification_loss: 0.6872\n",
      "209/500 [===========>..................] - ETA: 5:46 - loss: 2.3784 - regression_loss: 1.6914 - classification_loss: 0.6871\n",
      "210/500 [===========>..................] - ETA: 5:44 - loss: 2.3779 - regression_loss: 1.6915 - classification_loss: 0.6865\n",
      "211/500 [===========>..................] - ETA: 5:43 - loss: 2.3773 - regression_loss: 1.6918 - classification_loss: 0.6855\n",
      "212/500 [===========>..................] - ETA: 5:44 - loss: 2.3761 - regression_loss: 1.6899 - classification_loss: 0.6863\n",
      "213/500 [===========>..................] - ETA: 5:42 - loss: 2.3776 - regression_loss: 1.6918 - classification_loss: 0.6858\n",
      "214/500 [===========>..................] - ETA: 5:41 - loss: 2.3732 - regression_loss: 1.6883 - classification_loss: 0.6849\n",
      "215/500 [===========>..................] - ETA: 5:39 - loss: 2.3731 - regression_loss: 1.6878 - classification_loss: 0.6852\n",
      "216/500 [===========>..................] - ETA: 5:38 - loss: 2.3726 - regression_loss: 1.6869 - classification_loss: 0.6857\n",
      "217/500 [============>.................] - ETA: 5:36 - loss: 2.3765 - regression_loss: 1.6896 - classification_loss: 0.6869\n",
      "218/500 [============>.................] - ETA: 5:35 - loss: 2.3782 - regression_loss: 1.6915 - classification_loss: 0.6867\n",
      "219/500 [============>.................] - ETA: 5:34 - loss: 2.3753 - regression_loss: 1.6891 - classification_loss: 0.6862\n",
      "220/500 [============>.................] - ETA: 5:32 - loss: 2.3758 - regression_loss: 1.6896 - classification_loss: 0.6862\n",
      "221/500 [============>.................] - ETA: 5:31 - loss: 2.3803 - regression_loss: 1.6930 - classification_loss: 0.6874\n",
      "222/500 [============>.................] - ETA: 5:29 - loss: 2.3824 - regression_loss: 1.6934 - classification_loss: 0.6890\n",
      "223/500 [============>.................] - ETA: 5:28 - loss: 2.3841 - regression_loss: 1.6950 - classification_loss: 0.6891\n",
      "224/500 [============>.................] - ETA: 5:27 - loss: 2.3825 - regression_loss: 1.6935 - classification_loss: 0.6891\n",
      "225/500 [============>.................] - ETA: 5:25 - loss: 2.3837 - regression_loss: 1.6939 - classification_loss: 0.6898\n",
      "226/500 [============>.................] - ETA: 5:24 - loss: 2.3824 - regression_loss: 1.6922 - classification_loss: 0.6902\n",
      "227/500 [============>.................] - ETA: 5:22 - loss: 2.3833 - regression_loss: 1.6932 - classification_loss: 0.6901\n",
      "228/500 [============>.................] - ETA: 5:21 - loss: 2.3821 - regression_loss: 1.6931 - classification_loss: 0.6890\n",
      "229/500 [============>.................] - ETA: 5:20 - loss: 2.3843 - regression_loss: 1.6951 - classification_loss: 0.6892\n",
      "230/500 [============>.................] - ETA: 5:18 - loss: 2.3836 - regression_loss: 1.6946 - classification_loss: 0.6891\n",
      "231/500 [============>.................] - ETA: 5:17 - loss: 2.3835 - regression_loss: 1.6948 - classification_loss: 0.6887\n",
      "232/500 [============>.................] - ETA: 5:18 - loss: 2.3813 - regression_loss: 1.6930 - classification_loss: 0.6882\n",
      "233/500 [============>.................] - ETA: 5:16 - loss: 2.3831 - regression_loss: 1.6954 - classification_loss: 0.6877\n",
      "234/500 [=============>................] - ETA: 5:15 - loss: 2.3832 - regression_loss: 1.6958 - classification_loss: 0.6874\n",
      "235/500 [=============>................] - ETA: 5:14 - loss: 2.3833 - regression_loss: 1.6955 - classification_loss: 0.6878\n",
      "236/500 [=============>................] - ETA: 5:13 - loss: 2.3821 - regression_loss: 1.6946 - classification_loss: 0.6875\n",
      "237/500 [=============>................] - ETA: 5:11 - loss: 2.3803 - regression_loss: 1.6933 - classification_loss: 0.6870\n",
      "238/500 [=============>................] - ETA: 5:10 - loss: 2.3794 - regression_loss: 1.6927 - classification_loss: 0.6867\n",
      "239/500 [=============>................] - ETA: 5:09 - loss: 2.3777 - regression_loss: 1.6914 - classification_loss: 0.6864\n",
      "240/500 [=============>................] - ETA: 5:10 - loss: 2.3747 - regression_loss: 1.6890 - classification_loss: 0.6857\n",
      "241/500 [=============>................] - ETA: 5:09 - loss: 2.3732 - regression_loss: 1.6881 - classification_loss: 0.6851\n",
      "242/500 [=============>................] - ETA: 5:08 - loss: 2.3710 - regression_loss: 1.6862 - classification_loss: 0.6848\n",
      "243/500 [=============>................] - ETA: 5:06 - loss: 2.3695 - regression_loss: 1.6852 - classification_loss: 0.6843\n",
      "244/500 [=============>................] - ETA: 5:05 - loss: 2.3702 - regression_loss: 1.6865 - classification_loss: 0.6838\n",
      "245/500 [=============>................] - ETA: 5:04 - loss: 2.3720 - regression_loss: 1.6887 - classification_loss: 0.6833\n",
      "246/500 [=============>................] - ETA: 5:02 - loss: 2.3692 - regression_loss: 1.6864 - classification_loss: 0.6829\n",
      "247/500 [=============>................] - ETA: 5:01 - loss: 2.3681 - regression_loss: 1.6856 - classification_loss: 0.6826\n",
      "248/500 [=============>................] - ETA: 5:00 - loss: 2.3643 - regression_loss: 1.6828 - classification_loss: 0.6815\n",
      "249/500 [=============>................] - ETA: 4:58 - loss: 2.3629 - regression_loss: 1.6818 - classification_loss: 0.6811\n",
      "250/500 [==============>...............] - ETA: 4:57 - loss: 2.3587 - regression_loss: 1.6785 - classification_loss: 0.6802\n",
      "251/500 [==============>...............] - ETA: 4:56 - loss: 2.3630 - regression_loss: 1.6816 - classification_loss: 0.6814\n",
      "252/500 [==============>...............] - ETA: 4:56 - loss: 2.3638 - regression_loss: 1.6826 - classification_loss: 0.6812\n",
      "253/500 [==============>...............] - ETA: 4:55 - loss: 2.3639 - regression_loss: 1.6821 - classification_loss: 0.6818\n",
      "254/500 [==============>...............] - ETA: 4:54 - loss: 2.3629 - regression_loss: 1.6816 - classification_loss: 0.6813\n",
      "255/500 [==============>...............] - ETA: 4:52 - loss: 2.3661 - regression_loss: 1.6844 - classification_loss: 0.6817\n",
      "256/500 [==============>...............] - ETA: 4:51 - loss: 2.3688 - regression_loss: 1.6866 - classification_loss: 0.6822\n",
      "257/500 [==============>...............] - ETA: 4:50 - loss: 2.3691 - regression_loss: 1.6865 - classification_loss: 0.6826\n",
      "258/500 [==============>...............] - ETA: 4:49 - loss: 2.3662 - regression_loss: 1.6846 - classification_loss: 0.6816\n",
      "259/500 [==============>...............] - ETA: 4:47 - loss: 2.3670 - regression_loss: 1.6851 - classification_loss: 0.6820\n",
      "260/500 [==============>...............] - ETA: 4:46 - loss: 2.3666 - regression_loss: 1.6850 - classification_loss: 0.6816\n",
      "261/500 [==============>...............] - ETA: 4:44 - loss: 2.3656 - regression_loss: 1.6844 - classification_loss: 0.6813\n",
      "262/500 [==============>...............] - ETA: 4:43 - loss: 2.3654 - regression_loss: 1.6839 - classification_loss: 0.6815\n",
      "263/500 [==============>...............] - ETA: 4:42 - loss: 2.3660 - regression_loss: 1.6851 - classification_loss: 0.6809\n",
      "264/500 [==============>...............] - ETA: 4:41 - loss: 2.3688 - regression_loss: 1.6877 - classification_loss: 0.6811\n",
      "265/500 [==============>...............] - ETA: 4:41 - loss: 2.3694 - regression_loss: 1.6887 - classification_loss: 0.6807\n",
      "266/500 [==============>...............] - ETA: 4:40 - loss: 2.3708 - regression_loss: 1.6899 - classification_loss: 0.6809\n",
      "267/500 [===============>..............] - ETA: 4:39 - loss: 2.3680 - regression_loss: 1.6871 - classification_loss: 0.6809\n",
      "268/500 [===============>..............] - ETA: 4:37 - loss: 2.3701 - regression_loss: 1.6882 - classification_loss: 0.6819\n",
      "269/500 [===============>..............] - ETA: 4:36 - loss: 2.3687 - regression_loss: 1.6867 - classification_loss: 0.6820\n",
      "270/500 [===============>..............] - ETA: 4:34 - loss: 2.3701 - regression_loss: 1.6880 - classification_loss: 0.6822\n",
      "271/500 [===============>..............] - ETA: 4:33 - loss: 2.3672 - regression_loss: 1.6859 - classification_loss: 0.6814\n",
      "272/500 [===============>..............] - ETA: 4:32 - loss: 2.3644 - regression_loss: 1.6837 - classification_loss: 0.6807\n",
      "273/500 [===============>..............] - ETA: 4:31 - loss: 2.3664 - regression_loss: 1.6858 - classification_loss: 0.6806\n",
      "274/500 [===============>..............] - ETA: 4:29 - loss: 2.3648 - regression_loss: 1.6846 - classification_loss: 0.6803\n",
      "275/500 [===============>..............] - ETA: 4:28 - loss: 2.3632 - regression_loss: 1.6835 - classification_loss: 0.6798\n",
      "276/500 [===============>..............] - ETA: 4:27 - loss: 2.3591 - regression_loss: 1.6803 - classification_loss: 0.6788\n",
      "277/500 [===============>..............] - ETA: 4:25 - loss: 2.3557 - regression_loss: 1.6777 - classification_loss: 0.6781\n",
      "278/500 [===============>..............] - ETA: 4:24 - loss: 2.3541 - regression_loss: 1.6759 - classification_loss: 0.6782\n",
      "279/500 [===============>..............] - ETA: 4:25 - loss: 2.3545 - regression_loss: 1.6768 - classification_loss: 0.6777\n",
      "280/500 [===============>..............] - ETA: 4:24 - loss: 2.3537 - regression_loss: 1.6762 - classification_loss: 0.6775\n",
      "281/500 [===============>..............] - ETA: 4:22 - loss: 2.3540 - regression_loss: 1.6766 - classification_loss: 0.6774\n",
      "282/500 [===============>..............] - ETA: 4:21 - loss: 2.3527 - regression_loss: 1.6754 - classification_loss: 0.6773\n",
      "283/500 [===============>..............] - ETA: 4:20 - loss: 2.3498 - regression_loss: 1.6732 - classification_loss: 0.6765\n",
      "284/500 [================>.............] - ETA: 4:19 - loss: 2.3468 - regression_loss: 1.6708 - classification_loss: 0.6760\n",
      "285/500 [================>.............] - ETA: 4:17 - loss: 2.3473 - regression_loss: 1.6711 - classification_loss: 0.6762\n",
      "286/500 [================>.............] - ETA: 4:16 - loss: 2.3491 - regression_loss: 1.6731 - classification_loss: 0.6760\n",
      "287/500 [================>.............] - ETA: 4:15 - loss: 2.3475 - regression_loss: 1.6721 - classification_loss: 0.6754\n",
      "288/500 [================>.............] - ETA: 4:13 - loss: 2.3478 - regression_loss: 1.6726 - classification_loss: 0.6753\n",
      "289/500 [================>.............] - ETA: 4:12 - loss: 2.3454 - regression_loss: 1.6705 - classification_loss: 0.6749\n",
      "290/500 [================>.............] - ETA: 4:11 - loss: 2.3449 - regression_loss: 1.6705 - classification_loss: 0.6744\n",
      "291/500 [================>.............] - ETA: 4:09 - loss: 2.3460 - regression_loss: 1.6716 - classification_loss: 0.6744\n",
      "292/500 [================>.............] - ETA: 4:08 - loss: 2.3446 - regression_loss: 1.6705 - classification_loss: 0.6741\n",
      "293/500 [================>.............] - ETA: 4:07 - loss: 2.3474 - regression_loss: 1.6725 - classification_loss: 0.6748\n",
      "294/500 [================>.............] - ETA: 4:05 - loss: 2.3472 - regression_loss: 1.6728 - classification_loss: 0.6744\n",
      "295/500 [================>.............] - ETA: 4:04 - loss: 2.3482 - regression_loss: 1.6733 - classification_loss: 0.6749\n",
      "296/500 [================>.............] - ETA: 4:03 - loss: 2.3478 - regression_loss: 1.6729 - classification_loss: 0.6749\n",
      "297/500 [================>.............] - ETA: 4:01 - loss: 2.3476 - regression_loss: 1.6726 - classification_loss: 0.6750\n",
      "298/500 [================>.............] - ETA: 4:00 - loss: 2.3473 - regression_loss: 1.6730 - classification_loss: 0.6743\n",
      "299/500 [================>.............] - ETA: 3:59 - loss: 2.3465 - regression_loss: 1.6728 - classification_loss: 0.6737\n",
      "300/500 [=================>............] - ETA: 3:57 - loss: 2.3450 - regression_loss: 1.6719 - classification_loss: 0.6731\n",
      "301/500 [=================>............] - ETA: 3:56 - loss: 2.3469 - regression_loss: 1.6736 - classification_loss: 0.6733\n",
      "302/500 [=================>............] - ETA: 3:55 - loss: 2.3436 - regression_loss: 1.6710 - classification_loss: 0.6726\n",
      "303/500 [=================>............] - ETA: 3:54 - loss: 2.3414 - regression_loss: 1.6689 - classification_loss: 0.6725\n",
      "304/500 [=================>............] - ETA: 3:53 - loss: 2.3411 - regression_loss: 1.6691 - classification_loss: 0.6719\n",
      "305/500 [=================>............] - ETA: 3:51 - loss: 2.3370 - regression_loss: 1.6661 - classification_loss: 0.6709\n",
      "306/500 [=================>............] - ETA: 3:50 - loss: 2.3381 - regression_loss: 1.6670 - classification_loss: 0.6711\n",
      "307/500 [=================>............] - ETA: 3:49 - loss: 2.3389 - regression_loss: 1.6675 - classification_loss: 0.6714\n",
      "308/500 [=================>............] - ETA: 3:49 - loss: 2.3388 - regression_loss: 1.6676 - classification_loss: 0.6713\n",
      "309/500 [=================>............] - ETA: 3:48 - loss: 2.3388 - regression_loss: 1.6681 - classification_loss: 0.6708\n",
      "310/500 [=================>............] - ETA: 3:46 - loss: 2.3383 - regression_loss: 1.6675 - classification_loss: 0.6708\n",
      "311/500 [=================>............] - ETA: 3:45 - loss: 2.3373 - regression_loss: 1.6671 - classification_loss: 0.6703\n",
      "312/500 [=================>............] - ETA: 3:45 - loss: 2.3359 - regression_loss: 1.6661 - classification_loss: 0.6698\n",
      "313/500 [=================>............] - ETA: 3:44 - loss: 2.3365 - regression_loss: 1.6667 - classification_loss: 0.6698\n",
      "314/500 [=================>............] - ETA: 3:43 - loss: 2.3346 - regression_loss: 1.6653 - classification_loss: 0.6692\n",
      "315/500 [=================>............] - ETA: 3:42 - loss: 2.3348 - regression_loss: 1.6659 - classification_loss: 0.6689\n",
      "316/500 [=================>............] - ETA: 3:40 - loss: 2.3345 - regression_loss: 1.6659 - classification_loss: 0.6686\n",
      "317/500 [==================>...........] - ETA: 3:40 - loss: 2.3327 - regression_loss: 1.6645 - classification_loss: 0.6682\n",
      "318/500 [==================>...........] - ETA: 3:38 - loss: 2.3342 - regression_loss: 1.6654 - classification_loss: 0.6687\n",
      "319/500 [==================>...........] - ETA: 3:37 - loss: 2.3332 - regression_loss: 1.6645 - classification_loss: 0.6687\n",
      "320/500 [==================>...........] - ETA: 3:36 - loss: 2.3316 - regression_loss: 1.6632 - classification_loss: 0.6684\n",
      "321/500 [==================>...........] - ETA: 3:34 - loss: 2.3327 - regression_loss: 1.6632 - classification_loss: 0.6695\n",
      "322/500 [==================>...........] - ETA: 3:33 - loss: 2.3319 - regression_loss: 1.6628 - classification_loss: 0.6691\n",
      "323/500 [==================>...........] - ETA: 3:32 - loss: 2.3333 - regression_loss: 1.6645 - classification_loss: 0.6688\n",
      "324/500 [==================>...........] - ETA: 3:31 - loss: 2.3332 - regression_loss: 1.6644 - classification_loss: 0.6688\n",
      "325/500 [==================>...........] - ETA: 3:29 - loss: 2.3338 - regression_loss: 1.6651 - classification_loss: 0.6687\n",
      "326/500 [==================>...........] - ETA: 3:28 - loss: 2.3345 - regression_loss: 1.6659 - classification_loss: 0.6686\n",
      "327/500 [==================>...........] - ETA: 3:27 - loss: 2.3332 - regression_loss: 1.6653 - classification_loss: 0.6679\n",
      "328/500 [==================>...........] - ETA: 3:26 - loss: 2.3329 - regression_loss: 1.6653 - classification_loss: 0.6677\n",
      "329/500 [==================>...........] - ETA: 3:24 - loss: 2.3321 - regression_loss: 1.6651 - classification_loss: 0.6670\n",
      "330/500 [==================>...........] - ETA: 3:23 - loss: 2.3311 - regression_loss: 1.6645 - classification_loss: 0.6666\n",
      "331/500 [==================>...........] - ETA: 3:23 - loss: 2.3317 - regression_loss: 1.6651 - classification_loss: 0.6666\n",
      "332/500 [==================>...........] - ETA: 3:21 - loss: 2.3310 - regression_loss: 1.6644 - classification_loss: 0.6665\n",
      "333/500 [==================>...........] - ETA: 3:20 - loss: 2.3297 - regression_loss: 1.6639 - classification_loss: 0.6658\n",
      "334/500 [===================>..........] - ETA: 3:19 - loss: 2.3316 - regression_loss: 1.6652 - classification_loss: 0.6664\n",
      "335/500 [===================>..........] - ETA: 3:17 - loss: 2.3333 - regression_loss: 1.6666 - classification_loss: 0.6666\n",
      "336/500 [===================>..........] - ETA: 3:16 - loss: 2.3340 - regression_loss: 1.6671 - classification_loss: 0.6668\n",
      "337/500 [===================>..........] - ETA: 3:15 - loss: 2.3371 - regression_loss: 1.6694 - classification_loss: 0.6677\n",
      "338/500 [===================>..........] - ETA: 3:13 - loss: 2.3359 - regression_loss: 1.6686 - classification_loss: 0.6673\n",
      "339/500 [===================>..........] - ETA: 3:12 - loss: 2.3347 - regression_loss: 1.6679 - classification_loss: 0.6668\n",
      "340/500 [===================>..........] - ETA: 3:11 - loss: 2.3333 - regression_loss: 1.6665 - classification_loss: 0.6668\n",
      "341/500 [===================>..........] - ETA: 3:10 - loss: 2.3343 - regression_loss: 1.6673 - classification_loss: 0.6670\n",
      "342/500 [===================>..........] - ETA: 3:08 - loss: 2.3340 - regression_loss: 1.6676 - classification_loss: 0.6664\n",
      "343/500 [===================>..........] - ETA: 3:07 - loss: 2.3340 - regression_loss: 1.6674 - classification_loss: 0.6666\n",
      "344/500 [===================>..........] - ETA: 3:06 - loss: 2.3308 - regression_loss: 1.6651 - classification_loss: 0.6657\n",
      "345/500 [===================>..........] - ETA: 3:05 - loss: 2.3310 - regression_loss: 1.6656 - classification_loss: 0.6654\n",
      "346/500 [===================>..........] - ETA: 3:03 - loss: 2.3300 - regression_loss: 1.6647 - classification_loss: 0.6653\n",
      "347/500 [===================>..........] - ETA: 3:02 - loss: 2.3324 - regression_loss: 1.6661 - classification_loss: 0.6663\n",
      "348/500 [===================>..........] - ETA: 3:01 - loss: 2.3314 - regression_loss: 1.6652 - classification_loss: 0.6662\n",
      "349/500 [===================>..........] - ETA: 2:59 - loss: 2.3300 - regression_loss: 1.6640 - classification_loss: 0.6660\n",
      "350/500 [====================>.........] - ETA: 2:58 - loss: 2.3292 - regression_loss: 1.6636 - classification_loss: 0.6656\n",
      "351/500 [====================>.........] - ETA: 2:57 - loss: 2.3280 - regression_loss: 1.6620 - classification_loss: 0.6660\n",
      "352/500 [====================>.........] - ETA: 2:57 - loss: 2.3269 - regression_loss: 1.6613 - classification_loss: 0.6655\n",
      "353/500 [====================>.........] - ETA: 2:56 - loss: 2.3288 - regression_loss: 1.6629 - classification_loss: 0.6660\n",
      "354/500 [====================>.........] - ETA: 2:55 - loss: 2.3280 - regression_loss: 1.6620 - classification_loss: 0.6660\n",
      "355/500 [====================>.........] - ETA: 2:54 - loss: 2.3285 - regression_loss: 1.6625 - classification_loss: 0.6660\n",
      "356/500 [====================>.........] - ETA: 2:52 - loss: 2.3282 - regression_loss: 1.6622 - classification_loss: 0.6660\n",
      "357/500 [====================>.........] - ETA: 2:51 - loss: 2.3288 - regression_loss: 1.6625 - classification_loss: 0.6663\n",
      "358/500 [====================>.........] - ETA: 2:50 - loss: 2.3280 - regression_loss: 1.6619 - classification_loss: 0.6661\n",
      "359/500 [====================>.........] - ETA: 2:49 - loss: 2.3288 - regression_loss: 1.6627 - classification_loss: 0.6661\n",
      "360/500 [====================>.........] - ETA: 2:48 - loss: 2.3274 - regression_loss: 1.6621 - classification_loss: 0.6653\n",
      "361/500 [====================>.........] - ETA: 2:47 - loss: 2.3256 - regression_loss: 1.6605 - classification_loss: 0.6651\n",
      "362/500 [====================>.........] - ETA: 2:46 - loss: 2.3245 - regression_loss: 1.6597 - classification_loss: 0.6648\n",
      "363/500 [====================>.........] - ETA: 2:45 - loss: 2.3259 - regression_loss: 1.6606 - classification_loss: 0.6652\n",
      "364/500 [====================>.........] - ETA: 2:43 - loss: 2.3260 - regression_loss: 1.6609 - classification_loss: 0.6651\n",
      "365/500 [====================>.........] - ETA: 2:42 - loss: 2.3264 - regression_loss: 1.6611 - classification_loss: 0.6653\n",
      "366/500 [====================>.........] - ETA: 2:41 - loss: 2.3255 - regression_loss: 1.6604 - classification_loss: 0.6652\n",
      "367/500 [=====================>........] - ETA: 2:39 - loss: 2.3240 - regression_loss: 1.6594 - classification_loss: 0.6646\n",
      "368/500 [=====================>........] - ETA: 2:38 - loss: 2.3252 - regression_loss: 1.6599 - classification_loss: 0.6653\n",
      "369/500 [=====================>........] - ETA: 2:37 - loss: 2.3223 - regression_loss: 1.6576 - classification_loss: 0.6647\n",
      "370/500 [=====================>........] - ETA: 2:36 - loss: 2.3218 - regression_loss: 1.6572 - classification_loss: 0.6647\n",
      "371/500 [=====================>........] - ETA: 2:34 - loss: 2.3221 - regression_loss: 1.6575 - classification_loss: 0.6646\n",
      "372/500 [=====================>........] - ETA: 2:34 - loss: 2.3213 - regression_loss: 1.6571 - classification_loss: 0.6643\n",
      "373/500 [=====================>........] - ETA: 2:32 - loss: 2.3214 - regression_loss: 1.6570 - classification_loss: 0.6644\n",
      "374/500 [=====================>........] - ETA: 2:31 - loss: 2.3194 - regression_loss: 1.6557 - classification_loss: 0.6637\n",
      "375/500 [=====================>........] - ETA: 2:30 - loss: 2.3186 - regression_loss: 1.6553 - classification_loss: 0.6632\n",
      "376/500 [=====================>........] - ETA: 2:29 - loss: 2.3202 - regression_loss: 1.6566 - classification_loss: 0.6636\n",
      "377/500 [=====================>........] - ETA: 2:27 - loss: 2.3206 - regression_loss: 1.6568 - classification_loss: 0.6638\n",
      "378/500 [=====================>........] - ETA: 2:26 - loss: 2.3189 - regression_loss: 1.6554 - classification_loss: 0.6636\n",
      "379/500 [=====================>........] - ETA: 2:25 - loss: 2.3175 - regression_loss: 1.6544 - classification_loss: 0.6631\n",
      "380/500 [=====================>........] - ETA: 2:24 - loss: 2.3179 - regression_loss: 1.6554 - classification_loss: 0.6625\n",
      "381/500 [=====================>........] - ETA: 2:22 - loss: 2.3152 - regression_loss: 1.6531 - classification_loss: 0.6621\n",
      "382/500 [=====================>........] - ETA: 2:21 - loss: 2.3156 - regression_loss: 1.6536 - classification_loss: 0.6620\n",
      "383/500 [=====================>........] - ETA: 2:20 - loss: 2.3130 - regression_loss: 1.6516 - classification_loss: 0.6614\n",
      "384/500 [======================>.......] - ETA: 2:19 - loss: 2.3124 - regression_loss: 1.6506 - classification_loss: 0.6618\n",
      "385/500 [======================>.......] - ETA: 2:18 - loss: 2.3129 - regression_loss: 1.6507 - classification_loss: 0.6622\n",
      "386/500 [======================>.......] - ETA: 2:17 - loss: 2.3130 - regression_loss: 1.6506 - classification_loss: 0.6623\n",
      "387/500 [======================>.......] - ETA: 2:16 - loss: 2.3108 - regression_loss: 1.6491 - classification_loss: 0.6618\n",
      "388/500 [======================>.......] - ETA: 2:15 - loss: 2.3120 - regression_loss: 1.6500 - classification_loss: 0.6621\n",
      "389/500 [======================>.......] - ETA: 2:13 - loss: 2.3122 - regression_loss: 1.6501 - classification_loss: 0.6620\n",
      "390/500 [======================>.......] - ETA: 2:12 - loss: 2.3144 - regression_loss: 1.6515 - classification_loss: 0.6629\n",
      "391/500 [======================>.......] - ETA: 2:11 - loss: 2.3146 - regression_loss: 1.6517 - classification_loss: 0.6629\n",
      "392/500 [======================>.......] - ETA: 2:10 - loss: 2.3135 - regression_loss: 1.6510 - classification_loss: 0.6625\n",
      "393/500 [======================>.......] - ETA: 2:09 - loss: 2.3135 - regression_loss: 1.6509 - classification_loss: 0.6626\n",
      "394/500 [======================>.......] - ETA: 2:08 - loss: 2.3130 - regression_loss: 1.6507 - classification_loss: 0.6622\n",
      "395/500 [======================>.......] - ETA: 2:06 - loss: 2.3124 - regression_loss: 1.6496 - classification_loss: 0.6627\n",
      "396/500 [======================>.......] - ETA: 2:05 - loss: 2.3118 - regression_loss: 1.6491 - classification_loss: 0.6628\n",
      "397/500 [======================>.......] - ETA: 2:04 - loss: 2.3114 - regression_loss: 1.6487 - classification_loss: 0.6627\n",
      "398/500 [======================>.......] - ETA: 2:03 - loss: 2.3112 - regression_loss: 1.6486 - classification_loss: 0.6627\n",
      "399/500 [======================>.......] - ETA: 2:01 - loss: 2.3098 - regression_loss: 1.6473 - classification_loss: 0.6625\n",
      "400/500 [=======================>......] - ETA: 2:00 - loss: 2.3102 - regression_loss: 1.6475 - classification_loss: 0.6627\n",
      "401/500 [=======================>......] - ETA: 1:59 - loss: 2.3097 - regression_loss: 1.6473 - classification_loss: 0.6624\n",
      "402/500 [=======================>......] - ETA: 1:58 - loss: 2.3097 - regression_loss: 1.6473 - classification_loss: 0.6624\n",
      "403/500 [=======================>......] - ETA: 1:56 - loss: 2.3087 - regression_loss: 1.6464 - classification_loss: 0.6623\n",
      "404/500 [=======================>......] - ETA: 1:55 - loss: 2.3073 - regression_loss: 1.6454 - classification_loss: 0.6619\n",
      "405/500 [=======================>......] - ETA: 1:54 - loss: 2.3073 - regression_loss: 1.6454 - classification_loss: 0.6619\n",
      "406/500 [=======================>......] - ETA: 1:53 - loss: 2.3063 - regression_loss: 1.6444 - classification_loss: 0.6619\n",
      "407/500 [=======================>......] - ETA: 1:52 - loss: 2.3058 - regression_loss: 1.6442 - classification_loss: 0.6616\n",
      "408/500 [=======================>......] - ETA: 1:51 - loss: 2.3064 - regression_loss: 1.6451 - classification_loss: 0.6613\n",
      "409/500 [=======================>......] - ETA: 1:49 - loss: 2.3087 - regression_loss: 1.6469 - classification_loss: 0.6617\n",
      "410/500 [=======================>......] - ETA: 1:48 - loss: 2.3097 - regression_loss: 1.6479 - classification_loss: 0.6618\n",
      "411/500 [=======================>......] - ETA: 1:47 - loss: 2.3109 - regression_loss: 1.6488 - classification_loss: 0.6621\n",
      "412/500 [=======================>......] - ETA: 1:46 - loss: 2.3123 - regression_loss: 1.6502 - classification_loss: 0.6620\n",
      "413/500 [=======================>......] - ETA: 1:45 - loss: 2.3126 - regression_loss: 1.6504 - classification_loss: 0.6622\n",
      "414/500 [=======================>......] - ETA: 1:43 - loss: 2.3109 - regression_loss: 1.6492 - classification_loss: 0.6616\n",
      "415/500 [=======================>......] - ETA: 1:42 - loss: 2.3118 - regression_loss: 1.6498 - classification_loss: 0.6620\n",
      "416/500 [=======================>......] - ETA: 1:41 - loss: 2.3117 - regression_loss: 1.6496 - classification_loss: 0.6621\n",
      "417/500 [========================>.....] - ETA: 1:40 - loss: 2.3113 - regression_loss: 1.6494 - classification_loss: 0.6618\n",
      "418/500 [========================>.....] - ETA: 1:38 - loss: 2.3101 - regression_loss: 1.6483 - classification_loss: 0.6619\n",
      "419/500 [========================>.....] - ETA: 1:37 - loss: 2.3101 - regression_loss: 1.6485 - classification_loss: 0.6616\n",
      "420/500 [========================>.....] - ETA: 1:36 - loss: 2.3112 - regression_loss: 1.6497 - classification_loss: 0.6615\n",
      "421/500 [========================>.....] - ETA: 1:35 - loss: 2.3111 - regression_loss: 1.6498 - classification_loss: 0.6613\n",
      "422/500 [========================>.....] - ETA: 1:33 - loss: 2.3105 - regression_loss: 1.6493 - classification_loss: 0.6611\n",
      "423/500 [========================>.....] - ETA: 1:32 - loss: 2.3102 - regression_loss: 1.6493 - classification_loss: 0.6609\n",
      "424/500 [========================>.....] - ETA: 1:31 - loss: 2.3119 - regression_loss: 1.6508 - classification_loss: 0.6612\n",
      "425/500 [========================>.....] - ETA: 1:30 - loss: 2.3132 - regression_loss: 1.6518 - classification_loss: 0.6614\n",
      "426/500 [========================>.....] - ETA: 1:28 - loss: 2.3118 - regression_loss: 1.6507 - classification_loss: 0.6611\n",
      "427/500 [========================>.....] - ETA: 1:27 - loss: 2.3118 - regression_loss: 1.6510 - classification_loss: 0.6608\n",
      "428/500 [========================>.....] - ETA: 1:26 - loss: 2.3113 - regression_loss: 1.6504 - classification_loss: 0.6608\n",
      "429/500 [========================>.....] - ETA: 1:25 - loss: 2.3104 - regression_loss: 1.6498 - classification_loss: 0.6606\n",
      "430/500 [========================>.....] - ETA: 1:24 - loss: 2.3094 - regression_loss: 1.6486 - classification_loss: 0.6608\n",
      "431/500 [========================>.....] - ETA: 1:23 - loss: 2.3092 - regression_loss: 1.6482 - classification_loss: 0.6610\n",
      "432/500 [========================>.....] - ETA: 1:21 - loss: 2.3100 - regression_loss: 1.6486 - classification_loss: 0.6614\n",
      "433/500 [========================>.....] - ETA: 1:20 - loss: 2.3108 - regression_loss: 1.6495 - classification_loss: 0.6613\n",
      "434/500 [=========================>....] - ETA: 1:19 - loss: 2.3111 - regression_loss: 1.6497 - classification_loss: 0.6614\n",
      "435/500 [=========================>....] - ETA: 1:18 - loss: 2.3099 - regression_loss: 1.6488 - classification_loss: 0.6610\n",
      "436/500 [=========================>....] - ETA: 1:17 - loss: 2.3078 - regression_loss: 1.6473 - classification_loss: 0.6605\n",
      "437/500 [=========================>....] - ETA: 1:16 - loss: 2.3080 - regression_loss: 1.6474 - classification_loss: 0.6607\n",
      "438/500 [=========================>....] - ETA: 1:14 - loss: 2.3074 - regression_loss: 1.6468 - classification_loss: 0.6606\n",
      "439/500 [=========================>....] - ETA: 1:13 - loss: 2.3091 - regression_loss: 1.6483 - classification_loss: 0.6608\n",
      "440/500 [=========================>....] - ETA: 1:12 - loss: 2.3101 - regression_loss: 1.6491 - classification_loss: 0.6610\n",
      "441/500 [=========================>....] - ETA: 1:11 - loss: 2.3088 - regression_loss: 1.6481 - classification_loss: 0.6607\n",
      "442/500 [=========================>....] - ETA: 1:09 - loss: 2.3091 - regression_loss: 1.6483 - classification_loss: 0.6608\n",
      "443/500 [=========================>....] - ETA: 1:08 - loss: 2.3104 - regression_loss: 1.6493 - classification_loss: 0.6611\n",
      "444/500 [=========================>....] - ETA: 1:07 - loss: 2.3092 - regression_loss: 1.6481 - classification_loss: 0.6611\n",
      "445/500 [=========================>....] - ETA: 1:06 - loss: 2.3105 - regression_loss: 1.6491 - classification_loss: 0.6614\n",
      "446/500 [=========================>....] - ETA: 1:05 - loss: 2.3105 - regression_loss: 1.6493 - classification_loss: 0.6612\n",
      "447/500 [=========================>....] - ETA: 1:03 - loss: 2.3088 - regression_loss: 1.6481 - classification_loss: 0.6607\n",
      "448/500 [=========================>....] - ETA: 1:02 - loss: 2.3086 - regression_loss: 1.6481 - classification_loss: 0.6606\n",
      "449/500 [=========================>....] - ETA: 1:01 - loss: 2.3085 - regression_loss: 1.6483 - classification_loss: 0.6603\n",
      "450/500 [==========================>...] - ETA: 1:00 - loss: 2.3097 - regression_loss: 1.6493 - classification_loss: 0.6604\n",
      "451/500 [==========================>...] - ETA: 58s - loss: 2.3108 - regression_loss: 1.6499 - classification_loss: 0.6609 \n",
      "452/500 [==========================>...] - ETA: 57s - loss: 2.3104 - regression_loss: 1.6498 - classification_loss: 0.6605\n",
      "453/500 [==========================>...] - ETA: 56s - loss: 2.3104 - regression_loss: 1.6501 - classification_loss: 0.6603\n",
      "454/500 [==========================>...] - ETA: 55s - loss: 2.3089 - regression_loss: 1.6491 - classification_loss: 0.6598\n",
      "455/500 [==========================>...] - ETA: 54s - loss: 2.3080 - regression_loss: 1.6485 - classification_loss: 0.6595\n",
      "456/500 [==========================>...] - ETA: 52s - loss: 2.3065 - regression_loss: 1.6473 - classification_loss: 0.6592\n",
      "457/500 [==========================>...] - ETA: 51s - loss: 2.3058 - regression_loss: 1.6468 - classification_loss: 0.6590\n",
      "458/500 [==========================>...] - ETA: 50s - loss: 2.3058 - regression_loss: 1.6471 - classification_loss: 0.6588\n",
      "459/500 [==========================>...] - ETA: 49s - loss: 2.3049 - regression_loss: 1.6464 - classification_loss: 0.6585\n",
      "460/500 [==========================>...] - ETA: 47s - loss: 2.3054 - regression_loss: 1.6466 - classification_loss: 0.6588\n",
      "461/500 [==========================>...] - ETA: 46s - loss: 2.3056 - regression_loss: 1.6469 - classification_loss: 0.6587\n",
      "462/500 [==========================>...] - ETA: 45s - loss: 2.3051 - regression_loss: 1.6464 - classification_loss: 0.6587\n",
      "463/500 [==========================>...] - ETA: 44s - loss: 2.3065 - regression_loss: 1.6477 - classification_loss: 0.6588\n",
      "464/500 [==========================>...] - ETA: 43s - loss: 2.3054 - regression_loss: 1.6470 - classification_loss: 0.6584\n",
      "465/500 [==========================>...] - ETA: 41s - loss: 2.3056 - regression_loss: 1.6468 - classification_loss: 0.6588\n",
      "466/500 [==========================>...] - ETA: 40s - loss: 2.3043 - regression_loss: 1.6460 - classification_loss: 0.6584\n",
      "467/500 [===========================>..] - ETA: 39s - loss: 2.3031 - regression_loss: 1.6448 - classification_loss: 0.6583\n",
      "468/500 [===========================>..] - ETA: 38s - loss: 2.3026 - regression_loss: 1.6446 - classification_loss: 0.6580\n",
      "469/500 [===========================>..] - ETA: 37s - loss: 2.2993 - regression_loss: 1.6421 - classification_loss: 0.6572\n",
      "470/500 [===========================>..] - ETA: 35s - loss: 2.2977 - regression_loss: 1.6410 - classification_loss: 0.6567\n",
      "471/500 [===========================>..] - ETA: 34s - loss: 2.2987 - regression_loss: 1.6420 - classification_loss: 0.6566\n",
      "472/500 [===========================>..] - ETA: 33s - loss: 2.2969 - regression_loss: 1.6406 - classification_loss: 0.6563\n",
      "473/500 [===========================>..] - ETA: 32s - loss: 2.2960 - regression_loss: 1.6399 - classification_loss: 0.6562\n",
      "474/500 [===========================>..] - ETA: 31s - loss: 2.2968 - regression_loss: 1.6402 - classification_loss: 0.6566\n",
      "475/500 [===========================>..] - ETA: 30s - loss: 2.2976 - regression_loss: 1.6408 - classification_loss: 0.6568\n",
      "476/500 [===========================>..] - ETA: 28s - loss: 2.2986 - regression_loss: 1.6417 - classification_loss: 0.6568\n",
      "477/500 [===========================>..] - ETA: 27s - loss: 2.2975 - regression_loss: 1.6410 - classification_loss: 0.6566\n",
      "478/500 [===========================>..] - ETA: 26s - loss: 2.2975 - regression_loss: 1.6408 - classification_loss: 0.6567\n",
      "479/500 [===========================>..] - ETA: 25s - loss: 2.2985 - regression_loss: 1.6415 - classification_loss: 0.6570\n",
      "480/500 [===========================>..] - ETA: 24s - loss: 2.2980 - regression_loss: 1.6411 - classification_loss: 0.6570\n",
      "481/500 [===========================>..] - ETA: 22s - loss: 2.2998 - regression_loss: 1.6423 - classification_loss: 0.6575\n",
      "482/500 [===========================>..] - ETA: 21s - loss: 2.2989 - regression_loss: 1.6415 - classification_loss: 0.6574\n",
      "483/500 [===========================>..] - ETA: 20s - loss: 2.3002 - regression_loss: 1.6423 - classification_loss: 0.6579\n",
      "484/500 [============================>.] - ETA: 19s - loss: 2.2991 - regression_loss: 1.6417 - classification_loss: 0.6574\n",
      "485/500 [============================>.] - ETA: 18s - loss: 2.2973 - regression_loss: 1.6402 - classification_loss: 0.6571\n",
      "486/500 [============================>.] - ETA: 16s - loss: 2.2978 - regression_loss: 1.6410 - classification_loss: 0.6569\n",
      "487/500 [============================>.] - ETA: 15s - loss: 2.2966 - regression_loss: 1.6400 - classification_loss: 0.6566\n",
      "488/500 [============================>.] - ETA: 14s - loss: 2.2957 - regression_loss: 1.6393 - classification_loss: 0.6564\n",
      "489/500 [============================>.] - ETA: 13s - loss: 2.2973 - regression_loss: 1.6404 - classification_loss: 0.6569\n",
      "490/500 [============================>.] - ETA: 11s - loss: 2.2990 - regression_loss: 1.6418 - classification_loss: 0.6572\n",
      "491/500 [============================>.] - ETA: 10s - loss: 2.2989 - regression_loss: 1.6419 - classification_loss: 0.6570\n",
      "492/500 [============================>.] - ETA: 9s - loss: 2.2985 - regression_loss: 1.6413 - classification_loss: 0.6572 \n",
      "493/500 [============================>.] - ETA: 8s - loss: 2.2968 - regression_loss: 1.6400 - classification_loss: 0.6567\n",
      "494/500 [============================>.] - ETA: 7s - loss: 2.2958 - regression_loss: 1.6390 - classification_loss: 0.6568\n",
      "495/500 [============================>.] - ETA: 5s - loss: 2.2967 - regression_loss: 1.6395 - classification_loss: 0.6571\n",
      "496/500 [============================>.] - ETA: 4s - loss: 2.2955 - regression_loss: 1.6387 - classification_loss: 0.6568\n",
      "497/500 [============================>.] - ETA: 3s - loss: 2.2953 - regression_loss: 1.6387 - classification_loss: 0.6566\n",
      "498/500 [============================>.] - ETA: 2s - loss: 2.2937 - regression_loss: 1.6375 - classification_loss: 0.6561\n",
      "499/500 [============================>.] - ETA: 1s - loss: 2.2948 - regression_loss: 1.6382 - classification_loss: 0.6566\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.2958 - regression_loss: 1.6393 - classification_loss: 0.6565\n",
      "Epoch 00002: saving model to ./snapshots\\resnet50_csv_02.h5\n",
      "\n",
      "500/500 [==============================] - 601s 1s/step - loss: 2.2958 - regression_loss: 1.6393 - classification_loss: 0.6565\n",
      "Epoch 3/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 1.6267 - regression_loss: 1.0166 - classification_loss: 0.6101\n",
      "  2/500 [..............................] - ETA: 4:03 - loss: 2.1378 - regression_loss: 1.4646 - classification_loss: 0.6732\n",
      "  3/500 [..............................] - ETA: 5:52 - loss: 2.0892 - regression_loss: 1.4579 - classification_loss: 0.6313\n",
      "  4/500 [..............................] - ETA: 6:40 - loss: 2.0318 - regression_loss: 1.4304 - classification_loss: 0.6013\n",
      "  5/500 [..............................] - ETA: 7:06 - loss: 2.0690 - regression_loss: 1.4548 - classification_loss: 0.6142\n",
      "  6/500 [..............................] - ETA: 7:24 - loss: 2.0932 - regression_loss: 1.4724 - classification_loss: 0.6208\n",
      "  7/500 [..............................] - ETA: 7:29 - loss: 2.2646 - regression_loss: 1.5999 - classification_loss: 0.6647\n",
      "  8/500 [..............................] - ETA: 7:32 - loss: 2.1933 - regression_loss: 1.5446 - classification_loss: 0.6487\n",
      "  9/500 [..............................] - ETA: 7:35 - loss: 2.1799 - regression_loss: 1.5326 - classification_loss: 0.6473\n",
      " 10/500 [..............................] - ETA: 7:37 - loss: 2.1190 - regression_loss: 1.4886 - classification_loss: 0.6304\n",
      " 11/500 [..............................] - ETA: 7:38 - loss: 2.2011 - regression_loss: 1.5537 - classification_loss: 0.6475\n",
      " 12/500 [..............................] - ETA: 9:01 - loss: 2.1714 - regression_loss: 1.5346 - classification_loss: 0.6368\n",
      " 13/500 [..............................] - ETA: 8:55 - loss: 2.1995 - regression_loss: 1.5535 - classification_loss: 0.6460\n",
      " 14/500 [..............................] - ETA: 8:42 - loss: 2.2651 - regression_loss: 1.6065 - classification_loss: 0.6586\n",
      " 15/500 [..............................] - ETA: 8:44 - loss: 2.2871 - regression_loss: 1.6228 - classification_loss: 0.6643\n",
      " 16/500 [..............................] - ETA: 8:43 - loss: 2.2456 - regression_loss: 1.5974 - classification_loss: 0.6482\n",
      " 17/500 [>.............................] - ETA: 8:39 - loss: 2.2948 - regression_loss: 1.6365 - classification_loss: 0.6583\n",
      " 18/500 [>.............................] - ETA: 8:35 - loss: 2.2330 - regression_loss: 1.5935 - classification_loss: 0.6395\n",
      " 19/500 [>.............................] - ETA: 8:34 - loss: 2.2660 - regression_loss: 1.6203 - classification_loss: 0.6457\n",
      " 20/500 [>.............................] - ETA: 8:34 - loss: 2.3007 - regression_loss: 1.6456 - classification_loss: 0.6551\n",
      " 21/500 [>.............................] - ETA: 8:33 - loss: 2.3175 - regression_loss: 1.6606 - classification_loss: 0.6569\n",
      " 22/500 [>.............................] - ETA: 8:25 - loss: 2.2876 - regression_loss: 1.6392 - classification_loss: 0.6484\n",
      " 23/500 [>.............................] - ETA: 8:28 - loss: 2.2840 - regression_loss: 1.6418 - classification_loss: 0.6421\n",
      " 24/500 [>.............................] - ETA: 8:27 - loss: 2.2573 - regression_loss: 1.6227 - classification_loss: 0.6346\n",
      " 25/500 [>.............................] - ETA: 8:25 - loss: 2.2661 - regression_loss: 1.6227 - classification_loss: 0.6434\n",
      " 26/500 [>.............................] - ETA: 8:22 - loss: 2.2786 - regression_loss: 1.6280 - classification_loss: 0.6506\n",
      " 27/500 [>.............................] - ETA: 8:20 - loss: 2.2970 - regression_loss: 1.6426 - classification_loss: 0.6543\n",
      " 28/500 [>.............................] - ETA: 8:19 - loss: 2.2943 - regression_loss: 1.6412 - classification_loss: 0.6531\n",
      " 29/500 [>.............................] - ETA: 8:19 - loss: 2.3002 - regression_loss: 1.6466 - classification_loss: 0.6536\n",
      " 30/500 [>.............................] - ETA: 8:18 - loss: 2.2948 - regression_loss: 1.6423 - classification_loss: 0.6525\n",
      " 31/500 [>.............................] - ETA: 8:16 - loss: 2.2862 - regression_loss: 1.6352 - classification_loss: 0.6511\n",
      " 32/500 [>.............................] - ETA: 8:42 - loss: 2.2791 - regression_loss: 1.6281 - classification_loss: 0.6510\n",
      " 33/500 [>.............................] - ETA: 8:41 - loss: 2.2967 - regression_loss: 1.6407 - classification_loss: 0.6560\n",
      " 34/500 [=>............................] - ETA: 9:09 - loss: 2.3040 - regression_loss: 1.6460 - classification_loss: 0.6580\n",
      " 35/500 [=>............................] - ETA: 9:05 - loss: 2.3072 - regression_loss: 1.6522 - classification_loss: 0.6550\n",
      " 36/500 [=>............................] - ETA: 9:01 - loss: 2.2921 - regression_loss: 1.6446 - classification_loss: 0.6475\n",
      " 37/500 [=>............................] - ETA: 8:58 - loss: 2.2899 - regression_loss: 1.6440 - classification_loss: 0.6460\n",
      " 38/500 [=>............................] - ETA: 8:56 - loss: 2.2736 - regression_loss: 1.6338 - classification_loss: 0.6398\n",
      " 39/500 [=>............................] - ETA: 8:54 - loss: 2.2602 - regression_loss: 1.6279 - classification_loss: 0.6323\n",
      " 40/500 [=>............................] - ETA: 8:48 - loss: 2.2507 - regression_loss: 1.6172 - classification_loss: 0.6334\n",
      " 41/500 [=>............................] - ETA: 8:48 - loss: 2.2609 - regression_loss: 1.6224 - classification_loss: 0.6386\n",
      " 42/500 [=>............................] - ETA: 8:47 - loss: 2.2676 - regression_loss: 1.6260 - classification_loss: 0.6415\n",
      " 43/500 [=>............................] - ETA: 8:44 - loss: 2.2500 - regression_loss: 1.6160 - classification_loss: 0.6339\n",
      " 44/500 [=>............................] - ETA: 8:42 - loss: 2.2455 - regression_loss: 1.6125 - classification_loss: 0.6330\n",
      " 45/500 [=>............................] - ETA: 8:40 - loss: 2.2367 - regression_loss: 1.6055 - classification_loss: 0.6312\n",
      " 46/500 [=>............................] - ETA: 8:35 - loss: 2.2399 - regression_loss: 1.6104 - classification_loss: 0.6295\n",
      " 47/500 [=>............................] - ETA: 8:35 - loss: 2.2363 - regression_loss: 1.6077 - classification_loss: 0.6286\n",
      " 48/500 [=>............................] - ETA: 8:32 - loss: 2.2384 - regression_loss: 1.6122 - classification_loss: 0.6262\n",
      " 49/500 [=>............................] - ETA: 8:30 - loss: 2.2298 - regression_loss: 1.6025 - classification_loss: 0.6273\n",
      " 50/500 [==>...........................] - ETA: 8:28 - loss: 2.2321 - regression_loss: 1.6027 - classification_loss: 0.6294\n",
      " 51/500 [==>...........................] - ETA: 8:25 - loss: 2.2229 - regression_loss: 1.5976 - classification_loss: 0.6253\n",
      " 52/500 [==>...........................] - ETA: 8:21 - loss: 2.2141 - regression_loss: 1.5916 - classification_loss: 0.6225\n",
      " 53/500 [==>...........................] - ETA: 8:21 - loss: 2.2154 - regression_loss: 1.5929 - classification_loss: 0.6225\n",
      " 54/500 [==>...........................] - ETA: 8:17 - loss: 2.2093 - regression_loss: 1.5878 - classification_loss: 0.6215\n",
      " 55/500 [==>...........................] - ETA: 8:17 - loss: 2.2055 - regression_loss: 1.5848 - classification_loss: 0.6208\n",
      " 56/500 [==>...........................] - ETA: 8:15 - loss: 2.2036 - regression_loss: 1.5820 - classification_loss: 0.6216\n",
      " 57/500 [==>...........................] - ETA: 8:40 - loss: 2.2195 - regression_loss: 1.5932 - classification_loss: 0.6262\n",
      " 58/500 [==>...........................] - ETA: 8:38 - loss: 2.2154 - regression_loss: 1.5891 - classification_loss: 0.6263\n",
      " 59/500 [==>...........................] - ETA: 8:36 - loss: 2.2213 - regression_loss: 1.5923 - classification_loss: 0.6290\n",
      " 60/500 [==>...........................] - ETA: 8:32 - loss: 2.2141 - regression_loss: 1.5852 - classification_loss: 0.6289\n",
      " 61/500 [==>...........................] - ETA: 8:31 - loss: 2.2139 - regression_loss: 1.5851 - classification_loss: 0.6289\n",
      " 62/500 [==>...........................] - ETA: 8:28 - loss: 2.2071 - regression_loss: 1.5807 - classification_loss: 0.6264\n",
      " 63/500 [==>...........................] - ETA: 8:27 - loss: 2.2130 - regression_loss: 1.5866 - classification_loss: 0.6264\n",
      " 64/500 [==>...........................] - ETA: 8:24 - loss: 2.2253 - regression_loss: 1.5941 - classification_loss: 0.6312\n",
      " 65/500 [==>...........................] - ETA: 8:22 - loss: 2.2353 - regression_loss: 1.6037 - classification_loss: 0.6316\n",
      " 66/500 [==>...........................] - ETA: 8:20 - loss: 2.2458 - regression_loss: 1.6127 - classification_loss: 0.6331\n",
      " 67/500 [===>..........................] - ETA: 8:17 - loss: 2.2413 - regression_loss: 1.6091 - classification_loss: 0.6322\n",
      " 68/500 [===>..........................] - ETA: 8:15 - loss: 2.2473 - regression_loss: 1.6137 - classification_loss: 0.6336\n",
      " 69/500 [===>..........................] - ETA: 8:14 - loss: 2.2467 - regression_loss: 1.6126 - classification_loss: 0.6341\n",
      " 70/500 [===>..........................] - ETA: 8:12 - loss: 2.2373 - regression_loss: 1.6057 - classification_loss: 0.6316\n",
      " 71/500 [===>..........................] - ETA: 8:09 - loss: 2.2324 - regression_loss: 1.6008 - classification_loss: 0.6316\n",
      " 72/500 [===>..........................] - ETA: 8:08 - loss: 2.2276 - regression_loss: 1.5960 - classification_loss: 0.6316\n",
      " 73/500 [===>..........................] - ETA: 8:06 - loss: 2.2208 - regression_loss: 1.5928 - classification_loss: 0.6281\n",
      " 74/500 [===>..........................] - ETA: 8:04 - loss: 2.2203 - regression_loss: 1.5934 - classification_loss: 0.6269\n",
      " 75/500 [===>..........................] - ETA: 8:02 - loss: 2.2164 - regression_loss: 1.5900 - classification_loss: 0.6265\n",
      " 76/500 [===>..........................] - ETA: 8:00 - loss: 2.2304 - regression_loss: 1.5996 - classification_loss: 0.6308\n",
      " 77/500 [===>..........................] - ETA: 7:58 - loss: 2.2307 - regression_loss: 1.6022 - classification_loss: 0.6285\n",
      " 78/500 [===>..........................] - ETA: 7:59 - loss: 2.2360 - regression_loss: 1.6057 - classification_loss: 0.6303\n",
      " 79/500 [===>..........................] - ETA: 7:57 - loss: 2.2281 - regression_loss: 1.6001 - classification_loss: 0.6280\n",
      " 80/500 [===>..........................] - ETA: 7:55 - loss: 2.2301 - regression_loss: 1.6034 - classification_loss: 0.6267\n",
      " 81/500 [===>..........................] - ETA: 7:53 - loss: 2.2309 - regression_loss: 1.6048 - classification_loss: 0.6261\n",
      " 82/500 [===>..........................] - ETA: 7:51 - loss: 2.2323 - regression_loss: 1.6073 - classification_loss: 0.6249\n",
      " 83/500 [===>..........................] - ETA: 7:59 - loss: 2.2345 - regression_loss: 1.6103 - classification_loss: 0.6242\n",
      " 84/500 [====>.........................] - ETA: 7:57 - loss: 2.2283 - regression_loss: 1.6063 - classification_loss: 0.6220\n",
      " 85/500 [====>.........................] - ETA: 7:55 - loss: 2.2274 - regression_loss: 1.6069 - classification_loss: 0.6204\n",
      " 86/500 [====>.........................] - ETA: 7:53 - loss: 2.2216 - regression_loss: 1.6024 - classification_loss: 0.6192\n",
      " 87/500 [====>.........................] - ETA: 7:52 - loss: 2.2264 - regression_loss: 1.6063 - classification_loss: 0.6201\n",
      " 88/500 [====>.........................] - ETA: 7:50 - loss: 2.2316 - regression_loss: 1.6111 - classification_loss: 0.6205\n",
      " 89/500 [====>.........................] - ETA: 7:49 - loss: 2.2314 - regression_loss: 1.6123 - classification_loss: 0.6191\n",
      " 90/500 [====>.........................] - ETA: 7:47 - loss: 2.2286 - regression_loss: 1.6104 - classification_loss: 0.6181\n",
      " 91/500 [====>.........................] - ETA: 7:45 - loss: 2.2285 - regression_loss: 1.6103 - classification_loss: 0.6182\n",
      " 92/500 [====>.........................] - ETA: 7:44 - loss: 2.2263 - regression_loss: 1.6095 - classification_loss: 0.6168\n",
      " 93/500 [====>.........................] - ETA: 7:42 - loss: 2.2229 - regression_loss: 1.6071 - classification_loss: 0.6158\n",
      " 94/500 [====>.........................] - ETA: 7:40 - loss: 2.2210 - regression_loss: 1.6059 - classification_loss: 0.6151\n",
      " 95/500 [====>.........................] - ETA: 7:39 - loss: 2.2188 - regression_loss: 1.6047 - classification_loss: 0.6141\n",
      " 96/500 [====>.........................] - ETA: 7:37 - loss: 2.2190 - regression_loss: 1.6052 - classification_loss: 0.6137\n",
      " 97/500 [====>.........................] - ETA: 7:36 - loss: 2.2206 - regression_loss: 1.6043 - classification_loss: 0.6163\n",
      " 98/500 [====>.........................] - ETA: 7:33 - loss: 2.2223 - regression_loss: 1.6058 - classification_loss: 0.6165\n",
      " 99/500 [====>.........................] - ETA: 7:32 - loss: 2.2240 - regression_loss: 1.6070 - classification_loss: 0.6170\n",
      "100/500 [=====>........................] - ETA: 7:31 - loss: 2.2202 - regression_loss: 1.6049 - classification_loss: 0.6153\n",
      "101/500 [=====>........................] - ETA: 7:29 - loss: 2.2165 - regression_loss: 1.6028 - classification_loss: 0.6137\n",
      "102/500 [=====>........................] - ETA: 7:28 - loss: 2.2164 - regression_loss: 1.6021 - classification_loss: 0.6143\n",
      "103/500 [=====>........................] - ETA: 7:26 - loss: 2.2128 - regression_loss: 1.5980 - classification_loss: 0.6148\n",
      "104/500 [=====>........................] - ETA: 7:26 - loss: 2.2202 - regression_loss: 1.6031 - classification_loss: 0.6171\n",
      "105/500 [=====>........................] - ETA: 7:24 - loss: 2.2153 - regression_loss: 1.5985 - classification_loss: 0.6168\n",
      "106/500 [=====>........................] - ETA: 7:23 - loss: 2.2076 - regression_loss: 1.5925 - classification_loss: 0.6151\n",
      "107/500 [=====>........................] - ETA: 7:21 - loss: 2.2106 - regression_loss: 1.5950 - classification_loss: 0.6156\n",
      "108/500 [=====>........................] - ETA: 7:20 - loss: 2.2112 - regression_loss: 1.5951 - classification_loss: 0.6161\n",
      "109/500 [=====>........................] - ETA: 7:18 - loss: 2.2113 - regression_loss: 1.5950 - classification_loss: 0.6163\n",
      "110/500 [=====>........................] - ETA: 7:17 - loss: 2.2132 - regression_loss: 1.5954 - classification_loss: 0.6178\n",
      "111/500 [=====>........................] - ETA: 7:15 - loss: 2.2093 - regression_loss: 1.5918 - classification_loss: 0.6175\n",
      "112/500 [=====>........................] - ETA: 7:14 - loss: 2.2035 - regression_loss: 1.5872 - classification_loss: 0.6163\n",
      "113/500 [=====>........................] - ETA: 7:12 - loss: 2.2017 - regression_loss: 1.5854 - classification_loss: 0.6163\n",
      "114/500 [=====>........................] - ETA: 7:11 - loss: 2.2053 - regression_loss: 1.5890 - classification_loss: 0.6162\n",
      "115/500 [=====>........................] - ETA: 7:09 - loss: 2.2077 - regression_loss: 1.5899 - classification_loss: 0.6178\n",
      "116/500 [=====>........................] - ETA: 7:07 - loss: 2.2060 - regression_loss: 1.5892 - classification_loss: 0.6168\n",
      "117/500 [======>.......................] - ETA: 7:06 - loss: 2.2010 - regression_loss: 1.5855 - classification_loss: 0.6155\n",
      "118/500 [======>.......................] - ETA: 7:05 - loss: 2.2017 - regression_loss: 1.5851 - classification_loss: 0.6166\n",
      "119/500 [======>.......................] - ETA: 7:03 - loss: 2.1952 - regression_loss: 1.5795 - classification_loss: 0.6157\n",
      "120/500 [======>.......................] - ETA: 7:08 - loss: 2.1975 - regression_loss: 1.5809 - classification_loss: 0.6166\n",
      "121/500 [======>.......................] - ETA: 7:06 - loss: 2.2025 - regression_loss: 1.5857 - classification_loss: 0.6168\n",
      "122/500 [======>.......................] - ETA: 7:05 - loss: 2.1971 - regression_loss: 1.5818 - classification_loss: 0.6153\n",
      "123/500 [======>.......................] - ETA: 7:04 - loss: 2.2014 - regression_loss: 1.5847 - classification_loss: 0.6167\n",
      "124/500 [======>.......................] - ETA: 7:02 - loss: 2.1986 - regression_loss: 1.5831 - classification_loss: 0.6155\n",
      "125/500 [======>.......................] - ETA: 7:01 - loss: 2.1952 - regression_loss: 1.5804 - classification_loss: 0.6148\n",
      "126/500 [======>.......................] - ETA: 7:00 - loss: 2.1997 - regression_loss: 1.5826 - classification_loss: 0.6171\n",
      "127/500 [======>.......................] - ETA: 6:58 - loss: 2.2003 - regression_loss: 1.5831 - classification_loss: 0.6173\n",
      "128/500 [======>.......................] - ETA: 6:57 - loss: 2.2074 - regression_loss: 1.5880 - classification_loss: 0.6194\n",
      "129/500 [======>.......................] - ETA: 6:55 - loss: 2.2049 - regression_loss: 1.5870 - classification_loss: 0.6179\n",
      "130/500 [======>.......................] - ETA: 6:54 - loss: 2.2078 - regression_loss: 1.5898 - classification_loss: 0.6180\n",
      "131/500 [======>.......................] - ETA: 6:53 - loss: 2.2077 - regression_loss: 1.5904 - classification_loss: 0.6173\n",
      "132/500 [======>.......................] - ETA: 6:52 - loss: 2.2096 - regression_loss: 1.5930 - classification_loss: 0.6166\n",
      "133/500 [======>.......................] - ETA: 6:50 - loss: 2.2121 - regression_loss: 1.5939 - classification_loss: 0.6182\n",
      "134/500 [=======>......................] - ETA: 6:49 - loss: 2.2147 - regression_loss: 1.5974 - classification_loss: 0.6174\n",
      "135/500 [=======>......................] - ETA: 6:47 - loss: 2.2201 - regression_loss: 1.6017 - classification_loss: 0.6184\n",
      "136/500 [=======>......................] - ETA: 6:46 - loss: 2.2222 - regression_loss: 1.6030 - classification_loss: 0.6192\n",
      "137/500 [=======>......................] - ETA: 6:44 - loss: 2.2236 - regression_loss: 1.6046 - classification_loss: 0.6189\n",
      "138/500 [=======>......................] - ETA: 6:43 - loss: 2.2200 - regression_loss: 1.6018 - classification_loss: 0.6182\n",
      "139/500 [=======>......................] - ETA: 6:42 - loss: 2.2217 - regression_loss: 1.6026 - classification_loss: 0.6192\n",
      "140/500 [=======>......................] - ETA: 6:41 - loss: 2.2190 - regression_loss: 1.6007 - classification_loss: 0.6183\n",
      "141/500 [=======>......................] - ETA: 6:40 - loss: 2.2189 - regression_loss: 1.6007 - classification_loss: 0.6182\n",
      "142/500 [=======>......................] - ETA: 6:39 - loss: 2.2171 - regression_loss: 1.5996 - classification_loss: 0.6175\n",
      "143/500 [=======>......................] - ETA: 6:43 - loss: 2.2113 - regression_loss: 1.5953 - classification_loss: 0.6160\n",
      "144/500 [=======>......................] - ETA: 6:42 - loss: 2.2152 - regression_loss: 1.5978 - classification_loss: 0.6175\n",
      "145/500 [=======>......................] - ETA: 6:40 - loss: 2.2147 - regression_loss: 1.5971 - classification_loss: 0.6176\n",
      "146/500 [=======>......................] - ETA: 6:39 - loss: 2.2144 - regression_loss: 1.5953 - classification_loss: 0.6191\n",
      "147/500 [=======>......................] - ETA: 6:38 - loss: 2.2158 - regression_loss: 1.5975 - classification_loss: 0.6184\n",
      "148/500 [=======>......................] - ETA: 6:37 - loss: 2.2114 - regression_loss: 1.5941 - classification_loss: 0.6173\n",
      "149/500 [=======>......................] - ETA: 6:35 - loss: 2.2164 - regression_loss: 1.5988 - classification_loss: 0.6175\n",
      "150/500 [========>.....................] - ETA: 6:34 - loss: 2.2196 - regression_loss: 1.6014 - classification_loss: 0.6183\n",
      "151/500 [========>.....................] - ETA: 6:33 - loss: 2.2229 - regression_loss: 1.6029 - classification_loss: 0.6200\n",
      "152/500 [========>.....................] - ETA: 6:31 - loss: 2.2202 - regression_loss: 1.6013 - classification_loss: 0.6189\n",
      "153/500 [========>.....................] - ETA: 6:29 - loss: 2.2244 - regression_loss: 1.6047 - classification_loss: 0.6196\n",
      "154/500 [========>.....................] - ETA: 6:29 - loss: 2.2203 - regression_loss: 1.6020 - classification_loss: 0.6182\n",
      "155/500 [========>.....................] - ETA: 6:27 - loss: 2.2202 - regression_loss: 1.6026 - classification_loss: 0.6176\n",
      "156/500 [========>.....................] - ETA: 6:26 - loss: 2.2171 - regression_loss: 1.6003 - classification_loss: 0.6168\n",
      "157/500 [========>.....................] - ETA: 6:24 - loss: 2.2205 - regression_loss: 1.6023 - classification_loss: 0.6182\n",
      "158/500 [========>.....................] - ETA: 6:23 - loss: 2.2232 - regression_loss: 1.6044 - classification_loss: 0.6188\n",
      "159/500 [========>.....................] - ETA: 6:22 - loss: 2.2273 - regression_loss: 1.6069 - classification_loss: 0.6204\n",
      "160/500 [========>.....................] - ETA: 6:20 - loss: 2.2266 - regression_loss: 1.6070 - classification_loss: 0.6196\n",
      "161/500 [========>.....................] - ETA: 6:19 - loss: 2.2306 - regression_loss: 1.6101 - classification_loss: 0.6205\n",
      "162/500 [========>.....................] - ETA: 6:21 - loss: 2.2329 - regression_loss: 1.6127 - classification_loss: 0.6202\n",
      "163/500 [========>.....................] - ETA: 6:20 - loss: 2.2339 - regression_loss: 1.6137 - classification_loss: 0.6201\n",
      "164/500 [========>.....................] - ETA: 6:19 - loss: 2.2349 - regression_loss: 1.6143 - classification_loss: 0.6206\n",
      "165/500 [========>.....................] - ETA: 6:18 - loss: 2.2327 - regression_loss: 1.6134 - classification_loss: 0.6193\n",
      "166/500 [========>.....................] - ETA: 6:16 - loss: 2.2348 - regression_loss: 1.6153 - classification_loss: 0.6195\n",
      "167/500 [=========>....................] - ETA: 6:15 - loss: 2.2326 - regression_loss: 1.6123 - classification_loss: 0.6203\n",
      "168/500 [=========>....................] - ETA: 6:13 - loss: 2.2294 - regression_loss: 1.6103 - classification_loss: 0.6191\n",
      "169/500 [=========>....................] - ETA: 6:12 - loss: 2.2259 - regression_loss: 1.6077 - classification_loss: 0.6182\n",
      "170/500 [=========>....................] - ETA: 6:11 - loss: 2.2235 - regression_loss: 1.6056 - classification_loss: 0.6179\n",
      "171/500 [=========>....................] - ETA: 6:09 - loss: 2.2233 - regression_loss: 1.6059 - classification_loss: 0.6174\n",
      "172/500 [=========>....................] - ETA: 6:08 - loss: 2.2278 - regression_loss: 1.6094 - classification_loss: 0.6184\n",
      "173/500 [=========>....................] - ETA: 6:07 - loss: 2.2277 - regression_loss: 1.6099 - classification_loss: 0.6178\n",
      "174/500 [=========>....................] - ETA: 6:06 - loss: 2.2248 - regression_loss: 1.6079 - classification_loss: 0.6170\n",
      "175/500 [=========>....................] - ETA: 6:04 - loss: 2.2247 - regression_loss: 1.6064 - classification_loss: 0.6183\n",
      "176/500 [=========>....................] - ETA: 6:03 - loss: 2.2305 - regression_loss: 1.6106 - classification_loss: 0.6199\n",
      "177/500 [=========>....................] - ETA: 6:01 - loss: 2.2327 - regression_loss: 1.6127 - classification_loss: 0.6200\n",
      "178/500 [=========>....................] - ETA: 6:00 - loss: 2.2321 - regression_loss: 1.6129 - classification_loss: 0.6192\n",
      "179/500 [=========>....................] - ETA: 5:59 - loss: 2.2336 - regression_loss: 1.6153 - classification_loss: 0.6182\n",
      "180/500 [=========>....................] - ETA: 5:57 - loss: 2.2300 - regression_loss: 1.6120 - classification_loss: 0.6180\n",
      "181/500 [=========>....................] - ETA: 5:56 - loss: 2.2303 - regression_loss: 1.6125 - classification_loss: 0.6178\n",
      "182/500 [=========>....................] - ETA: 5:55 - loss: 2.2272 - regression_loss: 1.6105 - classification_loss: 0.6167\n",
      "183/500 [=========>....................] - ETA: 5:54 - loss: 2.2313 - regression_loss: 1.6139 - classification_loss: 0.6174\n",
      "184/500 [==========>...................] - ETA: 5:52 - loss: 2.2301 - regression_loss: 1.6128 - classification_loss: 0.6173\n",
      "185/500 [==========>...................] - ETA: 5:51 - loss: 2.2303 - regression_loss: 1.6128 - classification_loss: 0.6175\n",
      "186/500 [==========>...................] - ETA: 5:50 - loss: 2.2257 - regression_loss: 1.6098 - classification_loss: 0.6159\n",
      "187/500 [==========>...................] - ETA: 5:48 - loss: 2.2234 - regression_loss: 1.6083 - classification_loss: 0.6151\n",
      "188/500 [==========>...................] - ETA: 5:47 - loss: 2.2240 - regression_loss: 1.6093 - classification_loss: 0.6147\n",
      "189/500 [==========>...................] - ETA: 5:46 - loss: 2.2218 - regression_loss: 1.6066 - classification_loss: 0.6151\n",
      "190/500 [==========>...................] - ETA: 5:45 - loss: 2.2195 - regression_loss: 1.6052 - classification_loss: 0.6144\n",
      "191/500 [==========>...................] - ETA: 5:44 - loss: 2.2215 - regression_loss: 1.6061 - classification_loss: 0.6154\n",
      "192/500 [==========>...................] - ETA: 5:42 - loss: 2.2213 - regression_loss: 1.6055 - classification_loss: 0.6159\n",
      "193/500 [==========>...................] - ETA: 5:41 - loss: 2.2225 - regression_loss: 1.6070 - classification_loss: 0.6155\n",
      "194/500 [==========>...................] - ETA: 5:40 - loss: 2.2241 - regression_loss: 1.6084 - classification_loss: 0.6157\n",
      "195/500 [==========>...................] - ETA: 5:39 - loss: 2.2251 - regression_loss: 1.6096 - classification_loss: 0.6156\n",
      "196/500 [==========>...................] - ETA: 5:40 - loss: 2.2227 - regression_loss: 1.6080 - classification_loss: 0.6147\n",
      "197/500 [==========>...................] - ETA: 5:39 - loss: 2.2235 - regression_loss: 1.6082 - classification_loss: 0.6153\n",
      "198/500 [==========>...................] - ETA: 5:38 - loss: 2.2269 - regression_loss: 1.6105 - classification_loss: 0.6164\n",
      "199/500 [==========>...................] - ETA: 5:36 - loss: 2.2276 - regression_loss: 1.6109 - classification_loss: 0.6166\n",
      "200/500 [===========>..................] - ETA: 5:35 - loss: 2.2277 - regression_loss: 1.6111 - classification_loss: 0.6166\n",
      "201/500 [===========>..................] - ETA: 5:34 - loss: 2.2335 - regression_loss: 1.6159 - classification_loss: 0.6176\n",
      "202/500 [===========>..................] - ETA: 5:32 - loss: 2.2363 - regression_loss: 1.6188 - classification_loss: 0.6174\n",
      "203/500 [===========>..................] - ETA: 5:31 - loss: 2.2371 - regression_loss: 1.6197 - classification_loss: 0.6175\n",
      "204/500 [===========>..................] - ETA: 5:30 - loss: 2.2373 - regression_loss: 1.6197 - classification_loss: 0.6176\n",
      "205/500 [===========>..................] - ETA: 5:29 - loss: 2.2372 - regression_loss: 1.6202 - classification_loss: 0.6170\n",
      "206/500 [===========>..................] - ETA: 5:27 - loss: 2.2354 - regression_loss: 1.6194 - classification_loss: 0.6159\n",
      "207/500 [===========>..................] - ETA: 5:26 - loss: 2.2316 - regression_loss: 1.6173 - classification_loss: 0.6142\n",
      "208/500 [===========>..................] - ETA: 5:25 - loss: 2.2317 - regression_loss: 1.6171 - classification_loss: 0.6146\n",
      "209/500 [===========>..................] - ETA: 5:23 - loss: 2.2321 - regression_loss: 1.6171 - classification_loss: 0.6150\n",
      "210/500 [===========>..................] - ETA: 5:22 - loss: 2.2363 - regression_loss: 1.6195 - classification_loss: 0.6168\n",
      "211/500 [===========>..................] - ETA: 5:21 - loss: 2.2371 - regression_loss: 1.6207 - classification_loss: 0.6164\n",
      "212/500 [===========>..................] - ETA: 5:20 - loss: 2.2405 - regression_loss: 1.6243 - classification_loss: 0.6162\n",
      "213/500 [===========>..................] - ETA: 5:18 - loss: 2.2417 - regression_loss: 1.6255 - classification_loss: 0.6162\n",
      "214/500 [===========>..................] - ETA: 5:18 - loss: 2.2416 - regression_loss: 1.6261 - classification_loss: 0.6155\n",
      "215/500 [===========>..................] - ETA: 5:16 - loss: 2.2447 - regression_loss: 1.6282 - classification_loss: 0.6165\n",
      "216/500 [===========>..................] - ETA: 5:15 - loss: 2.2442 - regression_loss: 1.6277 - classification_loss: 0.6165\n",
      "217/500 [============>.................] - ETA: 5:14 - loss: 2.2433 - regression_loss: 1.6270 - classification_loss: 0.6163\n",
      "218/500 [============>.................] - ETA: 5:13 - loss: 2.2452 - regression_loss: 1.6287 - classification_loss: 0.6165\n",
      "219/500 [============>.................] - ETA: 5:11 - loss: 2.2493 - regression_loss: 1.6315 - classification_loss: 0.6178\n",
      "220/500 [============>.................] - ETA: 5:10 - loss: 2.2484 - regression_loss: 1.6306 - classification_loss: 0.6178\n",
      "221/500 [============>.................] - ETA: 5:09 - loss: 2.2508 - regression_loss: 1.6327 - classification_loss: 0.6181\n",
      "222/500 [============>.................] - ETA: 5:07 - loss: 2.2510 - regression_loss: 1.6323 - classification_loss: 0.6187\n",
      "223/500 [============>.................] - ETA: 5:06 - loss: 2.2520 - regression_loss: 1.6330 - classification_loss: 0.6190\n",
      "224/500 [============>.................] - ETA: 5:05 - loss: 2.2535 - regression_loss: 1.6341 - classification_loss: 0.6195\n",
      "225/500 [============>.................] - ETA: 5:04 - loss: 2.2546 - regression_loss: 1.6343 - classification_loss: 0.6202\n",
      "226/500 [============>.................] - ETA: 5:03 - loss: 2.2586 - regression_loss: 1.6373 - classification_loss: 0.6213\n",
      "227/500 [============>.................] - ETA: 5:02 - loss: 2.2566 - regression_loss: 1.6359 - classification_loss: 0.6208\n",
      "228/500 [============>.................] - ETA: 5:00 - loss: 2.2570 - regression_loss: 1.6356 - classification_loss: 0.6215\n",
      "229/500 [============>.................] - ETA: 4:59 - loss: 2.2550 - regression_loss: 1.6339 - classification_loss: 0.6211\n",
      "230/500 [============>.................] - ETA: 5:00 - loss: 2.2553 - regression_loss: 1.6340 - classification_loss: 0.6214\n",
      "231/500 [============>.................] - ETA: 4:59 - loss: 2.2547 - regression_loss: 1.6336 - classification_loss: 0.6211\n",
      "232/500 [============>.................] - ETA: 4:58 - loss: 2.2580 - regression_loss: 1.6355 - classification_loss: 0.6225\n",
      "233/500 [============>.................] - ETA: 4:57 - loss: 2.2561 - regression_loss: 1.6339 - classification_loss: 0.6223\n",
      "234/500 [=============>................] - ETA: 4:56 - loss: 2.2589 - regression_loss: 1.6354 - classification_loss: 0.6236\n",
      "235/500 [=============>................] - ETA: 4:54 - loss: 2.2608 - regression_loss: 1.6373 - classification_loss: 0.6236\n",
      "236/500 [=============>................] - ETA: 4:53 - loss: 2.2631 - regression_loss: 1.6392 - classification_loss: 0.6239\n",
      "237/500 [=============>................] - ETA: 4:52 - loss: 2.2602 - regression_loss: 1.6369 - classification_loss: 0.6233\n",
      "238/500 [=============>................] - ETA: 4:51 - loss: 2.2566 - regression_loss: 1.6343 - classification_loss: 0.6223\n",
      "239/500 [=============>................] - ETA: 4:50 - loss: 2.2558 - regression_loss: 1.6339 - classification_loss: 0.6219\n",
      "240/500 [=============>................] - ETA: 4:49 - loss: 2.2561 - regression_loss: 1.6344 - classification_loss: 0.6218\n",
      "241/500 [=============>................] - ETA: 4:47 - loss: 2.2568 - regression_loss: 1.6344 - classification_loss: 0.6223\n",
      "242/500 [=============>................] - ETA: 4:46 - loss: 2.2535 - regression_loss: 1.6321 - classification_loss: 0.6214\n",
      "243/500 [=============>................] - ETA: 4:45 - loss: 2.2533 - regression_loss: 1.6322 - classification_loss: 0.6211\n",
      "244/500 [=============>................] - ETA: 4:44 - loss: 2.2514 - regression_loss: 1.6311 - classification_loss: 0.6203\n",
      "245/500 [=============>................] - ETA: 4:43 - loss: 2.2536 - regression_loss: 1.6324 - classification_loss: 0.6212\n",
      "246/500 [=============>................] - ETA: 4:41 - loss: 2.2540 - regression_loss: 1.6326 - classification_loss: 0.6215\n",
      "247/500 [=============>................] - ETA: 4:40 - loss: 2.2523 - regression_loss: 1.6319 - classification_loss: 0.6203\n",
      "248/500 [=============>................] - ETA: 4:39 - loss: 2.2523 - regression_loss: 1.6324 - classification_loss: 0.6199\n",
      "249/500 [=============>................] - ETA: 4:38 - loss: 2.2540 - regression_loss: 1.6339 - classification_loss: 0.6201\n",
      "250/500 [==============>...............] - ETA: 4:37 - loss: 2.2507 - regression_loss: 1.6310 - classification_loss: 0.6197\n",
      "251/500 [==============>...............] - ETA: 4:35 - loss: 2.2498 - regression_loss: 1.6306 - classification_loss: 0.6192\n",
      "252/500 [==============>...............] - ETA: 4:34 - loss: 2.2486 - regression_loss: 1.6296 - classification_loss: 0.6190\n",
      "253/500 [==============>...............] - ETA: 4:33 - loss: 2.2536 - regression_loss: 1.6337 - classification_loss: 0.6199\n",
      "254/500 [==============>...............] - ETA: 4:32 - loss: 2.2519 - regression_loss: 1.6322 - classification_loss: 0.6197\n",
      "255/500 [==============>...............] - ETA: 4:31 - loss: 2.2528 - regression_loss: 1.6323 - classification_loss: 0.6204\n",
      "256/500 [==============>...............] - ETA: 4:29 - loss: 2.2544 - regression_loss: 1.6334 - classification_loss: 0.6209\n",
      "257/500 [==============>...............] - ETA: 4:28 - loss: 2.2560 - regression_loss: 1.6346 - classification_loss: 0.6215\n",
      "258/500 [==============>...............] - ETA: 4:27 - loss: 2.2581 - regression_loss: 1.6360 - classification_loss: 0.6221\n",
      "259/500 [==============>...............] - ETA: 4:26 - loss: 2.2585 - regression_loss: 1.6361 - classification_loss: 0.6224\n",
      "260/500 [==============>...............] - ETA: 4:25 - loss: 2.2580 - regression_loss: 1.6361 - classification_loss: 0.6219\n",
      "261/500 [==============>...............] - ETA: 4:23 - loss: 2.2599 - regression_loss: 1.6372 - classification_loss: 0.6227\n",
      "262/500 [==============>...............] - ETA: 4:22 - loss: 2.2596 - regression_loss: 1.6373 - classification_loss: 0.6223\n",
      "263/500 [==============>...............] - ETA: 4:21 - loss: 2.2589 - regression_loss: 1.6371 - classification_loss: 0.6218\n",
      "264/500 [==============>...............] - ETA: 4:20 - loss: 2.2573 - regression_loss: 1.6361 - classification_loss: 0.6212\n",
      "265/500 [==============>...............] - ETA: 4:19 - loss: 2.2590 - regression_loss: 1.6376 - classification_loss: 0.6214\n",
      "266/500 [==============>...............] - ETA: 4:18 - loss: 2.2585 - regression_loss: 1.6372 - classification_loss: 0.6213\n",
      "267/500 [===============>..............] - ETA: 4:19 - loss: 2.2570 - regression_loss: 1.6361 - classification_loss: 0.6209\n",
      "268/500 [===============>..............] - ETA: 4:17 - loss: 2.2556 - regression_loss: 1.6351 - classification_loss: 0.6205\n",
      "269/500 [===============>..............] - ETA: 4:16 - loss: 2.2581 - regression_loss: 1.6374 - classification_loss: 0.6207\n",
      "270/500 [===============>..............] - ETA: 4:15 - loss: 2.2601 - regression_loss: 1.6388 - classification_loss: 0.6213\n",
      "271/500 [===============>..............] - ETA: 4:14 - loss: 2.2592 - regression_loss: 1.6384 - classification_loss: 0.6209\n",
      "272/500 [===============>..............] - ETA: 4:13 - loss: 2.2619 - regression_loss: 1.6405 - classification_loss: 0.6214\n",
      "273/500 [===============>..............] - ETA: 4:12 - loss: 2.2628 - regression_loss: 1.6410 - classification_loss: 0.6218\n",
      "274/500 [===============>..............] - ETA: 4:10 - loss: 2.2603 - regression_loss: 1.6391 - classification_loss: 0.6212\n",
      "275/500 [===============>..............] - ETA: 4:09 - loss: 2.2615 - regression_loss: 1.6407 - classification_loss: 0.6208\n",
      "276/500 [===============>..............] - ETA: 4:08 - loss: 2.2599 - regression_loss: 1.6393 - classification_loss: 0.6206\n",
      "277/500 [===============>..............] - ETA: 4:07 - loss: 2.2601 - regression_loss: 1.6397 - classification_loss: 0.6204\n",
      "278/500 [===============>..............] - ETA: 4:06 - loss: 2.2589 - regression_loss: 1.6389 - classification_loss: 0.6201\n",
      "279/500 [===============>..............] - ETA: 4:04 - loss: 2.2588 - regression_loss: 1.6393 - classification_loss: 0.6195\n",
      "280/500 [===============>..............] - ETA: 4:03 - loss: 2.2568 - regression_loss: 1.6381 - classification_loss: 0.6187\n",
      "281/500 [===============>..............] - ETA: 4:02 - loss: 2.2572 - regression_loss: 1.6388 - classification_loss: 0.6184\n",
      "282/500 [===============>..............] - ETA: 4:01 - loss: 2.2554 - regression_loss: 1.6377 - classification_loss: 0.6177\n",
      "283/500 [===============>..............] - ETA: 4:00 - loss: 2.2548 - regression_loss: 1.6371 - classification_loss: 0.6177\n",
      "284/500 [================>.............] - ETA: 3:59 - loss: 2.2545 - regression_loss: 1.6371 - classification_loss: 0.6174\n",
      "285/500 [================>.............] - ETA: 3:57 - loss: 2.2536 - regression_loss: 1.6365 - classification_loss: 0.6171\n",
      "286/500 [================>.............] - ETA: 3:56 - loss: 2.2513 - regression_loss: 1.6351 - classification_loss: 0.6162\n",
      "287/500 [================>.............] - ETA: 3:55 - loss: 2.2546 - regression_loss: 1.6376 - classification_loss: 0.6170\n",
      "288/500 [================>.............] - ETA: 3:54 - loss: 2.2545 - regression_loss: 1.6377 - classification_loss: 0.6169\n",
      "289/500 [================>.............] - ETA: 3:53 - loss: 2.2568 - regression_loss: 1.6387 - classification_loss: 0.6181\n",
      "290/500 [================>.............] - ETA: 3:52 - loss: 2.2569 - regression_loss: 1.6387 - classification_loss: 0.6183\n",
      "291/500 [================>.............] - ETA: 3:50 - loss: 2.2587 - regression_loss: 1.6401 - classification_loss: 0.6186\n",
      "292/500 [================>.............] - ETA: 3:49 - loss: 2.2577 - regression_loss: 1.6395 - classification_loss: 0.6182\n",
      "293/500 [================>.............] - ETA: 3:48 - loss: 2.2579 - regression_loss: 1.6397 - classification_loss: 0.6182\n",
      "294/500 [================>.............] - ETA: 3:47 - loss: 2.2571 - regression_loss: 1.6393 - classification_loss: 0.6178\n",
      "295/500 [================>.............] - ETA: 3:46 - loss: 2.2601 - regression_loss: 1.6424 - classification_loss: 0.6177\n",
      "296/500 [================>.............] - ETA: 3:45 - loss: 2.2590 - regression_loss: 1.6414 - classification_loss: 0.6176\n",
      "297/500 [================>.............] - ETA: 3:43 - loss: 2.2559 - regression_loss: 1.6390 - classification_loss: 0.6169\n",
      "298/500 [================>.............] - ETA: 3:42 - loss: 2.2566 - regression_loss: 1.6396 - classification_loss: 0.6169\n",
      "299/500 [================>.............] - ETA: 3:41 - loss: 2.2549 - regression_loss: 1.6383 - classification_loss: 0.6166\n",
      "300/500 [=================>............] - ETA: 3:40 - loss: 2.2558 - regression_loss: 1.6391 - classification_loss: 0.6167\n",
      "301/500 [=================>............] - ETA: 3:39 - loss: 2.2575 - regression_loss: 1.6404 - classification_loss: 0.6170\n",
      "302/500 [=================>............] - ETA: 3:37 - loss: 2.2560 - regression_loss: 1.6392 - classification_loss: 0.6168\n",
      "303/500 [=================>............] - ETA: 3:36 - loss: 2.2562 - regression_loss: 1.6390 - classification_loss: 0.6172\n",
      "304/500 [=================>............] - ETA: 3:35 - loss: 2.2551 - regression_loss: 1.6383 - classification_loss: 0.6169\n",
      "305/500 [=================>............] - ETA: 3:34 - loss: 2.2551 - regression_loss: 1.6381 - classification_loss: 0.6170\n",
      "306/500 [=================>............] - ETA: 3:33 - loss: 2.2543 - regression_loss: 1.6372 - classification_loss: 0.6172\n",
      "307/500 [=================>............] - ETA: 3:32 - loss: 2.2546 - regression_loss: 1.6375 - classification_loss: 0.6172\n",
      "308/500 [=================>............] - ETA: 3:32 - loss: 2.2550 - regression_loss: 1.6369 - classification_loss: 0.6181\n",
      "309/500 [=================>............] - ETA: 3:31 - loss: 2.2573 - regression_loss: 1.6391 - classification_loss: 0.6182\n",
      "310/500 [=================>............] - ETA: 3:30 - loss: 2.2581 - regression_loss: 1.6396 - classification_loss: 0.6185\n",
      "311/500 [=================>............] - ETA: 3:29 - loss: 2.2593 - regression_loss: 1.6401 - classification_loss: 0.6191\n",
      "312/500 [=================>............] - ETA: 3:28 - loss: 2.2613 - regression_loss: 1.6418 - classification_loss: 0.6195\n",
      "313/500 [=================>............] - ETA: 3:27 - loss: 2.2612 - regression_loss: 1.6417 - classification_loss: 0.6195\n",
      "314/500 [=================>............] - ETA: 3:26 - loss: 2.2644 - regression_loss: 1.6439 - classification_loss: 0.6205\n",
      "315/500 [=================>............] - ETA: 3:25 - loss: 2.2617 - regression_loss: 1.6417 - classification_loss: 0.6200\n",
      "316/500 [=================>............] - ETA: 3:24 - loss: 2.2613 - regression_loss: 1.6414 - classification_loss: 0.6199\n",
      "317/500 [==================>...........] - ETA: 3:23 - loss: 2.2605 - regression_loss: 1.6408 - classification_loss: 0.6197\n",
      "318/500 [==================>...........] - ETA: 3:22 - loss: 2.2591 - regression_loss: 1.6397 - classification_loss: 0.6194\n",
      "319/500 [==================>...........] - ETA: 3:20 - loss: 2.2603 - regression_loss: 1.6404 - classification_loss: 0.6199\n",
      "320/500 [==================>...........] - ETA: 3:19 - loss: 2.2595 - regression_loss: 1.6397 - classification_loss: 0.6198\n",
      "321/500 [==================>...........] - ETA: 3:18 - loss: 2.2580 - regression_loss: 1.6389 - classification_loss: 0.6191\n",
      "322/500 [==================>...........] - ETA: 3:17 - loss: 2.2594 - regression_loss: 1.6399 - classification_loss: 0.6195\n",
      "323/500 [==================>...........] - ETA: 3:16 - loss: 2.2564 - regression_loss: 1.6377 - classification_loss: 0.6186\n",
      "324/500 [==================>...........] - ETA: 3:15 - loss: 2.2560 - regression_loss: 1.6374 - classification_loss: 0.6187\n",
      "325/500 [==================>...........] - ETA: 3:14 - loss: 2.2593 - regression_loss: 1.6390 - classification_loss: 0.6203\n",
      "326/500 [==================>...........] - ETA: 3:13 - loss: 2.2599 - regression_loss: 1.6393 - classification_loss: 0.6205\n",
      "327/500 [==================>...........] - ETA: 3:11 - loss: 2.2595 - regression_loss: 1.6388 - classification_loss: 0.6207\n",
      "328/500 [==================>...........] - ETA: 3:10 - loss: 2.2597 - regression_loss: 1.6390 - classification_loss: 0.6207\n",
      "329/500 [==================>...........] - ETA: 3:09 - loss: 2.2593 - regression_loss: 1.6386 - classification_loss: 0.6207\n",
      "330/500 [==================>...........] - ETA: 3:08 - loss: 2.2573 - regression_loss: 1.6368 - classification_loss: 0.6205\n",
      "331/500 [==================>...........] - ETA: 3:07 - loss: 2.2580 - regression_loss: 1.6372 - classification_loss: 0.6207\n",
      "332/500 [==================>...........] - ETA: 3:06 - loss: 2.2568 - regression_loss: 1.6363 - classification_loss: 0.6204\n",
      "333/500 [==================>...........] - ETA: 3:05 - loss: 2.2566 - regression_loss: 1.6362 - classification_loss: 0.6203\n",
      "334/500 [===================>..........] - ETA: 3:04 - loss: 2.2575 - regression_loss: 1.6370 - classification_loss: 0.6205\n",
      "335/500 [===================>..........] - ETA: 3:02 - loss: 2.2582 - regression_loss: 1.6376 - classification_loss: 0.6206\n",
      "336/500 [===================>..........] - ETA: 3:01 - loss: 2.2592 - regression_loss: 1.6386 - classification_loss: 0.6206\n",
      "337/500 [===================>..........] - ETA: 3:00 - loss: 2.2577 - regression_loss: 1.6380 - classification_loss: 0.6197\n",
      "338/500 [===================>..........] - ETA: 2:59 - loss: 2.2572 - regression_loss: 1.6375 - classification_loss: 0.6197\n",
      "339/500 [===================>..........] - ETA: 2:58 - loss: 2.2567 - regression_loss: 1.6374 - classification_loss: 0.6193\n",
      "340/500 [===================>..........] - ETA: 2:57 - loss: 2.2563 - regression_loss: 1.6374 - classification_loss: 0.6189\n",
      "341/500 [===================>..........] - ETA: 2:55 - loss: 2.2557 - regression_loss: 1.6366 - classification_loss: 0.6191\n",
      "342/500 [===================>..........] - ETA: 2:54 - loss: 2.2575 - regression_loss: 1.6382 - classification_loss: 0.6193\n",
      "343/500 [===================>..........] - ETA: 2:54 - loss: 2.2579 - regression_loss: 1.6381 - classification_loss: 0.6198\n",
      "344/500 [===================>..........] - ETA: 2:53 - loss: 2.2568 - regression_loss: 1.6373 - classification_loss: 0.6195\n",
      "345/500 [===================>..........] - ETA: 2:52 - loss: 2.2571 - regression_loss: 1.6377 - classification_loss: 0.6193\n",
      "346/500 [===================>..........] - ETA: 2:51 - loss: 2.2571 - regression_loss: 1.6380 - classification_loss: 0.6192\n",
      "347/500 [===================>..........] - ETA: 2:50 - loss: 2.2578 - regression_loss: 1.6383 - classification_loss: 0.6195\n",
      "348/500 [===================>..........] - ETA: 2:48 - loss: 2.2586 - regression_loss: 1.6384 - classification_loss: 0.6201\n",
      "349/500 [===================>..........] - ETA: 2:48 - loss: 2.2578 - regression_loss: 1.6380 - classification_loss: 0.6199\n",
      "350/500 [====================>.........] - ETA: 2:47 - loss: 2.2552 - regression_loss: 1.6360 - classification_loss: 0.6192\n",
      "351/500 [====================>.........] - ETA: 2:46 - loss: 2.2520 - regression_loss: 1.6335 - classification_loss: 0.6185\n",
      "352/500 [====================>.........] - ETA: 2:45 - loss: 2.2546 - regression_loss: 1.6353 - classification_loss: 0.6193\n",
      "353/500 [====================>.........] - ETA: 2:44 - loss: 2.2570 - regression_loss: 1.6366 - classification_loss: 0.6204\n",
      "354/500 [====================>.........] - ETA: 2:42 - loss: 2.2560 - regression_loss: 1.6361 - classification_loss: 0.6200\n",
      "355/500 [====================>.........] - ETA: 2:41 - loss: 2.2562 - regression_loss: 1.6362 - classification_loss: 0.6201\n",
      "356/500 [====================>.........] - ETA: 2:40 - loss: 2.2557 - regression_loss: 1.6357 - classification_loss: 0.6199\n",
      "357/500 [====================>.........] - ETA: 2:41 - loss: 2.2565 - regression_loss: 1.6364 - classification_loss: 0.6201\n",
      "358/500 [====================>.........] - ETA: 2:40 - loss: 2.2580 - regression_loss: 1.6376 - classification_loss: 0.6204\n",
      "359/500 [====================>.........] - ETA: 2:39 - loss: 2.2573 - regression_loss: 1.6372 - classification_loss: 0.6201\n",
      "360/500 [====================>.........] - ETA: 2:38 - loss: 2.2577 - regression_loss: 1.6373 - classification_loss: 0.6204\n",
      "361/500 [====================>.........] - ETA: 2:37 - loss: 2.2560 - regression_loss: 1.6356 - classification_loss: 0.6204\n",
      "362/500 [====================>.........] - ETA: 2:36 - loss: 2.2553 - regression_loss: 1.6351 - classification_loss: 0.6202\n",
      "363/500 [====================>.........] - ETA: 2:34 - loss: 2.2574 - regression_loss: 1.6366 - classification_loss: 0.6208\n",
      "364/500 [====================>.........] - ETA: 2:33 - loss: 2.2546 - regression_loss: 1.6346 - classification_loss: 0.6200\n",
      "365/500 [====================>.........] - ETA: 2:32 - loss: 2.2541 - regression_loss: 1.6342 - classification_loss: 0.6199\n",
      "366/500 [====================>.........] - ETA: 2:31 - loss: 2.2544 - regression_loss: 1.6343 - classification_loss: 0.6201\n",
      "367/500 [=====================>........] - ETA: 2:30 - loss: 2.2527 - regression_loss: 1.6331 - classification_loss: 0.6196\n",
      "368/500 [=====================>........] - ETA: 2:28 - loss: 2.2523 - regression_loss: 1.6329 - classification_loss: 0.6195\n",
      "369/500 [=====================>........] - ETA: 2:27 - loss: 2.2516 - regression_loss: 1.6324 - classification_loss: 0.6193\n",
      "370/500 [=====================>........] - ETA: 2:26 - loss: 2.2503 - regression_loss: 1.6312 - classification_loss: 0.6191\n",
      "371/500 [=====================>........] - ETA: 2:25 - loss: 2.2503 - regression_loss: 1.6313 - classification_loss: 0.6191\n",
      "372/500 [=====================>........] - ETA: 2:24 - loss: 2.2503 - regression_loss: 1.6312 - classification_loss: 0.6191\n",
      "373/500 [=====================>........] - ETA: 2:23 - loss: 2.2503 - regression_loss: 1.6312 - classification_loss: 0.6191\n",
      "374/500 [=====================>........] - ETA: 2:22 - loss: 2.2502 - regression_loss: 1.6308 - classification_loss: 0.6194\n",
      "375/500 [=====================>........] - ETA: 2:20 - loss: 2.2482 - regression_loss: 1.6294 - classification_loss: 0.6188\n",
      "376/500 [=====================>........] - ETA: 2:19 - loss: 2.2477 - regression_loss: 1.6293 - classification_loss: 0.6184\n",
      "377/500 [=====================>........] - ETA: 2:18 - loss: 2.2475 - regression_loss: 1.6290 - classification_loss: 0.6185\n",
      "378/500 [=====================>........] - ETA: 2:17 - loss: 2.2462 - regression_loss: 1.6280 - classification_loss: 0.6182\n",
      "379/500 [=====================>........] - ETA: 2:16 - loss: 2.2466 - regression_loss: 1.6286 - classification_loss: 0.6180\n",
      "380/500 [=====================>........] - ETA: 2:14 - loss: 2.2456 - regression_loss: 1.6281 - classification_loss: 0.6175\n",
      "381/500 [=====================>........] - ETA: 2:13 - loss: 2.2466 - regression_loss: 1.6291 - classification_loss: 0.6175\n",
      "382/500 [=====================>........] - ETA: 2:12 - loss: 2.2456 - regression_loss: 1.6279 - classification_loss: 0.6177\n",
      "383/500 [=====================>........] - ETA: 2:11 - loss: 2.2433 - regression_loss: 1.6260 - classification_loss: 0.6173\n",
      "384/500 [======================>.......] - ETA: 2:10 - loss: 2.2425 - regression_loss: 1.6254 - classification_loss: 0.6171\n",
      "385/500 [======================>.......] - ETA: 2:09 - loss: 2.2429 - regression_loss: 1.6258 - classification_loss: 0.6170\n",
      "386/500 [======================>.......] - ETA: 2:08 - loss: 2.2409 - regression_loss: 1.6243 - classification_loss: 0.6166\n",
      "387/500 [======================>.......] - ETA: 2:06 - loss: 2.2402 - regression_loss: 1.6238 - classification_loss: 0.6163\n",
      "388/500 [======================>.......] - ETA: 2:05 - loss: 2.2413 - regression_loss: 1.6250 - classification_loss: 0.6163\n",
      "389/500 [======================>.......] - ETA: 2:04 - loss: 2.2410 - regression_loss: 1.6247 - classification_loss: 0.6163\n",
      "390/500 [======================>.......] - ETA: 2:03 - loss: 2.2391 - regression_loss: 1.6229 - classification_loss: 0.6162\n",
      "391/500 [======================>.......] - ETA: 2:02 - loss: 2.2358 - regression_loss: 1.6204 - classification_loss: 0.6154\n",
      "392/500 [======================>.......] - ETA: 2:01 - loss: 2.2379 - regression_loss: 1.6217 - classification_loss: 0.6162\n",
      "393/500 [======================>.......] - ETA: 2:00 - loss: 2.2382 - regression_loss: 1.6222 - classification_loss: 0.6159\n",
      "394/500 [======================>.......] - ETA: 1:58 - loss: 2.2377 - regression_loss: 1.6221 - classification_loss: 0.6157\n",
      "395/500 [======================>.......] - ETA: 1:57 - loss: 2.2387 - regression_loss: 1.6221 - classification_loss: 0.6166\n",
      "396/500 [======================>.......] - ETA: 1:56 - loss: 2.2367 - regression_loss: 1.6206 - classification_loss: 0.6161\n",
      "397/500 [======================>.......] - ETA: 1:55 - loss: 2.2378 - regression_loss: 1.6212 - classification_loss: 0.6166\n",
      "398/500 [======================>.......] - ETA: 1:54 - loss: 2.2383 - regression_loss: 1.6218 - classification_loss: 0.6165\n",
      "399/500 [======================>.......] - ETA: 1:53 - loss: 2.2366 - regression_loss: 1.6207 - classification_loss: 0.6159\n",
      "400/500 [=======================>......] - ETA: 1:52 - loss: 2.2355 - regression_loss: 1.6199 - classification_loss: 0.6156\n",
      "401/500 [=======================>......] - ETA: 1:50 - loss: 2.2331 - regression_loss: 1.6178 - classification_loss: 0.6153\n",
      "402/500 [=======================>......] - ETA: 1:49 - loss: 2.2327 - regression_loss: 1.6176 - classification_loss: 0.6151\n",
      "403/500 [=======================>......] - ETA: 1:48 - loss: 2.2311 - regression_loss: 1.6161 - classification_loss: 0.6150\n",
      "404/500 [=======================>......] - ETA: 1:47 - loss: 2.2315 - regression_loss: 1.6165 - classification_loss: 0.6150\n",
      "405/500 [=======================>......] - ETA: 1:46 - loss: 2.2308 - regression_loss: 1.6164 - classification_loss: 0.6145\n",
      "406/500 [=======================>......] - ETA: 1:45 - loss: 2.2317 - regression_loss: 1.6172 - classification_loss: 0.6145\n",
      "407/500 [=======================>......] - ETA: 1:44 - loss: 2.2307 - regression_loss: 1.6168 - classification_loss: 0.6139\n",
      "408/500 [=======================>......] - ETA: 1:42 - loss: 2.2306 - regression_loss: 1.6167 - classification_loss: 0.6139\n",
      "409/500 [=======================>......] - ETA: 1:41 - loss: 2.2303 - regression_loss: 1.6166 - classification_loss: 0.6137\n",
      "410/500 [=======================>......] - ETA: 1:40 - loss: 2.2300 - regression_loss: 1.6167 - classification_loss: 0.6132\n",
      "411/500 [=======================>......] - ETA: 1:39 - loss: 2.2290 - regression_loss: 1.6162 - classification_loss: 0.6128\n",
      "412/500 [=======================>......] - ETA: 1:38 - loss: 2.2281 - regression_loss: 1.6154 - classification_loss: 0.6127\n",
      "413/500 [=======================>......] - ETA: 1:37 - loss: 2.2276 - regression_loss: 1.6149 - classification_loss: 0.6127\n",
      "414/500 [=======================>......] - ETA: 1:36 - loss: 2.2253 - regression_loss: 1.6132 - classification_loss: 0.6122\n",
      "415/500 [=======================>......] - ETA: 1:35 - loss: 2.2263 - regression_loss: 1.6140 - classification_loss: 0.6122\n",
      "416/500 [=======================>......] - ETA: 1:34 - loss: 2.2273 - regression_loss: 1.6152 - classification_loss: 0.6121\n",
      "417/500 [========================>.....] - ETA: 1:33 - loss: 2.2292 - regression_loss: 1.6168 - classification_loss: 0.6124\n",
      "418/500 [========================>.....] - ETA: 1:32 - loss: 2.2281 - regression_loss: 1.6163 - classification_loss: 0.6117\n",
      "419/500 [========================>.....] - ETA: 1:31 - loss: 2.2298 - regression_loss: 1.6175 - classification_loss: 0.6123\n",
      "420/500 [========================>.....] - ETA: 1:29 - loss: 2.2300 - regression_loss: 1.6179 - classification_loss: 0.6120\n",
      "421/500 [========================>.....] - ETA: 1:28 - loss: 2.2290 - regression_loss: 1.6170 - classification_loss: 0.6120\n",
      "422/500 [========================>.....] - ETA: 1:27 - loss: 2.2308 - regression_loss: 1.6186 - classification_loss: 0.6122\n",
      "423/500 [========================>.....] - ETA: 1:26 - loss: 2.2308 - regression_loss: 1.6184 - classification_loss: 0.6123\n",
      "424/500 [========================>.....] - ETA: 1:25 - loss: 2.2295 - regression_loss: 1.6171 - classification_loss: 0.6123\n",
      "425/500 [========================>.....] - ETA: 1:24 - loss: 2.2316 - regression_loss: 1.6185 - classification_loss: 0.6131\n",
      "426/500 [========================>.....] - ETA: 1:23 - loss: 2.2327 - regression_loss: 1.6197 - classification_loss: 0.6129\n",
      "427/500 [========================>.....] - ETA: 1:21 - loss: 2.2344 - regression_loss: 1.6210 - classification_loss: 0.6134\n",
      "428/500 [========================>.....] - ETA: 1:20 - loss: 2.2370 - regression_loss: 1.6235 - classification_loss: 0.6135\n",
      "429/500 [========================>.....] - ETA: 1:19 - loss: 2.2362 - regression_loss: 1.6229 - classification_loss: 0.6132\n",
      "430/500 [========================>.....] - ETA: 1:18 - loss: 2.2357 - regression_loss: 1.6225 - classification_loss: 0.6132\n",
      "431/500 [========================>.....] - ETA: 1:17 - loss: 2.2343 - regression_loss: 1.6214 - classification_loss: 0.6129\n",
      "432/500 [========================>.....] - ETA: 1:16 - loss: 2.2357 - regression_loss: 1.6226 - classification_loss: 0.6132\n",
      "433/500 [========================>.....] - ETA: 1:15 - loss: 2.2342 - regression_loss: 1.6215 - classification_loss: 0.6127\n",
      "434/500 [=========================>....] - ETA: 1:13 - loss: 2.2336 - regression_loss: 1.6213 - classification_loss: 0.6124\n",
      "435/500 [=========================>....] - ETA: 1:12 - loss: 2.2328 - regression_loss: 1.6208 - classification_loss: 0.6120\n",
      "436/500 [=========================>....] - ETA: 1:11 - loss: 2.2305 - regression_loss: 1.6190 - classification_loss: 0.6115\n",
      "437/500 [=========================>....] - ETA: 1:10 - loss: 2.2312 - regression_loss: 1.6194 - classification_loss: 0.6118\n",
      "438/500 [=========================>....] - ETA: 1:09 - loss: 2.2312 - regression_loss: 1.6196 - classification_loss: 0.6116\n",
      "439/500 [=========================>....] - ETA: 1:08 - loss: 2.2324 - regression_loss: 1.6197 - classification_loss: 0.6127\n",
      "440/500 [=========================>....] - ETA: 1:07 - loss: 2.2313 - regression_loss: 1.6188 - classification_loss: 0.6125\n",
      "441/500 [=========================>....] - ETA: 1:06 - loss: 2.2303 - regression_loss: 1.6181 - classification_loss: 0.6122\n",
      "442/500 [=========================>....] - ETA: 1:04 - loss: 2.2317 - regression_loss: 1.6191 - classification_loss: 0.6126\n",
      "443/500 [=========================>....] - ETA: 1:03 - loss: 2.2309 - regression_loss: 1.6180 - classification_loss: 0.6129\n",
      "444/500 [=========================>....] - ETA: 1:02 - loss: 2.2319 - regression_loss: 1.6188 - classification_loss: 0.6131\n",
      "445/500 [=========================>....] - ETA: 1:01 - loss: 2.2337 - regression_loss: 1.6201 - classification_loss: 0.6136\n",
      "446/500 [=========================>....] - ETA: 1:00 - loss: 2.2319 - regression_loss: 1.6188 - classification_loss: 0.6131\n",
      "447/500 [=========================>....] - ETA: 59s - loss: 2.2311 - regression_loss: 1.6181 - classification_loss: 0.6130 \n",
      "448/500 [=========================>....] - ETA: 58s - loss: 2.2304 - regression_loss: 1.6175 - classification_loss: 0.6129\n",
      "449/500 [=========================>....] - ETA: 57s - loss: 2.2321 - regression_loss: 1.6188 - classification_loss: 0.6133\n",
      "450/500 [==========================>...] - ETA: 55s - loss: 2.2314 - regression_loss: 1.6179 - classification_loss: 0.6135\n",
      "451/500 [==========================>...] - ETA: 54s - loss: 2.2322 - regression_loss: 1.6187 - classification_loss: 0.6134\n",
      "452/500 [==========================>...] - ETA: 53s - loss: 2.2309 - regression_loss: 1.6180 - classification_loss: 0.6129\n",
      "453/500 [==========================>...] - ETA: 52s - loss: 2.2312 - regression_loss: 1.6181 - classification_loss: 0.6131\n",
      "454/500 [==========================>...] - ETA: 51s - loss: 2.2319 - regression_loss: 1.6188 - classification_loss: 0.6131\n",
      "455/500 [==========================>...] - ETA: 50s - loss: 2.2319 - regression_loss: 1.6186 - classification_loss: 0.6133\n",
      "456/500 [==========================>...] - ETA: 49s - loss: 2.2324 - regression_loss: 1.6193 - classification_loss: 0.6131\n",
      "457/500 [==========================>...] - ETA: 48s - loss: 2.2324 - regression_loss: 1.6193 - classification_loss: 0.6131\n",
      "458/500 [==========================>...] - ETA: 46s - loss: 2.2316 - regression_loss: 1.6188 - classification_loss: 0.6128\n",
      "459/500 [==========================>...] - ETA: 45s - loss: 2.2302 - regression_loss: 1.6180 - classification_loss: 0.6122\n",
      "460/500 [==========================>...] - ETA: 44s - loss: 2.2289 - regression_loss: 1.6169 - classification_loss: 0.6119\n",
      "461/500 [==========================>...] - ETA: 43s - loss: 2.2274 - regression_loss: 1.6159 - classification_loss: 0.6115\n",
      "462/500 [==========================>...] - ETA: 42s - loss: 2.2266 - regression_loss: 1.6154 - classification_loss: 0.6112\n",
      "463/500 [==========================>...] - ETA: 41s - loss: 2.2262 - regression_loss: 1.6150 - classification_loss: 0.6112\n",
      "464/500 [==========================>...] - ETA: 40s - loss: 2.2255 - regression_loss: 1.6145 - classification_loss: 0.6109\n",
      "465/500 [==========================>...] - ETA: 39s - loss: 2.2235 - regression_loss: 1.6129 - classification_loss: 0.6106\n",
      "466/500 [==========================>...] - ETA: 37s - loss: 2.2238 - regression_loss: 1.6133 - classification_loss: 0.6105\n",
      "467/500 [===========================>..] - ETA: 36s - loss: 2.2222 - regression_loss: 1.6120 - classification_loss: 0.6103\n",
      "468/500 [===========================>..] - ETA: 35s - loss: 2.2202 - regression_loss: 1.6105 - classification_loss: 0.6096\n",
      "469/500 [===========================>..] - ETA: 34s - loss: 2.2183 - regression_loss: 1.6090 - classification_loss: 0.6093\n",
      "470/500 [===========================>..] - ETA: 33s - loss: 2.2179 - regression_loss: 1.6085 - classification_loss: 0.6094\n",
      "471/500 [===========================>..] - ETA: 32s - loss: 2.2166 - regression_loss: 1.6072 - classification_loss: 0.6094\n",
      "472/500 [===========================>..] - ETA: 31s - loss: 2.2173 - regression_loss: 1.6082 - classification_loss: 0.6091\n",
      "473/500 [===========================>..] - ETA: 30s - loss: 2.2159 - regression_loss: 1.6073 - classification_loss: 0.6086\n",
      "474/500 [===========================>..] - ETA: 28s - loss: 2.2167 - regression_loss: 1.6077 - classification_loss: 0.6090\n",
      "475/500 [===========================>..] - ETA: 27s - loss: 2.2165 - regression_loss: 1.6076 - classification_loss: 0.6089\n",
      "476/500 [===========================>..] - ETA: 26s - loss: 2.2172 - regression_loss: 1.6080 - classification_loss: 0.6092\n",
      "477/500 [===========================>..] - ETA: 25s - loss: 2.2174 - regression_loss: 1.6078 - classification_loss: 0.6096\n",
      "478/500 [===========================>..] - ETA: 24s - loss: 2.2175 - regression_loss: 1.6081 - classification_loss: 0.6093\n",
      "479/500 [===========================>..] - ETA: 23s - loss: 2.2177 - regression_loss: 1.6088 - classification_loss: 0.6090\n",
      "480/500 [===========================>..] - ETA: 22s - loss: 2.2185 - regression_loss: 1.6094 - classification_loss: 0.6091\n",
      "481/500 [===========================>..] - ETA: 21s - loss: 2.2177 - regression_loss: 1.6090 - classification_loss: 0.6087\n",
      "482/500 [===========================>..] - ETA: 20s - loss: 2.2170 - regression_loss: 1.6086 - classification_loss: 0.6084\n",
      "483/500 [===========================>..] - ETA: 18s - loss: 2.2170 - regression_loss: 1.6085 - classification_loss: 0.6085\n",
      "484/500 [============================>.] - ETA: 17s - loss: 2.2163 - regression_loss: 1.6083 - classification_loss: 0.6080\n",
      "485/500 [============================>.] - ETA: 16s - loss: 2.2154 - regression_loss: 1.6079 - classification_loss: 0.6076\n",
      "486/500 [============================>.] - ETA: 15s - loss: 2.2162 - regression_loss: 1.6086 - classification_loss: 0.6075\n",
      "487/500 [============================>.] - ETA: 14s - loss: 2.2156 - regression_loss: 1.6084 - classification_loss: 0.6072\n",
      "488/500 [============================>.] - ETA: 13s - loss: 2.2155 - regression_loss: 1.6084 - classification_loss: 0.6071\n",
      "489/500 [============================>.] - ETA: 12s - loss: 2.2147 - regression_loss: 1.6078 - classification_loss: 0.6068\n",
      "490/500 [============================>.] - ETA: 11s - loss: 2.2167 - regression_loss: 1.6096 - classification_loss: 0.6071\n",
      "491/500 [============================>.] - ETA: 10s - loss: 2.2172 - regression_loss: 1.6098 - classification_loss: 0.6074\n",
      "492/500 [============================>.] - ETA: 8s - loss: 2.2183 - regression_loss: 1.6106 - classification_loss: 0.6077 \n",
      "493/500 [============================>.] - ETA: 7s - loss: 2.2200 - regression_loss: 1.6118 - classification_loss: 0.6083\n",
      "494/500 [============================>.] - ETA: 6s - loss: 2.2206 - regression_loss: 1.6124 - classification_loss: 0.6083\n",
      "495/500 [============================>.] - ETA: 5s - loss: 2.2207 - regression_loss: 1.6126 - classification_loss: 0.6081\n",
      "496/500 [============================>.] - ETA: 4s - loss: 2.2204 - regression_loss: 1.6125 - classification_loss: 0.6080\n",
      "497/500 [============================>.] - ETA: 3s - loss: 2.2205 - regression_loss: 1.6128 - classification_loss: 0.6077\n",
      "498/500 [============================>.] - ETA: 2s - loss: 2.2199 - regression_loss: 1.6123 - classification_loss: 0.6076\n",
      "499/500 [============================>.] - ETA: 1s - loss: 2.2189 - regression_loss: 1.6118 - classification_loss: 0.6071\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.2175 - regression_loss: 1.6107 - classification_loss: 0.6068\n",
      "Epoch 00003: saving model to ./snapshots\\resnet50_csv_03.h5\n",
      "\n",
      "500/500 [==============================] - 555s 1s/step - loss: 2.2175 - regression_loss: 1.6107 - classification_loss: 0.6068\n",
      "Epoch 4/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 2.6702 - regression_loss: 1.9473 - classification_loss: 0.7229\n",
      "  2/500 [..............................] - ETA: 4:02 - loss: 2.3628 - regression_loss: 1.6948 - classification_loss: 0.6680\n",
      "  3/500 [..............................] - ETA: 4:56 - loss: 2.0941 - regression_loss: 1.4933 - classification_loss: 0.6008\n",
      "  4/500 [..............................] - ETA: 6:15 - loss: 2.0512 - regression_loss: 1.4750 - classification_loss: 0.5762\n",
      "  5/500 [..............................] - ETA: 6:48 - loss: 2.1153 - regression_loss: 1.5212 - classification_loss: 0.5941\n",
      "  6/500 [..............................] - ETA: 7:00 - loss: 2.2608 - regression_loss: 1.6346 - classification_loss: 0.6262\n",
      "  7/500 [..............................] - ETA: 6:53 - loss: 2.1549 - regression_loss: 1.5339 - classification_loss: 0.6210\n",
      "  8/500 [..............................] - ETA: 7:11 - loss: 2.1692 - regression_loss: 1.5567 - classification_loss: 0.6125\n",
      "  9/500 [..............................] - ETA: 7:15 - loss: 2.2010 - regression_loss: 1.5710 - classification_loss: 0.6299\n",
      " 10/500 [..............................] - ETA: 7:20 - loss: 2.1201 - regression_loss: 1.5221 - classification_loss: 0.5980\n",
      " 11/500 [..............................] - ETA: 7:28 - loss: 2.1427 - regression_loss: 1.5427 - classification_loss: 0.6000\n",
      " 12/500 [..............................] - ETA: 7:35 - loss: 2.2214 - regression_loss: 1.6045 - classification_loss: 0.6169\n",
      " 13/500 [..............................] - ETA: 7:36 - loss: 2.2105 - regression_loss: 1.5847 - classification_loss: 0.6258\n",
      " 14/500 [..............................] - ETA: 7:41 - loss: 2.1991 - regression_loss: 1.5843 - classification_loss: 0.6149\n",
      " 15/500 [..............................] - ETA: 7:41 - loss: 2.2113 - regression_loss: 1.5854 - classification_loss: 0.6259\n",
      " 16/500 [..............................] - ETA: 7:41 - loss: 2.1905 - regression_loss: 1.5746 - classification_loss: 0.6159\n",
      " 17/500 [>.............................] - ETA: 7:41 - loss: 2.2012 - regression_loss: 1.5862 - classification_loss: 0.6150\n",
      " 18/500 [>.............................] - ETA: 7:37 - loss: 2.1945 - regression_loss: 1.5758 - classification_loss: 0.6188\n",
      " 19/500 [>.............................] - ETA: 7:43 - loss: 2.1818 - regression_loss: 1.5649 - classification_loss: 0.6169\n",
      " 20/500 [>.............................] - ETA: 7:45 - loss: 2.1542 - regression_loss: 1.5382 - classification_loss: 0.6160\n",
      " 21/500 [>.............................] - ETA: 7:47 - loss: 2.1819 - regression_loss: 1.5617 - classification_loss: 0.6202\n",
      " 22/500 [>.............................] - ETA: 7:46 - loss: 2.1745 - regression_loss: 1.5578 - classification_loss: 0.6167\n",
      " 23/500 [>.............................] - ETA: 7:45 - loss: 2.1740 - regression_loss: 1.5566 - classification_loss: 0.6174\n",
      " 24/500 [>.............................] - ETA: 7:46 - loss: 2.1733 - regression_loss: 1.5591 - classification_loss: 0.6142\n",
      " 25/500 [>.............................] - ETA: 7:47 - loss: 2.1773 - regression_loss: 1.5644 - classification_loss: 0.6130\n",
      " 26/500 [>.............................] - ETA: 7:46 - loss: 2.1953 - regression_loss: 1.5778 - classification_loss: 0.6174\n",
      " 27/500 [>.............................] - ETA: 7:45 - loss: 2.1798 - regression_loss: 1.5655 - classification_loss: 0.6142\n",
      " 28/500 [>.............................] - ETA: 7:44 - loss: 2.1708 - regression_loss: 1.5610 - classification_loss: 0.6098\n",
      " 29/500 [>.............................] - ETA: 7:43 - loss: 2.1715 - regression_loss: 1.5615 - classification_loss: 0.6100\n",
      " 30/500 [>.............................] - ETA: 7:42 - loss: 2.1547 - regression_loss: 1.5522 - classification_loss: 0.6025\n",
      " 31/500 [>.............................] - ETA: 8:39 - loss: 2.1739 - regression_loss: 1.5643 - classification_loss: 0.6095\n",
      " 32/500 [>.............................] - ETA: 9:10 - loss: 2.1570 - regression_loss: 1.5561 - classification_loss: 0.6009\n",
      " 33/500 [>.............................] - ETA: 9:08 - loss: 2.1924 - regression_loss: 1.5871 - classification_loss: 0.6053\n",
      " 34/500 [=>............................] - ETA: 9:04 - loss: 2.2015 - regression_loss: 1.5908 - classification_loss: 0.6107\n",
      " 35/500 [=>............................] - ETA: 9:02 - loss: 2.2183 - regression_loss: 1.6031 - classification_loss: 0.6151\n",
      " 36/500 [=>............................] - ETA: 8:58 - loss: 2.2217 - regression_loss: 1.6104 - classification_loss: 0.6113\n",
      " 37/500 [=>............................] - ETA: 8:56 - loss: 2.2358 - regression_loss: 1.6219 - classification_loss: 0.6139\n",
      " 38/500 [=>............................] - ETA: 8:53 - loss: 2.2416 - regression_loss: 1.6241 - classification_loss: 0.6175\n",
      " 39/500 [=>............................] - ETA: 8:50 - loss: 2.2554 - regression_loss: 1.6404 - classification_loss: 0.6150\n",
      " 40/500 [=>............................] - ETA: 8:47 - loss: 2.2672 - regression_loss: 1.6504 - classification_loss: 0.6168\n",
      " 41/500 [=>............................] - ETA: 8:45 - loss: 2.2733 - regression_loss: 1.6547 - classification_loss: 0.6186\n",
      " 42/500 [=>............................] - ETA: 8:42 - loss: 2.2574 - regression_loss: 1.6442 - classification_loss: 0.6132\n",
      " 43/500 [=>............................] - ETA: 8:40 - loss: 2.2579 - regression_loss: 1.6417 - classification_loss: 0.6161\n",
      " 44/500 [=>............................] - ETA: 8:37 - loss: 2.2601 - regression_loss: 1.6448 - classification_loss: 0.6153\n",
      " 45/500 [=>............................] - ETA: 8:34 - loss: 2.2517 - regression_loss: 1.6369 - classification_loss: 0.6147\n",
      " 46/500 [=>............................] - ETA: 8:32 - loss: 2.2522 - regression_loss: 1.6348 - classification_loss: 0.6174\n",
      " 47/500 [=>............................] - ETA: 8:30 - loss: 2.2523 - regression_loss: 1.6369 - classification_loss: 0.6153\n",
      " 48/500 [=>............................] - ETA: 8:28 - loss: 2.2474 - regression_loss: 1.6353 - classification_loss: 0.6121\n",
      " 49/500 [=>............................] - ETA: 8:26 - loss: 2.2385 - regression_loss: 1.6273 - classification_loss: 0.6112\n",
      " 50/500 [==>...........................] - ETA: 8:24 - loss: 2.2522 - regression_loss: 1.6358 - classification_loss: 0.6164\n",
      " 51/500 [==>...........................] - ETA: 8:21 - loss: 2.2497 - regression_loss: 1.6313 - classification_loss: 0.6184\n",
      " 52/500 [==>...........................] - ETA: 8:20 - loss: 2.2573 - regression_loss: 1.6322 - classification_loss: 0.6251\n",
      " 53/500 [==>...........................] - ETA: 8:18 - loss: 2.2582 - regression_loss: 1.6345 - classification_loss: 0.6236\n",
      " 54/500 [==>...........................] - ETA: 8:15 - loss: 2.2569 - regression_loss: 1.6338 - classification_loss: 0.6231\n",
      " 55/500 [==>...........................] - ETA: 8:14 - loss: 2.2633 - regression_loss: 1.6392 - classification_loss: 0.6241\n",
      " 56/500 [==>...........................] - ETA: 8:13 - loss: 2.2716 - regression_loss: 1.6451 - classification_loss: 0.6265\n",
      " 57/500 [==>...........................] - ETA: 8:11 - loss: 2.2650 - regression_loss: 1.6411 - classification_loss: 0.6239\n",
      " 58/500 [==>...........................] - ETA: 8:10 - loss: 2.2551 - regression_loss: 1.6335 - classification_loss: 0.6216\n",
      " 59/500 [==>...........................] - ETA: 8:08 - loss: 2.2437 - regression_loss: 1.6252 - classification_loss: 0.6185\n",
      " 60/500 [==>...........................] - ETA: 8:07 - loss: 2.2438 - regression_loss: 1.6256 - classification_loss: 0.6181\n",
      " 61/500 [==>...........................] - ETA: 8:06 - loss: 2.2500 - regression_loss: 1.6212 - classification_loss: 0.6288\n",
      " 62/500 [==>...........................] - ETA: 8:02 - loss: 2.2551 - regression_loss: 1.6215 - classification_loss: 0.6336\n",
      " 63/500 [==>...........................] - ETA: 8:01 - loss: 2.2435 - regression_loss: 1.6123 - classification_loss: 0.6313\n",
      " 64/500 [==>...........................] - ETA: 8:00 - loss: 2.2462 - regression_loss: 1.6156 - classification_loss: 0.6306\n",
      " 65/500 [==>...........................] - ETA: 7:58 - loss: 2.2264 - regression_loss: 1.5995 - classification_loss: 0.6269\n",
      " 66/500 [==>...........................] - ETA: 7:57 - loss: 2.2321 - regression_loss: 1.6028 - classification_loss: 0.6293\n",
      " 67/500 [===>..........................] - ETA: 7:59 - loss: 2.2289 - regression_loss: 1.6005 - classification_loss: 0.6284\n",
      " 68/500 [===>..........................] - ETA: 7:56 - loss: 2.2374 - regression_loss: 1.6080 - classification_loss: 0.6294\n",
      " 69/500 [===>..........................] - ETA: 7:55 - loss: 2.2394 - regression_loss: 1.6117 - classification_loss: 0.6277\n",
      " 70/500 [===>..........................] - ETA: 7:53 - loss: 2.2384 - regression_loss: 1.6115 - classification_loss: 0.6270\n",
      " 71/500 [===>..........................] - ETA: 7:52 - loss: 2.2327 - regression_loss: 1.6069 - classification_loss: 0.6258\n",
      " 72/500 [===>..........................] - ETA: 7:49 - loss: 2.2332 - regression_loss: 1.6086 - classification_loss: 0.6246\n",
      " 73/500 [===>..........................] - ETA: 7:49 - loss: 2.2352 - regression_loss: 1.6121 - classification_loss: 0.6231\n",
      " 74/500 [===>..........................] - ETA: 7:47 - loss: 2.2461 - regression_loss: 1.6171 - classification_loss: 0.6290\n",
      " 75/500 [===>..........................] - ETA: 7:45 - loss: 2.2424 - regression_loss: 1.6142 - classification_loss: 0.6282\n",
      " 76/500 [===>..........................] - ETA: 7:43 - loss: 2.2337 - regression_loss: 1.6085 - classification_loss: 0.6252\n",
      " 77/500 [===>..........................] - ETA: 7:42 - loss: 2.2277 - regression_loss: 1.6048 - classification_loss: 0.6229\n",
      " 78/500 [===>..........................] - ETA: 7:41 - loss: 2.2210 - regression_loss: 1.6008 - classification_loss: 0.6202\n",
      " 79/500 [===>..........................] - ETA: 7:39 - loss: 2.2187 - regression_loss: 1.5988 - classification_loss: 0.6199\n",
      " 80/500 [===>..........................] - ETA: 7:38 - loss: 2.2276 - regression_loss: 1.6085 - classification_loss: 0.6192\n",
      " 81/500 [===>..........................] - ETA: 7:37 - loss: 2.2304 - regression_loss: 1.6097 - classification_loss: 0.6207\n",
      " 82/500 [===>..........................] - ETA: 7:35 - loss: 2.2432 - regression_loss: 1.6181 - classification_loss: 0.6251\n",
      " 83/500 [===>..........................] - ETA: 7:33 - loss: 2.2362 - regression_loss: 1.6138 - classification_loss: 0.6224\n",
      " 84/500 [====>.........................] - ETA: 7:31 - loss: 2.2311 - regression_loss: 1.6113 - classification_loss: 0.6198\n",
      " 85/500 [====>.........................] - ETA: 7:31 - loss: 2.2254 - regression_loss: 1.6069 - classification_loss: 0.6185\n",
      " 86/500 [====>.........................] - ETA: 7:44 - loss: 2.2178 - regression_loss: 1.6017 - classification_loss: 0.6161\n",
      " 87/500 [====>.........................] - ETA: 7:45 - loss: 2.2112 - regression_loss: 1.5972 - classification_loss: 0.6140\n",
      " 88/500 [====>.........................] - ETA: 7:43 - loss: 2.2106 - regression_loss: 1.5988 - classification_loss: 0.6118\n",
      " 89/500 [====>.........................] - ETA: 7:42 - loss: 2.2188 - regression_loss: 1.6053 - classification_loss: 0.6135\n",
      " 90/500 [====>.........................] - ETA: 7:40 - loss: 2.2236 - regression_loss: 1.6080 - classification_loss: 0.6157\n",
      " 91/500 [====>.........................] - ETA: 7:39 - loss: 2.2260 - regression_loss: 1.6102 - classification_loss: 0.6158\n",
      " 92/500 [====>.........................] - ETA: 7:37 - loss: 2.2222 - regression_loss: 1.6089 - classification_loss: 0.6133\n",
      " 93/500 [====>.........................] - ETA: 7:35 - loss: 2.2225 - regression_loss: 1.6109 - classification_loss: 0.6116\n",
      " 94/500 [====>.........................] - ETA: 7:33 - loss: 2.2260 - regression_loss: 1.6142 - classification_loss: 0.6118\n",
      " 95/500 [====>.........................] - ETA: 7:32 - loss: 2.2292 - regression_loss: 1.6176 - classification_loss: 0.6116\n",
      " 96/500 [====>.........................] - ETA: 7:30 - loss: 2.2201 - regression_loss: 1.6106 - classification_loss: 0.6096\n",
      " 97/500 [====>.........................] - ETA: 7:29 - loss: 2.2133 - regression_loss: 1.6054 - classification_loss: 0.6079\n",
      " 98/500 [====>.........................] - ETA: 7:28 - loss: 2.2179 - regression_loss: 1.6092 - classification_loss: 0.6086\n",
      " 99/500 [====>.........................] - ETA: 7:26 - loss: 2.2110 - regression_loss: 1.6049 - classification_loss: 0.6061\n",
      "100/500 [=====>........................] - ETA: 7:25 - loss: 2.2167 - regression_loss: 1.6089 - classification_loss: 0.6078\n",
      "101/500 [=====>........................] - ETA: 7:22 - loss: 2.2128 - regression_loss: 1.6060 - classification_loss: 0.6067\n",
      "102/500 [=====>........................] - ETA: 7:22 - loss: 2.2189 - regression_loss: 1.6118 - classification_loss: 0.6071\n",
      "103/500 [=====>........................] - ETA: 7:20 - loss: 2.2179 - regression_loss: 1.6121 - classification_loss: 0.6058\n",
      "104/500 [=====>........................] - ETA: 7:19 - loss: 2.2205 - regression_loss: 1.6140 - classification_loss: 0.6065\n",
      "105/500 [=====>........................] - ETA: 7:17 - loss: 2.2128 - regression_loss: 1.6073 - classification_loss: 0.6055\n",
      "106/500 [=====>........................] - ETA: 7:16 - loss: 2.2046 - regression_loss: 1.6008 - classification_loss: 0.6039\n",
      "107/500 [=====>........................] - ETA: 7:15 - loss: 2.2036 - regression_loss: 1.6011 - classification_loss: 0.6025\n",
      "108/500 [=====>........................] - ETA: 7:14 - loss: 2.2037 - regression_loss: 1.6010 - classification_loss: 0.6026\n",
      "109/500 [=====>........................] - ETA: 7:11 - loss: 2.2091 - regression_loss: 1.6047 - classification_loss: 0.6045\n",
      "110/500 [=====>........................] - ETA: 7:10 - loss: 2.2014 - regression_loss: 1.5981 - classification_loss: 0.6033\n",
      "111/500 [=====>........................] - ETA: 7:10 - loss: 2.2045 - regression_loss: 1.6011 - classification_loss: 0.6033\n",
      "112/500 [=====>........................] - ETA: 7:09 - loss: 2.2040 - regression_loss: 1.6015 - classification_loss: 0.6025\n",
      "113/500 [=====>........................] - ETA: 7:07 - loss: 2.1997 - regression_loss: 1.5982 - classification_loss: 0.6015\n",
      "114/500 [=====>........................] - ETA: 7:06 - loss: 2.1979 - regression_loss: 1.5979 - classification_loss: 0.6000\n",
      "115/500 [=====>........................] - ETA: 7:05 - loss: 2.1966 - regression_loss: 1.5974 - classification_loss: 0.5992\n",
      "116/500 [=====>........................] - ETA: 7:03 - loss: 2.1931 - regression_loss: 1.5948 - classification_loss: 0.5983\n",
      "117/500 [======>.......................] - ETA: 7:02 - loss: 2.1894 - regression_loss: 1.5906 - classification_loss: 0.5988\n",
      "118/500 [======>.......................] - ETA: 7:00 - loss: 2.1928 - regression_loss: 1.5926 - classification_loss: 0.6002\n",
      "119/500 [======>.......................] - ETA: 6:59 - loss: 2.1948 - regression_loss: 1.5955 - classification_loss: 0.5994\n",
      "120/500 [======>.......................] - ETA: 6:57 - loss: 2.1974 - regression_loss: 1.5979 - classification_loss: 0.5995\n",
      "121/500 [======>.......................] - ETA: 6:55 - loss: 2.1990 - regression_loss: 1.5978 - classification_loss: 0.6012\n",
      "122/500 [======>.......................] - ETA: 6:55 - loss: 2.1954 - regression_loss: 1.5954 - classification_loss: 0.6000\n",
      "123/500 [======>.......................] - ETA: 6:53 - loss: 2.1980 - regression_loss: 1.5986 - classification_loss: 0.5994\n",
      "124/500 [======>.......................] - ETA: 6:51 - loss: 2.2002 - regression_loss: 1.6001 - classification_loss: 0.6000\n",
      "125/500 [======>.......................] - ETA: 6:51 - loss: 2.2002 - regression_loss: 1.6002 - classification_loss: 0.6000\n",
      "126/500 [======>.......................] - ETA: 6:50 - loss: 2.2013 - regression_loss: 1.6022 - classification_loss: 0.5991\n",
      "127/500 [======>.......................] - ETA: 6:49 - loss: 2.2045 - regression_loss: 1.6054 - classification_loss: 0.5991\n",
      "128/500 [======>.......................] - ETA: 6:47 - loss: 2.2003 - regression_loss: 1.6026 - classification_loss: 0.5977\n",
      "129/500 [======>.......................] - ETA: 6:46 - loss: 2.2052 - regression_loss: 1.6053 - classification_loss: 0.6000\n",
      "130/500 [======>.......................] - ETA: 6:45 - loss: 2.2080 - regression_loss: 1.6066 - classification_loss: 0.6014\n",
      "131/500 [======>.......................] - ETA: 6:44 - loss: 2.2100 - regression_loss: 1.6089 - classification_loss: 0.6011\n",
      "132/500 [======>.......................] - ETA: 6:42 - loss: 2.2109 - regression_loss: 1.6090 - classification_loss: 0.6019\n",
      "133/500 [======>.......................] - ETA: 6:41 - loss: 2.2157 - regression_loss: 1.6121 - classification_loss: 0.6036\n",
      "134/500 [=======>......................] - ETA: 6:40 - loss: 2.2156 - regression_loss: 1.6114 - classification_loss: 0.6042\n",
      "135/500 [=======>......................] - ETA: 6:39 - loss: 2.2160 - regression_loss: 1.6115 - classification_loss: 0.6046\n",
      "136/500 [=======>......................] - ETA: 6:38 - loss: 2.2097 - regression_loss: 1.6064 - classification_loss: 0.6033\n",
      "137/500 [=======>......................] - ETA: 6:36 - loss: 2.2078 - regression_loss: 1.6048 - classification_loss: 0.6030\n",
      "138/500 [=======>......................] - ETA: 6:36 - loss: 2.2043 - regression_loss: 1.6024 - classification_loss: 0.6019\n",
      "139/500 [=======>......................] - ETA: 6:35 - loss: 2.2024 - regression_loss: 1.6024 - classification_loss: 0.6000\n",
      "140/500 [=======>......................] - ETA: 6:34 - loss: 2.2005 - regression_loss: 1.6017 - classification_loss: 0.5988\n",
      "141/500 [=======>......................] - ETA: 6:40 - loss: 2.1994 - regression_loss: 1.6012 - classification_loss: 0.5983\n",
      "142/500 [=======>......................] - ETA: 6:39 - loss: 2.2023 - regression_loss: 1.6034 - classification_loss: 0.5989\n",
      "143/500 [=======>......................] - ETA: 6:38 - loss: 2.2015 - regression_loss: 1.6036 - classification_loss: 0.5979\n",
      "144/500 [=======>......................] - ETA: 6:40 - loss: 2.1957 - regression_loss: 1.5985 - classification_loss: 0.5972\n",
      "145/500 [=======>......................] - ETA: 6:39 - loss: 2.1912 - regression_loss: 1.5949 - classification_loss: 0.5963\n",
      "146/500 [=======>......................] - ETA: 6:37 - loss: 2.1895 - regression_loss: 1.5937 - classification_loss: 0.5957\n",
      "147/500 [=======>......................] - ETA: 6:37 - loss: 2.1895 - regression_loss: 1.5935 - classification_loss: 0.5960\n",
      "148/500 [=======>......................] - ETA: 6:36 - loss: 2.1904 - regression_loss: 1.5943 - classification_loss: 0.5961\n",
      "149/500 [=======>......................] - ETA: 6:35 - loss: 2.1827 - regression_loss: 1.5887 - classification_loss: 0.5939\n",
      "150/500 [========>.....................] - ETA: 6:33 - loss: 2.1822 - regression_loss: 1.5889 - classification_loss: 0.5933\n",
      "151/500 [========>.....................] - ETA: 6:32 - loss: 2.1847 - regression_loss: 1.5911 - classification_loss: 0.5936\n",
      "152/500 [========>.....................] - ETA: 6:30 - loss: 2.1813 - regression_loss: 1.5879 - classification_loss: 0.5934\n",
      "153/500 [========>.....................] - ETA: 6:29 - loss: 2.1847 - regression_loss: 1.5909 - classification_loss: 0.5938\n",
      "154/500 [========>.....................] - ETA: 6:28 - loss: 2.1852 - regression_loss: 1.5918 - classification_loss: 0.5933\n",
      "155/500 [========>.....................] - ETA: 6:26 - loss: 2.1901 - regression_loss: 1.5956 - classification_loss: 0.5945\n",
      "156/500 [========>.....................] - ETA: 6:25 - loss: 2.1857 - regression_loss: 1.5925 - classification_loss: 0.5932\n",
      "157/500 [========>.....................] - ETA: 6:24 - loss: 2.1830 - regression_loss: 1.5905 - classification_loss: 0.5924\n",
      "158/500 [========>.....................] - ETA: 6:23 - loss: 2.1802 - regression_loss: 1.5885 - classification_loss: 0.5916\n",
      "159/500 [========>.....................] - ETA: 6:21 - loss: 2.1879 - regression_loss: 1.5938 - classification_loss: 0.5941\n",
      "160/500 [========>.....................] - ETA: 6:20 - loss: 2.1915 - regression_loss: 1.5964 - classification_loss: 0.5951\n",
      "161/500 [========>.....................] - ETA: 6:19 - loss: 2.1944 - regression_loss: 1.5983 - classification_loss: 0.5961\n",
      "162/500 [========>.....................] - ETA: 6:17 - loss: 2.1917 - regression_loss: 1.5960 - classification_loss: 0.5957\n",
      "163/500 [========>.....................] - ETA: 6:16 - loss: 2.1859 - regression_loss: 1.5910 - classification_loss: 0.5949\n",
      "164/500 [========>.....................] - ETA: 6:15 - loss: 2.1903 - regression_loss: 1.5937 - classification_loss: 0.5965\n",
      "165/500 [========>.....................] - ETA: 6:14 - loss: 2.1900 - regression_loss: 1.5929 - classification_loss: 0.5971\n",
      "166/500 [========>.....................] - ETA: 6:12 - loss: 2.1867 - regression_loss: 1.5904 - classification_loss: 0.5963\n",
      "167/500 [=========>....................] - ETA: 6:11 - loss: 2.1896 - regression_loss: 1.5936 - classification_loss: 0.5960\n",
      "168/500 [=========>....................] - ETA: 6:10 - loss: 2.1943 - regression_loss: 1.5979 - classification_loss: 0.5964\n",
      "169/500 [=========>....................] - ETA: 6:09 - loss: 2.1996 - regression_loss: 1.6018 - classification_loss: 0.5978\n",
      "170/500 [=========>....................] - ETA: 6:07 - loss: 2.1998 - regression_loss: 1.6027 - classification_loss: 0.5971\n",
      "171/500 [=========>....................] - ETA: 6:06 - loss: 2.1996 - regression_loss: 1.6034 - classification_loss: 0.5963\n",
      "172/500 [=========>....................] - ETA: 6:05 - loss: 2.2055 - regression_loss: 1.6072 - classification_loss: 0.5982\n",
      "173/500 [=========>....................] - ETA: 6:04 - loss: 2.2039 - regression_loss: 1.6067 - classification_loss: 0.5972\n",
      "174/500 [=========>....................] - ETA: 6:03 - loss: 2.2080 - regression_loss: 1.6102 - classification_loss: 0.5979\n",
      "175/500 [=========>....................] - ETA: 6:01 - loss: 2.2072 - regression_loss: 1.6093 - classification_loss: 0.5979\n",
      "176/500 [=========>....................] - ETA: 6:00 - loss: 2.2049 - regression_loss: 1.6067 - classification_loss: 0.5982\n",
      "177/500 [=========>....................] - ETA: 5:59 - loss: 2.2072 - regression_loss: 1.6080 - classification_loss: 0.5993\n",
      "178/500 [=========>....................] - ETA: 5:58 - loss: 2.2108 - regression_loss: 1.6108 - classification_loss: 0.5999\n",
      "179/500 [=========>....................] - ETA: 5:57 - loss: 2.2108 - regression_loss: 1.6117 - classification_loss: 0.5991\n",
      "180/500 [=========>....................] - ETA: 5:55 - loss: 2.2103 - regression_loss: 1.6118 - classification_loss: 0.5985\n",
      "181/500 [=========>....................] - ETA: 5:54 - loss: 2.2078 - regression_loss: 1.6104 - classification_loss: 0.5974\n",
      "182/500 [=========>....................] - ETA: 5:53 - loss: 2.2044 - regression_loss: 1.6077 - classification_loss: 0.5968\n",
      "183/500 [=========>....................] - ETA: 5:51 - loss: 2.2044 - regression_loss: 1.6078 - classification_loss: 0.5966\n",
      "184/500 [==========>...................] - ETA: 5:50 - loss: 2.2038 - regression_loss: 1.6071 - classification_loss: 0.5967\n",
      "185/500 [==========>...................] - ETA: 5:49 - loss: 2.2048 - regression_loss: 1.6075 - classification_loss: 0.5973\n",
      "186/500 [==========>...................] - ETA: 5:48 - loss: 2.2037 - regression_loss: 1.6070 - classification_loss: 0.5967\n",
      "187/500 [==========>...................] - ETA: 5:47 - loss: 2.1997 - regression_loss: 1.6038 - classification_loss: 0.5959\n",
      "188/500 [==========>...................] - ETA: 5:46 - loss: 2.1977 - regression_loss: 1.6025 - classification_loss: 0.5952\n",
      "189/500 [==========>...................] - ETA: 5:44 - loss: 2.1976 - regression_loss: 1.6019 - classification_loss: 0.5957\n",
      "190/500 [==========>...................] - ETA: 5:43 - loss: 2.1979 - regression_loss: 1.6020 - classification_loss: 0.5959\n",
      "191/500 [==========>...................] - ETA: 5:42 - loss: 2.1946 - regression_loss: 1.5993 - classification_loss: 0.5954\n",
      "192/500 [==========>...................] - ETA: 5:41 - loss: 2.1950 - regression_loss: 1.5991 - classification_loss: 0.5959\n",
      "193/500 [==========>...................] - ETA: 5:39 - loss: 2.1969 - regression_loss: 1.6016 - classification_loss: 0.5954\n",
      "194/500 [==========>...................] - ETA: 5:38 - loss: 2.1972 - regression_loss: 1.6014 - classification_loss: 0.5958\n",
      "195/500 [==========>...................] - ETA: 5:37 - loss: 2.1966 - regression_loss: 1.6011 - classification_loss: 0.5955\n",
      "196/500 [==========>...................] - ETA: 5:36 - loss: 2.1974 - regression_loss: 1.6015 - classification_loss: 0.5960\n",
      "197/500 [==========>...................] - ETA: 5:34 - loss: 2.1994 - regression_loss: 1.6027 - classification_loss: 0.5967\n",
      "198/500 [==========>...................] - ETA: 5:33 - loss: 2.1984 - regression_loss: 1.6017 - classification_loss: 0.5967\n",
      "199/500 [==========>...................] - ETA: 5:32 - loss: 2.1994 - regression_loss: 1.6030 - classification_loss: 0.5965\n",
      "200/500 [===========>..................] - ETA: 5:31 - loss: 2.1980 - regression_loss: 1.6020 - classification_loss: 0.5960\n",
      "201/500 [===========>..................] - ETA: 5:30 - loss: 2.2002 - regression_loss: 1.6034 - classification_loss: 0.5967\n",
      "202/500 [===========>..................] - ETA: 5:29 - loss: 2.2002 - regression_loss: 1.6041 - classification_loss: 0.5961\n",
      "203/500 [===========>..................] - ETA: 5:27 - loss: 2.2029 - regression_loss: 1.6055 - classification_loss: 0.5973\n",
      "204/500 [===========>..................] - ETA: 5:26 - loss: 2.2019 - regression_loss: 1.6052 - classification_loss: 0.5967\n",
      "205/500 [===========>..................] - ETA: 5:25 - loss: 2.2021 - regression_loss: 1.6059 - classification_loss: 0.5962\n",
      "206/500 [===========>..................] - ETA: 5:24 - loss: 2.2039 - regression_loss: 1.6073 - classification_loss: 0.5966\n",
      "207/500 [===========>..................] - ETA: 5:23 - loss: 2.2032 - regression_loss: 1.6061 - classification_loss: 0.5971\n",
      "208/500 [===========>..................] - ETA: 5:22 - loss: 2.2033 - regression_loss: 1.6062 - classification_loss: 0.5971\n",
      "209/500 [===========>..................] - ETA: 5:20 - loss: 2.2023 - regression_loss: 1.6055 - classification_loss: 0.5968\n",
      "210/500 [===========>..................] - ETA: 5:19 - loss: 2.2059 - regression_loss: 1.6086 - classification_loss: 0.5974\n",
      "211/500 [===========>..................] - ETA: 5:18 - loss: 2.2017 - regression_loss: 1.6063 - classification_loss: 0.5954\n",
      "212/500 [===========>..................] - ETA: 5:17 - loss: 2.2022 - regression_loss: 1.6071 - classification_loss: 0.5950\n",
      "213/500 [===========>..................] - ETA: 5:16 - loss: 2.2000 - regression_loss: 1.6056 - classification_loss: 0.5944\n",
      "214/500 [===========>..................] - ETA: 5:15 - loss: 2.2034 - regression_loss: 1.6087 - classification_loss: 0.5947\n",
      "215/500 [===========>..................] - ETA: 5:13 - loss: 2.2033 - regression_loss: 1.6088 - classification_loss: 0.5945\n",
      "216/500 [===========>..................] - ETA: 5:12 - loss: 2.2061 - regression_loss: 1.6105 - classification_loss: 0.5956\n",
      "217/500 [============>.................] - ETA: 5:11 - loss: 2.2084 - regression_loss: 1.6129 - classification_loss: 0.5955\n",
      "218/500 [============>.................] - ETA: 5:10 - loss: 2.2061 - regression_loss: 1.6115 - classification_loss: 0.5946\n",
      "219/500 [============>.................] - ETA: 5:09 - loss: 2.2079 - regression_loss: 1.6126 - classification_loss: 0.5953\n",
      "220/500 [============>.................] - ETA: 5:08 - loss: 2.2104 - regression_loss: 1.6146 - classification_loss: 0.5958\n",
      "221/500 [============>.................] - ETA: 5:06 - loss: 2.2132 - regression_loss: 1.6167 - classification_loss: 0.5965\n",
      "222/500 [============>.................] - ETA: 5:05 - loss: 2.2129 - regression_loss: 1.6165 - classification_loss: 0.5964\n",
      "223/500 [============>.................] - ETA: 5:04 - loss: 2.2110 - regression_loss: 1.6146 - classification_loss: 0.5964\n",
      "224/500 [============>.................] - ETA: 5:03 - loss: 2.2110 - regression_loss: 1.6147 - classification_loss: 0.5963\n",
      "225/500 [============>.................] - ETA: 5:04 - loss: 2.2118 - regression_loss: 1.6154 - classification_loss: 0.5964\n",
      "226/500 [============>.................] - ETA: 5:03 - loss: 2.2117 - regression_loss: 1.6155 - classification_loss: 0.5962\n",
      "227/500 [============>.................] - ETA: 5:02 - loss: 2.2155 - regression_loss: 1.6181 - classification_loss: 0.5973\n",
      "228/500 [============>.................] - ETA: 5:00 - loss: 2.2163 - regression_loss: 1.6190 - classification_loss: 0.5974\n",
      "229/500 [============>.................] - ETA: 4:59 - loss: 2.2129 - regression_loss: 1.6157 - classification_loss: 0.5972\n",
      "230/500 [============>.................] - ETA: 4:58 - loss: 2.2116 - regression_loss: 1.6153 - classification_loss: 0.5964\n",
      "231/500 [============>.................] - ETA: 4:57 - loss: 2.2141 - regression_loss: 1.6173 - classification_loss: 0.5968\n",
      "232/500 [============>.................] - ETA: 4:56 - loss: 2.2131 - regression_loss: 1.6169 - classification_loss: 0.5962\n",
      "233/500 [============>.................] - ETA: 4:55 - loss: 2.2121 - regression_loss: 1.6163 - classification_loss: 0.5958\n",
      "234/500 [=============>................] - ETA: 4:53 - loss: 2.2132 - regression_loss: 1.6168 - classification_loss: 0.5964\n",
      "235/500 [=============>................] - ETA: 4:52 - loss: 2.2092 - regression_loss: 1.6139 - classification_loss: 0.5953\n",
      "236/500 [=============>................] - ETA: 4:51 - loss: 2.2074 - regression_loss: 1.6123 - classification_loss: 0.5951\n",
      "237/500 [=============>................] - ETA: 4:50 - loss: 2.2076 - regression_loss: 1.6132 - classification_loss: 0.5944\n",
      "238/500 [=============>................] - ETA: 4:49 - loss: 2.2057 - regression_loss: 1.6114 - classification_loss: 0.5943\n",
      "239/500 [=============>................] - ETA: 4:47 - loss: 2.2030 - regression_loss: 1.6098 - classification_loss: 0.5932\n",
      "240/500 [=============>................] - ETA: 4:46 - loss: 2.2034 - regression_loss: 1.6103 - classification_loss: 0.5931\n",
      "241/500 [=============>................] - ETA: 4:45 - loss: 2.2005 - regression_loss: 1.6079 - classification_loss: 0.5926\n",
      "242/500 [=============>................] - ETA: 4:44 - loss: 2.2025 - regression_loss: 1.6099 - classification_loss: 0.5927\n",
      "243/500 [=============>................] - ETA: 4:42 - loss: 2.2037 - regression_loss: 1.6104 - classification_loss: 0.5933\n",
      "244/500 [=============>................] - ETA: 4:41 - loss: 2.2054 - regression_loss: 1.6121 - classification_loss: 0.5934\n",
      "245/500 [=============>................] - ETA: 4:40 - loss: 2.2021 - regression_loss: 1.6092 - classification_loss: 0.5929\n",
      "246/500 [=============>................] - ETA: 4:39 - loss: 2.2017 - regression_loss: 1.6088 - classification_loss: 0.5930\n",
      "247/500 [=============>................] - ETA: 4:38 - loss: 2.1989 - regression_loss: 1.6063 - classification_loss: 0.5925\n",
      "248/500 [=============>................] - ETA: 4:37 - loss: 2.2024 - regression_loss: 1.6089 - classification_loss: 0.5935\n",
      "249/500 [=============>................] - ETA: 4:35 - loss: 2.2011 - regression_loss: 1.6082 - classification_loss: 0.5929\n",
      "250/500 [==============>...............] - ETA: 4:34 - loss: 2.2002 - regression_loss: 1.6076 - classification_loss: 0.5925\n",
      "251/500 [==============>...............] - ETA: 4:33 - loss: 2.2007 - regression_loss: 1.6085 - classification_loss: 0.5923\n",
      "252/500 [==============>...............] - ETA: 4:32 - loss: 2.1997 - regression_loss: 1.6077 - classification_loss: 0.5920\n",
      "253/500 [==============>...............] - ETA: 4:31 - loss: 2.1997 - regression_loss: 1.6072 - classification_loss: 0.5926\n",
      "254/500 [==============>...............] - ETA: 4:29 - loss: 2.2021 - regression_loss: 1.6087 - classification_loss: 0.5934\n",
      "255/500 [==============>...............] - ETA: 4:28 - loss: 2.2004 - regression_loss: 1.6076 - classification_loss: 0.5928\n",
      "256/500 [==============>...............] - ETA: 4:27 - loss: 2.2002 - regression_loss: 1.6077 - classification_loss: 0.5924\n",
      "257/500 [==============>...............] - ETA: 4:26 - loss: 2.1986 - regression_loss: 1.6061 - classification_loss: 0.5925\n",
      "258/500 [==============>...............] - ETA: 4:25 - loss: 2.1973 - regression_loss: 1.6051 - classification_loss: 0.5922\n",
      "259/500 [==============>...............] - ETA: 4:24 - loss: 2.1964 - regression_loss: 1.6047 - classification_loss: 0.5918\n",
      "260/500 [==============>...............] - ETA: 4:23 - loss: 2.1974 - regression_loss: 1.6054 - classification_loss: 0.5920\n",
      "261/500 [==============>...............] - ETA: 4:21 - loss: 2.1961 - regression_loss: 1.6045 - classification_loss: 0.5915\n",
      "262/500 [==============>...............] - ETA: 4:20 - loss: 2.1967 - regression_loss: 1.6052 - classification_loss: 0.5915\n",
      "263/500 [==============>...............] - ETA: 4:19 - loss: 2.1958 - regression_loss: 1.6036 - classification_loss: 0.5922\n",
      "264/500 [==============>...............] - ETA: 4:18 - loss: 2.1929 - regression_loss: 1.6015 - classification_loss: 0.5914\n",
      "265/500 [==============>...............] - ETA: 4:17 - loss: 2.1882 - regression_loss: 1.5979 - classification_loss: 0.5903\n",
      "266/500 [==============>...............] - ETA: 4:16 - loss: 2.1906 - regression_loss: 1.6001 - classification_loss: 0.5905\n",
      "267/500 [===============>..............] - ETA: 4:14 - loss: 2.1893 - regression_loss: 1.5993 - classification_loss: 0.5900\n",
      "268/500 [===============>..............] - ETA: 4:13 - loss: 2.1922 - regression_loss: 1.6011 - classification_loss: 0.5911\n",
      "269/500 [===============>..............] - ETA: 4:12 - loss: 2.1929 - regression_loss: 1.6013 - classification_loss: 0.5916\n",
      "270/500 [===============>..............] - ETA: 4:11 - loss: 2.1938 - regression_loss: 1.6022 - classification_loss: 0.5916\n",
      "271/500 [===============>..............] - ETA: 4:10 - loss: 2.1952 - regression_loss: 1.6026 - classification_loss: 0.5926\n",
      "272/500 [===============>..............] - ETA: 4:09 - loss: 2.1950 - regression_loss: 1.6025 - classification_loss: 0.5925\n",
      "273/500 [===============>..............] - ETA: 4:08 - loss: 2.1979 - regression_loss: 1.6047 - classification_loss: 0.5932\n",
      "274/500 [===============>..............] - ETA: 4:06 - loss: 2.1955 - regression_loss: 1.6031 - classification_loss: 0.5923\n",
      "275/500 [===============>..............] - ETA: 4:05 - loss: 2.1967 - regression_loss: 1.6041 - classification_loss: 0.5926\n",
      "276/500 [===============>..............] - ETA: 4:04 - loss: 2.1948 - regression_loss: 1.6030 - classification_loss: 0.5918\n",
      "277/500 [===============>..............] - ETA: 4:03 - loss: 2.1944 - regression_loss: 1.6034 - classification_loss: 0.5910\n",
      "278/500 [===============>..............] - ETA: 4:02 - loss: 2.1914 - regression_loss: 1.6015 - classification_loss: 0.5899\n",
      "279/500 [===============>..............] - ETA: 4:01 - loss: 2.1887 - regression_loss: 1.5994 - classification_loss: 0.5893\n",
      "280/500 [===============>..............] - ETA: 4:00 - loss: 2.1883 - regression_loss: 1.5993 - classification_loss: 0.5890\n",
      "281/500 [===============>..............] - ETA: 3:58 - loss: 2.1840 - regression_loss: 1.5960 - classification_loss: 0.5880\n",
      "282/500 [===============>..............] - ETA: 3:57 - loss: 2.1842 - regression_loss: 1.5967 - classification_loss: 0.5875\n",
      "283/500 [===============>..............] - ETA: 3:56 - loss: 2.1846 - regression_loss: 1.5969 - classification_loss: 0.5877\n",
      "284/500 [================>.............] - ETA: 3:55 - loss: 2.1843 - regression_loss: 1.5968 - classification_loss: 0.5875\n",
      "285/500 [================>.............] - ETA: 3:54 - loss: 2.1833 - regression_loss: 1.5964 - classification_loss: 0.5869\n",
      "286/500 [================>.............] - ETA: 3:53 - loss: 2.1834 - regression_loss: 1.5968 - classification_loss: 0.5866\n",
      "287/500 [================>.............] - ETA: 3:52 - loss: 2.1839 - regression_loss: 1.5974 - classification_loss: 0.5864\n",
      "288/500 [================>.............] - ETA: 3:50 - loss: 2.1849 - regression_loss: 1.5985 - classification_loss: 0.5865\n",
      "289/500 [================>.............] - ETA: 3:49 - loss: 2.1883 - regression_loss: 1.6007 - classification_loss: 0.5875\n",
      "290/500 [================>.............] - ETA: 3:48 - loss: 2.1897 - regression_loss: 1.6015 - classification_loss: 0.5881\n",
      "291/500 [================>.............] - ETA: 3:47 - loss: 2.1883 - regression_loss: 1.6006 - classification_loss: 0.5877\n",
      "292/500 [================>.............] - ETA: 3:46 - loss: 2.1893 - regression_loss: 1.6015 - classification_loss: 0.5878\n",
      "293/500 [================>.............] - ETA: 3:45 - loss: 2.1920 - regression_loss: 1.6027 - classification_loss: 0.5893\n",
      "294/500 [================>.............] - ETA: 3:44 - loss: 2.1905 - regression_loss: 1.6011 - classification_loss: 0.5894\n",
      "295/500 [================>.............] - ETA: 3:43 - loss: 2.1917 - regression_loss: 1.6025 - classification_loss: 0.5892\n",
      "296/500 [================>.............] - ETA: 3:41 - loss: 2.1904 - regression_loss: 1.6022 - classification_loss: 0.5883\n",
      "297/500 [================>.............] - ETA: 3:40 - loss: 2.1914 - regression_loss: 1.6029 - classification_loss: 0.5886\n",
      "298/500 [================>.............] - ETA: 3:39 - loss: 2.1921 - regression_loss: 1.6035 - classification_loss: 0.5886\n",
      "299/500 [================>.............] - ETA: 3:38 - loss: 2.1921 - regression_loss: 1.6039 - classification_loss: 0.5883\n",
      "300/500 [=================>............] - ETA: 3:37 - loss: 2.1909 - regression_loss: 1.6023 - classification_loss: 0.5886\n",
      "301/500 [=================>............] - ETA: 3:36 - loss: 2.1893 - regression_loss: 1.6011 - classification_loss: 0.5883\n",
      "302/500 [=================>............] - ETA: 3:34 - loss: 2.1874 - regression_loss: 1.5998 - classification_loss: 0.5876\n",
      "303/500 [=================>............] - ETA: 3:33 - loss: 2.1861 - regression_loss: 1.5985 - classification_loss: 0.5876\n",
      "304/500 [=================>............] - ETA: 3:32 - loss: 2.1857 - regression_loss: 1.5989 - classification_loss: 0.5868\n",
      "305/500 [=================>............] - ETA: 3:31 - loss: 2.1838 - regression_loss: 1.5978 - classification_loss: 0.5860\n",
      "306/500 [=================>............] - ETA: 3:30 - loss: 2.1827 - regression_loss: 1.5968 - classification_loss: 0.5859\n",
      "307/500 [=================>............] - ETA: 3:29 - loss: 2.1823 - regression_loss: 1.5964 - classification_loss: 0.5859\n",
      "308/500 [=================>............] - ETA: 3:28 - loss: 2.1861 - regression_loss: 1.5997 - classification_loss: 0.5864\n",
      "309/500 [=================>............] - ETA: 3:27 - loss: 2.1860 - regression_loss: 1.5996 - classification_loss: 0.5865\n",
      "310/500 [=================>............] - ETA: 3:26 - loss: 2.1823 - regression_loss: 1.5968 - classification_loss: 0.5854\n",
      "311/500 [=================>............] - ETA: 3:24 - loss: 2.1829 - regression_loss: 1.5975 - classification_loss: 0.5854\n",
      "312/500 [=================>............] - ETA: 3:23 - loss: 2.1832 - regression_loss: 1.5980 - classification_loss: 0.5853\n",
      "313/500 [=================>............] - ETA: 3:22 - loss: 2.1859 - regression_loss: 1.6004 - classification_loss: 0.5855\n",
      "314/500 [=================>............] - ETA: 3:21 - loss: 2.1867 - regression_loss: 1.6008 - classification_loss: 0.5858\n",
      "315/500 [=================>............] - ETA: 3:20 - loss: 2.1848 - regression_loss: 1.5998 - classification_loss: 0.5851\n",
      "316/500 [=================>............] - ETA: 3:19 - loss: 2.1832 - regression_loss: 1.5986 - classification_loss: 0.5846\n",
      "317/500 [==================>...........] - ETA: 3:18 - loss: 2.1806 - regression_loss: 1.5963 - classification_loss: 0.5843\n",
      "318/500 [==================>...........] - ETA: 3:17 - loss: 2.1815 - regression_loss: 1.5971 - classification_loss: 0.5843\n",
      "319/500 [==================>...........] - ETA: 3:15 - loss: 2.1821 - regression_loss: 1.5977 - classification_loss: 0.5844\n",
      "320/500 [==================>...........] - ETA: 3:14 - loss: 2.1816 - regression_loss: 1.5976 - classification_loss: 0.5841\n",
      "321/500 [==================>...........] - ETA: 3:13 - loss: 2.1822 - regression_loss: 1.5978 - classification_loss: 0.5843\n",
      "322/500 [==================>...........] - ETA: 3:12 - loss: 2.1803 - regression_loss: 1.5966 - classification_loss: 0.5836\n",
      "323/500 [==================>...........] - ETA: 3:11 - loss: 2.1786 - regression_loss: 1.5957 - classification_loss: 0.5829\n",
      "324/500 [==================>...........] - ETA: 3:10 - loss: 2.1771 - regression_loss: 1.5948 - classification_loss: 0.5823\n",
      "325/500 [==================>...........] - ETA: 3:09 - loss: 2.1770 - regression_loss: 1.5946 - classification_loss: 0.5824\n",
      "326/500 [==================>...........] - ETA: 3:08 - loss: 2.1782 - regression_loss: 1.5954 - classification_loss: 0.5828\n",
      "327/500 [==================>...........] - ETA: 3:07 - loss: 2.1787 - regression_loss: 1.5957 - classification_loss: 0.5830\n",
      "328/500 [==================>...........] - ETA: 3:06 - loss: 2.1831 - regression_loss: 1.5988 - classification_loss: 0.5843\n",
      "329/500 [==================>...........] - ETA: 3:05 - loss: 2.1824 - regression_loss: 1.5982 - classification_loss: 0.5842\n",
      "330/500 [==================>...........] - ETA: 3:04 - loss: 2.1842 - regression_loss: 1.5993 - classification_loss: 0.5849\n",
      "331/500 [==================>...........] - ETA: 3:03 - loss: 2.1865 - regression_loss: 1.6004 - classification_loss: 0.5861\n",
      "332/500 [==================>...........] - ETA: 3:02 - loss: 2.1830 - regression_loss: 1.5975 - classification_loss: 0.5855\n",
      "333/500 [==================>...........] - ETA: 3:00 - loss: 2.1829 - regression_loss: 1.5975 - classification_loss: 0.5855\n",
      "334/500 [===================>..........] - ETA: 2:59 - loss: 2.1857 - regression_loss: 1.5993 - classification_loss: 0.5864\n",
      "335/500 [===================>..........] - ETA: 2:58 - loss: 2.1836 - regression_loss: 1.5977 - classification_loss: 0.5859\n",
      "336/500 [===================>..........] - ETA: 2:57 - loss: 2.1839 - regression_loss: 1.5982 - classification_loss: 0.5857\n",
      "337/500 [===================>..........] - ETA: 2:56 - loss: 2.1835 - regression_loss: 1.5979 - classification_loss: 0.5856\n",
      "338/500 [===================>..........] - ETA: 2:55 - loss: 2.1828 - regression_loss: 1.5973 - classification_loss: 0.5855\n",
      "339/500 [===================>..........] - ETA: 2:54 - loss: 2.1816 - regression_loss: 1.5963 - classification_loss: 0.5853\n",
      "340/500 [===================>..........] - ETA: 2:54 - loss: 2.1820 - regression_loss: 1.5967 - classification_loss: 0.5853\n",
      "341/500 [===================>..........] - ETA: 2:52 - loss: 2.1815 - regression_loss: 1.5961 - classification_loss: 0.5854\n",
      "342/500 [===================>..........] - ETA: 2:51 - loss: 2.1779 - regression_loss: 1.5931 - classification_loss: 0.5848\n",
      "343/500 [===================>..........] - ETA: 2:50 - loss: 2.1785 - regression_loss: 1.5931 - classification_loss: 0.5853\n",
      "344/500 [===================>..........] - ETA: 2:49 - loss: 2.1781 - regression_loss: 1.5928 - classification_loss: 0.5852\n",
      "345/500 [===================>..........] - ETA: 2:48 - loss: 2.1795 - regression_loss: 1.5941 - classification_loss: 0.5853\n",
      "346/500 [===================>..........] - ETA: 2:47 - loss: 2.1785 - regression_loss: 1.5937 - classification_loss: 0.5847\n",
      "347/500 [===================>..........] - ETA: 2:46 - loss: 2.1772 - regression_loss: 1.5931 - classification_loss: 0.5841\n",
      "348/500 [===================>..........] - ETA: 2:45 - loss: 2.1781 - regression_loss: 1.5943 - classification_loss: 0.5839\n",
      "349/500 [===================>..........] - ETA: 2:44 - loss: 2.1767 - regression_loss: 1.5932 - classification_loss: 0.5835\n",
      "350/500 [====================>.........] - ETA: 2:43 - loss: 2.1739 - regression_loss: 1.5912 - classification_loss: 0.5827\n",
      "351/500 [====================>.........] - ETA: 2:41 - loss: 2.1761 - regression_loss: 1.5930 - classification_loss: 0.5831\n",
      "352/500 [====================>.........] - ETA: 2:40 - loss: 2.1764 - regression_loss: 1.5933 - classification_loss: 0.5831\n",
      "353/500 [====================>.........] - ETA: 2:39 - loss: 2.1748 - regression_loss: 1.5923 - classification_loss: 0.5825\n",
      "354/500 [====================>.........] - ETA: 2:38 - loss: 2.1746 - regression_loss: 1.5924 - classification_loss: 0.5822\n",
      "355/500 [====================>.........] - ETA: 2:37 - loss: 2.1771 - regression_loss: 1.5944 - classification_loss: 0.5827\n",
      "356/500 [====================>.........] - ETA: 2:36 - loss: 2.1754 - regression_loss: 1.5933 - classification_loss: 0.5821\n",
      "357/500 [====================>.........] - ETA: 2:35 - loss: 2.1746 - regression_loss: 1.5927 - classification_loss: 0.5819\n",
      "358/500 [====================>.........] - ETA: 2:34 - loss: 2.1740 - regression_loss: 1.5921 - classification_loss: 0.5819\n",
      "359/500 [====================>.........] - ETA: 2:33 - loss: 2.1768 - regression_loss: 1.5945 - classification_loss: 0.5823\n",
      "360/500 [====================>.........] - ETA: 2:32 - loss: 2.1781 - regression_loss: 1.5957 - classification_loss: 0.5824\n",
      "361/500 [====================>.........] - ETA: 2:30 - loss: 2.1790 - regression_loss: 1.5964 - classification_loss: 0.5826\n",
      "362/500 [====================>.........] - ETA: 2:29 - loss: 2.1800 - regression_loss: 1.5974 - classification_loss: 0.5826\n",
      "363/500 [====================>.........] - ETA: 2:28 - loss: 2.1806 - regression_loss: 1.5979 - classification_loss: 0.5827\n",
      "364/500 [====================>.........] - ETA: 2:27 - loss: 2.1826 - regression_loss: 1.5994 - classification_loss: 0.5832\n",
      "365/500 [====================>.........] - ETA: 2:26 - loss: 2.1841 - regression_loss: 1.6005 - classification_loss: 0.5836\n",
      "366/500 [====================>.........] - ETA: 2:25 - loss: 2.1845 - regression_loss: 1.6008 - classification_loss: 0.5838\n",
      "367/500 [=====================>........] - ETA: 2:24 - loss: 2.1831 - regression_loss: 1.5997 - classification_loss: 0.5834\n",
      "368/500 [=====================>........] - ETA: 2:23 - loss: 2.1847 - regression_loss: 1.6009 - classification_loss: 0.5838\n",
      "369/500 [=====================>........] - ETA: 2:22 - loss: 2.1840 - regression_loss: 1.6004 - classification_loss: 0.5836\n",
      "370/500 [=====================>........] - ETA: 2:20 - loss: 2.1837 - regression_loss: 1.6003 - classification_loss: 0.5834\n",
      "371/500 [=====================>........] - ETA: 2:19 - loss: 2.1821 - regression_loss: 1.5990 - classification_loss: 0.5831\n",
      "372/500 [=====================>........] - ETA: 2:18 - loss: 2.1840 - regression_loss: 1.6005 - classification_loss: 0.5836\n",
      "373/500 [=====================>........] - ETA: 2:17 - loss: 2.1826 - regression_loss: 1.5992 - classification_loss: 0.5834\n",
      "374/500 [=====================>........] - ETA: 2:16 - loss: 2.1833 - regression_loss: 1.5998 - classification_loss: 0.5835\n",
      "375/500 [=====================>........] - ETA: 2:15 - loss: 2.1829 - regression_loss: 1.5994 - classification_loss: 0.5835\n",
      "376/500 [=====================>........] - ETA: 2:14 - loss: 2.1844 - regression_loss: 1.6005 - classification_loss: 0.5839\n",
      "377/500 [=====================>........] - ETA: 2:13 - loss: 2.1856 - regression_loss: 1.6018 - classification_loss: 0.5838\n",
      "378/500 [=====================>........] - ETA: 2:12 - loss: 2.1866 - regression_loss: 1.6025 - classification_loss: 0.5841\n",
      "379/500 [=====================>........] - ETA: 2:11 - loss: 2.1881 - regression_loss: 1.6037 - classification_loss: 0.5844\n",
      "380/500 [=====================>........] - ETA: 2:09 - loss: 2.1880 - regression_loss: 1.6036 - classification_loss: 0.5843\n",
      "381/500 [=====================>........] - ETA: 2:08 - loss: 2.1881 - regression_loss: 1.6040 - classification_loss: 0.5841\n",
      "382/500 [=====================>........] - ETA: 2:07 - loss: 2.1871 - regression_loss: 1.6031 - classification_loss: 0.5840\n",
      "383/500 [=====================>........] - ETA: 2:06 - loss: 2.1850 - regression_loss: 1.6016 - classification_loss: 0.5834\n",
      "384/500 [======================>.......] - ETA: 2:05 - loss: 2.1840 - regression_loss: 1.6010 - classification_loss: 0.5830\n",
      "385/500 [======================>.......] - ETA: 2:04 - loss: 2.1827 - regression_loss: 1.6001 - classification_loss: 0.5826\n",
      "386/500 [======================>.......] - ETA: 2:03 - loss: 2.1836 - regression_loss: 1.6004 - classification_loss: 0.5832\n",
      "387/500 [======================>.......] - ETA: 2:02 - loss: 2.1829 - regression_loss: 1.5994 - classification_loss: 0.5835\n",
      "388/500 [======================>.......] - ETA: 2:00 - loss: 2.1807 - regression_loss: 1.5975 - classification_loss: 0.5832\n",
      "389/500 [======================>.......] - ETA: 1:59 - loss: 2.1819 - regression_loss: 1.5982 - classification_loss: 0.5836\n",
      "390/500 [======================>.......] - ETA: 1:58 - loss: 2.1815 - regression_loss: 1.5982 - classification_loss: 0.5833\n",
      "391/500 [======================>.......] - ETA: 1:57 - loss: 2.1825 - regression_loss: 1.5987 - classification_loss: 0.5839\n",
      "392/500 [======================>.......] - ETA: 1:56 - loss: 2.1850 - regression_loss: 1.6003 - classification_loss: 0.5847\n",
      "393/500 [======================>.......] - ETA: 1:55 - loss: 2.1851 - regression_loss: 1.6003 - classification_loss: 0.5848\n",
      "394/500 [======================>.......] - ETA: 1:54 - loss: 2.1858 - regression_loss: 1.6010 - classification_loss: 0.5848\n",
      "395/500 [======================>.......] - ETA: 1:53 - loss: 2.1852 - regression_loss: 1.6007 - classification_loss: 0.5845\n",
      "396/500 [======================>.......] - ETA: 1:52 - loss: 2.1852 - regression_loss: 1.6009 - classification_loss: 0.5843\n",
      "397/500 [======================>.......] - ETA: 1:51 - loss: 2.1822 - regression_loss: 1.5987 - classification_loss: 0.5835\n",
      "398/500 [======================>.......] - ETA: 1:50 - loss: 2.1805 - regression_loss: 1.5972 - classification_loss: 0.5833\n",
      "399/500 [======================>.......] - ETA: 1:48 - loss: 2.1803 - regression_loss: 1.5974 - classification_loss: 0.5829\n",
      "400/500 [=======================>......] - ETA: 1:47 - loss: 2.1782 - regression_loss: 1.5958 - classification_loss: 0.5823\n",
      "401/500 [=======================>......] - ETA: 1:46 - loss: 2.1790 - regression_loss: 1.5959 - classification_loss: 0.5830\n",
      "402/500 [=======================>......] - ETA: 1:45 - loss: 2.1788 - regression_loss: 1.5959 - classification_loss: 0.5829\n",
      "403/500 [=======================>......] - ETA: 1:44 - loss: 2.1775 - regression_loss: 1.5951 - classification_loss: 0.5823\n",
      "404/500 [=======================>......] - ETA: 1:43 - loss: 2.1773 - regression_loss: 1.5949 - classification_loss: 0.5823\n",
      "405/500 [=======================>......] - ETA: 1:42 - loss: 2.1767 - regression_loss: 1.5945 - classification_loss: 0.5823\n",
      "406/500 [=======================>......] - ETA: 1:41 - loss: 2.1771 - regression_loss: 1.5949 - classification_loss: 0.5822\n",
      "407/500 [=======================>......] - ETA: 1:40 - loss: 2.1766 - regression_loss: 1.5946 - classification_loss: 0.5820\n",
      "408/500 [=======================>......] - ETA: 1:39 - loss: 2.1768 - regression_loss: 1.5947 - classification_loss: 0.5821\n",
      "409/500 [=======================>......] - ETA: 1:38 - loss: 2.1775 - regression_loss: 1.5949 - classification_loss: 0.5826\n",
      "410/500 [=======================>......] - ETA: 1:36 - loss: 2.1770 - regression_loss: 1.5946 - classification_loss: 0.5824\n",
      "411/500 [=======================>......] - ETA: 1:35 - loss: 2.1758 - regression_loss: 1.5935 - classification_loss: 0.5824\n",
      "412/500 [=======================>......] - ETA: 1:34 - loss: 2.1751 - regression_loss: 1.5934 - classification_loss: 0.5817\n",
      "413/500 [=======================>......] - ETA: 1:33 - loss: 2.1776 - regression_loss: 1.5952 - classification_loss: 0.5824\n",
      "414/500 [=======================>......] - ETA: 1:32 - loss: 2.1789 - regression_loss: 1.5964 - classification_loss: 0.5825\n",
      "415/500 [=======================>......] - ETA: 1:31 - loss: 2.1806 - regression_loss: 1.5977 - classification_loss: 0.5829\n",
      "416/500 [=======================>......] - ETA: 1:30 - loss: 2.1810 - regression_loss: 1.5982 - classification_loss: 0.5828\n",
      "417/500 [========================>.....] - ETA: 1:29 - loss: 2.1785 - regression_loss: 1.5964 - classification_loss: 0.5821\n",
      "418/500 [========================>.....] - ETA: 1:28 - loss: 2.1781 - regression_loss: 1.5962 - classification_loss: 0.5819\n",
      "419/500 [========================>.....] - ETA: 1:27 - loss: 2.1787 - regression_loss: 1.5966 - classification_loss: 0.5821\n",
      "420/500 [========================>.....] - ETA: 1:26 - loss: 2.1773 - regression_loss: 1.5956 - classification_loss: 0.5817\n",
      "421/500 [========================>.....] - ETA: 1:24 - loss: 2.1790 - regression_loss: 1.5971 - classification_loss: 0.5819\n",
      "422/500 [========================>.....] - ETA: 1:23 - loss: 2.1800 - regression_loss: 1.5978 - classification_loss: 0.5822\n",
      "423/500 [========================>.....] - ETA: 1:22 - loss: 2.1795 - regression_loss: 1.5977 - classification_loss: 0.5818\n",
      "424/500 [========================>.....] - ETA: 1:21 - loss: 2.1819 - regression_loss: 1.5993 - classification_loss: 0.5826\n",
      "425/500 [========================>.....] - ETA: 1:20 - loss: 2.1834 - regression_loss: 1.6004 - classification_loss: 0.5829\n",
      "426/500 [========================>.....] - ETA: 1:19 - loss: 2.1816 - regression_loss: 1.5990 - classification_loss: 0.5826\n",
      "427/500 [========================>.....] - ETA: 1:18 - loss: 2.1788 - regression_loss: 1.5968 - classification_loss: 0.5820\n",
      "428/500 [========================>.....] - ETA: 1:17 - loss: 2.1792 - regression_loss: 1.5972 - classification_loss: 0.5820\n",
      "429/500 [========================>.....] - ETA: 1:16 - loss: 2.1786 - regression_loss: 1.5965 - classification_loss: 0.5821\n",
      "430/500 [========================>.....] - ETA: 1:15 - loss: 2.1795 - regression_loss: 1.5970 - classification_loss: 0.5825\n",
      "431/500 [========================>.....] - ETA: 1:14 - loss: 2.1772 - regression_loss: 1.5954 - classification_loss: 0.5818\n",
      "432/500 [========================>.....] - ETA: 1:13 - loss: 2.1778 - regression_loss: 1.5957 - classification_loss: 0.5821\n",
      "433/500 [========================>.....] - ETA: 1:11 - loss: 2.1778 - regression_loss: 1.5955 - classification_loss: 0.5823\n",
      "434/500 [=========================>....] - ETA: 1:10 - loss: 2.1778 - regression_loss: 1.5959 - classification_loss: 0.5819\n",
      "435/500 [=========================>....] - ETA: 1:09 - loss: 2.1788 - regression_loss: 1.5963 - classification_loss: 0.5825\n",
      "436/500 [=========================>....] - ETA: 1:08 - loss: 2.1796 - regression_loss: 1.5968 - classification_loss: 0.5828\n",
      "437/500 [=========================>....] - ETA: 1:07 - loss: 2.1794 - regression_loss: 1.5968 - classification_loss: 0.5826\n",
      "438/500 [=========================>....] - ETA: 1:06 - loss: 2.1804 - regression_loss: 1.5976 - classification_loss: 0.5828\n",
      "439/500 [=========================>....] - ETA: 1:05 - loss: 2.1802 - regression_loss: 1.5975 - classification_loss: 0.5828\n",
      "440/500 [=========================>....] - ETA: 1:04 - loss: 2.1790 - regression_loss: 1.5965 - classification_loss: 0.5825\n",
      "441/500 [=========================>....] - ETA: 1:03 - loss: 2.1804 - regression_loss: 1.5972 - classification_loss: 0.5832\n",
      "442/500 [=========================>....] - ETA: 1:02 - loss: 2.1811 - regression_loss: 1.5977 - classification_loss: 0.5833\n",
      "443/500 [=========================>....] - ETA: 1:01 - loss: 2.1835 - regression_loss: 1.5995 - classification_loss: 0.5840\n",
      "444/500 [=========================>....] - ETA: 1:00 - loss: 2.1825 - regression_loss: 1.5989 - classification_loss: 0.5836\n",
      "445/500 [=========================>....] - ETA: 59s - loss: 2.1825 - regression_loss: 1.5990 - classification_loss: 0.5835 \n",
      "446/500 [=========================>....] - ETA: 57s - loss: 2.1816 - regression_loss: 1.5982 - classification_loss: 0.5834\n",
      "447/500 [=========================>....] - ETA: 56s - loss: 2.1823 - regression_loss: 1.5983 - classification_loss: 0.5840\n",
      "448/500 [=========================>....] - ETA: 55s - loss: 2.1848 - regression_loss: 1.6000 - classification_loss: 0.5848\n",
      "449/500 [=========================>....] - ETA: 54s - loss: 2.1858 - regression_loss: 1.6008 - classification_loss: 0.5850\n",
      "450/500 [==========================>...] - ETA: 53s - loss: 2.1856 - regression_loss: 1.6009 - classification_loss: 0.5847\n",
      "451/500 [==========================>...] - ETA: 52s - loss: 2.1852 - regression_loss: 1.6006 - classification_loss: 0.5845\n",
      "452/500 [==========================>...] - ETA: 51s - loss: 2.1833 - regression_loss: 1.5991 - classification_loss: 0.5841\n",
      "453/500 [==========================>...] - ETA: 50s - loss: 2.1825 - regression_loss: 1.5984 - classification_loss: 0.5841\n",
      "454/500 [==========================>...] - ETA: 49s - loss: 2.1830 - regression_loss: 1.5989 - classification_loss: 0.5841\n",
      "455/500 [==========================>...] - ETA: 48s - loss: 2.1827 - regression_loss: 1.5988 - classification_loss: 0.5838\n",
      "456/500 [==========================>...] - ETA: 47s - loss: 2.1810 - regression_loss: 1.5978 - classification_loss: 0.5833\n",
      "457/500 [==========================>...] - ETA: 46s - loss: 2.1814 - regression_loss: 1.5983 - classification_loss: 0.5831\n",
      "458/500 [==========================>...] - ETA: 45s - loss: 2.1801 - regression_loss: 1.5973 - classification_loss: 0.5828\n",
      "459/500 [==========================>...] - ETA: 43s - loss: 2.1799 - regression_loss: 1.5971 - classification_loss: 0.5829\n",
      "460/500 [==========================>...] - ETA: 42s - loss: 2.1811 - regression_loss: 1.5978 - classification_loss: 0.5834\n",
      "461/500 [==========================>...] - ETA: 41s - loss: 2.1805 - regression_loss: 1.5973 - classification_loss: 0.5832\n",
      "462/500 [==========================>...] - ETA: 40s - loss: 2.1792 - regression_loss: 1.5961 - classification_loss: 0.5831\n",
      "463/500 [==========================>...] - ETA: 39s - loss: 2.1793 - regression_loss: 1.5963 - classification_loss: 0.5830\n",
      "464/500 [==========================>...] - ETA: 38s - loss: 2.1819 - regression_loss: 1.5986 - classification_loss: 0.5833\n",
      "465/500 [==========================>...] - ETA: 37s - loss: 2.1818 - regression_loss: 1.5981 - classification_loss: 0.5837\n",
      "466/500 [==========================>...] - ETA: 36s - loss: 2.1804 - regression_loss: 1.5970 - classification_loss: 0.5835\n",
      "467/500 [===========================>..] - ETA: 35s - loss: 2.1800 - regression_loss: 1.5964 - classification_loss: 0.5836\n",
      "468/500 [===========================>..] - ETA: 34s - loss: 2.1792 - regression_loss: 1.5957 - classification_loss: 0.5834\n",
      "469/500 [===========================>..] - ETA: 33s - loss: 2.1790 - regression_loss: 1.5955 - classification_loss: 0.5835\n",
      "470/500 [===========================>..] - ETA: 32s - loss: 2.1772 - regression_loss: 1.5940 - classification_loss: 0.5832\n",
      "471/500 [===========================>..] - ETA: 31s - loss: 2.1771 - regression_loss: 1.5939 - classification_loss: 0.5831\n",
      "472/500 [===========================>..] - ETA: 30s - loss: 2.1775 - regression_loss: 1.5945 - classification_loss: 0.5830\n",
      "473/500 [===========================>..] - ETA: 29s - loss: 2.1764 - regression_loss: 1.5939 - classification_loss: 0.5825\n",
      "474/500 [===========================>..] - ETA: 27s - loss: 2.1769 - regression_loss: 1.5942 - classification_loss: 0.5827\n",
      "475/500 [===========================>..] - ETA: 26s - loss: 2.1773 - regression_loss: 1.5948 - classification_loss: 0.5826\n",
      "476/500 [===========================>..] - ETA: 25s - loss: 2.1777 - regression_loss: 1.5947 - classification_loss: 0.5830\n",
      "477/500 [===========================>..] - ETA: 24s - loss: 2.1767 - regression_loss: 1.5938 - classification_loss: 0.5829\n",
      "478/500 [===========================>..] - ETA: 23s - loss: 2.1756 - regression_loss: 1.5929 - classification_loss: 0.5826\n",
      "479/500 [===========================>..] - ETA: 22s - loss: 2.1739 - regression_loss: 1.5918 - classification_loss: 0.5821\n",
      "480/500 [===========================>..] - ETA: 21s - loss: 2.1743 - regression_loss: 1.5923 - classification_loss: 0.5820\n",
      "481/500 [===========================>..] - ETA: 20s - loss: 2.1753 - regression_loss: 1.5936 - classification_loss: 0.5817\n",
      "482/500 [===========================>..] - ETA: 19s - loss: 2.1738 - regression_loss: 1.5925 - classification_loss: 0.5813\n",
      "483/500 [===========================>..] - ETA: 18s - loss: 2.1744 - regression_loss: 1.5928 - classification_loss: 0.5816\n",
      "484/500 [============================>.] - ETA: 17s - loss: 2.1736 - regression_loss: 1.5924 - classification_loss: 0.5812\n",
      "485/500 [============================>.] - ETA: 16s - loss: 2.1720 - regression_loss: 1.5911 - classification_loss: 0.5809\n",
      "486/500 [============================>.] - ETA: 15s - loss: 2.1721 - regression_loss: 1.5912 - classification_loss: 0.5809\n",
      "487/500 [============================>.] - ETA: 13s - loss: 2.1723 - regression_loss: 1.5915 - classification_loss: 0.5808\n",
      "488/500 [============================>.] - ETA: 12s - loss: 2.1716 - regression_loss: 1.5912 - classification_loss: 0.5804\n",
      "489/500 [============================>.] - ETA: 11s - loss: 2.1719 - regression_loss: 1.5916 - classification_loss: 0.5803\n",
      "490/500 [============================>.] - ETA: 10s - loss: 2.1717 - regression_loss: 1.5909 - classification_loss: 0.5808\n",
      "491/500 [============================>.] - ETA: 9s - loss: 2.1733 - regression_loss: 1.5921 - classification_loss: 0.5813 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 2.1741 - regression_loss: 1.5922 - classification_loss: 0.5819\n",
      "493/500 [============================>.] - ETA: 7s - loss: 2.1735 - regression_loss: 1.5918 - classification_loss: 0.5817\n",
      "494/500 [============================>.] - ETA: 6s - loss: 2.1745 - regression_loss: 1.5921 - classification_loss: 0.5824\n",
      "495/500 [============================>.] - ETA: 5s - loss: 2.1743 - regression_loss: 1.5919 - classification_loss: 0.5823\n",
      "496/500 [============================>.] - ETA: 4s - loss: 2.1732 - regression_loss: 1.5913 - classification_loss: 0.5819\n",
      "497/500 [============================>.] - ETA: 3s - loss: 2.1724 - regression_loss: 1.5905 - classification_loss: 0.5819\n",
      "498/500 [============================>.] - ETA: 2s - loss: 2.1736 - regression_loss: 1.5911 - classification_loss: 0.5825\n",
      "499/500 [============================>.] - ETA: 1s - loss: 2.1735 - regression_loss: 1.5910 - classification_loss: 0.5825\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.1735 - regression_loss: 1.5911 - classification_loss: 0.5824\n",
      "Epoch 00004: saving model to ./snapshots\\resnet50_csv_04.h5\n",
      "\n",
      "500/500 [==============================] - 536s 1s/step - loss: 2.1735 - regression_loss: 1.5911 - classification_loss: 0.5824\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 2.5884 - regression_loss: 1.8897 - classification_loss: 0.6987\n",
      "  2/500 [..............................] - ETA: 4:31 - loss: 2.3794 - regression_loss: 1.7563 - classification_loss: 0.6231\n",
      "  3/500 [..............................] - ETA: 5:43 - loss: 2.3348 - regression_loss: 1.6925 - classification_loss: 0.6423\n",
      "  4/500 [..............................] - ETA: 6:30 - loss: 2.4751 - regression_loss: 1.8082 - classification_loss: 0.6670\n",
      "  5/500 [..............................] - ETA: 6:49 - loss: 2.2016 - regression_loss: 1.6185 - classification_loss: 0.5831\n",
      "  6/500 [..............................] - ETA: 7:29 - loss: 2.1943 - regression_loss: 1.6078 - classification_loss: 0.5864\n",
      "  7/500 [..............................] - ETA: 7:17 - loss: 2.2012 - regression_loss: 1.6203 - classification_loss: 0.5808\n",
      "  8/500 [..............................] - ETA: 7:34 - loss: 2.2266 - regression_loss: 1.6374 - classification_loss: 0.5892\n",
      "  9/500 [..............................] - ETA: 7:36 - loss: 2.1540 - regression_loss: 1.5799 - classification_loss: 0.5741\n",
      " 10/500 [..............................] - ETA: 7:42 - loss: 2.1793 - regression_loss: 1.5990 - classification_loss: 0.5803\n",
      " 11/500 [..............................] - ETA: 7:48 - loss: 2.1104 - regression_loss: 1.5436 - classification_loss: 0.5668\n",
      " 12/500 [..............................] - ETA: 7:39 - loss: 2.0827 - regression_loss: 1.5135 - classification_loss: 0.5692\n",
      " 13/500 [..............................] - ETA: 7:46 - loss: 2.0827 - regression_loss: 1.5206 - classification_loss: 0.5621\n",
      " 14/500 [..............................] - ETA: 7:46 - loss: 2.0282 - regression_loss: 1.4727 - classification_loss: 0.5555\n",
      " 15/500 [..............................] - ETA: 7:49 - loss: 2.0321 - regression_loss: 1.4744 - classification_loss: 0.5578\n",
      " 16/500 [..............................] - ETA: 7:48 - loss: 1.9983 - regression_loss: 1.4563 - classification_loss: 0.5420\n",
      " 17/500 [>.............................] - ETA: 7:41 - loss: 1.9907 - regression_loss: 1.4455 - classification_loss: 0.5451\n",
      " 18/500 [>.............................] - ETA: 7:42 - loss: 2.0089 - regression_loss: 1.4570 - classification_loss: 0.5519\n",
      " 19/500 [>.............................] - ETA: 7:52 - loss: 2.0022 - regression_loss: 1.4474 - classification_loss: 0.5549\n",
      " 20/500 [>.............................] - ETA: 7:53 - loss: 1.9929 - regression_loss: 1.4395 - classification_loss: 0.5533\n",
      " 21/500 [>.............................] - ETA: 7:52 - loss: 1.9986 - regression_loss: 1.4472 - classification_loss: 0.5515\n",
      " 22/500 [>.............................] - ETA: 7:51 - loss: 1.9547 - regression_loss: 1.4105 - classification_loss: 0.5443\n",
      " 23/500 [>.............................] - ETA: 7:52 - loss: 1.9864 - regression_loss: 1.4386 - classification_loss: 0.5478\n",
      " 24/500 [>.............................] - ETA: 7:53 - loss: 1.9455 - regression_loss: 1.4087 - classification_loss: 0.5367\n",
      " 25/500 [>.............................] - ETA: 7:52 - loss: 1.9960 - regression_loss: 1.4557 - classification_loss: 0.5403\n",
      " 26/500 [>.............................] - ETA: 7:50 - loss: 1.9829 - regression_loss: 1.4486 - classification_loss: 0.5343\n",
      " 27/500 [>.............................] - ETA: 7:51 - loss: 1.9591 - regression_loss: 1.4272 - classification_loss: 0.5319\n",
      " 28/500 [>.............................] - ETA: 7:49 - loss: 1.9811 - regression_loss: 1.4490 - classification_loss: 0.5322\n",
      " 29/500 [>.............................] - ETA: 7:49 - loss: 1.9987 - regression_loss: 1.4649 - classification_loss: 0.5338\n",
      " 30/500 [>.............................] - ETA: 7:48 - loss: 1.9813 - regression_loss: 1.4483 - classification_loss: 0.5331\n",
      " 31/500 [>.............................] - ETA: 7:47 - loss: 1.9948 - regression_loss: 1.4530 - classification_loss: 0.5419\n",
      " 32/500 [>.............................] - ETA: 7:45 - loss: 2.0246 - regression_loss: 1.4745 - classification_loss: 0.5501\n",
      " 33/500 [>.............................] - ETA: 7:45 - loss: 2.0076 - regression_loss: 1.4597 - classification_loss: 0.5479\n",
      " 34/500 [=>............................] - ETA: 7:45 - loss: 1.9998 - regression_loss: 1.4568 - classification_loss: 0.5430\n",
      " 35/500 [=>............................] - ETA: 7:44 - loss: 1.9935 - regression_loss: 1.4533 - classification_loss: 0.5402\n",
      " 36/500 [=>............................] - ETA: 7:43 - loss: 2.0018 - regression_loss: 1.4619 - classification_loss: 0.5400\n",
      " 37/500 [=>............................] - ETA: 7:40 - loss: 2.0124 - regression_loss: 1.4680 - classification_loss: 0.5444\n",
      " 38/500 [=>............................] - ETA: 7:39 - loss: 2.0386 - regression_loss: 1.4884 - classification_loss: 0.5502\n",
      " 39/500 [=>............................] - ETA: 7:36 - loss: 2.0296 - regression_loss: 1.4849 - classification_loss: 0.5447\n",
      " 40/500 [=>............................] - ETA: 7:37 - loss: 2.0148 - regression_loss: 1.4734 - classification_loss: 0.5414\n",
      " 41/500 [=>............................] - ETA: 7:38 - loss: 2.0097 - regression_loss: 1.4701 - classification_loss: 0.5396\n",
      " 42/500 [=>............................] - ETA: 7:37 - loss: 1.9889 - regression_loss: 1.4544 - classification_loss: 0.5345\n",
      " 43/500 [=>............................] - ETA: 7:36 - loss: 1.9793 - regression_loss: 1.4513 - classification_loss: 0.5280\n",
      " 44/500 [=>............................] - ETA: 7:35 - loss: 1.9894 - regression_loss: 1.4595 - classification_loss: 0.5300\n",
      " 45/500 [=>............................] - ETA: 7:33 - loss: 1.9926 - regression_loss: 1.4597 - classification_loss: 0.5328\n",
      " 46/500 [=>............................] - ETA: 7:32 - loss: 2.0045 - regression_loss: 1.4663 - classification_loss: 0.5383\n",
      " 47/500 [=>............................] - ETA: 7:30 - loss: 2.0132 - regression_loss: 1.4748 - classification_loss: 0.5384\n",
      " 48/500 [=>............................] - ETA: 7:30 - loss: 2.0023 - regression_loss: 1.4651 - classification_loss: 0.5372\n",
      " 49/500 [=>............................] - ETA: 7:29 - loss: 1.9913 - regression_loss: 1.4591 - classification_loss: 0.5322\n",
      " 50/500 [==>...........................] - ETA: 7:29 - loss: 1.9919 - regression_loss: 1.4619 - classification_loss: 0.5300\n",
      " 51/500 [==>...........................] - ETA: 7:29 - loss: 2.0124 - regression_loss: 1.4782 - classification_loss: 0.5342\n",
      " 52/500 [==>...........................] - ETA: 7:28 - loss: 1.9968 - regression_loss: 1.4666 - classification_loss: 0.5302\n",
      " 53/500 [==>...........................] - ETA: 7:27 - loss: 1.9984 - regression_loss: 1.4672 - classification_loss: 0.5312\n",
      " 54/500 [==>...........................] - ETA: 7:26 - loss: 1.9948 - regression_loss: 1.4653 - classification_loss: 0.5295\n",
      " 55/500 [==>...........................] - ETA: 7:26 - loss: 1.9992 - regression_loss: 1.4715 - classification_loss: 0.5277\n",
      " 56/500 [==>...........................] - ETA: 7:26 - loss: 2.0132 - regression_loss: 1.4811 - classification_loss: 0.5322\n",
      " 57/500 [==>...........................] - ETA: 7:25 - loss: 2.0224 - regression_loss: 1.4897 - classification_loss: 0.5327\n",
      " 58/500 [==>...........................] - ETA: 7:23 - loss: 2.0189 - regression_loss: 1.4856 - classification_loss: 0.5333\n",
      " 59/500 [==>...........................] - ETA: 7:23 - loss: 2.0156 - regression_loss: 1.4813 - classification_loss: 0.5344\n",
      " 60/500 [==>...........................] - ETA: 7:22 - loss: 2.0215 - regression_loss: 1.4829 - classification_loss: 0.5386\n",
      " 61/500 [==>...........................] - ETA: 7:23 - loss: 2.0386 - regression_loss: 1.4972 - classification_loss: 0.5414\n",
      " 62/500 [==>...........................] - ETA: 7:23 - loss: 2.0534 - regression_loss: 1.5039 - classification_loss: 0.5495\n",
      " 63/500 [==>...........................] - ETA: 7:22 - loss: 2.0515 - regression_loss: 1.5019 - classification_loss: 0.5496\n",
      " 64/500 [==>...........................] - ETA: 7:20 - loss: 2.0527 - regression_loss: 1.5024 - classification_loss: 0.5502\n",
      " 65/500 [==>...........................] - ETA: 7:19 - loss: 2.0527 - regression_loss: 1.5028 - classification_loss: 0.5500\n",
      " 66/500 [==>...........................] - ETA: 7:18 - loss: 2.0567 - regression_loss: 1.5044 - classification_loss: 0.5523\n",
      " 67/500 [===>..........................] - ETA: 7:17 - loss: 2.0488 - regression_loss: 1.4981 - classification_loss: 0.5508\n",
      " 68/500 [===>..........................] - ETA: 7:16 - loss: 2.0447 - regression_loss: 1.4935 - classification_loss: 0.5512\n",
      " 69/500 [===>..........................] - ETA: 7:14 - loss: 2.0491 - regression_loss: 1.4974 - classification_loss: 0.5517\n",
      " 70/500 [===>..........................] - ETA: 7:12 - loss: 2.0554 - regression_loss: 1.4984 - classification_loss: 0.5569\n",
      " 71/500 [===>..........................] - ETA: 7:12 - loss: 2.0431 - regression_loss: 1.4901 - classification_loss: 0.5530\n",
      " 72/500 [===>..........................] - ETA: 7:11 - loss: 2.0371 - regression_loss: 1.4852 - classification_loss: 0.5519\n",
      " 73/500 [===>..........................] - ETA: 7:11 - loss: 2.0424 - regression_loss: 1.4914 - classification_loss: 0.5510\n",
      " 74/500 [===>..........................] - ETA: 7:10 - loss: 2.0544 - regression_loss: 1.4995 - classification_loss: 0.5549\n",
      " 75/500 [===>..........................] - ETA: 7:09 - loss: 2.0620 - regression_loss: 1.5053 - classification_loss: 0.5566\n",
      " 76/500 [===>..........................] - ETA: 7:17 - loss: 2.0600 - regression_loss: 1.5048 - classification_loss: 0.5552\n",
      " 77/500 [===>..........................] - ETA: 7:16 - loss: 2.0703 - regression_loss: 1.5112 - classification_loss: 0.5592\n",
      " 78/500 [===>..........................] - ETA: 7:16 - loss: 2.0683 - regression_loss: 1.5078 - classification_loss: 0.5605\n",
      " 79/500 [===>..........................] - ETA: 7:15 - loss: 2.0845 - regression_loss: 1.5195 - classification_loss: 0.5650\n",
      " 80/500 [===>..........................] - ETA: 7:14 - loss: 2.0797 - regression_loss: 1.5155 - classification_loss: 0.5641\n",
      " 81/500 [===>..........................] - ETA: 7:13 - loss: 2.0891 - regression_loss: 1.5231 - classification_loss: 0.5660\n",
      " 82/500 [===>..........................] - ETA: 7:12 - loss: 2.0819 - regression_loss: 1.5180 - classification_loss: 0.5638\n",
      " 83/500 [===>..........................] - ETA: 7:10 - loss: 2.0744 - regression_loss: 1.5123 - classification_loss: 0.5621\n",
      " 84/500 [====>.........................] - ETA: 7:10 - loss: 2.0689 - regression_loss: 1.5083 - classification_loss: 0.5606\n",
      " 85/500 [====>.........................] - ETA: 7:09 - loss: 2.0805 - regression_loss: 1.5171 - classification_loss: 0.5634\n",
      " 86/500 [====>.........................] - ETA: 7:09 - loss: 2.0739 - regression_loss: 1.5120 - classification_loss: 0.5619\n",
      " 87/500 [====>.........................] - ETA: 7:08 - loss: 2.0763 - regression_loss: 1.5125 - classification_loss: 0.5638\n",
      " 88/500 [====>.........................] - ETA: 7:06 - loss: 2.0690 - regression_loss: 1.5077 - classification_loss: 0.5613\n",
      " 89/500 [====>.........................] - ETA: 7:06 - loss: 2.0753 - regression_loss: 1.5144 - classification_loss: 0.5610\n",
      " 90/500 [====>.........................] - ETA: 7:04 - loss: 2.0746 - regression_loss: 1.5144 - classification_loss: 0.5602\n",
      " 91/500 [====>.........................] - ETA: 7:03 - loss: 2.0835 - regression_loss: 1.5193 - classification_loss: 0.5642\n",
      " 92/500 [====>.........................] - ETA: 7:03 - loss: 2.0812 - regression_loss: 1.5166 - classification_loss: 0.5646\n",
      " 93/500 [====>.........................] - ETA: 7:02 - loss: 2.0807 - regression_loss: 1.5162 - classification_loss: 0.5645\n",
      " 94/500 [====>.........................] - ETA: 7:00 - loss: 2.0786 - regression_loss: 1.5147 - classification_loss: 0.5639\n",
      " 95/500 [====>.........................] - ETA: 7:00 - loss: 2.0719 - regression_loss: 1.5098 - classification_loss: 0.5621\n",
      " 96/500 [====>.........................] - ETA: 6:59 - loss: 2.0820 - regression_loss: 1.5183 - classification_loss: 0.5637\n",
      " 97/500 [====>.........................] - ETA: 6:58 - loss: 2.0857 - regression_loss: 1.5213 - classification_loss: 0.5644\n",
      " 98/500 [====>.........................] - ETA: 6:57 - loss: 2.0867 - regression_loss: 1.5202 - classification_loss: 0.5664\n",
      " 99/500 [====>.........................] - ETA: 6:56 - loss: 2.0930 - regression_loss: 1.5250 - classification_loss: 0.5680\n",
      "100/500 [=====>........................] - ETA: 6:54 - loss: 2.0971 - regression_loss: 1.5283 - classification_loss: 0.5688\n",
      "101/500 [=====>........................] - ETA: 6:53 - loss: 2.0967 - regression_loss: 1.5277 - classification_loss: 0.5690\n",
      "102/500 [=====>........................] - ETA: 6:52 - loss: 2.0965 - regression_loss: 1.5275 - classification_loss: 0.5690\n",
      "103/500 [=====>........................] - ETA: 6:51 - loss: 2.0917 - regression_loss: 1.5246 - classification_loss: 0.5671\n",
      "104/500 [=====>........................] - ETA: 6:50 - loss: 2.0901 - regression_loss: 1.5238 - classification_loss: 0.5663\n",
      "105/500 [=====>........................] - ETA: 6:49 - loss: 2.0897 - regression_loss: 1.5240 - classification_loss: 0.5657\n",
      "106/500 [=====>........................] - ETA: 6:48 - loss: 2.0914 - regression_loss: 1.5250 - classification_loss: 0.5664\n",
      "107/500 [=====>........................] - ETA: 6:47 - loss: 2.0849 - regression_loss: 1.5206 - classification_loss: 0.5644\n",
      "108/500 [=====>........................] - ETA: 6:46 - loss: 2.0800 - regression_loss: 1.5166 - classification_loss: 0.5634\n",
      "109/500 [=====>........................] - ETA: 6:44 - loss: 2.0781 - regression_loss: 1.5156 - classification_loss: 0.5625\n",
      "110/500 [=====>........................] - ETA: 6:44 - loss: 2.0795 - regression_loss: 1.5164 - classification_loss: 0.5632\n",
      "111/500 [=====>........................] - ETA: 6:43 - loss: 2.0808 - regression_loss: 1.5170 - classification_loss: 0.5638\n",
      "112/500 [=====>........................] - ETA: 6:42 - loss: 2.0815 - regression_loss: 1.5173 - classification_loss: 0.5642\n",
      "113/500 [=====>........................] - ETA: 6:41 - loss: 2.0707 - regression_loss: 1.5086 - classification_loss: 0.5621\n",
      "114/500 [=====>........................] - ETA: 6:40 - loss: 2.0686 - regression_loss: 1.5062 - classification_loss: 0.5624\n",
      "115/500 [=====>........................] - ETA: 6:39 - loss: 2.0725 - regression_loss: 1.5091 - classification_loss: 0.5634\n",
      "116/500 [=====>........................] - ETA: 6:38 - loss: 2.0740 - regression_loss: 1.5108 - classification_loss: 0.5632\n",
      "117/500 [======>.......................] - ETA: 6:37 - loss: 2.0713 - regression_loss: 1.5094 - classification_loss: 0.5619\n",
      "118/500 [======>.......................] - ETA: 6:36 - loss: 2.0772 - regression_loss: 1.5144 - classification_loss: 0.5627\n",
      "119/500 [======>.......................] - ETA: 6:35 - loss: 2.0747 - regression_loss: 1.5124 - classification_loss: 0.5623\n",
      "120/500 [======>.......................] - ETA: 6:35 - loss: 2.0811 - regression_loss: 1.5172 - classification_loss: 0.5640\n",
      "121/500 [======>.......................] - ETA: 6:34 - loss: 2.0849 - regression_loss: 1.5213 - classification_loss: 0.5636\n",
      "122/500 [======>.......................] - ETA: 6:33 - loss: 2.0839 - regression_loss: 1.5203 - classification_loss: 0.5636\n",
      "123/500 [======>.......................] - ETA: 6:32 - loss: 2.0806 - regression_loss: 1.5178 - classification_loss: 0.5628\n",
      "124/500 [======>.......................] - ETA: 6:31 - loss: 2.0838 - regression_loss: 1.5193 - classification_loss: 0.5646\n",
      "125/500 [======>.......................] - ETA: 6:29 - loss: 2.0839 - regression_loss: 1.5197 - classification_loss: 0.5642\n",
      "126/500 [======>.......................] - ETA: 6:35 - loss: 2.0857 - regression_loss: 1.5204 - classification_loss: 0.5653\n",
      "127/500 [======>.......................] - ETA: 6:34 - loss: 2.0796 - regression_loss: 1.5157 - classification_loss: 0.5638\n",
      "128/500 [======>.......................] - ETA: 6:33 - loss: 2.0789 - regression_loss: 1.5143 - classification_loss: 0.5647\n",
      "129/500 [======>.......................] - ETA: 6:32 - loss: 2.0858 - regression_loss: 1.5208 - classification_loss: 0.5650\n",
      "130/500 [======>.......................] - ETA: 6:31 - loss: 2.0821 - regression_loss: 1.5185 - classification_loss: 0.5636\n",
      "131/500 [======>.......................] - ETA: 6:30 - loss: 2.0799 - regression_loss: 1.5167 - classification_loss: 0.5632\n",
      "132/500 [======>.......................] - ETA: 6:29 - loss: 2.0817 - regression_loss: 1.5178 - classification_loss: 0.5639\n",
      "133/500 [======>.......................] - ETA: 6:28 - loss: 2.0776 - regression_loss: 1.5148 - classification_loss: 0.5627\n",
      "134/500 [=======>......................] - ETA: 6:27 - loss: 2.0846 - regression_loss: 1.5202 - classification_loss: 0.5644\n",
      "135/500 [=======>......................] - ETA: 6:26 - loss: 2.0796 - regression_loss: 1.5162 - classification_loss: 0.5634\n",
      "136/500 [=======>......................] - ETA: 6:24 - loss: 2.0845 - regression_loss: 1.5196 - classification_loss: 0.5650\n",
      "137/500 [=======>......................] - ETA: 6:23 - loss: 2.0863 - regression_loss: 1.5224 - classification_loss: 0.5639\n",
      "138/500 [=======>......................] - ETA: 6:22 - loss: 2.0859 - regression_loss: 1.5227 - classification_loss: 0.5632\n",
      "139/500 [=======>......................] - ETA: 6:21 - loss: 2.0868 - regression_loss: 1.5247 - classification_loss: 0.5621\n",
      "140/500 [=======>......................] - ETA: 6:20 - loss: 2.0879 - regression_loss: 1.5266 - classification_loss: 0.5614\n",
      "141/500 [=======>......................] - ETA: 6:19 - loss: 2.0851 - regression_loss: 1.5239 - classification_loss: 0.5612\n",
      "142/500 [=======>......................] - ETA: 6:18 - loss: 2.0880 - regression_loss: 1.5262 - classification_loss: 0.5618\n",
      "143/500 [=======>......................] - ETA: 6:17 - loss: 2.0925 - regression_loss: 1.5292 - classification_loss: 0.5634\n",
      "144/500 [=======>......................] - ETA: 6:16 - loss: 2.0915 - regression_loss: 1.5278 - classification_loss: 0.5637\n",
      "145/500 [=======>......................] - ETA: 6:15 - loss: 2.0924 - regression_loss: 1.5286 - classification_loss: 0.5638\n",
      "146/500 [=======>......................] - ETA: 6:14 - loss: 2.0897 - regression_loss: 1.5260 - classification_loss: 0.5637\n",
      "147/500 [=======>......................] - ETA: 6:13 - loss: 2.0874 - regression_loss: 1.5243 - classification_loss: 0.5631\n",
      "148/500 [=======>......................] - ETA: 6:12 - loss: 2.0941 - regression_loss: 1.5282 - classification_loss: 0.5659\n",
      "149/500 [=======>......................] - ETA: 6:11 - loss: 2.0936 - regression_loss: 1.5283 - classification_loss: 0.5653\n",
      "150/500 [========>.....................] - ETA: 6:09 - loss: 2.0888 - regression_loss: 1.5249 - classification_loss: 0.5639\n",
      "151/500 [========>.....................] - ETA: 6:09 - loss: 2.0872 - regression_loss: 1.5243 - classification_loss: 0.5629\n",
      "152/500 [========>.....................] - ETA: 6:07 - loss: 2.0866 - regression_loss: 1.5238 - classification_loss: 0.5629\n",
      "153/500 [========>.....................] - ETA: 6:06 - loss: 2.0844 - regression_loss: 1.5220 - classification_loss: 0.5624\n",
      "154/500 [========>.....................] - ETA: 6:05 - loss: 2.0840 - regression_loss: 1.5224 - classification_loss: 0.5617\n",
      "155/500 [========>.....................] - ETA: 6:04 - loss: 2.0876 - regression_loss: 1.5244 - classification_loss: 0.5632\n",
      "156/500 [========>.....................] - ETA: 6:04 - loss: 2.0876 - regression_loss: 1.5250 - classification_loss: 0.5626\n",
      "157/500 [========>.....................] - ETA: 6:03 - loss: 2.0858 - regression_loss: 1.5237 - classification_loss: 0.5621\n",
      "158/500 [========>.....................] - ETA: 6:02 - loss: 2.0932 - regression_loss: 1.5291 - classification_loss: 0.5640\n",
      "159/500 [========>.....................] - ETA: 6:00 - loss: 2.0945 - regression_loss: 1.5300 - classification_loss: 0.5645\n",
      "160/500 [========>.....................] - ETA: 5:59 - loss: 2.0940 - regression_loss: 1.5299 - classification_loss: 0.5641\n",
      "161/500 [========>.....................] - ETA: 5:58 - loss: 2.0924 - regression_loss: 1.5282 - classification_loss: 0.5642\n",
      "162/500 [========>.....................] - ETA: 5:57 - loss: 2.0934 - regression_loss: 1.5284 - classification_loss: 0.5650\n",
      "163/500 [========>.....................] - ETA: 5:56 - loss: 2.0966 - regression_loss: 1.5312 - classification_loss: 0.5654\n",
      "164/500 [========>.....................] - ETA: 5:55 - loss: 2.0946 - regression_loss: 1.5293 - classification_loss: 0.5653\n",
      "165/500 [========>.....................] - ETA: 5:54 - loss: 2.0997 - regression_loss: 1.5329 - classification_loss: 0.5667\n",
      "166/500 [========>.....................] - ETA: 5:53 - loss: 2.1002 - regression_loss: 1.5334 - classification_loss: 0.5668\n",
      "167/500 [=========>....................] - ETA: 5:52 - loss: 2.1008 - regression_loss: 1.5338 - classification_loss: 0.5670\n",
      "168/500 [=========>....................] - ETA: 5:50 - loss: 2.0986 - regression_loss: 1.5325 - classification_loss: 0.5661\n",
      "169/500 [=========>....................] - ETA: 5:49 - loss: 2.0979 - regression_loss: 1.5327 - classification_loss: 0.5652\n",
      "170/500 [=========>....................] - ETA: 5:48 - loss: 2.0935 - regression_loss: 1.5294 - classification_loss: 0.5641\n",
      "171/500 [=========>....................] - ETA: 5:53 - loss: 2.0927 - regression_loss: 1.5297 - classification_loss: 0.5630\n",
      "172/500 [=========>....................] - ETA: 5:52 - loss: 2.0959 - regression_loss: 1.5332 - classification_loss: 0.5627\n",
      "173/500 [=========>....................] - ETA: 5:51 - loss: 2.0955 - regression_loss: 1.5323 - classification_loss: 0.5632\n",
      "174/500 [=========>....................] - ETA: 5:49 - loss: 2.0957 - regression_loss: 1.5329 - classification_loss: 0.5629\n",
      "175/500 [=========>....................] - ETA: 5:48 - loss: 2.0985 - regression_loss: 1.5344 - classification_loss: 0.5641\n",
      "176/500 [=========>....................] - ETA: 5:47 - loss: 2.1021 - regression_loss: 1.5374 - classification_loss: 0.5647\n",
      "177/500 [=========>....................] - ETA: 5:46 - loss: 2.1056 - regression_loss: 1.5401 - classification_loss: 0.5656\n",
      "178/500 [=========>....................] - ETA: 5:44 - loss: 2.1070 - regression_loss: 1.5416 - classification_loss: 0.5654\n",
      "179/500 [=========>....................] - ETA: 5:43 - loss: 2.1005 - regression_loss: 1.5367 - classification_loss: 0.5638\n",
      "180/500 [=========>....................] - ETA: 5:42 - loss: 2.1021 - regression_loss: 1.5375 - classification_loss: 0.5646\n",
      "181/500 [=========>....................] - ETA: 5:41 - loss: 2.1023 - regression_loss: 1.5370 - classification_loss: 0.5654\n",
      "182/500 [=========>....................] - ETA: 5:40 - loss: 2.1000 - regression_loss: 1.5354 - classification_loss: 0.5646\n",
      "183/500 [=========>....................] - ETA: 5:39 - loss: 2.0989 - regression_loss: 1.5348 - classification_loss: 0.5641\n",
      "184/500 [==========>...................] - ETA: 5:38 - loss: 2.0988 - regression_loss: 1.5339 - classification_loss: 0.5648\n",
      "185/500 [==========>...................] - ETA: 5:37 - loss: 2.0988 - regression_loss: 1.5342 - classification_loss: 0.5646\n",
      "186/500 [==========>...................] - ETA: 5:36 - loss: 2.0996 - regression_loss: 1.5348 - classification_loss: 0.5648\n",
      "187/500 [==========>...................] - ETA: 5:35 - loss: 2.1066 - regression_loss: 1.5401 - classification_loss: 0.5665\n",
      "188/500 [==========>...................] - ETA: 5:34 - loss: 2.1058 - regression_loss: 1.5387 - classification_loss: 0.5671\n",
      "189/500 [==========>...................] - ETA: 5:32 - loss: 2.1038 - regression_loss: 1.5382 - classification_loss: 0.5656\n",
      "190/500 [==========>...................] - ETA: 5:31 - loss: 2.1038 - regression_loss: 1.5389 - classification_loss: 0.5650\n",
      "191/500 [==========>...................] - ETA: 5:30 - loss: 2.1076 - regression_loss: 1.5429 - classification_loss: 0.5648\n",
      "192/500 [==========>...................] - ETA: 5:29 - loss: 2.1103 - regression_loss: 1.5448 - classification_loss: 0.5655\n",
      "193/500 [==========>...................] - ETA: 5:28 - loss: 2.1087 - regression_loss: 1.5435 - classification_loss: 0.5651\n",
      "194/500 [==========>...................] - ETA: 5:27 - loss: 2.1160 - regression_loss: 1.5488 - classification_loss: 0.5671\n",
      "195/500 [==========>...................] - ETA: 5:26 - loss: 2.1184 - regression_loss: 1.5512 - classification_loss: 0.5672\n",
      "196/500 [==========>...................] - ETA: 5:25 - loss: 2.1175 - regression_loss: 1.5514 - classification_loss: 0.5661\n",
      "197/500 [==========>...................] - ETA: 5:24 - loss: 2.1195 - regression_loss: 1.5533 - classification_loss: 0.5662\n",
      "198/500 [==========>...................] - ETA: 5:23 - loss: 2.1212 - regression_loss: 1.5543 - classification_loss: 0.5670\n",
      "199/500 [==========>...................] - ETA: 5:22 - loss: 2.1186 - regression_loss: 1.5526 - classification_loss: 0.5660\n",
      "200/500 [===========>..................] - ETA: 5:21 - loss: 2.1125 - regression_loss: 1.5481 - classification_loss: 0.5644\n",
      "201/500 [===========>..................] - ETA: 5:19 - loss: 2.1154 - regression_loss: 1.5500 - classification_loss: 0.5654\n",
      "202/500 [===========>..................] - ETA: 5:18 - loss: 2.1150 - regression_loss: 1.5486 - classification_loss: 0.5664\n",
      "203/500 [===========>..................] - ETA: 5:17 - loss: 2.1169 - regression_loss: 1.5502 - classification_loss: 0.5668\n",
      "204/500 [===========>..................] - ETA: 5:16 - loss: 2.1196 - regression_loss: 1.5528 - classification_loss: 0.5668\n",
      "205/500 [===========>..................] - ETA: 5:15 - loss: 2.1187 - regression_loss: 1.5518 - classification_loss: 0.5668\n",
      "206/500 [===========>..................] - ETA: 5:13 - loss: 2.1217 - regression_loss: 1.5547 - classification_loss: 0.5670\n",
      "207/500 [===========>..................] - ETA: 5:12 - loss: 2.1248 - regression_loss: 1.5570 - classification_loss: 0.5677\n",
      "208/500 [===========>..................] - ETA: 5:11 - loss: 2.1250 - regression_loss: 1.5565 - classification_loss: 0.5685\n",
      "209/500 [===========>..................] - ETA: 5:10 - loss: 2.1239 - regression_loss: 1.5557 - classification_loss: 0.5682\n",
      "210/500 [===========>..................] - ETA: 5:09 - loss: 2.1256 - regression_loss: 1.5574 - classification_loss: 0.5682\n",
      "211/500 [===========>..................] - ETA: 5:08 - loss: 2.1245 - regression_loss: 1.5568 - classification_loss: 0.5677\n",
      "212/500 [===========>..................] - ETA: 5:07 - loss: 2.1247 - regression_loss: 1.5570 - classification_loss: 0.5677\n",
      "213/500 [===========>..................] - ETA: 5:06 - loss: 2.1272 - regression_loss: 1.5592 - classification_loss: 0.5680\n",
      "214/500 [===========>..................] - ETA: 5:04 - loss: 2.1283 - regression_loss: 1.5606 - classification_loss: 0.5677\n",
      "215/500 [===========>..................] - ETA: 5:03 - loss: 2.1272 - regression_loss: 1.5594 - classification_loss: 0.5678\n",
      "216/500 [===========>..................] - ETA: 5:02 - loss: 2.1259 - regression_loss: 1.5579 - classification_loss: 0.5680\n",
      "217/500 [============>.................] - ETA: 5:01 - loss: 2.1258 - regression_loss: 1.5577 - classification_loss: 0.5682\n",
      "218/500 [============>.................] - ETA: 5:00 - loss: 2.1257 - regression_loss: 1.5576 - classification_loss: 0.5681\n",
      "219/500 [============>.................] - ETA: 4:59 - loss: 2.1241 - regression_loss: 1.5562 - classification_loss: 0.5679\n",
      "220/500 [============>.................] - ETA: 4:58 - loss: 2.1232 - regression_loss: 1.5549 - classification_loss: 0.5683\n",
      "221/500 [============>.................] - ETA: 4:57 - loss: 2.1205 - regression_loss: 1.5526 - classification_loss: 0.5679\n",
      "222/500 [============>.................] - ETA: 4:55 - loss: 2.1176 - regression_loss: 1.5508 - classification_loss: 0.5668\n",
      "223/500 [============>.................] - ETA: 4:55 - loss: 2.1181 - regression_loss: 1.5511 - classification_loss: 0.5670\n",
      "224/500 [============>.................] - ETA: 4:53 - loss: 2.1227 - regression_loss: 1.5548 - classification_loss: 0.5679\n",
      "225/500 [============>.................] - ETA: 4:52 - loss: 2.1212 - regression_loss: 1.5537 - classification_loss: 0.5675\n",
      "226/500 [============>.................] - ETA: 4:51 - loss: 2.1190 - regression_loss: 1.5518 - classification_loss: 0.5672\n",
      "227/500 [============>.................] - ETA: 4:50 - loss: 2.1173 - regression_loss: 1.5502 - classification_loss: 0.5671\n",
      "228/500 [============>.................] - ETA: 4:49 - loss: 2.1174 - regression_loss: 1.5505 - classification_loss: 0.5669\n",
      "229/500 [============>.................] - ETA: 4:48 - loss: 2.1147 - regression_loss: 1.5486 - classification_loss: 0.5661\n",
      "230/500 [============>.................] - ETA: 4:47 - loss: 2.1131 - regression_loss: 1.5477 - classification_loss: 0.5654\n",
      "231/500 [============>.................] - ETA: 4:46 - loss: 2.1105 - regression_loss: 1.5460 - classification_loss: 0.5645\n",
      "232/500 [============>.................] - ETA: 4:44 - loss: 2.1086 - regression_loss: 1.5444 - classification_loss: 0.5642\n",
      "233/500 [============>.................] - ETA: 4:43 - loss: 2.1115 - regression_loss: 1.5467 - classification_loss: 0.5649\n",
      "234/500 [=============>................] - ETA: 4:42 - loss: 2.1133 - regression_loss: 1.5485 - classification_loss: 0.5648\n",
      "235/500 [=============>................] - ETA: 4:41 - loss: 2.1130 - regression_loss: 1.5484 - classification_loss: 0.5646\n",
      "236/500 [=============>................] - ETA: 4:40 - loss: 2.1113 - regression_loss: 1.5475 - classification_loss: 0.5639\n",
      "237/500 [=============>................] - ETA: 4:39 - loss: 2.1108 - regression_loss: 1.5472 - classification_loss: 0.5636\n",
      "238/500 [=============>................] - ETA: 4:38 - loss: 2.1122 - regression_loss: 1.5486 - classification_loss: 0.5636\n",
      "239/500 [=============>................] - ETA: 4:37 - loss: 2.1094 - regression_loss: 1.5466 - classification_loss: 0.5628\n",
      "240/500 [=============>................] - ETA: 4:35 - loss: 2.1128 - regression_loss: 1.5494 - classification_loss: 0.5634\n",
      "241/500 [=============>................] - ETA: 4:34 - loss: 2.1108 - regression_loss: 1.5478 - classification_loss: 0.5630\n",
      "242/500 [=============>................] - ETA: 4:33 - loss: 2.1087 - regression_loss: 1.5465 - classification_loss: 0.5622\n",
      "243/500 [=============>................] - ETA: 4:32 - loss: 2.1085 - regression_loss: 1.5465 - classification_loss: 0.5620\n",
      "244/500 [=============>................] - ETA: 4:31 - loss: 2.1076 - regression_loss: 1.5461 - classification_loss: 0.5616\n",
      "245/500 [=============>................] - ETA: 4:30 - loss: 2.1062 - regression_loss: 1.5443 - classification_loss: 0.5619\n",
      "246/500 [=============>................] - ETA: 4:29 - loss: 2.1040 - regression_loss: 1.5424 - classification_loss: 0.5616\n",
      "247/500 [=============>................] - ETA: 4:28 - loss: 2.1036 - regression_loss: 1.5411 - classification_loss: 0.5625\n",
      "248/500 [=============>................] - ETA: 4:27 - loss: 2.1055 - regression_loss: 1.5431 - classification_loss: 0.5624\n",
      "249/500 [=============>................] - ETA: 4:26 - loss: 2.1049 - regression_loss: 1.5431 - classification_loss: 0.5618\n",
      "250/500 [==============>...............] - ETA: 4:25 - loss: 2.1047 - regression_loss: 1.5434 - classification_loss: 0.5612\n",
      "251/500 [==============>...............] - ETA: 4:24 - loss: 2.1065 - regression_loss: 1.5451 - classification_loss: 0.5614\n",
      "252/500 [==============>...............] - ETA: 4:22 - loss: 2.1031 - regression_loss: 1.5424 - classification_loss: 0.5606\n",
      "253/500 [==============>...............] - ETA: 4:21 - loss: 2.1017 - regression_loss: 1.5413 - classification_loss: 0.5605\n",
      "254/500 [==============>...............] - ETA: 4:20 - loss: 2.1025 - regression_loss: 1.5418 - classification_loss: 0.5607\n",
      "255/500 [==============>...............] - ETA: 4:19 - loss: 2.1024 - regression_loss: 1.5405 - classification_loss: 0.5619\n",
      "256/500 [==============>...............] - ETA: 4:18 - loss: 2.1026 - regression_loss: 1.5412 - classification_loss: 0.5614\n",
      "257/500 [==============>...............] - ETA: 4:17 - loss: 2.1021 - regression_loss: 1.5413 - classification_loss: 0.5608\n",
      "258/500 [==============>...............] - ETA: 4:16 - loss: 2.1038 - regression_loss: 1.5422 - classification_loss: 0.5615\n",
      "259/500 [==============>...............] - ETA: 4:15 - loss: 2.1030 - regression_loss: 1.5415 - classification_loss: 0.5615\n",
      "260/500 [==============>...............] - ETA: 4:14 - loss: 2.1010 - regression_loss: 1.5399 - classification_loss: 0.5611\n",
      "261/500 [==============>...............] - ETA: 4:12 - loss: 2.1011 - regression_loss: 1.5393 - classification_loss: 0.5618\n",
      "262/500 [==============>...............] - ETA: 4:11 - loss: 2.1015 - regression_loss: 1.5396 - classification_loss: 0.5619\n",
      "263/500 [==============>...............] - ETA: 4:10 - loss: 2.1011 - regression_loss: 1.5394 - classification_loss: 0.5616\n",
      "264/500 [==============>...............] - ETA: 4:09 - loss: 2.0996 - regression_loss: 1.5384 - classification_loss: 0.5613\n",
      "265/500 [==============>...............] - ETA: 4:08 - loss: 2.1026 - regression_loss: 1.5405 - classification_loss: 0.5621\n",
      "266/500 [==============>...............] - ETA: 4:07 - loss: 2.1030 - regression_loss: 1.5405 - classification_loss: 0.5625\n",
      "267/500 [===============>..............] - ETA: 4:06 - loss: 2.1015 - regression_loss: 1.5398 - classification_loss: 0.5617\n",
      "268/500 [===============>..............] - ETA: 4:05 - loss: 2.1010 - regression_loss: 1.5392 - classification_loss: 0.5618\n",
      "269/500 [===============>..............] - ETA: 4:04 - loss: 2.1015 - regression_loss: 1.5391 - classification_loss: 0.5624\n",
      "270/500 [===============>..............] - ETA: 4:03 - loss: 2.0987 - regression_loss: 1.5370 - classification_loss: 0.5617\n",
      "271/500 [===============>..............] - ETA: 4:01 - loss: 2.0962 - regression_loss: 1.5356 - classification_loss: 0.5606\n",
      "272/500 [===============>..............] - ETA: 4:00 - loss: 2.0935 - regression_loss: 1.5337 - classification_loss: 0.5597\n",
      "273/500 [===============>..............] - ETA: 3:59 - loss: 2.0932 - regression_loss: 1.5324 - classification_loss: 0.5608\n",
      "274/500 [===============>..............] - ETA: 3:58 - loss: 2.0911 - regression_loss: 1.5310 - classification_loss: 0.5601\n",
      "275/500 [===============>..............] - ETA: 3:57 - loss: 2.0892 - regression_loss: 1.5293 - classification_loss: 0.5599\n",
      "276/500 [===============>..............] - ETA: 3:56 - loss: 2.0895 - regression_loss: 1.5294 - classification_loss: 0.5601\n",
      "277/500 [===============>..............] - ETA: 3:55 - loss: 2.0889 - regression_loss: 1.5292 - classification_loss: 0.5597\n",
      "278/500 [===============>..............] - ETA: 3:54 - loss: 2.0881 - regression_loss: 1.5288 - classification_loss: 0.5593\n",
      "279/500 [===============>..............] - ETA: 3:53 - loss: 2.0874 - regression_loss: 1.5280 - classification_loss: 0.5594\n",
      "280/500 [===============>..............] - ETA: 3:52 - loss: 2.0851 - regression_loss: 1.5268 - classification_loss: 0.5583\n",
      "281/500 [===============>..............] - ETA: 3:51 - loss: 2.0896 - regression_loss: 1.5300 - classification_loss: 0.5596\n",
      "282/500 [===============>..............] - ETA: 3:50 - loss: 2.0906 - regression_loss: 1.5309 - classification_loss: 0.5597\n",
      "283/500 [===============>..............] - ETA: 3:49 - loss: 2.0891 - regression_loss: 1.5301 - classification_loss: 0.5589\n",
      "284/500 [================>.............] - ETA: 3:48 - loss: 2.0886 - regression_loss: 1.5296 - classification_loss: 0.5590\n",
      "285/500 [================>.............] - ETA: 3:46 - loss: 2.0871 - regression_loss: 1.5284 - classification_loss: 0.5587\n",
      "286/500 [================>.............] - ETA: 3:46 - loss: 2.0872 - regression_loss: 1.5284 - classification_loss: 0.5588\n",
      "287/500 [================>.............] - ETA: 3:45 - loss: 2.0870 - regression_loss: 1.5286 - classification_loss: 0.5583\n",
      "288/500 [================>.............] - ETA: 3:43 - loss: 2.0895 - regression_loss: 1.5312 - classification_loss: 0.5583\n",
      "289/500 [================>.............] - ETA: 3:42 - loss: 2.0865 - regression_loss: 1.5287 - classification_loss: 0.5578\n",
      "290/500 [================>.............] - ETA: 3:41 - loss: 2.0883 - regression_loss: 1.5302 - classification_loss: 0.5580\n",
      "291/500 [================>.............] - ETA: 3:40 - loss: 2.0892 - regression_loss: 1.5311 - classification_loss: 0.5581\n",
      "292/500 [================>.............] - ETA: 3:39 - loss: 2.0876 - regression_loss: 1.5299 - classification_loss: 0.5577\n",
      "293/500 [================>.............] - ETA: 3:38 - loss: 2.0856 - regression_loss: 1.5287 - classification_loss: 0.5570\n",
      "294/500 [================>.............] - ETA: 3:37 - loss: 2.0848 - regression_loss: 1.5280 - classification_loss: 0.5568\n",
      "295/500 [================>.............] - ETA: 3:36 - loss: 2.0867 - regression_loss: 1.5286 - classification_loss: 0.5581\n",
      "296/500 [================>.............] - ETA: 3:35 - loss: 2.0857 - regression_loss: 1.5280 - classification_loss: 0.5577\n",
      "297/500 [================>.............] - ETA: 3:34 - loss: 2.0862 - regression_loss: 1.5283 - classification_loss: 0.5579\n",
      "298/500 [================>.............] - ETA: 3:33 - loss: 2.0846 - regression_loss: 1.5265 - classification_loss: 0.5580\n",
      "299/500 [================>.............] - ETA: 3:32 - loss: 2.0835 - regression_loss: 1.5261 - classification_loss: 0.5575\n",
      "300/500 [=================>............] - ETA: 3:30 - loss: 2.0820 - regression_loss: 1.5252 - classification_loss: 0.5569\n",
      "301/500 [=================>............] - ETA: 3:29 - loss: 2.0821 - regression_loss: 1.5250 - classification_loss: 0.5570\n",
      "302/500 [=================>............] - ETA: 3:28 - loss: 2.0831 - regression_loss: 1.5262 - classification_loss: 0.5569\n",
      "303/500 [=================>............] - ETA: 3:27 - loss: 2.0835 - regression_loss: 1.5266 - classification_loss: 0.5569\n",
      "304/500 [=================>............] - ETA: 3:26 - loss: 2.0811 - regression_loss: 1.5245 - classification_loss: 0.5566\n",
      "305/500 [=================>............] - ETA: 3:25 - loss: 2.0786 - regression_loss: 1.5224 - classification_loss: 0.5562\n",
      "306/500 [=================>............] - ETA: 3:24 - loss: 2.0779 - regression_loss: 1.5224 - classification_loss: 0.5555\n",
      "307/500 [=================>............] - ETA: 3:23 - loss: 2.0784 - regression_loss: 1.5225 - classification_loss: 0.5559\n",
      "308/500 [=================>............] - ETA: 3:22 - loss: 2.0766 - regression_loss: 1.5213 - classification_loss: 0.5553\n",
      "309/500 [=================>............] - ETA: 3:21 - loss: 2.0768 - regression_loss: 1.5214 - classification_loss: 0.5555\n",
      "310/500 [=================>............] - ETA: 3:20 - loss: 2.0769 - regression_loss: 1.5214 - classification_loss: 0.5555\n",
      "311/500 [=================>............] - ETA: 3:19 - loss: 2.0760 - regression_loss: 1.5203 - classification_loss: 0.5557\n",
      "312/500 [=================>............] - ETA: 3:18 - loss: 2.0755 - regression_loss: 1.5196 - classification_loss: 0.5559\n",
      "313/500 [=================>............] - ETA: 3:16 - loss: 2.0756 - regression_loss: 1.5201 - classification_loss: 0.5555\n",
      "314/500 [=================>............] - ETA: 3:15 - loss: 2.0736 - regression_loss: 1.5186 - classification_loss: 0.5550\n",
      "315/500 [=================>............] - ETA: 3:14 - loss: 2.0731 - regression_loss: 1.5178 - classification_loss: 0.5552\n",
      "316/500 [=================>............] - ETA: 3:13 - loss: 2.0729 - regression_loss: 1.5176 - classification_loss: 0.5553\n",
      "317/500 [==================>...........] - ETA: 3:12 - loss: 2.0720 - regression_loss: 1.5167 - classification_loss: 0.5553\n",
      "318/500 [==================>...........] - ETA: 3:11 - loss: 2.0717 - regression_loss: 1.5167 - classification_loss: 0.5551\n",
      "319/500 [==================>...........] - ETA: 3:10 - loss: 2.0705 - regression_loss: 1.5158 - classification_loss: 0.5546\n",
      "320/500 [==================>...........] - ETA: 3:09 - loss: 2.0710 - regression_loss: 1.5165 - classification_loss: 0.5545\n",
      "321/500 [==================>...........] - ETA: 3:08 - loss: 2.0702 - regression_loss: 1.5163 - classification_loss: 0.5539\n",
      "322/500 [==================>...........] - ETA: 3:07 - loss: 2.0732 - regression_loss: 1.5178 - classification_loss: 0.5554\n",
      "323/500 [==================>...........] - ETA: 3:06 - loss: 2.0746 - regression_loss: 1.5191 - classification_loss: 0.5556\n",
      "324/500 [==================>...........] - ETA: 3:05 - loss: 2.0740 - regression_loss: 1.5186 - classification_loss: 0.5553\n",
      "325/500 [==================>...........] - ETA: 3:04 - loss: 2.0749 - regression_loss: 1.5196 - classification_loss: 0.5554\n",
      "326/500 [==================>...........] - ETA: 3:03 - loss: 2.0749 - regression_loss: 1.5199 - classification_loss: 0.5550\n",
      "327/500 [==================>...........] - ETA: 3:02 - loss: 2.0768 - regression_loss: 1.5216 - classification_loss: 0.5551\n",
      "328/500 [==================>...........] - ETA: 3:00 - loss: 2.0782 - regression_loss: 1.5229 - classification_loss: 0.5553\n",
      "329/500 [==================>...........] - ETA: 2:59 - loss: 2.0762 - regression_loss: 1.5214 - classification_loss: 0.5549\n",
      "330/500 [==================>...........] - ETA: 2:58 - loss: 2.0778 - regression_loss: 1.5224 - classification_loss: 0.5554\n",
      "331/500 [==================>...........] - ETA: 2:57 - loss: 2.0790 - regression_loss: 1.5231 - classification_loss: 0.5559\n",
      "332/500 [==================>...........] - ETA: 2:56 - loss: 2.0801 - regression_loss: 1.5238 - classification_loss: 0.5563\n",
      "333/500 [==================>...........] - ETA: 2:55 - loss: 2.0795 - regression_loss: 1.5228 - classification_loss: 0.5567\n",
      "334/500 [===================>..........] - ETA: 2:54 - loss: 2.0796 - regression_loss: 1.5227 - classification_loss: 0.5568\n",
      "335/500 [===================>..........] - ETA: 2:53 - loss: 2.0791 - regression_loss: 1.5222 - classification_loss: 0.5569\n",
      "336/500 [===================>..........] - ETA: 2:52 - loss: 2.0804 - regression_loss: 1.5230 - classification_loss: 0.5574\n",
      "337/500 [===================>..........] - ETA: 2:51 - loss: 2.0791 - regression_loss: 1.5221 - classification_loss: 0.5569\n",
      "338/500 [===================>..........] - ETA: 2:50 - loss: 2.0803 - regression_loss: 1.5231 - classification_loss: 0.5572\n",
      "339/500 [===================>..........] - ETA: 2:49 - loss: 2.0810 - regression_loss: 1.5236 - classification_loss: 0.5574\n",
      "340/500 [===================>..........] - ETA: 2:48 - loss: 2.0790 - regression_loss: 1.5219 - classification_loss: 0.5571\n",
      "341/500 [===================>..........] - ETA: 2:47 - loss: 2.0784 - regression_loss: 1.5216 - classification_loss: 0.5568\n",
      "342/500 [===================>..........] - ETA: 2:46 - loss: 2.0757 - regression_loss: 1.5193 - classification_loss: 0.5564\n",
      "343/500 [===================>..........] - ETA: 2:45 - loss: 2.0757 - regression_loss: 1.5193 - classification_loss: 0.5563\n",
      "344/500 [===================>..........] - ETA: 2:44 - loss: 2.0737 - regression_loss: 1.5181 - classification_loss: 0.5556\n",
      "345/500 [===================>..........] - ETA: 2:43 - loss: 2.0750 - regression_loss: 1.5190 - classification_loss: 0.5560\n",
      "346/500 [===================>..........] - ETA: 2:42 - loss: 2.0763 - regression_loss: 1.5202 - classification_loss: 0.5561\n",
      "347/500 [===================>..........] - ETA: 2:41 - loss: 2.0754 - regression_loss: 1.5196 - classification_loss: 0.5558\n",
      "348/500 [===================>..........] - ETA: 2:40 - loss: 2.0769 - regression_loss: 1.5209 - classification_loss: 0.5560\n",
      "349/500 [===================>..........] - ETA: 2:38 - loss: 2.0760 - regression_loss: 1.5203 - classification_loss: 0.5556\n",
      "350/500 [====================>.........] - ETA: 2:37 - loss: 2.0764 - regression_loss: 1.5208 - classification_loss: 0.5556\n",
      "351/500 [====================>.........] - ETA: 2:36 - loss: 2.0791 - regression_loss: 1.5225 - classification_loss: 0.5566\n",
      "352/500 [====================>.........] - ETA: 2:35 - loss: 2.0812 - regression_loss: 1.5244 - classification_loss: 0.5568\n",
      "353/500 [====================>.........] - ETA: 2:34 - loss: 2.0830 - regression_loss: 1.5256 - classification_loss: 0.5574\n",
      "354/500 [====================>.........] - ETA: 2:33 - loss: 2.0840 - regression_loss: 1.5261 - classification_loss: 0.5579\n",
      "355/500 [====================>.........] - ETA: 2:32 - loss: 2.0863 - regression_loss: 1.5279 - classification_loss: 0.5583\n",
      "356/500 [====================>.........] - ETA: 2:31 - loss: 2.0857 - regression_loss: 1.5271 - classification_loss: 0.5586\n",
      "357/500 [====================>.........] - ETA: 2:30 - loss: 2.0867 - regression_loss: 1.5277 - classification_loss: 0.5590\n",
      "358/500 [====================>.........] - ETA: 2:29 - loss: 2.0864 - regression_loss: 1.5274 - classification_loss: 0.5590\n",
      "359/500 [====================>.........] - ETA: 2:28 - loss: 2.0843 - regression_loss: 1.5259 - classification_loss: 0.5584\n",
      "360/500 [====================>.........] - ETA: 2:27 - loss: 2.0842 - regression_loss: 1.5259 - classification_loss: 0.5583\n",
      "361/500 [====================>.........] - ETA: 2:26 - loss: 2.0833 - regression_loss: 1.5252 - classification_loss: 0.5581\n",
      "362/500 [====================>.........] - ETA: 2:25 - loss: 2.0817 - regression_loss: 1.5240 - classification_loss: 0.5577\n",
      "363/500 [====================>.........] - ETA: 2:24 - loss: 2.0831 - regression_loss: 1.5251 - classification_loss: 0.5579\n",
      "364/500 [====================>.........] - ETA: 2:22 - loss: 2.0818 - regression_loss: 1.5238 - classification_loss: 0.5580\n",
      "365/500 [====================>.........] - ETA: 2:21 - loss: 2.0826 - regression_loss: 1.5246 - classification_loss: 0.5581\n",
      "366/500 [====================>.........] - ETA: 2:20 - loss: 2.0852 - regression_loss: 1.5269 - classification_loss: 0.5583\n",
      "367/500 [=====================>........] - ETA: 2:19 - loss: 2.0863 - regression_loss: 1.5281 - classification_loss: 0.5583\n",
      "368/500 [=====================>........] - ETA: 2:18 - loss: 2.0862 - regression_loss: 1.5282 - classification_loss: 0.5579\n",
      "369/500 [=====================>........] - ETA: 2:17 - loss: 2.0869 - regression_loss: 1.5287 - classification_loss: 0.5582\n",
      "370/500 [=====================>........] - ETA: 2:16 - loss: 2.0875 - regression_loss: 1.5292 - classification_loss: 0.5583\n",
      "371/500 [=====================>........] - ETA: 2:15 - loss: 2.0867 - regression_loss: 1.5285 - classification_loss: 0.5582\n",
      "372/500 [=====================>........] - ETA: 2:14 - loss: 2.0864 - regression_loss: 1.5281 - classification_loss: 0.5583\n",
      "373/500 [=====================>........] - ETA: 2:13 - loss: 2.0855 - regression_loss: 1.5276 - classification_loss: 0.5580\n",
      "374/500 [=====================>........] - ETA: 2:12 - loss: 2.0860 - regression_loss: 1.5276 - classification_loss: 0.5584\n",
      "375/500 [=====================>........] - ETA: 2:11 - loss: 2.0870 - regression_loss: 1.5284 - classification_loss: 0.5586\n",
      "376/500 [=====================>........] - ETA: 2:10 - loss: 2.0912 - regression_loss: 1.5310 - classification_loss: 0.5603\n",
      "377/500 [=====================>........] - ETA: 2:09 - loss: 2.0911 - regression_loss: 1.5311 - classification_loss: 0.5600\n",
      "378/500 [=====================>........] - ETA: 2:08 - loss: 2.0907 - regression_loss: 1.5308 - classification_loss: 0.5599\n",
      "379/500 [=====================>........] - ETA: 2:07 - loss: 2.0917 - regression_loss: 1.5319 - classification_loss: 0.5598\n",
      "380/500 [=====================>........] - ETA: 2:06 - loss: 2.0941 - regression_loss: 1.5338 - classification_loss: 0.5603\n",
      "381/500 [=====================>........] - ETA: 2:05 - loss: 2.0948 - regression_loss: 1.5343 - classification_loss: 0.5605\n",
      "382/500 [=====================>........] - ETA: 2:03 - loss: 2.0947 - regression_loss: 1.5343 - classification_loss: 0.5604\n",
      "383/500 [=====================>........] - ETA: 2:02 - loss: 2.0956 - regression_loss: 1.5352 - classification_loss: 0.5605\n",
      "384/500 [======================>.......] - ETA: 2:01 - loss: 2.0929 - regression_loss: 1.5331 - classification_loss: 0.5598\n",
      "385/500 [======================>.......] - ETA: 2:00 - loss: 2.0915 - regression_loss: 1.5318 - classification_loss: 0.5597\n",
      "386/500 [======================>.......] - ETA: 1:59 - loss: 2.0915 - regression_loss: 1.5317 - classification_loss: 0.5598\n",
      "387/500 [======================>.......] - ETA: 1:58 - loss: 2.0903 - regression_loss: 1.5310 - classification_loss: 0.5593\n",
      "388/500 [======================>.......] - ETA: 1:57 - loss: 2.0897 - regression_loss: 1.5306 - classification_loss: 0.5591\n",
      "389/500 [======================>.......] - ETA: 1:56 - loss: 2.0892 - regression_loss: 1.5301 - classification_loss: 0.5591\n",
      "390/500 [======================>.......] - ETA: 1:55 - loss: 2.0918 - regression_loss: 1.5324 - classification_loss: 0.5594\n",
      "391/500 [======================>.......] - ETA: 1:54 - loss: 2.0901 - regression_loss: 1.5312 - classification_loss: 0.5590\n",
      "392/500 [======================>.......] - ETA: 1:53 - loss: 2.0880 - regression_loss: 1.5296 - classification_loss: 0.5584\n",
      "393/500 [======================>.......] - ETA: 1:52 - loss: 2.0887 - regression_loss: 1.5301 - classification_loss: 0.5586\n",
      "394/500 [======================>.......] - ETA: 1:51 - loss: 2.0863 - regression_loss: 1.5284 - classification_loss: 0.5580\n",
      "395/500 [======================>.......] - ETA: 1:50 - loss: 2.0879 - regression_loss: 1.5296 - classification_loss: 0.5583\n",
      "396/500 [======================>.......] - ETA: 1:49 - loss: 2.0880 - regression_loss: 1.5293 - classification_loss: 0.5587\n",
      "397/500 [======================>.......] - ETA: 1:48 - loss: 2.0871 - regression_loss: 1.5287 - classification_loss: 0.5584\n",
      "398/500 [======================>.......] - ETA: 1:47 - loss: 2.0854 - regression_loss: 1.5272 - classification_loss: 0.5582\n",
      "399/500 [======================>.......] - ETA: 1:45 - loss: 2.0866 - regression_loss: 1.5282 - classification_loss: 0.5584\n",
      "400/500 [=======================>......] - ETA: 1:44 - loss: 2.0858 - regression_loss: 1.5277 - classification_loss: 0.5581\n",
      "401/500 [=======================>......] - ETA: 1:43 - loss: 2.0882 - regression_loss: 1.5297 - classification_loss: 0.5585\n",
      "402/500 [=======================>......] - ETA: 1:42 - loss: 2.0898 - regression_loss: 1.5308 - classification_loss: 0.5590\n",
      "403/500 [=======================>......] - ETA: 1:41 - loss: 2.0881 - regression_loss: 1.5296 - classification_loss: 0.5585\n",
      "404/500 [=======================>......] - ETA: 1:40 - loss: 2.0894 - regression_loss: 1.5302 - classification_loss: 0.5592\n",
      "405/500 [=======================>......] - ETA: 1:39 - loss: 2.0898 - regression_loss: 1.5308 - classification_loss: 0.5591\n",
      "406/500 [=======================>......] - ETA: 1:38 - loss: 2.0903 - regression_loss: 1.5312 - classification_loss: 0.5591\n",
      "407/500 [=======================>......] - ETA: 1:37 - loss: 2.0900 - regression_loss: 1.5311 - classification_loss: 0.5588\n",
      "408/500 [=======================>......] - ETA: 1:36 - loss: 2.0896 - regression_loss: 1.5311 - classification_loss: 0.5585\n",
      "409/500 [=======================>......] - ETA: 1:35 - loss: 2.0902 - regression_loss: 1.5313 - classification_loss: 0.5589\n",
      "410/500 [=======================>......] - ETA: 1:34 - loss: 2.0889 - regression_loss: 1.5306 - classification_loss: 0.5583\n",
      "411/500 [=======================>......] - ETA: 1:33 - loss: 2.0889 - regression_loss: 1.5306 - classification_loss: 0.5583\n",
      "412/500 [=======================>......] - ETA: 1:32 - loss: 2.0893 - regression_loss: 1.5307 - classification_loss: 0.5586\n",
      "413/500 [=======================>......] - ETA: 1:31 - loss: 2.0885 - regression_loss: 1.5302 - classification_loss: 0.5583\n",
      "414/500 [=======================>......] - ETA: 1:30 - loss: 2.0879 - regression_loss: 1.5298 - classification_loss: 0.5581\n",
      "415/500 [=======================>......] - ETA: 1:29 - loss: 2.0890 - regression_loss: 1.5307 - classification_loss: 0.5583\n",
      "416/500 [=======================>......] - ETA: 1:28 - loss: 2.0882 - regression_loss: 1.5301 - classification_loss: 0.5580\n",
      "417/500 [========================>.....] - ETA: 1:27 - loss: 2.0875 - regression_loss: 1.5298 - classification_loss: 0.5578\n",
      "418/500 [========================>.....] - ETA: 1:26 - loss: 2.0848 - regression_loss: 1.5278 - classification_loss: 0.5569\n",
      "419/500 [========================>.....] - ETA: 1:25 - loss: 2.0858 - regression_loss: 1.5288 - classification_loss: 0.5570\n",
      "420/500 [========================>.....] - ETA: 1:24 - loss: 2.0860 - regression_loss: 1.5292 - classification_loss: 0.5569\n",
      "421/500 [========================>.....] - ETA: 1:23 - loss: 2.0860 - regression_loss: 1.5292 - classification_loss: 0.5568\n",
      "422/500 [========================>.....] - ETA: 1:22 - loss: 2.0875 - regression_loss: 1.5302 - classification_loss: 0.5573\n",
      "423/500 [========================>.....] - ETA: 1:21 - loss: 2.0875 - regression_loss: 1.5302 - classification_loss: 0.5573\n",
      "424/500 [========================>.....] - ETA: 1:19 - loss: 2.0881 - regression_loss: 1.5309 - classification_loss: 0.5572\n",
      "425/500 [========================>.....] - ETA: 1:18 - loss: 2.0909 - regression_loss: 1.5330 - classification_loss: 0.5580\n",
      "426/500 [========================>.....] - ETA: 1:17 - loss: 2.0888 - regression_loss: 1.5316 - classification_loss: 0.5572\n",
      "427/500 [========================>.....] - ETA: 1:16 - loss: 2.0909 - regression_loss: 1.5326 - classification_loss: 0.5582\n",
      "428/500 [========================>.....] - ETA: 1:15 - loss: 2.0893 - regression_loss: 1.5313 - classification_loss: 0.5580\n",
      "429/500 [========================>.....] - ETA: 1:14 - loss: 2.0881 - regression_loss: 1.5305 - classification_loss: 0.5576\n",
      "430/500 [========================>.....] - ETA: 1:13 - loss: 2.0874 - regression_loss: 1.5301 - classification_loss: 0.5574\n",
      "431/500 [========================>.....] - ETA: 1:12 - loss: 2.0867 - regression_loss: 1.5299 - classification_loss: 0.5568\n",
      "432/500 [========================>.....] - ETA: 1:11 - loss: 2.0862 - regression_loss: 1.5290 - classification_loss: 0.5571\n",
      "433/500 [========================>.....] - ETA: 1:10 - loss: 2.0882 - regression_loss: 1.5305 - classification_loss: 0.5577\n",
      "434/500 [=========================>....] - ETA: 1:09 - loss: 2.0860 - regression_loss: 1.5289 - classification_loss: 0.5571\n",
      "435/500 [=========================>....] - ETA: 1:08 - loss: 2.0871 - regression_loss: 1.5292 - classification_loss: 0.5579\n",
      "436/500 [=========================>....] - ETA: 1:07 - loss: 2.0888 - regression_loss: 1.5305 - classification_loss: 0.5582\n",
      "437/500 [=========================>....] - ETA: 1:06 - loss: 2.0881 - regression_loss: 1.5302 - classification_loss: 0.5579\n",
      "438/500 [=========================>....] - ETA: 1:05 - loss: 2.0885 - regression_loss: 1.5309 - classification_loss: 0.5577\n",
      "439/500 [=========================>....] - ETA: 1:04 - loss: 2.0893 - regression_loss: 1.5307 - classification_loss: 0.5586\n",
      "440/500 [=========================>....] - ETA: 1:03 - loss: 2.0892 - regression_loss: 1.5307 - classification_loss: 0.5584\n",
      "441/500 [=========================>....] - ETA: 1:02 - loss: 2.0888 - regression_loss: 1.5302 - classification_loss: 0.5586\n",
      "442/500 [=========================>....] - ETA: 1:00 - loss: 2.0909 - regression_loss: 1.5316 - classification_loss: 0.5593\n",
      "443/500 [=========================>....] - ETA: 59s - loss: 2.0904 - regression_loss: 1.5311 - classification_loss: 0.5593 \n",
      "444/500 [=========================>....] - ETA: 58s - loss: 2.0901 - regression_loss: 1.5308 - classification_loss: 0.5593\n",
      "445/500 [=========================>....] - ETA: 57s - loss: 2.0912 - regression_loss: 1.5316 - classification_loss: 0.5597\n",
      "446/500 [=========================>....] - ETA: 56s - loss: 2.0897 - regression_loss: 1.5303 - classification_loss: 0.5593\n",
      "447/500 [=========================>....] - ETA: 55s - loss: 2.0915 - regression_loss: 1.5314 - classification_loss: 0.5601\n",
      "448/500 [=========================>....] - ETA: 54s - loss: 2.0908 - regression_loss: 1.5310 - classification_loss: 0.5598\n",
      "449/500 [=========================>....] - ETA: 53s - loss: 2.0930 - regression_loss: 1.5327 - classification_loss: 0.5604\n",
      "450/500 [==========================>...] - ETA: 52s - loss: 2.0919 - regression_loss: 1.5320 - classification_loss: 0.5600\n",
      "451/500 [==========================>...] - ETA: 51s - loss: 2.0910 - regression_loss: 1.5313 - classification_loss: 0.5597\n",
      "452/500 [==========================>...] - ETA: 50s - loss: 2.0912 - regression_loss: 1.5314 - classification_loss: 0.5597\n",
      "453/500 [==========================>...] - ETA: 49s - loss: 2.0909 - regression_loss: 1.5313 - classification_loss: 0.5597\n",
      "454/500 [==========================>...] - ETA: 48s - loss: 2.0915 - regression_loss: 1.5320 - classification_loss: 0.5595\n",
      "455/500 [==========================>...] - ETA: 47s - loss: 2.0907 - regression_loss: 1.5314 - classification_loss: 0.5593\n",
      "456/500 [==========================>...] - ETA: 46s - loss: 2.0924 - regression_loss: 1.5326 - classification_loss: 0.5597\n",
      "457/500 [==========================>...] - ETA: 45s - loss: 2.0946 - regression_loss: 1.5342 - classification_loss: 0.5604\n",
      "458/500 [==========================>...] - ETA: 44s - loss: 2.0942 - regression_loss: 1.5337 - classification_loss: 0.5605\n",
      "459/500 [==========================>...] - ETA: 43s - loss: 2.0937 - regression_loss: 1.5332 - classification_loss: 0.5605\n",
      "460/500 [==========================>...] - ETA: 42s - loss: 2.0930 - regression_loss: 1.5327 - classification_loss: 0.5603\n",
      "461/500 [==========================>...] - ETA: 41s - loss: 2.0942 - regression_loss: 1.5336 - classification_loss: 0.5606\n",
      "462/500 [==========================>...] - ETA: 40s - loss: 2.0935 - regression_loss: 1.5333 - classification_loss: 0.5603\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 2.0947 - regression_loss: 1.5342 - classification_loss: 0.5605\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 2.0955 - regression_loss: 1.5342 - classification_loss: 0.5613\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 2.0968 - regression_loss: 1.5352 - classification_loss: 0.5616\n",
      "466/500 [==========================>...] - ETA: 35s - loss: 2.0956 - regression_loss: 1.5345 - classification_loss: 0.5612\n",
      "467/500 [===========================>..] - ETA: 34s - loss: 2.0948 - regression_loss: 1.5334 - classification_loss: 0.5614\n",
      "468/500 [===========================>..] - ETA: 33s - loss: 2.0936 - regression_loss: 1.5325 - classification_loss: 0.5611\n",
      "469/500 [===========================>..] - ETA: 32s - loss: 2.0942 - regression_loss: 1.5325 - classification_loss: 0.5617\n",
      "470/500 [===========================>..] - ETA: 31s - loss: 2.0944 - regression_loss: 1.5328 - classification_loss: 0.5616\n",
      "471/500 [===========================>..] - ETA: 30s - loss: 2.0958 - regression_loss: 1.5340 - classification_loss: 0.5618\n",
      "472/500 [===========================>..] - ETA: 29s - loss: 2.0969 - regression_loss: 1.5348 - classification_loss: 0.5621\n",
      "473/500 [===========================>..] - ETA: 28s - loss: 2.0975 - regression_loss: 1.5354 - classification_loss: 0.5621\n",
      "474/500 [===========================>..] - ETA: 27s - loss: 2.0971 - regression_loss: 1.5352 - classification_loss: 0.5619\n",
      "475/500 [===========================>..] - ETA: 26s - loss: 2.0970 - regression_loss: 1.5353 - classification_loss: 0.5617\n",
      "476/500 [===========================>..] - ETA: 25s - loss: 2.0968 - regression_loss: 1.5351 - classification_loss: 0.5616\n",
      "477/500 [===========================>..] - ETA: 24s - loss: 2.0955 - regression_loss: 1.5342 - classification_loss: 0.5614\n",
      "478/500 [===========================>..] - ETA: 23s - loss: 2.0966 - regression_loss: 1.5349 - classification_loss: 0.5617\n",
      "479/500 [===========================>..] - ETA: 22s - loss: 2.0973 - regression_loss: 1.5355 - classification_loss: 0.5619\n",
      "480/500 [===========================>..] - ETA: 21s - loss: 2.0979 - regression_loss: 1.5359 - classification_loss: 0.5620\n",
      "481/500 [===========================>..] - ETA: 20s - loss: 2.0990 - regression_loss: 1.5367 - classification_loss: 0.5623\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 2.0993 - regression_loss: 1.5369 - classification_loss: 0.5623\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 2.0981 - regression_loss: 1.5362 - classification_loss: 0.5619\n",
      "484/500 [============================>.] - ETA: 16s - loss: 2.0991 - regression_loss: 1.5367 - classification_loss: 0.5624\n",
      "485/500 [============================>.] - ETA: 15s - loss: 2.0984 - regression_loss: 1.5364 - classification_loss: 0.5621\n",
      "486/500 [============================>.] - ETA: 14s - loss: 2.0989 - regression_loss: 1.5367 - classification_loss: 0.5622\n",
      "487/500 [============================>.] - ETA: 13s - loss: 2.0978 - regression_loss: 1.5359 - classification_loss: 0.5619\n",
      "488/500 [============================>.] - ETA: 12s - loss: 2.0977 - regression_loss: 1.5359 - classification_loss: 0.5618\n",
      "489/500 [============================>.] - ETA: 11s - loss: 2.0969 - regression_loss: 1.5353 - classification_loss: 0.5616\n",
      "490/500 [============================>.] - ETA: 10s - loss: 2.0976 - regression_loss: 1.5361 - classification_loss: 0.5614\n",
      "491/500 [============================>.] - ETA: 9s - loss: 2.0965 - regression_loss: 1.5356 - classification_loss: 0.5609 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 2.0954 - regression_loss: 1.5348 - classification_loss: 0.5606\n",
      "493/500 [============================>.] - ETA: 7s - loss: 2.0941 - regression_loss: 1.5338 - classification_loss: 0.5603\n",
      "494/500 [============================>.] - ETA: 6s - loss: 2.0952 - regression_loss: 1.5348 - classification_loss: 0.5603\n",
      "495/500 [============================>.] - ETA: 5s - loss: 2.0943 - regression_loss: 1.5344 - classification_loss: 0.5599\n",
      "496/500 [============================>.] - ETA: 4s - loss: 2.0940 - regression_loss: 1.5343 - classification_loss: 0.5598\n",
      "497/500 [============================>.] - ETA: 3s - loss: 2.0929 - regression_loss: 1.5336 - classification_loss: 0.5593\n",
      "498/500 [============================>.] - ETA: 2s - loss: 2.0932 - regression_loss: 1.5339 - classification_loss: 0.5593\n",
      "499/500 [============================>.] - ETA: 1s - loss: 2.0934 - regression_loss: 1.5342 - classification_loss: 0.5592\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.0930 - regression_loss: 1.5340 - classification_loss: 0.5590\n",
      "Epoch 00005: saving model to ./snapshots\\resnet50_csv_05.h5\n",
      "\n",
      "500/500 [==============================] - 527s 1s/step - loss: 2.0930 - regression_loss: 1.5340 - classification_loss: 0.5590\n",
      "Epoch 6/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 2.1540 - regression_loss: 1.5043 - classification_loss: 0.6497\n",
      "  2/500 [..............................] - ETA: 4:07 - loss: 1.9873 - regression_loss: 1.3800 - classification_loss: 0.6073\n",
      "  3/500 [..............................] - ETA: 4:48 - loss: 1.9507 - regression_loss: 1.3900 - classification_loss: 0.5607\n",
      "  4/500 [..............................] - ETA: 5:47 - loss: 1.7232 - regression_loss: 1.2161 - classification_loss: 0.5071\n",
      "  5/500 [..............................] - ETA: 6:38 - loss: 1.9279 - regression_loss: 1.3457 - classification_loss: 0.5822\n",
      "  6/500 [..............................] - ETA: 6:59 - loss: 1.8969 - regression_loss: 1.3425 - classification_loss: 0.5544\n",
      "  7/500 [..............................] - ETA: 7:15 - loss: 1.9434 - regression_loss: 1.3910 - classification_loss: 0.5525\n",
      "  8/500 [..............................] - ETA: 7:20 - loss: 1.9390 - regression_loss: 1.3986 - classification_loss: 0.5403\n",
      "  9/500 [..............................] - ETA: 7:25 - loss: 1.8809 - regression_loss: 1.3626 - classification_loss: 0.5183\n",
      " 10/500 [..............................] - ETA: 7:33 - loss: 1.8200 - regression_loss: 1.3112 - classification_loss: 0.5087\n",
      " 11/500 [..............................] - ETA: 7:35 - loss: 1.8608 - regression_loss: 1.3576 - classification_loss: 0.5031\n",
      " 12/500 [..............................] - ETA: 7:40 - loss: 1.8624 - regression_loss: 1.3505 - classification_loss: 0.5119\n",
      " 13/500 [..............................] - ETA: 7:44 - loss: 1.8916 - regression_loss: 1.3700 - classification_loss: 0.5216\n",
      " 14/500 [..............................] - ETA: 7:48 - loss: 1.8944 - regression_loss: 1.3699 - classification_loss: 0.5245\n",
      " 15/500 [..............................] - ETA: 7:47 - loss: 1.9142 - regression_loss: 1.3743 - classification_loss: 0.5398\n",
      " 16/500 [..............................] - ETA: 7:50 - loss: 1.8899 - regression_loss: 1.3636 - classification_loss: 0.5263\n",
      " 17/500 [>.............................] - ETA: 7:49 - loss: 1.9105 - regression_loss: 1.3810 - classification_loss: 0.5295\n",
      " 18/500 [>.............................] - ETA: 7:42 - loss: 1.8780 - regression_loss: 1.3601 - classification_loss: 0.5180\n",
      " 19/500 [>.............................] - ETA: 7:48 - loss: 1.8662 - regression_loss: 1.3551 - classification_loss: 0.5111\n",
      " 20/500 [>.............................] - ETA: 7:48 - loss: 1.8838 - regression_loss: 1.3730 - classification_loss: 0.5108\n",
      " 21/500 [>.............................] - ETA: 7:47 - loss: 1.8621 - regression_loss: 1.3571 - classification_loss: 0.5050\n",
      " 22/500 [>.............................] - ETA: 7:49 - loss: 1.8591 - regression_loss: 1.3578 - classification_loss: 0.5014\n",
      " 23/500 [>.............................] - ETA: 7:43 - loss: 1.8614 - regression_loss: 1.3596 - classification_loss: 0.5018\n",
      " 24/500 [>.............................] - ETA: 7:47 - loss: 1.8783 - regression_loss: 1.3699 - classification_loss: 0.5084\n",
      " 25/500 [>.............................] - ETA: 7:48 - loss: 1.8708 - regression_loss: 1.3655 - classification_loss: 0.5053\n",
      " 26/500 [>.............................] - ETA: 7:47 - loss: 1.8597 - regression_loss: 1.3538 - classification_loss: 0.5059\n",
      " 27/500 [>.............................] - ETA: 7:48 - loss: 1.8507 - regression_loss: 1.3461 - classification_loss: 0.5045\n",
      " 28/500 [>.............................] - ETA: 7:48 - loss: 1.8777 - regression_loss: 1.3659 - classification_loss: 0.5118\n",
      " 29/500 [>.............................] - ETA: 7:47 - loss: 1.8804 - regression_loss: 1.3682 - classification_loss: 0.5122\n",
      " 30/500 [>.............................] - ETA: 7:48 - loss: 1.8650 - regression_loss: 1.3567 - classification_loss: 0.5083\n",
      " 31/500 [>.............................] - ETA: 7:48 - loss: 1.8663 - regression_loss: 1.3601 - classification_loss: 0.5063\n",
      " 32/500 [>.............................] - ETA: 7:48 - loss: 1.8625 - regression_loss: 1.3510 - classification_loss: 0.5114\n",
      " 33/500 [>.............................] - ETA: 7:47 - loss: 1.8595 - regression_loss: 1.3491 - classification_loss: 0.5104\n",
      " 34/500 [=>............................] - ETA: 7:46 - loss: 1.8672 - regression_loss: 1.3564 - classification_loss: 0.5108\n",
      " 35/500 [=>............................] - ETA: 7:43 - loss: 1.8568 - regression_loss: 1.3497 - classification_loss: 0.5071\n",
      " 36/500 [=>............................] - ETA: 7:44 - loss: 1.8430 - regression_loss: 1.3389 - classification_loss: 0.5041\n",
      " 37/500 [=>............................] - ETA: 7:44 - loss: 1.8312 - regression_loss: 1.3295 - classification_loss: 0.5017\n",
      " 38/500 [=>............................] - ETA: 7:43 - loss: 1.8510 - regression_loss: 1.3458 - classification_loss: 0.5051\n",
      " 39/500 [=>............................] - ETA: 7:41 - loss: 1.8724 - regression_loss: 1.3634 - classification_loss: 0.5089\n",
      " 40/500 [=>............................] - ETA: 7:41 - loss: 1.8837 - regression_loss: 1.3734 - classification_loss: 0.5104\n",
      " 41/500 [=>............................] - ETA: 7:38 - loss: 1.8757 - regression_loss: 1.3637 - classification_loss: 0.5120\n",
      " 42/500 [=>............................] - ETA: 7:39 - loss: 1.8864 - regression_loss: 1.3738 - classification_loss: 0.5125\n",
      " 43/500 [=>............................] - ETA: 7:38 - loss: 1.8951 - regression_loss: 1.3824 - classification_loss: 0.5127\n",
      " 44/500 [=>............................] - ETA: 7:35 - loss: 1.8846 - regression_loss: 1.3741 - classification_loss: 0.5105\n",
      " 45/500 [=>............................] - ETA: 7:36 - loss: 1.9009 - regression_loss: 1.3873 - classification_loss: 0.5136\n",
      " 46/500 [=>............................] - ETA: 7:36 - loss: 1.9128 - regression_loss: 1.3966 - classification_loss: 0.5163\n",
      " 47/500 [=>............................] - ETA: 7:35 - loss: 1.9128 - regression_loss: 1.3960 - classification_loss: 0.5168\n",
      " 48/500 [=>............................] - ETA: 7:35 - loss: 1.9290 - regression_loss: 1.4060 - classification_loss: 0.5230\n",
      " 49/500 [=>............................] - ETA: 7:35 - loss: 1.9159 - regression_loss: 1.3963 - classification_loss: 0.5196\n",
      " 50/500 [==>...........................] - ETA: 7:34 - loss: 1.9308 - regression_loss: 1.4067 - classification_loss: 0.5241\n",
      " 51/500 [==>...........................] - ETA: 7:33 - loss: 1.9247 - regression_loss: 1.4056 - classification_loss: 0.5192\n",
      " 52/500 [==>...........................] - ETA: 7:33 - loss: 1.9245 - regression_loss: 1.4036 - classification_loss: 0.5209\n",
      " 53/500 [==>...........................] - ETA: 7:32 - loss: 1.9182 - regression_loss: 1.3981 - classification_loss: 0.5200\n",
      " 54/500 [==>...........................] - ETA: 7:31 - loss: 1.9100 - regression_loss: 1.3919 - classification_loss: 0.5181\n",
      " 55/500 [==>...........................] - ETA: 7:30 - loss: 1.9116 - regression_loss: 1.3936 - classification_loss: 0.5179\n",
      " 56/500 [==>...........................] - ETA: 7:27 - loss: 1.9111 - regression_loss: 1.3950 - classification_loss: 0.5160\n",
      " 57/500 [==>...........................] - ETA: 7:28 - loss: 1.8986 - regression_loss: 1.3865 - classification_loss: 0.5120\n",
      " 58/500 [==>...........................] - ETA: 7:27 - loss: 1.9006 - regression_loss: 1.3895 - classification_loss: 0.5111\n",
      " 59/500 [==>...........................] - ETA: 7:27 - loss: 1.9123 - regression_loss: 1.3994 - classification_loss: 0.5129\n",
      " 60/500 [==>...........................] - ETA: 7:26 - loss: 1.9241 - regression_loss: 1.4090 - classification_loss: 0.5151\n",
      " 61/500 [==>...........................] - ETA: 7:26 - loss: 1.9210 - regression_loss: 1.4067 - classification_loss: 0.5143\n",
      " 62/500 [==>...........................] - ETA: 7:25 - loss: 1.9207 - regression_loss: 1.4073 - classification_loss: 0.5133\n",
      " 63/500 [==>...........................] - ETA: 7:24 - loss: 1.9385 - regression_loss: 1.4187 - classification_loss: 0.5198\n",
      " 64/500 [==>...........................] - ETA: 7:23 - loss: 1.9445 - regression_loss: 1.4242 - classification_loss: 0.5203\n",
      " 65/500 [==>...........................] - ETA: 7:21 - loss: 1.9384 - regression_loss: 1.4200 - classification_loss: 0.5184\n",
      " 66/500 [==>...........................] - ETA: 7:20 - loss: 1.9321 - regression_loss: 1.4156 - classification_loss: 0.5165\n",
      " 67/500 [===>..........................] - ETA: 7:19 - loss: 1.9408 - regression_loss: 1.4224 - classification_loss: 0.5185\n",
      " 68/500 [===>..........................] - ETA: 7:18 - loss: 1.9409 - regression_loss: 1.4249 - classification_loss: 0.5161\n",
      " 69/500 [===>..........................] - ETA: 7:17 - loss: 1.9480 - regression_loss: 1.4306 - classification_loss: 0.5174\n",
      " 70/500 [===>..........................] - ETA: 7:15 - loss: 1.9443 - regression_loss: 1.4293 - classification_loss: 0.5151\n",
      " 71/500 [===>..........................] - ETA: 7:15 - loss: 1.9600 - regression_loss: 1.4402 - classification_loss: 0.5197\n",
      " 72/500 [===>..........................] - ETA: 7:14 - loss: 1.9567 - regression_loss: 1.4373 - classification_loss: 0.5194\n",
      " 73/500 [===>..........................] - ETA: 7:13 - loss: 1.9520 - regression_loss: 1.4345 - classification_loss: 0.5175\n",
      " 74/500 [===>..........................] - ETA: 7:13 - loss: 1.9576 - regression_loss: 1.4400 - classification_loss: 0.5176\n",
      " 75/500 [===>..........................] - ETA: 7:11 - loss: 1.9669 - regression_loss: 1.4477 - classification_loss: 0.5193\n",
      " 76/500 [===>..........................] - ETA: 7:11 - loss: 1.9778 - regression_loss: 1.4557 - classification_loss: 0.5221\n",
      " 77/500 [===>..........................] - ETA: 7:10 - loss: 1.9721 - regression_loss: 1.4504 - classification_loss: 0.5217\n",
      " 78/500 [===>..........................] - ETA: 7:09 - loss: 1.9655 - regression_loss: 1.4461 - classification_loss: 0.5195\n",
      " 79/500 [===>..........................] - ETA: 7:08 - loss: 1.9671 - regression_loss: 1.4473 - classification_loss: 0.5199\n",
      " 80/500 [===>..........................] - ETA: 7:08 - loss: 1.9759 - regression_loss: 1.4521 - classification_loss: 0.5238\n",
      " 81/500 [===>..........................] - ETA: 7:07 - loss: 1.9919 - regression_loss: 1.4615 - classification_loss: 0.5304\n",
      " 82/500 [===>..........................] - ETA: 7:06 - loss: 1.9982 - regression_loss: 1.4660 - classification_loss: 0.5323\n",
      " 83/500 [===>..........................] - ETA: 7:04 - loss: 2.0021 - regression_loss: 1.4679 - classification_loss: 0.5341\n",
      " 84/500 [====>.........................] - ETA: 7:04 - loss: 1.9989 - regression_loss: 1.4660 - classification_loss: 0.5330\n",
      " 85/500 [====>.........................] - ETA: 7:03 - loss: 1.9963 - regression_loss: 1.4636 - classification_loss: 0.5328\n",
      " 86/500 [====>.........................] - ETA: 7:02 - loss: 2.0102 - regression_loss: 1.4728 - classification_loss: 0.5374\n",
      " 87/500 [====>.........................] - ETA: 7:01 - loss: 2.0185 - regression_loss: 1.4765 - classification_loss: 0.5420\n",
      " 88/500 [====>.........................] - ETA: 7:00 - loss: 2.0193 - regression_loss: 1.4787 - classification_loss: 0.5406\n",
      " 89/500 [====>.........................] - ETA: 6:59 - loss: 2.0136 - regression_loss: 1.4743 - classification_loss: 0.5392\n",
      " 90/500 [====>.........................] - ETA: 6:58 - loss: 2.0233 - regression_loss: 1.4809 - classification_loss: 0.5423\n",
      " 91/500 [====>.........................] - ETA: 6:57 - loss: 2.0187 - regression_loss: 1.4761 - classification_loss: 0.5426\n",
      " 92/500 [====>.........................] - ETA: 6:56 - loss: 2.0201 - regression_loss: 1.4770 - classification_loss: 0.5431\n",
      " 93/500 [====>.........................] - ETA: 6:55 - loss: 2.0271 - regression_loss: 1.4833 - classification_loss: 0.5438\n",
      " 94/500 [====>.........................] - ETA: 6:53 - loss: 2.0344 - regression_loss: 1.4880 - classification_loss: 0.5464\n",
      " 95/500 [====>.........................] - ETA: 6:53 - loss: 2.0456 - regression_loss: 1.4977 - classification_loss: 0.5479\n",
      " 96/500 [====>.........................] - ETA: 6:52 - loss: 2.0477 - regression_loss: 1.4988 - classification_loss: 0.5489\n",
      " 97/500 [====>.........................] - ETA: 6:51 - loss: 2.0498 - regression_loss: 1.4999 - classification_loss: 0.5499\n",
      " 98/500 [====>.........................] - ETA: 6:50 - loss: 2.0473 - regression_loss: 1.4989 - classification_loss: 0.5484\n",
      " 99/500 [====>.........................] - ETA: 6:48 - loss: 2.0554 - regression_loss: 1.5044 - classification_loss: 0.5510\n",
      "100/500 [=====>........................] - ETA: 6:48 - loss: 2.0578 - regression_loss: 1.5070 - classification_loss: 0.5508\n",
      "101/500 [=====>........................] - ETA: 6:47 - loss: 2.0668 - regression_loss: 1.5137 - classification_loss: 0.5531\n",
      "102/500 [=====>........................] - ETA: 6:46 - loss: 2.0645 - regression_loss: 1.5137 - classification_loss: 0.5508\n",
      "103/500 [=====>........................] - ETA: 6:44 - loss: 2.0642 - regression_loss: 1.5133 - classification_loss: 0.5509\n",
      "104/500 [=====>........................] - ETA: 6:50 - loss: 2.0654 - regression_loss: 1.5121 - classification_loss: 0.5533\n",
      "105/500 [=====>........................] - ETA: 6:49 - loss: 2.0607 - regression_loss: 1.5082 - classification_loss: 0.5525\n",
      "106/500 [=====>........................] - ETA: 6:47 - loss: 2.0613 - regression_loss: 1.5093 - classification_loss: 0.5520\n",
      "107/500 [=====>........................] - ETA: 6:47 - loss: 2.0657 - regression_loss: 1.5128 - classification_loss: 0.5529\n",
      "108/500 [=====>........................] - ETA: 6:46 - loss: 2.0668 - regression_loss: 1.5133 - classification_loss: 0.5535\n",
      "109/500 [=====>........................] - ETA: 6:45 - loss: 2.0684 - regression_loss: 1.5143 - classification_loss: 0.5540\n",
      "110/500 [=====>........................] - ETA: 6:44 - loss: 2.0712 - regression_loss: 1.5164 - classification_loss: 0.5549\n",
      "111/500 [=====>........................] - ETA: 6:43 - loss: 2.0741 - regression_loss: 1.5186 - classification_loss: 0.5555\n",
      "112/500 [=====>........................] - ETA: 6:42 - loss: 2.0707 - regression_loss: 1.5155 - classification_loss: 0.5552\n",
      "113/500 [=====>........................] - ETA: 6:41 - loss: 2.0739 - regression_loss: 1.5190 - classification_loss: 0.5548\n",
      "114/500 [=====>........................] - ETA: 6:40 - loss: 2.0768 - regression_loss: 1.5204 - classification_loss: 0.5564\n",
      "115/500 [=====>........................] - ETA: 6:39 - loss: 2.0728 - regression_loss: 1.5167 - classification_loss: 0.5561\n",
      "116/500 [=====>........................] - ETA: 6:39 - loss: 2.0681 - regression_loss: 1.5133 - classification_loss: 0.5548\n",
      "117/500 [======>.......................] - ETA: 6:38 - loss: 2.0683 - regression_loss: 1.5133 - classification_loss: 0.5549\n",
      "118/500 [======>.......................] - ETA: 6:36 - loss: 2.0743 - regression_loss: 1.5169 - classification_loss: 0.5575\n",
      "119/500 [======>.......................] - ETA: 6:35 - loss: 2.0790 - regression_loss: 1.5203 - classification_loss: 0.5587\n",
      "120/500 [======>.......................] - ETA: 6:35 - loss: 2.0796 - regression_loss: 1.5209 - classification_loss: 0.5587\n",
      "121/500 [======>.......................] - ETA: 6:33 - loss: 2.0824 - regression_loss: 1.5222 - classification_loss: 0.5603\n",
      "122/500 [======>.......................] - ETA: 6:35 - loss: 2.0940 - regression_loss: 1.5305 - classification_loss: 0.5635\n",
      "123/500 [======>.......................] - ETA: 6:34 - loss: 2.0921 - regression_loss: 1.5282 - classification_loss: 0.5639\n",
      "124/500 [======>.......................] - ETA: 6:33 - loss: 2.0977 - regression_loss: 1.5334 - classification_loss: 0.5643\n",
      "125/500 [======>.......................] - ETA: 6:32 - loss: 2.0919 - regression_loss: 1.5293 - classification_loss: 0.5626\n",
      "126/500 [======>.......................] - ETA: 6:31 - loss: 2.0851 - regression_loss: 1.5242 - classification_loss: 0.5609\n",
      "127/500 [======>.......................] - ETA: 6:29 - loss: 2.0798 - regression_loss: 1.5202 - classification_loss: 0.5597\n",
      "128/500 [======>.......................] - ETA: 6:28 - loss: 2.0776 - regression_loss: 1.5197 - classification_loss: 0.5579\n",
      "129/500 [======>.......................] - ETA: 6:27 - loss: 2.0842 - regression_loss: 1.5245 - classification_loss: 0.5597\n",
      "130/500 [======>.......................] - ETA: 6:26 - loss: 2.0914 - regression_loss: 1.5293 - classification_loss: 0.5621\n",
      "131/500 [======>.......................] - ETA: 6:25 - loss: 2.0917 - regression_loss: 1.5300 - classification_loss: 0.5617\n",
      "132/500 [======>.......................] - ETA: 6:24 - loss: 2.0909 - regression_loss: 1.5302 - classification_loss: 0.5607\n",
      "133/500 [======>.......................] - ETA: 6:23 - loss: 2.0844 - regression_loss: 1.5247 - classification_loss: 0.5597\n",
      "134/500 [=======>......................] - ETA: 6:21 - loss: 2.0839 - regression_loss: 1.5243 - classification_loss: 0.5596\n",
      "135/500 [=======>......................] - ETA: 6:20 - loss: 2.0830 - regression_loss: 1.5241 - classification_loss: 0.5588\n",
      "136/500 [=======>......................] - ETA: 6:19 - loss: 2.0800 - regression_loss: 1.5224 - classification_loss: 0.5576\n",
      "137/500 [=======>......................] - ETA: 6:18 - loss: 2.0848 - regression_loss: 1.5254 - classification_loss: 0.5594\n",
      "138/500 [=======>......................] - ETA: 6:21 - loss: 2.0829 - regression_loss: 1.5229 - classification_loss: 0.5599\n",
      "139/500 [=======>......................] - ETA: 6:20 - loss: 2.0862 - regression_loss: 1.5258 - classification_loss: 0.5604\n",
      "140/500 [=======>......................] - ETA: 6:19 - loss: 2.0903 - regression_loss: 1.5283 - classification_loss: 0.5620\n",
      "141/500 [=======>......................] - ETA: 6:18 - loss: 2.0896 - regression_loss: 1.5280 - classification_loss: 0.5615\n",
      "142/500 [=======>......................] - ETA: 6:17 - loss: 2.0905 - regression_loss: 1.5288 - classification_loss: 0.5617\n",
      "143/500 [=======>......................] - ETA: 6:16 - loss: 2.0852 - regression_loss: 1.5250 - classification_loss: 0.5602\n",
      "144/500 [=======>......................] - ETA: 6:15 - loss: 2.0873 - regression_loss: 1.5280 - classification_loss: 0.5594\n",
      "145/500 [=======>......................] - ETA: 6:13 - loss: 2.0854 - regression_loss: 1.5270 - classification_loss: 0.5584\n",
      "146/500 [=======>......................] - ETA: 6:12 - loss: 2.0857 - regression_loss: 1.5271 - classification_loss: 0.5586\n",
      "147/500 [=======>......................] - ETA: 6:11 - loss: 2.0829 - regression_loss: 1.5255 - classification_loss: 0.5574\n",
      "148/500 [=======>......................] - ETA: 6:10 - loss: 2.0800 - regression_loss: 1.5232 - classification_loss: 0.5568\n",
      "149/500 [=======>......................] - ETA: 6:09 - loss: 2.0784 - regression_loss: 1.5208 - classification_loss: 0.5576\n",
      "150/500 [========>.....................] - ETA: 6:08 - loss: 2.0787 - regression_loss: 1.5200 - classification_loss: 0.5587\n",
      "151/500 [========>.....................] - ETA: 6:07 - loss: 2.0783 - regression_loss: 1.5200 - classification_loss: 0.5584\n",
      "152/500 [========>.....................] - ETA: 6:06 - loss: 2.0831 - regression_loss: 1.5215 - classification_loss: 0.5615\n",
      "153/500 [========>.....................] - ETA: 6:05 - loss: 2.0834 - regression_loss: 1.5212 - classification_loss: 0.5621\n",
      "154/500 [========>.....................] - ETA: 6:04 - loss: 2.0813 - regression_loss: 1.5202 - classification_loss: 0.5610\n",
      "155/500 [========>.....................] - ETA: 6:03 - loss: 2.0819 - regression_loss: 1.5207 - classification_loss: 0.5612\n",
      "156/500 [========>.....................] - ETA: 6:02 - loss: 2.0863 - regression_loss: 1.5232 - classification_loss: 0.5631\n",
      "157/500 [========>.....................] - ETA: 6:01 - loss: 2.0882 - regression_loss: 1.5238 - classification_loss: 0.5644\n",
      "158/500 [========>.....................] - ETA: 6:00 - loss: 2.0861 - regression_loss: 1.5215 - classification_loss: 0.5645\n",
      "159/500 [========>.....................] - ETA: 5:58 - loss: 2.0860 - regression_loss: 1.5205 - classification_loss: 0.5655\n",
      "160/500 [========>.....................] - ETA: 5:57 - loss: 2.0853 - regression_loss: 1.5204 - classification_loss: 0.5649\n",
      "161/500 [========>.....................] - ETA: 5:56 - loss: 2.0833 - regression_loss: 1.5186 - classification_loss: 0.5646\n",
      "162/500 [========>.....................] - ETA: 5:55 - loss: 2.0873 - regression_loss: 1.5221 - classification_loss: 0.5652\n",
      "163/500 [========>.....................] - ETA: 5:54 - loss: 2.0905 - regression_loss: 1.5242 - classification_loss: 0.5664\n",
      "164/500 [========>.....................] - ETA: 5:53 - loss: 2.0869 - regression_loss: 1.5203 - classification_loss: 0.5666\n",
      "165/500 [========>.....................] - ETA: 5:52 - loss: 2.0891 - regression_loss: 1.5216 - classification_loss: 0.5675\n",
      "166/500 [========>.....................] - ETA: 5:51 - loss: 2.0940 - regression_loss: 1.5266 - classification_loss: 0.5674\n",
      "167/500 [=========>....................] - ETA: 5:49 - loss: 2.0959 - regression_loss: 1.5289 - classification_loss: 0.5670\n",
      "168/500 [=========>....................] - ETA: 5:48 - loss: 2.0998 - regression_loss: 1.5325 - classification_loss: 0.5673\n",
      "169/500 [=========>....................] - ETA: 5:47 - loss: 2.0965 - regression_loss: 1.5305 - classification_loss: 0.5660\n",
      "170/500 [=========>....................] - ETA: 5:46 - loss: 2.0991 - regression_loss: 1.5317 - classification_loss: 0.5674\n",
      "171/500 [=========>....................] - ETA: 5:44 - loss: 2.1023 - regression_loss: 1.5330 - classification_loss: 0.5693\n",
      "172/500 [=========>....................] - ETA: 5:44 - loss: 2.1043 - regression_loss: 1.5349 - classification_loss: 0.5694\n",
      "173/500 [=========>....................] - ETA: 5:42 - loss: 2.1049 - regression_loss: 1.5359 - classification_loss: 0.5690\n",
      "174/500 [=========>....................] - ETA: 5:41 - loss: 2.1044 - regression_loss: 1.5362 - classification_loss: 0.5681\n",
      "175/500 [=========>....................] - ETA: 5:40 - loss: 2.1048 - regression_loss: 1.5372 - classification_loss: 0.5676\n",
      "176/500 [=========>....................] - ETA: 5:39 - loss: 2.1050 - regression_loss: 1.5376 - classification_loss: 0.5674\n",
      "177/500 [=========>....................] - ETA: 5:38 - loss: 2.0984 - regression_loss: 1.5327 - classification_loss: 0.5657\n",
      "178/500 [=========>....................] - ETA: 5:37 - loss: 2.0984 - regression_loss: 1.5325 - classification_loss: 0.5658\n",
      "179/500 [=========>....................] - ETA: 5:36 - loss: 2.1008 - regression_loss: 1.5351 - classification_loss: 0.5657\n",
      "180/500 [=========>....................] - ETA: 5:35 - loss: 2.1042 - regression_loss: 1.5374 - classification_loss: 0.5668\n",
      "181/500 [=========>....................] - ETA: 5:34 - loss: 2.1001 - regression_loss: 1.5340 - classification_loss: 0.5660\n",
      "182/500 [=========>....................] - ETA: 5:33 - loss: 2.0983 - regression_loss: 1.5331 - classification_loss: 0.5651\n",
      "183/500 [=========>....................] - ETA: 5:32 - loss: 2.0979 - regression_loss: 1.5324 - classification_loss: 0.5655\n",
      "184/500 [==========>...................] - ETA: 5:31 - loss: 2.0984 - regression_loss: 1.5337 - classification_loss: 0.5647\n",
      "185/500 [==========>...................] - ETA: 5:30 - loss: 2.0956 - regression_loss: 1.5319 - classification_loss: 0.5637\n",
      "186/500 [==========>...................] - ETA: 5:29 - loss: 2.1040 - regression_loss: 1.5380 - classification_loss: 0.5660\n",
      "187/500 [==========>...................] - ETA: 5:28 - loss: 2.1029 - regression_loss: 1.5380 - classification_loss: 0.5649\n",
      "188/500 [==========>...................] - ETA: 5:27 - loss: 2.1019 - regression_loss: 1.5378 - classification_loss: 0.5641\n",
      "189/500 [==========>...................] - ETA: 5:26 - loss: 2.1032 - regression_loss: 1.5389 - classification_loss: 0.5643\n",
      "190/500 [==========>...................] - ETA: 5:25 - loss: 2.1077 - regression_loss: 1.5426 - classification_loss: 0.5651\n",
      "191/500 [==========>...................] - ETA: 5:24 - loss: 2.1106 - regression_loss: 1.5447 - classification_loss: 0.5659\n",
      "192/500 [==========>...................] - ETA: 5:22 - loss: 2.1102 - regression_loss: 1.5449 - classification_loss: 0.5653\n",
      "193/500 [==========>...................] - ETA: 5:21 - loss: 2.1131 - regression_loss: 1.5480 - classification_loss: 0.5650\n",
      "194/500 [==========>...................] - ETA: 5:20 - loss: 2.1144 - regression_loss: 1.5493 - classification_loss: 0.5651\n",
      "195/500 [==========>...................] - ETA: 5:19 - loss: 2.1189 - regression_loss: 1.5518 - classification_loss: 0.5672\n",
      "196/500 [==========>...................] - ETA: 5:18 - loss: 2.1206 - regression_loss: 1.5530 - classification_loss: 0.5676\n",
      "197/500 [==========>...................] - ETA: 5:17 - loss: 2.1154 - regression_loss: 1.5491 - classification_loss: 0.5664\n",
      "198/500 [==========>...................] - ETA: 5:16 - loss: 2.1128 - regression_loss: 1.5470 - classification_loss: 0.5658\n",
      "199/500 [==========>...................] - ETA: 5:15 - loss: 2.1133 - regression_loss: 1.5481 - classification_loss: 0.5653\n",
      "200/500 [===========>..................] - ETA: 5:14 - loss: 2.1118 - regression_loss: 1.5470 - classification_loss: 0.5648\n",
      "201/500 [===========>..................] - ETA: 5:13 - loss: 2.1063 - regression_loss: 1.5431 - classification_loss: 0.5632\n",
      "202/500 [===========>..................] - ETA: 5:12 - loss: 2.1042 - regression_loss: 1.5413 - classification_loss: 0.5629\n",
      "203/500 [===========>..................] - ETA: 5:11 - loss: 2.1043 - regression_loss: 1.5417 - classification_loss: 0.5626\n",
      "204/500 [===========>..................] - ETA: 5:09 - loss: 2.1089 - regression_loss: 1.5452 - classification_loss: 0.5637\n",
      "205/500 [===========>..................] - ETA: 5:08 - loss: 2.1132 - regression_loss: 1.5484 - classification_loss: 0.5649\n",
      "206/500 [===========>..................] - ETA: 5:07 - loss: 2.1126 - regression_loss: 1.5480 - classification_loss: 0.5645\n",
      "207/500 [===========>..................] - ETA: 5:06 - loss: 2.1100 - regression_loss: 1.5464 - classification_loss: 0.5636\n",
      "208/500 [===========>..................] - ETA: 5:05 - loss: 2.1078 - regression_loss: 1.5451 - classification_loss: 0.5627\n",
      "209/500 [===========>..................] - ETA: 5:04 - loss: 2.1045 - regression_loss: 1.5430 - classification_loss: 0.5615\n",
      "210/500 [===========>..................] - ETA: 5:03 - loss: 2.1024 - regression_loss: 1.5411 - classification_loss: 0.5613\n",
      "211/500 [===========>..................] - ETA: 5:02 - loss: 2.1018 - regression_loss: 1.5408 - classification_loss: 0.5610\n",
      "212/500 [===========>..................] - ETA: 5:00 - loss: 2.1017 - regression_loss: 1.5410 - classification_loss: 0.5607\n",
      "213/500 [===========>..................] - ETA: 4:59 - loss: 2.1014 - regression_loss: 1.5410 - classification_loss: 0.5605\n",
      "214/500 [===========>..................] - ETA: 4:58 - loss: 2.0974 - regression_loss: 1.5379 - classification_loss: 0.5595\n",
      "215/500 [===========>..................] - ETA: 4:57 - loss: 2.0955 - regression_loss: 1.5366 - classification_loss: 0.5589\n",
      "216/500 [===========>..................] - ETA: 4:56 - loss: 2.0963 - regression_loss: 1.5370 - classification_loss: 0.5593\n",
      "217/500 [============>.................] - ETA: 4:55 - loss: 2.0957 - regression_loss: 1.5367 - classification_loss: 0.5590\n",
      "218/500 [============>.................] - ETA: 4:54 - loss: 2.1000 - regression_loss: 1.5406 - classification_loss: 0.5594\n",
      "219/500 [============>.................] - ETA: 4:53 - loss: 2.1007 - regression_loss: 1.5413 - classification_loss: 0.5594\n",
      "220/500 [============>.................] - ETA: 4:52 - loss: 2.1004 - regression_loss: 1.5419 - classification_loss: 0.5586\n",
      "221/500 [============>.................] - ETA: 4:51 - loss: 2.0973 - regression_loss: 1.5399 - classification_loss: 0.5574\n",
      "222/500 [============>.................] - ETA: 4:50 - loss: 2.0990 - regression_loss: 1.5420 - classification_loss: 0.5569\n",
      "223/500 [============>.................] - ETA: 4:49 - loss: 2.0986 - regression_loss: 1.5419 - classification_loss: 0.5567\n",
      "224/500 [============>.................] - ETA: 4:48 - loss: 2.0998 - regression_loss: 1.5427 - classification_loss: 0.5571\n",
      "225/500 [============>.................] - ETA: 4:47 - loss: 2.0994 - regression_loss: 1.5429 - classification_loss: 0.5565\n",
      "226/500 [============>.................] - ETA: 4:46 - loss: 2.1000 - regression_loss: 1.5435 - classification_loss: 0.5566\n",
      "227/500 [============>.................] - ETA: 4:45 - loss: 2.0966 - regression_loss: 1.5408 - classification_loss: 0.5558\n",
      "228/500 [============>.................] - ETA: 4:43 - loss: 2.0956 - regression_loss: 1.5401 - classification_loss: 0.5555\n",
      "229/500 [============>.................] - ETA: 4:42 - loss: 2.0932 - regression_loss: 1.5384 - classification_loss: 0.5548\n",
      "230/500 [============>.................] - ETA: 4:41 - loss: 2.0967 - regression_loss: 1.5410 - classification_loss: 0.5557\n",
      "231/500 [============>.................] - ETA: 4:40 - loss: 2.0959 - regression_loss: 1.5403 - classification_loss: 0.5556\n",
      "232/500 [============>.................] - ETA: 4:39 - loss: 2.0927 - regression_loss: 1.5377 - classification_loss: 0.5550\n",
      "233/500 [============>.................] - ETA: 4:38 - loss: 2.0954 - regression_loss: 1.5396 - classification_loss: 0.5558\n",
      "234/500 [=============>................] - ETA: 4:37 - loss: 2.0945 - regression_loss: 1.5389 - classification_loss: 0.5556\n",
      "235/500 [=============>................] - ETA: 4:36 - loss: 2.0935 - regression_loss: 1.5376 - classification_loss: 0.5559\n",
      "236/500 [=============>................] - ETA: 4:35 - loss: 2.0942 - regression_loss: 1.5387 - classification_loss: 0.5555\n",
      "237/500 [=============>................] - ETA: 4:34 - loss: 2.0916 - regression_loss: 1.5368 - classification_loss: 0.5548\n",
      "238/500 [=============>................] - ETA: 4:33 - loss: 2.0926 - regression_loss: 1.5369 - classification_loss: 0.5557\n",
      "239/500 [=============>................] - ETA: 4:32 - loss: 2.0972 - regression_loss: 1.5402 - classification_loss: 0.5570\n",
      "240/500 [=============>................] - ETA: 4:31 - loss: 2.0952 - regression_loss: 1.5390 - classification_loss: 0.5562\n",
      "241/500 [=============>................] - ETA: 4:30 - loss: 2.0936 - regression_loss: 1.5379 - classification_loss: 0.5557\n",
      "242/500 [=============>................] - ETA: 4:28 - loss: 2.0913 - regression_loss: 1.5359 - classification_loss: 0.5553\n",
      "243/500 [=============>................] - ETA: 4:27 - loss: 2.0946 - regression_loss: 1.5372 - classification_loss: 0.5574\n",
      "244/500 [=============>................] - ETA: 4:26 - loss: 2.0923 - regression_loss: 1.5359 - classification_loss: 0.5564\n",
      "245/500 [=============>................] - ETA: 4:25 - loss: 2.0955 - regression_loss: 1.5380 - classification_loss: 0.5574\n",
      "246/500 [=============>................] - ETA: 4:24 - loss: 2.0990 - regression_loss: 1.5410 - classification_loss: 0.5580\n",
      "247/500 [=============>................] - ETA: 4:23 - loss: 2.1030 - regression_loss: 1.5441 - classification_loss: 0.5590\n",
      "248/500 [=============>................] - ETA: 4:22 - loss: 2.1036 - regression_loss: 1.5446 - classification_loss: 0.5591\n",
      "249/500 [=============>................] - ETA: 4:21 - loss: 2.1047 - regression_loss: 1.5452 - classification_loss: 0.5595\n",
      "250/500 [==============>...............] - ETA: 4:20 - loss: 2.1036 - regression_loss: 1.5447 - classification_loss: 0.5589\n",
      "251/500 [==============>...............] - ETA: 4:19 - loss: 2.1006 - regression_loss: 1.5421 - classification_loss: 0.5585\n",
      "252/500 [==============>...............] - ETA: 4:18 - loss: 2.0989 - regression_loss: 1.5409 - classification_loss: 0.5579\n",
      "253/500 [==============>...............] - ETA: 4:17 - loss: 2.0957 - regression_loss: 1.5386 - classification_loss: 0.5571\n",
      "254/500 [==============>...............] - ETA: 4:16 - loss: 2.0932 - regression_loss: 1.5361 - classification_loss: 0.5571\n",
      "255/500 [==============>...............] - ETA: 4:15 - loss: 2.0908 - regression_loss: 1.5347 - classification_loss: 0.5561\n",
      "256/500 [==============>...............] - ETA: 4:13 - loss: 2.0900 - regression_loss: 1.5339 - classification_loss: 0.5561\n",
      "257/500 [==============>...............] - ETA: 4:12 - loss: 2.0876 - regression_loss: 1.5323 - classification_loss: 0.5553\n",
      "258/500 [==============>...............] - ETA: 4:11 - loss: 2.0892 - regression_loss: 1.5337 - classification_loss: 0.5555\n",
      "259/500 [==============>...............] - ETA: 4:10 - loss: 2.0876 - regression_loss: 1.5326 - classification_loss: 0.5549\n",
      "260/500 [==============>...............] - ETA: 4:09 - loss: 2.0873 - regression_loss: 1.5327 - classification_loss: 0.5545\n",
      "261/500 [==============>...............] - ETA: 4:08 - loss: 2.0906 - regression_loss: 1.5350 - classification_loss: 0.5556\n",
      "262/500 [==============>...............] - ETA: 4:07 - loss: 2.0919 - regression_loss: 1.5365 - classification_loss: 0.5554\n",
      "263/500 [==============>...............] - ETA: 4:06 - loss: 2.0927 - regression_loss: 1.5372 - classification_loss: 0.5555\n",
      "264/500 [==============>...............] - ETA: 4:05 - loss: 2.0905 - regression_loss: 1.5358 - classification_loss: 0.5548\n",
      "265/500 [==============>...............] - ETA: 4:04 - loss: 2.0913 - regression_loss: 1.5365 - classification_loss: 0.5548\n",
      "266/500 [==============>...............] - ETA: 4:03 - loss: 2.0907 - regression_loss: 1.5364 - classification_loss: 0.5543\n",
      "267/500 [===============>..............] - ETA: 4:02 - loss: 2.0909 - regression_loss: 1.5364 - classification_loss: 0.5545\n",
      "268/500 [===============>..............] - ETA: 4:01 - loss: 2.0907 - regression_loss: 1.5361 - classification_loss: 0.5547\n",
      "269/500 [===============>..............] - ETA: 4:00 - loss: 2.0900 - regression_loss: 1.5361 - classification_loss: 0.5539\n",
      "270/500 [===============>..............] - ETA: 3:59 - loss: 2.0884 - regression_loss: 1.5352 - classification_loss: 0.5532\n",
      "271/500 [===============>..............] - ETA: 3:58 - loss: 2.0898 - regression_loss: 1.5367 - classification_loss: 0.5532\n",
      "272/500 [===============>..............] - ETA: 3:57 - loss: 2.0878 - regression_loss: 1.5356 - classification_loss: 0.5522\n",
      "273/500 [===============>..............] - ETA: 3:56 - loss: 2.0882 - regression_loss: 1.5358 - classification_loss: 0.5524\n",
      "274/500 [===============>..............] - ETA: 3:55 - loss: 2.0884 - regression_loss: 1.5363 - classification_loss: 0.5521\n",
      "275/500 [===============>..............] - ETA: 3:54 - loss: 2.0903 - regression_loss: 1.5369 - classification_loss: 0.5534\n",
      "276/500 [===============>..............] - ETA: 3:53 - loss: 2.0933 - regression_loss: 1.5388 - classification_loss: 0.5545\n",
      "277/500 [===============>..............] - ETA: 3:52 - loss: 2.0928 - regression_loss: 1.5386 - classification_loss: 0.5541\n",
      "278/500 [===============>..............] - ETA: 3:50 - loss: 2.0919 - regression_loss: 1.5379 - classification_loss: 0.5540\n",
      "279/500 [===============>..............] - ETA: 3:49 - loss: 2.0933 - regression_loss: 1.5391 - classification_loss: 0.5542\n",
      "280/500 [===============>..............] - ETA: 3:48 - loss: 2.0924 - regression_loss: 1.5386 - classification_loss: 0.5538\n",
      "281/500 [===============>..............] - ETA: 3:47 - loss: 2.0895 - regression_loss: 1.5368 - classification_loss: 0.5527\n",
      "282/500 [===============>..............] - ETA: 3:46 - loss: 2.0895 - regression_loss: 1.5364 - classification_loss: 0.5530\n",
      "283/500 [===============>..............] - ETA: 3:45 - loss: 2.0893 - regression_loss: 1.5363 - classification_loss: 0.5531\n",
      "284/500 [================>.............] - ETA: 3:44 - loss: 2.0875 - regression_loss: 1.5346 - classification_loss: 0.5529\n",
      "285/500 [================>.............] - ETA: 3:43 - loss: 2.0890 - regression_loss: 1.5365 - classification_loss: 0.5525\n",
      "286/500 [================>.............] - ETA: 3:42 - loss: 2.0868 - regression_loss: 1.5347 - classification_loss: 0.5521\n",
      "287/500 [================>.............] - ETA: 3:41 - loss: 2.0892 - regression_loss: 1.5371 - classification_loss: 0.5521\n",
      "288/500 [================>.............] - ETA: 3:40 - loss: 2.0874 - regression_loss: 1.5360 - classification_loss: 0.5513\n",
      "289/500 [================>.............] - ETA: 3:39 - loss: 2.0877 - regression_loss: 1.5355 - classification_loss: 0.5522\n",
      "290/500 [================>.............] - ETA: 3:38 - loss: 2.0885 - regression_loss: 1.5361 - classification_loss: 0.5525\n",
      "291/500 [================>.............] - ETA: 3:37 - loss: 2.0899 - regression_loss: 1.5368 - classification_loss: 0.5531\n",
      "292/500 [================>.............] - ETA: 3:36 - loss: 2.0887 - regression_loss: 1.5357 - classification_loss: 0.5530\n",
      "293/500 [================>.............] - ETA: 3:35 - loss: 2.0867 - regression_loss: 1.5340 - classification_loss: 0.5527\n",
      "294/500 [================>.............] - ETA: 3:34 - loss: 2.0884 - regression_loss: 1.5351 - classification_loss: 0.5533\n",
      "295/500 [================>.............] - ETA: 3:32 - loss: 2.0891 - regression_loss: 1.5351 - classification_loss: 0.5540\n",
      "296/500 [================>.............] - ETA: 3:31 - loss: 2.0895 - regression_loss: 1.5344 - classification_loss: 0.5551\n",
      "297/500 [================>.............] - ETA: 3:30 - loss: 2.0885 - regression_loss: 1.5335 - classification_loss: 0.5550\n",
      "298/500 [================>.............] - ETA: 3:29 - loss: 2.0884 - regression_loss: 1.5335 - classification_loss: 0.5548\n",
      "299/500 [================>.............] - ETA: 3:28 - loss: 2.0870 - regression_loss: 1.5325 - classification_loss: 0.5545\n",
      "300/500 [=================>............] - ETA: 3:27 - loss: 2.0877 - regression_loss: 1.5331 - classification_loss: 0.5547\n",
      "301/500 [=================>............] - ETA: 3:26 - loss: 2.0852 - regression_loss: 1.5311 - classification_loss: 0.5541\n",
      "302/500 [=================>............] - ETA: 3:25 - loss: 2.0881 - regression_loss: 1.5326 - classification_loss: 0.5555\n",
      "303/500 [=================>............] - ETA: 3:24 - loss: 2.0873 - regression_loss: 1.5322 - classification_loss: 0.5552\n",
      "304/500 [=================>............] - ETA: 3:23 - loss: 2.0865 - regression_loss: 1.5318 - classification_loss: 0.5548\n",
      "305/500 [=================>............] - ETA: 3:22 - loss: 2.0875 - regression_loss: 1.5325 - classification_loss: 0.5551\n",
      "306/500 [=================>............] - ETA: 3:21 - loss: 2.0854 - regression_loss: 1.5310 - classification_loss: 0.5544\n",
      "307/500 [=================>............] - ETA: 3:20 - loss: 2.0855 - regression_loss: 1.5312 - classification_loss: 0.5543\n",
      "308/500 [=================>............] - ETA: 3:19 - loss: 2.0874 - regression_loss: 1.5323 - classification_loss: 0.5551\n",
      "309/500 [=================>............] - ETA: 3:18 - loss: 2.0899 - regression_loss: 1.5340 - classification_loss: 0.5558\n",
      "310/500 [=================>............] - ETA: 3:17 - loss: 2.0885 - regression_loss: 1.5331 - classification_loss: 0.5555\n",
      "311/500 [=================>............] - ETA: 3:16 - loss: 2.0885 - regression_loss: 1.5335 - classification_loss: 0.5550\n",
      "312/500 [=================>............] - ETA: 3:15 - loss: 2.0882 - regression_loss: 1.5328 - classification_loss: 0.5554\n",
      "313/500 [=================>............] - ETA: 3:14 - loss: 2.0862 - regression_loss: 1.5313 - classification_loss: 0.5548\n",
      "314/500 [=================>............] - ETA: 3:13 - loss: 2.0838 - regression_loss: 1.5293 - classification_loss: 0.5545\n",
      "315/500 [=================>............] - ETA: 3:12 - loss: 2.0874 - regression_loss: 1.5319 - classification_loss: 0.5555\n",
      "316/500 [=================>............] - ETA: 3:11 - loss: 2.0870 - regression_loss: 1.5315 - classification_loss: 0.5555\n",
      "317/500 [==================>...........] - ETA: 3:10 - loss: 2.0842 - regression_loss: 1.5297 - classification_loss: 0.5545\n",
      "318/500 [==================>...........] - ETA: 3:08 - loss: 2.0819 - regression_loss: 1.5281 - classification_loss: 0.5539\n",
      "319/500 [==================>...........] - ETA: 3:07 - loss: 2.0814 - regression_loss: 1.5276 - classification_loss: 0.5538\n",
      "320/500 [==================>...........] - ETA: 3:06 - loss: 2.0833 - regression_loss: 1.5292 - classification_loss: 0.5541\n",
      "321/500 [==================>...........] - ETA: 3:05 - loss: 2.0848 - regression_loss: 1.5309 - classification_loss: 0.5539\n",
      "322/500 [==================>...........] - ETA: 3:04 - loss: 2.0851 - regression_loss: 1.5311 - classification_loss: 0.5540\n",
      "323/500 [==================>...........] - ETA: 3:03 - loss: 2.0874 - regression_loss: 1.5329 - classification_loss: 0.5545\n",
      "324/500 [==================>...........] - ETA: 3:02 - loss: 2.0876 - regression_loss: 1.5331 - classification_loss: 0.5544\n",
      "325/500 [==================>...........] - ETA: 3:01 - loss: 2.0885 - regression_loss: 1.5344 - classification_loss: 0.5542\n",
      "326/500 [==================>...........] - ETA: 3:00 - loss: 2.0879 - regression_loss: 1.5340 - classification_loss: 0.5538\n",
      "327/500 [==================>...........] - ETA: 2:59 - loss: 2.0877 - regression_loss: 1.5344 - classification_loss: 0.5533\n",
      "328/500 [==================>...........] - ETA: 2:58 - loss: 2.0884 - regression_loss: 1.5346 - classification_loss: 0.5538\n",
      "329/500 [==================>...........] - ETA: 2:57 - loss: 2.0905 - regression_loss: 1.5359 - classification_loss: 0.5546\n",
      "330/500 [==================>...........] - ETA: 2:56 - loss: 2.0919 - regression_loss: 1.5375 - classification_loss: 0.5544\n",
      "331/500 [==================>...........] - ETA: 2:55 - loss: 2.0904 - regression_loss: 1.5361 - classification_loss: 0.5543\n",
      "332/500 [==================>...........] - ETA: 2:54 - loss: 2.0896 - regression_loss: 1.5359 - classification_loss: 0.5537\n",
      "333/500 [==================>...........] - ETA: 2:53 - loss: 2.0885 - regression_loss: 1.5353 - classification_loss: 0.5531\n",
      "334/500 [===================>..........] - ETA: 2:52 - loss: 2.0907 - regression_loss: 1.5377 - classification_loss: 0.5530\n",
      "335/500 [===================>..........] - ETA: 2:51 - loss: 2.0935 - regression_loss: 1.5397 - classification_loss: 0.5538\n",
      "336/500 [===================>..........] - ETA: 2:50 - loss: 2.0929 - regression_loss: 1.5394 - classification_loss: 0.5535\n",
      "337/500 [===================>..........] - ETA: 2:49 - loss: 2.0908 - regression_loss: 1.5379 - classification_loss: 0.5529\n",
      "338/500 [===================>..........] - ETA: 2:48 - loss: 2.0948 - regression_loss: 1.5407 - classification_loss: 0.5541\n",
      "339/500 [===================>..........] - ETA: 2:47 - loss: 2.0949 - regression_loss: 1.5404 - classification_loss: 0.5545\n",
      "340/500 [===================>..........] - ETA: 2:46 - loss: 2.0947 - regression_loss: 1.5404 - classification_loss: 0.5543\n",
      "341/500 [===================>..........] - ETA: 2:45 - loss: 2.0966 - regression_loss: 1.5417 - classification_loss: 0.5549\n",
      "342/500 [===================>..........] - ETA: 2:43 - loss: 2.0954 - regression_loss: 1.5409 - classification_loss: 0.5545\n",
      "343/500 [===================>..........] - ETA: 2:42 - loss: 2.0931 - regression_loss: 1.5393 - classification_loss: 0.5538\n",
      "344/500 [===================>..........] - ETA: 2:41 - loss: 2.0928 - regression_loss: 1.5396 - classification_loss: 0.5531\n",
      "345/500 [===================>..........] - ETA: 2:40 - loss: 2.0904 - regression_loss: 1.5378 - classification_loss: 0.5526\n",
      "346/500 [===================>..........] - ETA: 2:39 - loss: 2.0889 - regression_loss: 1.5367 - classification_loss: 0.5521\n",
      "347/500 [===================>..........] - ETA: 2:38 - loss: 2.0882 - regression_loss: 1.5360 - classification_loss: 0.5522\n",
      "348/500 [===================>..........] - ETA: 2:37 - loss: 2.0869 - regression_loss: 1.5348 - classification_loss: 0.5521\n",
      "349/500 [===================>..........] - ETA: 2:36 - loss: 2.0870 - regression_loss: 1.5350 - classification_loss: 0.5520\n",
      "350/500 [====================>.........] - ETA: 2:35 - loss: 2.0868 - regression_loss: 1.5354 - classification_loss: 0.5514\n",
      "351/500 [====================>.........] - ETA: 2:34 - loss: 2.0855 - regression_loss: 1.5346 - classification_loss: 0.5509\n",
      "352/500 [====================>.........] - ETA: 2:33 - loss: 2.0840 - regression_loss: 1.5335 - classification_loss: 0.5504\n",
      "353/500 [====================>.........] - ETA: 2:32 - loss: 2.0817 - regression_loss: 1.5319 - classification_loss: 0.5498\n",
      "354/500 [====================>.........] - ETA: 2:31 - loss: 2.0798 - regression_loss: 1.5303 - classification_loss: 0.5496\n",
      "355/500 [====================>.........] - ETA: 2:30 - loss: 2.0789 - regression_loss: 1.5295 - classification_loss: 0.5494\n",
      "356/500 [====================>.........] - ETA: 2:29 - loss: 2.0785 - regression_loss: 1.5293 - classification_loss: 0.5491\n",
      "357/500 [====================>.........] - ETA: 2:28 - loss: 2.0795 - regression_loss: 1.5301 - classification_loss: 0.5494\n",
      "358/500 [====================>.........] - ETA: 2:27 - loss: 2.0802 - regression_loss: 1.5300 - classification_loss: 0.5502\n",
      "359/500 [====================>.........] - ETA: 2:26 - loss: 2.0819 - regression_loss: 1.5311 - classification_loss: 0.5509\n",
      "360/500 [====================>.........] - ETA: 2:25 - loss: 2.0812 - regression_loss: 1.5305 - classification_loss: 0.5507\n",
      "361/500 [====================>.........] - ETA: 2:24 - loss: 2.0799 - regression_loss: 1.5294 - classification_loss: 0.5505\n",
      "362/500 [====================>.........] - ETA: 2:23 - loss: 2.0805 - regression_loss: 1.5302 - classification_loss: 0.5503\n",
      "363/500 [====================>.........] - ETA: 2:22 - loss: 2.0807 - regression_loss: 1.5299 - classification_loss: 0.5508\n",
      "364/500 [====================>.........] - ETA: 2:21 - loss: 2.0804 - regression_loss: 1.5297 - classification_loss: 0.5507\n",
      "365/500 [====================>.........] - ETA: 2:20 - loss: 2.0822 - regression_loss: 1.5312 - classification_loss: 0.5510\n",
      "366/500 [====================>.........] - ETA: 2:18 - loss: 2.0830 - regression_loss: 1.5320 - classification_loss: 0.5510\n",
      "367/500 [=====================>........] - ETA: 2:17 - loss: 2.0855 - regression_loss: 1.5337 - classification_loss: 0.5518\n",
      "368/500 [=====================>........] - ETA: 2:16 - loss: 2.0845 - regression_loss: 1.5333 - classification_loss: 0.5512\n",
      "369/500 [=====================>........] - ETA: 2:15 - loss: 2.0837 - regression_loss: 1.5326 - classification_loss: 0.5511\n",
      "370/500 [=====================>........] - ETA: 2:14 - loss: 2.0834 - regression_loss: 1.5328 - classification_loss: 0.5506\n",
      "371/500 [=====================>........] - ETA: 2:13 - loss: 2.0829 - regression_loss: 1.5327 - classification_loss: 0.5502\n",
      "372/500 [=====================>........] - ETA: 2:12 - loss: 2.0821 - regression_loss: 1.5324 - classification_loss: 0.5496\n",
      "373/500 [=====================>........] - ETA: 2:11 - loss: 2.0845 - regression_loss: 1.5344 - classification_loss: 0.5501\n",
      "374/500 [=====================>........] - ETA: 2:10 - loss: 2.0858 - regression_loss: 1.5359 - classification_loss: 0.5499\n",
      "375/500 [=====================>........] - ETA: 2:09 - loss: 2.0849 - regression_loss: 1.5354 - classification_loss: 0.5494\n",
      "376/500 [=====================>........] - ETA: 2:08 - loss: 2.0833 - regression_loss: 1.5344 - classification_loss: 0.5489\n",
      "377/500 [=====================>........] - ETA: 2:07 - loss: 2.0821 - regression_loss: 1.5332 - classification_loss: 0.5490\n",
      "378/500 [=====================>........] - ETA: 2:06 - loss: 2.0830 - regression_loss: 1.5339 - classification_loss: 0.5491\n",
      "379/500 [=====================>........] - ETA: 2:05 - loss: 2.0848 - regression_loss: 1.5345 - classification_loss: 0.5503\n",
      "380/500 [=====================>........] - ETA: 2:04 - loss: 2.0872 - regression_loss: 1.5363 - classification_loss: 0.5509\n",
      "381/500 [=====================>........] - ETA: 2:03 - loss: 2.0872 - regression_loss: 1.5365 - classification_loss: 0.5507\n",
      "382/500 [=====================>........] - ETA: 2:03 - loss: 2.0876 - regression_loss: 1.5367 - classification_loss: 0.5509\n",
      "383/500 [=====================>........] - ETA: 2:01 - loss: 2.0883 - regression_loss: 1.5370 - classification_loss: 0.5513\n",
      "384/500 [======================>.......] - ETA: 2:00 - loss: 2.0899 - regression_loss: 1.5381 - classification_loss: 0.5518\n",
      "385/500 [======================>.......] - ETA: 1:59 - loss: 2.0880 - regression_loss: 1.5369 - classification_loss: 0.5510\n",
      "386/500 [======================>.......] - ETA: 1:58 - loss: 2.0872 - regression_loss: 1.5367 - classification_loss: 0.5505\n",
      "387/500 [======================>.......] - ETA: 1:57 - loss: 2.0873 - regression_loss: 1.5368 - classification_loss: 0.5505\n",
      "388/500 [======================>.......] - ETA: 1:56 - loss: 2.0895 - regression_loss: 1.5388 - classification_loss: 0.5507\n",
      "389/500 [======================>.......] - ETA: 1:55 - loss: 2.0892 - regression_loss: 1.5386 - classification_loss: 0.5507\n",
      "390/500 [======================>.......] - ETA: 1:54 - loss: 2.0886 - regression_loss: 1.5377 - classification_loss: 0.5509\n",
      "391/500 [======================>.......] - ETA: 1:53 - loss: 2.0882 - regression_loss: 1.5373 - classification_loss: 0.5509\n",
      "392/500 [======================>.......] - ETA: 1:52 - loss: 2.0881 - regression_loss: 1.5371 - classification_loss: 0.5511\n",
      "393/500 [======================>.......] - ETA: 1:51 - loss: 2.0895 - regression_loss: 1.5376 - classification_loss: 0.5518\n",
      "394/500 [======================>.......] - ETA: 1:50 - loss: 2.0895 - regression_loss: 1.5380 - classification_loss: 0.5515\n",
      "395/500 [======================>.......] - ETA: 1:49 - loss: 2.0889 - regression_loss: 1.5376 - classification_loss: 0.5512\n",
      "396/500 [======================>.......] - ETA: 1:48 - loss: 2.0900 - regression_loss: 1.5385 - classification_loss: 0.5514\n",
      "397/500 [======================>.......] - ETA: 1:47 - loss: 2.0897 - regression_loss: 1.5387 - classification_loss: 0.5510\n",
      "398/500 [======================>.......] - ETA: 1:46 - loss: 2.0905 - regression_loss: 1.5395 - classification_loss: 0.5510\n",
      "399/500 [======================>.......] - ETA: 1:45 - loss: 2.0909 - regression_loss: 1.5395 - classification_loss: 0.5513\n",
      "400/500 [=======================>......] - ETA: 1:44 - loss: 2.0907 - regression_loss: 1.5393 - classification_loss: 0.5513\n",
      "401/500 [=======================>......] - ETA: 1:43 - loss: 2.0915 - regression_loss: 1.5403 - classification_loss: 0.5512\n",
      "402/500 [=======================>......] - ETA: 1:42 - loss: 2.0909 - regression_loss: 1.5398 - classification_loss: 0.5511\n",
      "403/500 [=======================>......] - ETA: 1:41 - loss: 2.0912 - regression_loss: 1.5398 - classification_loss: 0.5514\n",
      "404/500 [=======================>......] - ETA: 1:39 - loss: 2.0889 - regression_loss: 1.5385 - classification_loss: 0.5504\n",
      "405/500 [=======================>......] - ETA: 1:38 - loss: 2.0882 - regression_loss: 1.5376 - classification_loss: 0.5506\n",
      "406/500 [=======================>......] - ETA: 1:37 - loss: 2.0887 - regression_loss: 1.5381 - classification_loss: 0.5507\n",
      "407/500 [=======================>......] - ETA: 1:36 - loss: 2.0893 - regression_loss: 1.5387 - classification_loss: 0.5506\n",
      "408/500 [=======================>......] - ETA: 1:35 - loss: 2.0885 - regression_loss: 1.5380 - classification_loss: 0.5505\n",
      "409/500 [=======================>......] - ETA: 1:34 - loss: 2.0910 - regression_loss: 1.5397 - classification_loss: 0.5513\n",
      "410/500 [=======================>......] - ETA: 1:33 - loss: 2.0911 - regression_loss: 1.5395 - classification_loss: 0.5517\n",
      "411/500 [=======================>......] - ETA: 1:32 - loss: 2.0904 - regression_loss: 1.5391 - classification_loss: 0.5513\n",
      "412/500 [=======================>......] - ETA: 1:31 - loss: 2.0886 - regression_loss: 1.5377 - classification_loss: 0.5508\n",
      "413/500 [=======================>......] - ETA: 1:30 - loss: 2.0883 - regression_loss: 1.5377 - classification_loss: 0.5506\n",
      "414/500 [=======================>......] - ETA: 1:29 - loss: 2.0877 - regression_loss: 1.5373 - classification_loss: 0.5504\n",
      "415/500 [=======================>......] - ETA: 1:28 - loss: 2.0883 - regression_loss: 1.5375 - classification_loss: 0.5507\n",
      "416/500 [=======================>......] - ETA: 1:27 - loss: 2.0886 - regression_loss: 1.5372 - classification_loss: 0.5514\n",
      "417/500 [========================>.....] - ETA: 1:26 - loss: 2.0891 - regression_loss: 1.5373 - classification_loss: 0.5519\n",
      "418/500 [========================>.....] - ETA: 1:25 - loss: 2.0887 - regression_loss: 1.5369 - classification_loss: 0.5518\n",
      "419/500 [========================>.....] - ETA: 1:24 - loss: 2.0899 - regression_loss: 1.5379 - classification_loss: 0.5519\n",
      "420/500 [========================>.....] - ETA: 1:23 - loss: 2.0904 - regression_loss: 1.5386 - classification_loss: 0.5519\n",
      "421/500 [========================>.....] - ETA: 1:22 - loss: 2.0888 - regression_loss: 1.5370 - classification_loss: 0.5518\n",
      "422/500 [========================>.....] - ETA: 1:21 - loss: 2.0879 - regression_loss: 1.5365 - classification_loss: 0.5514\n",
      "423/500 [========================>.....] - ETA: 1:20 - loss: 2.0879 - regression_loss: 1.5363 - classification_loss: 0.5516\n",
      "424/500 [========================>.....] - ETA: 1:19 - loss: 2.0879 - regression_loss: 1.5365 - classification_loss: 0.5514\n",
      "425/500 [========================>.....] - ETA: 1:17 - loss: 2.0869 - regression_loss: 1.5357 - classification_loss: 0.5512\n",
      "426/500 [========================>.....] - ETA: 1:16 - loss: 2.0873 - regression_loss: 1.5363 - classification_loss: 0.5510\n",
      "427/500 [========================>.....] - ETA: 1:15 - loss: 2.0874 - regression_loss: 1.5358 - classification_loss: 0.5515\n",
      "428/500 [========================>.....] - ETA: 1:14 - loss: 2.0893 - regression_loss: 1.5372 - classification_loss: 0.5521\n",
      "429/500 [========================>.....] - ETA: 1:13 - loss: 2.0886 - regression_loss: 1.5364 - classification_loss: 0.5521\n",
      "430/500 [========================>.....] - ETA: 1:12 - loss: 2.0886 - regression_loss: 1.5363 - classification_loss: 0.5523\n",
      "431/500 [========================>.....] - ETA: 1:11 - loss: 2.0885 - regression_loss: 1.5357 - classification_loss: 0.5527\n",
      "432/500 [========================>.....] - ETA: 1:10 - loss: 2.0876 - regression_loss: 1.5351 - classification_loss: 0.5525\n",
      "433/500 [========================>.....] - ETA: 1:09 - loss: 2.0895 - regression_loss: 1.5361 - classification_loss: 0.5535\n",
      "434/500 [=========================>....] - ETA: 1:08 - loss: 2.0899 - regression_loss: 1.5364 - classification_loss: 0.5535\n",
      "435/500 [=========================>....] - ETA: 1:07 - loss: 2.0893 - regression_loss: 1.5362 - classification_loss: 0.5532\n",
      "436/500 [=========================>....] - ETA: 1:06 - loss: 2.0901 - regression_loss: 1.5364 - classification_loss: 0.5536\n",
      "437/500 [=========================>....] - ETA: 1:05 - loss: 2.0913 - regression_loss: 1.5372 - classification_loss: 0.5540\n",
      "438/500 [=========================>....] - ETA: 1:04 - loss: 2.0898 - regression_loss: 1.5361 - classification_loss: 0.5537\n",
      "439/500 [=========================>....] - ETA: 1:03 - loss: 2.0918 - regression_loss: 1.5375 - classification_loss: 0.5543\n",
      "440/500 [=========================>....] - ETA: 1:02 - loss: 2.0923 - regression_loss: 1.5380 - classification_loss: 0.5544\n",
      "441/500 [=========================>....] - ETA: 1:01 - loss: 2.0914 - regression_loss: 1.5375 - classification_loss: 0.5539\n",
      "442/500 [=========================>....] - ETA: 1:00 - loss: 2.0903 - regression_loss: 1.5368 - classification_loss: 0.5535\n",
      "443/500 [=========================>....] - ETA: 59s - loss: 2.0893 - regression_loss: 1.5362 - classification_loss: 0.5531 \n",
      "444/500 [=========================>....] - ETA: 58s - loss: 2.0881 - regression_loss: 1.5352 - classification_loss: 0.5528\n",
      "445/500 [=========================>....] - ETA: 57s - loss: 2.0882 - regression_loss: 1.5355 - classification_loss: 0.5527\n",
      "446/500 [=========================>....] - ETA: 56s - loss: 2.0900 - regression_loss: 1.5367 - classification_loss: 0.5533\n",
      "447/500 [=========================>....] - ETA: 55s - loss: 2.0891 - regression_loss: 1.5359 - classification_loss: 0.5532\n",
      "448/500 [=========================>....] - ETA: 54s - loss: 2.0877 - regression_loss: 1.5347 - classification_loss: 0.5529\n",
      "449/500 [=========================>....] - ETA: 53s - loss: 2.0866 - regression_loss: 1.5340 - classification_loss: 0.5526\n",
      "450/500 [==========================>...] - ETA: 51s - loss: 2.0877 - regression_loss: 1.5347 - classification_loss: 0.5530\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 2.0890 - regression_loss: 1.5361 - classification_loss: 0.5529\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 2.0913 - regression_loss: 1.5375 - classification_loss: 0.5538\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 2.0932 - regression_loss: 1.5384 - classification_loss: 0.5548\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 2.0921 - regression_loss: 1.5374 - classification_loss: 0.5547\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 2.0920 - regression_loss: 1.5375 - classification_loss: 0.5545\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 2.0930 - regression_loss: 1.5383 - classification_loss: 0.5548\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 2.0927 - regression_loss: 1.5381 - classification_loss: 0.5546\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 2.0916 - regression_loss: 1.5371 - classification_loss: 0.5544\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 2.0914 - regression_loss: 1.5370 - classification_loss: 0.5544\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 2.0912 - regression_loss: 1.5368 - classification_loss: 0.5544\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 2.0910 - regression_loss: 1.5366 - classification_loss: 0.5543\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 2.0913 - regression_loss: 1.5368 - classification_loss: 0.5545\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 2.0899 - regression_loss: 1.5358 - classification_loss: 0.5541\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 2.0901 - regression_loss: 1.5362 - classification_loss: 0.5538\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 2.0893 - regression_loss: 1.5358 - classification_loss: 0.5536\n",
      "466/500 [==========================>...] - ETA: 35s - loss: 2.0903 - regression_loss: 1.5367 - classification_loss: 0.5536\n",
      "467/500 [===========================>..] - ETA: 34s - loss: 2.0895 - regression_loss: 1.5361 - classification_loss: 0.5533\n",
      "468/500 [===========================>..] - ETA: 33s - loss: 2.0880 - regression_loss: 1.5351 - classification_loss: 0.5529\n",
      "469/500 [===========================>..] - ETA: 32s - loss: 2.0888 - regression_loss: 1.5359 - classification_loss: 0.5529\n",
      "470/500 [===========================>..] - ETA: 31s - loss: 2.0889 - regression_loss: 1.5361 - classification_loss: 0.5528\n",
      "471/500 [===========================>..] - ETA: 30s - loss: 2.0894 - regression_loss: 1.5363 - classification_loss: 0.5530\n",
      "472/500 [===========================>..] - ETA: 29s - loss: 2.0902 - regression_loss: 1.5369 - classification_loss: 0.5533\n",
      "473/500 [===========================>..] - ETA: 28s - loss: 2.0892 - regression_loss: 1.5360 - classification_loss: 0.5533\n",
      "474/500 [===========================>..] - ETA: 27s - loss: 2.0882 - regression_loss: 1.5352 - classification_loss: 0.5531\n",
      "475/500 [===========================>..] - ETA: 25s - loss: 2.0875 - regression_loss: 1.5346 - classification_loss: 0.5529\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 2.0870 - regression_loss: 1.5343 - classification_loss: 0.5527\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 2.0849 - regression_loss: 1.5327 - classification_loss: 0.5522\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 2.0842 - regression_loss: 1.5321 - classification_loss: 0.5521\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 2.0844 - regression_loss: 1.5325 - classification_loss: 0.5519\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 2.0867 - regression_loss: 1.5341 - classification_loss: 0.5526\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 2.0855 - regression_loss: 1.5333 - classification_loss: 0.5522\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 2.0852 - regression_loss: 1.5331 - classification_loss: 0.5520\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 2.0858 - regression_loss: 1.5335 - classification_loss: 0.5523\n",
      "484/500 [============================>.] - ETA: 16s - loss: 2.0868 - regression_loss: 1.5345 - classification_loss: 0.5523\n",
      "485/500 [============================>.] - ETA: 15s - loss: 2.0852 - regression_loss: 1.5334 - classification_loss: 0.5518\n",
      "486/500 [============================>.] - ETA: 14s - loss: 2.0864 - regression_loss: 1.5345 - classification_loss: 0.5519\n",
      "487/500 [============================>.] - ETA: 13s - loss: 2.0856 - regression_loss: 1.5334 - classification_loss: 0.5522\n",
      "488/500 [============================>.] - ETA: 12s - loss: 2.0846 - regression_loss: 1.5327 - classification_loss: 0.5518\n",
      "489/500 [============================>.] - ETA: 11s - loss: 2.0836 - regression_loss: 1.5321 - classification_loss: 0.5516\n",
      "490/500 [============================>.] - ETA: 10s - loss: 2.0857 - regression_loss: 1.5337 - classification_loss: 0.5520\n",
      "491/500 [============================>.] - ETA: 9s - loss: 2.0842 - regression_loss: 1.5327 - classification_loss: 0.5515 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 2.0840 - regression_loss: 1.5328 - classification_loss: 0.5512\n",
      "493/500 [============================>.] - ETA: 7s - loss: 2.0826 - regression_loss: 1.5316 - classification_loss: 0.5511\n",
      "494/500 [============================>.] - ETA: 6s - loss: 2.0822 - regression_loss: 1.5309 - classification_loss: 0.5513\n",
      "495/500 [============================>.] - ETA: 5s - loss: 2.0806 - regression_loss: 1.5298 - classification_loss: 0.5508\n",
      "496/500 [============================>.] - ETA: 4s - loss: 2.0809 - regression_loss: 1.5301 - classification_loss: 0.5508\n",
      "497/500 [============================>.] - ETA: 3s - loss: 2.0815 - regression_loss: 1.5307 - classification_loss: 0.5508\n",
      "498/500 [============================>.] - ETA: 2s - loss: 2.0831 - regression_loss: 1.5317 - classification_loss: 0.5514\n",
      "499/500 [============================>.] - ETA: 1s - loss: 2.0829 - regression_loss: 1.5315 - classification_loss: 0.5514\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.0828 - regression_loss: 1.5313 - classification_loss: 0.5516\n",
      "Epoch 00006: saving model to ./snapshots\\resnet50_csv_06.h5\n",
      "\n",
      "500/500 [==============================] - 522s 1s/step - loss: 2.0828 - regression_loss: 1.5313 - classification_loss: 0.5516\n",
      "Epoch 7/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 1.9045 - regression_loss: 1.4231 - classification_loss: 0.4813\n",
      "  2/500 [..............................] - ETA: 4:28 - loss: 2.3964 - regression_loss: 1.7977 - classification_loss: 0.5987\n",
      "  3/500 [..............................] - ETA: 5:57 - loss: 2.1475 - regression_loss: 1.6275 - classification_loss: 0.5199\n",
      "  4/500 [..............................] - ETA: 6:30 - loss: 1.8135 - regression_loss: 1.3840 - classification_loss: 0.4295\n",
      "  5/500 [..............................] - ETA: 6:58 - loss: 1.8237 - regression_loss: 1.3736 - classification_loss: 0.4502\n",
      "  6/500 [..............................] - ETA: 7:08 - loss: 2.0112 - regression_loss: 1.5276 - classification_loss: 0.4836\n",
      "  7/500 [..............................] - ETA: 7:15 - loss: 2.0631 - regression_loss: 1.5581 - classification_loss: 0.5049\n",
      "  8/500 [..............................] - ETA: 7:27 - loss: 2.0877 - regression_loss: 1.5715 - classification_loss: 0.5163\n",
      "  9/500 [..............................] - ETA: 7:36 - loss: 2.1018 - regression_loss: 1.5705 - classification_loss: 0.5313\n",
      " 10/500 [..............................] - ETA: 7:43 - loss: 2.1542 - regression_loss: 1.6021 - classification_loss: 0.5521\n",
      " 11/500 [..............................] - ETA: 7:44 - loss: 2.0994 - regression_loss: 1.5655 - classification_loss: 0.5339\n",
      " 12/500 [..............................] - ETA: 7:48 - loss: 2.0364 - regression_loss: 1.5108 - classification_loss: 0.5256\n",
      " 13/500 [..............................] - ETA: 7:48 - loss: 2.0015 - regression_loss: 1.4808 - classification_loss: 0.5207\n",
      " 14/500 [..............................] - ETA: 7:48 - loss: 1.9895 - regression_loss: 1.4660 - classification_loss: 0.5234\n",
      " 15/500 [..............................] - ETA: 7:47 - loss: 1.9906 - regression_loss: 1.4657 - classification_loss: 0.5250\n",
      " 16/500 [..............................] - ETA: 7:47 - loss: 2.0150 - regression_loss: 1.4785 - classification_loss: 0.5365\n",
      " 17/500 [>.............................] - ETA: 7:46 - loss: 2.0409 - regression_loss: 1.4984 - classification_loss: 0.5425\n",
      " 18/500 [>.............................] - ETA: 7:43 - loss: 2.0207 - regression_loss: 1.4919 - classification_loss: 0.5288\n",
      " 19/500 [>.............................] - ETA: 7:47 - loss: 2.0779 - regression_loss: 1.5350 - classification_loss: 0.5429\n",
      " 20/500 [>.............................] - ETA: 7:46 - loss: 2.0719 - regression_loss: 1.5281 - classification_loss: 0.5438\n",
      " 21/500 [>.............................] - ETA: 7:45 - loss: 2.0763 - regression_loss: 1.5340 - classification_loss: 0.5423\n",
      " 22/500 [>.............................] - ETA: 7:47 - loss: 2.0735 - regression_loss: 1.5303 - classification_loss: 0.5433\n",
      " 23/500 [>.............................] - ETA: 7:48 - loss: 2.0965 - regression_loss: 1.5420 - classification_loss: 0.5545\n",
      " 24/500 [>.............................] - ETA: 7:49 - loss: 2.0845 - regression_loss: 1.5355 - classification_loss: 0.5490\n",
      " 25/500 [>.............................] - ETA: 7:48 - loss: 2.0625 - regression_loss: 1.5201 - classification_loss: 0.5424\n",
      " 26/500 [>.............................] - ETA: 7:48 - loss: 2.0726 - regression_loss: 1.5327 - classification_loss: 0.5399\n",
      " 27/500 [>.............................] - ETA: 7:49 - loss: 2.0799 - regression_loss: 1.5339 - classification_loss: 0.5460\n",
      " 28/500 [>.............................] - ETA: 7:48 - loss: 2.1008 - regression_loss: 1.5483 - classification_loss: 0.5525\n",
      " 29/500 [>.............................] - ETA: 7:47 - loss: 2.1028 - regression_loss: 1.5526 - classification_loss: 0.5502\n",
      " 30/500 [>.............................] - ETA: 7:46 - loss: 2.1028 - regression_loss: 1.5521 - classification_loss: 0.5507\n",
      " 31/500 [>.............................] - ETA: 7:45 - loss: 2.0955 - regression_loss: 1.5463 - classification_loss: 0.5492\n",
      " 32/500 [>.............................] - ETA: 7:45 - loss: 2.0845 - regression_loss: 1.5392 - classification_loss: 0.5453\n",
      " 33/500 [>.............................] - ETA: 7:48 - loss: 2.1074 - regression_loss: 1.5538 - classification_loss: 0.5536\n",
      " 34/500 [=>............................] - ETA: 7:48 - loss: 2.1249 - regression_loss: 1.5675 - classification_loss: 0.5573\n",
      " 35/500 [=>............................] - ETA: 7:48 - loss: 2.1011 - regression_loss: 1.5435 - classification_loss: 0.5576\n",
      " 36/500 [=>............................] - ETA: 7:47 - loss: 2.1011 - regression_loss: 1.5433 - classification_loss: 0.5578\n",
      " 37/500 [=>............................] - ETA: 7:45 - loss: 2.0908 - regression_loss: 1.5306 - classification_loss: 0.5602\n",
      " 38/500 [=>............................] - ETA: 7:46 - loss: 2.0789 - regression_loss: 1.5220 - classification_loss: 0.5569\n",
      " 39/500 [=>............................] - ETA: 7:44 - loss: 2.0606 - regression_loss: 1.5090 - classification_loss: 0.5516\n",
      " 40/500 [=>............................] - ETA: 7:44 - loss: 2.0510 - regression_loss: 1.5046 - classification_loss: 0.5464\n",
      " 41/500 [=>............................] - ETA: 7:44 - loss: 2.0837 - regression_loss: 1.5281 - classification_loss: 0.5555\n",
      " 42/500 [=>............................] - ETA: 7:43 - loss: 2.0787 - regression_loss: 1.5221 - classification_loss: 0.5565\n",
      " 43/500 [=>............................] - ETA: 7:43 - loss: 2.0950 - regression_loss: 1.5342 - classification_loss: 0.5608\n",
      " 44/500 [=>............................] - ETA: 7:41 - loss: 2.1009 - regression_loss: 1.5398 - classification_loss: 0.5610\n",
      " 45/500 [=>............................] - ETA: 7:40 - loss: 2.1011 - regression_loss: 1.5412 - classification_loss: 0.5599\n",
      " 46/500 [=>............................] - ETA: 7:39 - loss: 2.0906 - regression_loss: 1.5321 - classification_loss: 0.5585\n",
      " 47/500 [=>............................] - ETA: 7:38 - loss: 2.0959 - regression_loss: 1.5366 - classification_loss: 0.5593\n",
      " 48/500 [=>............................] - ETA: 7:37 - loss: 2.0790 - regression_loss: 1.5242 - classification_loss: 0.5548\n",
      " 49/500 [=>............................] - ETA: 7:34 - loss: 2.0880 - regression_loss: 1.5355 - classification_loss: 0.5526\n",
      " 50/500 [==>...........................] - ETA: 7:35 - loss: 2.0926 - regression_loss: 1.5393 - classification_loss: 0.5533\n",
      " 51/500 [==>...........................] - ETA: 7:33 - loss: 2.0768 - regression_loss: 1.5266 - classification_loss: 0.5502\n",
      " 52/500 [==>...........................] - ETA: 7:33 - loss: 2.0870 - regression_loss: 1.5328 - classification_loss: 0.5543\n",
      " 53/500 [==>...........................] - ETA: 7:31 - loss: 2.0737 - regression_loss: 1.5223 - classification_loss: 0.5514\n",
      " 54/500 [==>...........................] - ETA: 7:30 - loss: 2.0757 - regression_loss: 1.5243 - classification_loss: 0.5514\n",
      " 55/500 [==>...........................] - ETA: 7:29 - loss: 2.0846 - regression_loss: 1.5300 - classification_loss: 0.5546\n",
      " 56/500 [==>...........................] - ETA: 7:27 - loss: 2.0858 - regression_loss: 1.5300 - classification_loss: 0.5558\n",
      " 57/500 [==>...........................] - ETA: 7:26 - loss: 2.0795 - regression_loss: 1.5258 - classification_loss: 0.5538\n",
      " 58/500 [==>...........................] - ETA: 7:25 - loss: 2.0702 - regression_loss: 1.5188 - classification_loss: 0.5513\n",
      " 59/500 [==>...........................] - ETA: 7:24 - loss: 2.0779 - regression_loss: 1.5266 - classification_loss: 0.5513\n",
      " 60/500 [==>...........................] - ETA: 7:23 - loss: 2.0699 - regression_loss: 1.5187 - classification_loss: 0.5512\n",
      " 61/500 [==>...........................] - ETA: 7:22 - loss: 2.0769 - regression_loss: 1.5240 - classification_loss: 0.5529\n",
      " 62/500 [==>...........................] - ETA: 7:20 - loss: 2.0639 - regression_loss: 1.5151 - classification_loss: 0.5488\n",
      " 63/500 [==>...........................] - ETA: 7:19 - loss: 2.0568 - regression_loss: 1.5093 - classification_loss: 0.5475\n",
      " 64/500 [==>...........................] - ETA: 7:18 - loss: 2.0741 - regression_loss: 1.5200 - classification_loss: 0.5541\n",
      " 65/500 [==>...........................] - ETA: 7:18 - loss: 2.0728 - regression_loss: 1.5199 - classification_loss: 0.5530\n",
      " 66/500 [==>...........................] - ETA: 7:16 - loss: 2.0778 - regression_loss: 1.5243 - classification_loss: 0.5536\n",
      " 67/500 [===>..........................] - ETA: 7:15 - loss: 2.0707 - regression_loss: 1.5194 - classification_loss: 0.5513\n",
      " 68/500 [===>..........................] - ETA: 7:14 - loss: 2.0760 - regression_loss: 1.5254 - classification_loss: 0.5507\n",
      " 69/500 [===>..........................] - ETA: 7:13 - loss: 2.0849 - regression_loss: 1.5330 - classification_loss: 0.5519\n",
      " 70/500 [===>..........................] - ETA: 7:12 - loss: 2.0760 - regression_loss: 1.5262 - classification_loss: 0.5498\n",
      " 71/500 [===>..........................] - ETA: 7:11 - loss: 2.0750 - regression_loss: 1.5252 - classification_loss: 0.5499\n",
      " 72/500 [===>..........................] - ETA: 7:10 - loss: 2.0884 - regression_loss: 1.5354 - classification_loss: 0.5530\n",
      " 73/500 [===>..........................] - ETA: 7:09 - loss: 2.0788 - regression_loss: 1.5271 - classification_loss: 0.5517\n",
      " 74/500 [===>..........................] - ETA: 7:08 - loss: 2.0841 - regression_loss: 1.5332 - classification_loss: 0.5509\n",
      " 75/500 [===>..........................] - ETA: 7:07 - loss: 2.0762 - regression_loss: 1.5230 - classification_loss: 0.5532\n",
      " 76/500 [===>..........................] - ETA: 7:07 - loss: 2.0665 - regression_loss: 1.5164 - classification_loss: 0.5502\n",
      " 77/500 [===>..........................] - ETA: 7:06 - loss: 2.0580 - regression_loss: 1.5092 - classification_loss: 0.5488\n",
      " 78/500 [===>..........................] - ETA: 7:06 - loss: 2.0650 - regression_loss: 1.5114 - classification_loss: 0.5536\n",
      " 79/500 [===>..........................] - ETA: 7:04 - loss: 2.0642 - regression_loss: 1.5094 - classification_loss: 0.5549\n",
      " 80/500 [===>..........................] - ETA: 7:03 - loss: 2.0624 - regression_loss: 1.5086 - classification_loss: 0.5538\n",
      " 81/500 [===>..........................] - ETA: 7:03 - loss: 2.0727 - regression_loss: 1.5171 - classification_loss: 0.5556\n",
      " 82/500 [===>..........................] - ETA: 7:02 - loss: 2.0698 - regression_loss: 1.5167 - classification_loss: 0.5532\n",
      " 83/500 [===>..........................] - ETA: 7:01 - loss: 2.0724 - regression_loss: 1.5158 - classification_loss: 0.5565\n",
      " 84/500 [====>.........................] - ETA: 7:00 - loss: 2.0717 - regression_loss: 1.5144 - classification_loss: 0.5572\n",
      " 85/500 [====>.........................] - ETA: 6:59 - loss: 2.0718 - regression_loss: 1.5162 - classification_loss: 0.5556\n",
      " 86/500 [====>.........................] - ETA: 6:58 - loss: 2.0703 - regression_loss: 1.5143 - classification_loss: 0.5560\n",
      " 87/500 [====>.........................] - ETA: 6:57 - loss: 2.0670 - regression_loss: 1.5132 - classification_loss: 0.5538\n",
      " 88/500 [====>.........................] - ETA: 6:56 - loss: 2.0620 - regression_loss: 1.5090 - classification_loss: 0.5530\n",
      " 89/500 [====>.........................] - ETA: 6:55 - loss: 2.0632 - regression_loss: 1.5098 - classification_loss: 0.5534\n",
      " 90/500 [====>.........................] - ETA: 6:54 - loss: 2.0670 - regression_loss: 1.5145 - classification_loss: 0.5525\n",
      " 91/500 [====>.........................] - ETA: 6:53 - loss: 2.0635 - regression_loss: 1.5115 - classification_loss: 0.5520\n",
      " 92/500 [====>.........................] - ETA: 6:52 - loss: 2.0640 - regression_loss: 1.5137 - classification_loss: 0.5503\n",
      " 93/500 [====>.........................] - ETA: 6:50 - loss: 2.0608 - regression_loss: 1.5118 - classification_loss: 0.5490\n",
      " 94/500 [====>.........................] - ETA: 6:50 - loss: 2.0603 - regression_loss: 1.5114 - classification_loss: 0.5489\n",
      " 95/500 [====>.........................] - ETA: 6:49 - loss: 2.0736 - regression_loss: 1.5213 - classification_loss: 0.5522\n",
      " 96/500 [====>.........................] - ETA: 6:48 - loss: 2.0744 - regression_loss: 1.5205 - classification_loss: 0.5540\n",
      " 97/500 [====>.........................] - ETA: 6:47 - loss: 2.0755 - regression_loss: 1.5224 - classification_loss: 0.5531\n",
      " 98/500 [====>.........................] - ETA: 6:46 - loss: 2.0759 - regression_loss: 1.5235 - classification_loss: 0.5525\n",
      " 99/500 [====>.........................] - ETA: 6:46 - loss: 2.0830 - regression_loss: 1.5277 - classification_loss: 0.5553\n",
      "100/500 [=====>........................] - ETA: 6:45 - loss: 2.0899 - regression_loss: 1.5329 - classification_loss: 0.5570\n",
      "101/500 [=====>........................] - ETA: 6:44 - loss: 2.0907 - regression_loss: 1.5335 - classification_loss: 0.5572\n",
      "102/500 [=====>........................] - ETA: 6:43 - loss: 2.0951 - regression_loss: 1.5368 - classification_loss: 0.5582\n",
      "103/500 [=====>........................] - ETA: 6:41 - loss: 2.0887 - regression_loss: 1.5313 - classification_loss: 0.5574\n",
      "104/500 [=====>........................] - ETA: 6:41 - loss: 2.0925 - regression_loss: 1.5348 - classification_loss: 0.5578\n",
      "105/500 [=====>........................] - ETA: 6:40 - loss: 2.0987 - regression_loss: 1.5398 - classification_loss: 0.5589\n",
      "106/500 [=====>........................] - ETA: 6:39 - loss: 2.1005 - regression_loss: 1.5411 - classification_loss: 0.5594\n",
      "107/500 [=====>........................] - ETA: 6:38 - loss: 2.1001 - regression_loss: 1.5411 - classification_loss: 0.5591\n",
      "108/500 [=====>........................] - ETA: 6:37 - loss: 2.0950 - regression_loss: 1.5382 - classification_loss: 0.5568\n",
      "109/500 [=====>........................] - ETA: 6:35 - loss: 2.0946 - regression_loss: 1.5391 - classification_loss: 0.5555\n",
      "110/500 [=====>........................] - ETA: 6:35 - loss: 2.0993 - regression_loss: 1.5432 - classification_loss: 0.5561\n",
      "111/500 [=====>........................] - ETA: 6:33 - loss: 2.0988 - regression_loss: 1.5418 - classification_loss: 0.5571\n",
      "112/500 [=====>........................] - ETA: 6:33 - loss: 2.0966 - regression_loss: 1.5401 - classification_loss: 0.5565\n",
      "113/500 [=====>........................] - ETA: 6:32 - loss: 2.0930 - regression_loss: 1.5376 - classification_loss: 0.5555\n",
      "114/500 [=====>........................] - ETA: 6:30 - loss: 2.0923 - regression_loss: 1.5375 - classification_loss: 0.5548\n",
      "115/500 [=====>........................] - ETA: 6:29 - loss: 2.0805 - regression_loss: 1.5283 - classification_loss: 0.5522\n",
      "116/500 [=====>........................] - ETA: 6:28 - loss: 2.0793 - regression_loss: 1.5284 - classification_loss: 0.5509\n",
      "117/500 [======>.......................] - ETA: 6:27 - loss: 2.0754 - regression_loss: 1.5258 - classification_loss: 0.5496\n",
      "118/500 [======>.......................] - ETA: 6:26 - loss: 2.0747 - regression_loss: 1.5250 - classification_loss: 0.5497\n",
      "119/500 [======>.......................] - ETA: 6:25 - loss: 2.0780 - regression_loss: 1.5283 - classification_loss: 0.5497\n",
      "120/500 [======>.......................] - ETA: 6:24 - loss: 2.0835 - regression_loss: 1.5317 - classification_loss: 0.5519\n",
      "121/500 [======>.......................] - ETA: 6:23 - loss: 2.0843 - regression_loss: 1.5323 - classification_loss: 0.5520\n",
      "122/500 [======>.......................] - ETA: 6:23 - loss: 2.0777 - regression_loss: 1.5277 - classification_loss: 0.5500\n",
      "123/500 [======>.......................] - ETA: 6:22 - loss: 2.0826 - regression_loss: 1.5299 - classification_loss: 0.5527\n",
      "124/500 [======>.......................] - ETA: 6:21 - loss: 2.0920 - regression_loss: 1.5369 - classification_loss: 0.5551\n",
      "125/500 [======>.......................] - ETA: 6:20 - loss: 2.0969 - regression_loss: 1.5402 - classification_loss: 0.5567\n",
      "126/500 [======>.......................] - ETA: 6:19 - loss: 2.1032 - regression_loss: 1.5452 - classification_loss: 0.5579\n",
      "127/500 [======>.......................] - ETA: 6:18 - loss: 2.1072 - regression_loss: 1.5491 - classification_loss: 0.5581\n",
      "128/500 [======>.......................] - ETA: 6:17 - loss: 2.1133 - regression_loss: 1.5529 - classification_loss: 0.5604\n",
      "129/500 [======>.......................] - ETA: 6:16 - loss: 2.1108 - regression_loss: 1.5510 - classification_loss: 0.5597\n",
      "130/500 [======>.......................] - ETA: 6:15 - loss: 2.1157 - regression_loss: 1.5551 - classification_loss: 0.5606\n",
      "131/500 [======>.......................] - ETA: 6:14 - loss: 2.1224 - regression_loss: 1.5595 - classification_loss: 0.5629\n",
      "132/500 [======>.......................] - ETA: 6:13 - loss: 2.1213 - regression_loss: 1.5585 - classification_loss: 0.5628\n",
      "133/500 [======>.......................] - ETA: 6:12 - loss: 2.1175 - regression_loss: 1.5547 - classification_loss: 0.5628\n",
      "134/500 [=======>......................] - ETA: 6:11 - loss: 2.1163 - regression_loss: 1.5530 - classification_loss: 0.5633\n",
      "135/500 [=======>......................] - ETA: 6:10 - loss: 2.1163 - regression_loss: 1.5531 - classification_loss: 0.5632\n",
      "136/500 [=======>......................] - ETA: 6:09 - loss: 2.1133 - regression_loss: 1.5511 - classification_loss: 0.5622\n",
      "137/500 [=======>......................] - ETA: 6:08 - loss: 2.1077 - regression_loss: 1.5478 - classification_loss: 0.5599\n",
      "138/500 [=======>......................] - ETA: 6:07 - loss: 2.1020 - regression_loss: 1.5433 - classification_loss: 0.5587\n",
      "139/500 [=======>......................] - ETA: 6:06 - loss: 2.1017 - regression_loss: 1.5427 - classification_loss: 0.5590\n",
      "140/500 [=======>......................] - ETA: 6:05 - loss: 2.1019 - regression_loss: 1.5437 - classification_loss: 0.5582\n",
      "141/500 [=======>......................] - ETA: 6:05 - loss: 2.1091 - regression_loss: 1.5491 - classification_loss: 0.5600\n",
      "142/500 [=======>......................] - ETA: 6:04 - loss: 2.1048 - regression_loss: 1.5449 - classification_loss: 0.5599\n",
      "143/500 [=======>......................] - ETA: 6:03 - loss: 2.1038 - regression_loss: 1.5446 - classification_loss: 0.5591\n",
      "144/500 [=======>......................] - ETA: 6:01 - loss: 2.0982 - regression_loss: 1.5396 - classification_loss: 0.5587\n",
      "145/500 [=======>......................] - ETA: 6:01 - loss: 2.0927 - regression_loss: 1.5354 - classification_loss: 0.5573\n",
      "146/500 [=======>......................] - ETA: 6:00 - loss: 2.0979 - regression_loss: 1.5390 - classification_loss: 0.5588\n",
      "147/500 [=======>......................] - ETA: 5:59 - loss: 2.0976 - regression_loss: 1.5392 - classification_loss: 0.5584\n",
      "148/500 [=======>......................] - ETA: 5:58 - loss: 2.0967 - regression_loss: 1.5387 - classification_loss: 0.5579\n",
      "149/500 [=======>......................] - ETA: 5:57 - loss: 2.0929 - regression_loss: 1.5367 - classification_loss: 0.5562\n",
      "150/500 [========>.....................] - ETA: 5:56 - loss: 2.0929 - regression_loss: 1.5369 - classification_loss: 0.5561\n",
      "151/500 [========>.....................] - ETA: 5:55 - loss: 2.0933 - regression_loss: 1.5378 - classification_loss: 0.5555\n",
      "152/500 [========>.....................] - ETA: 5:54 - loss: 2.0956 - regression_loss: 1.5403 - classification_loss: 0.5554\n",
      "153/500 [========>.....................] - ETA: 5:53 - loss: 2.0949 - regression_loss: 1.5397 - classification_loss: 0.5552\n",
      "154/500 [========>.....................] - ETA: 5:52 - loss: 2.0951 - regression_loss: 1.5396 - classification_loss: 0.5555\n",
      "155/500 [========>.....................] - ETA: 5:51 - loss: 2.0905 - regression_loss: 1.5357 - classification_loss: 0.5547\n",
      "156/500 [========>.....................] - ETA: 5:50 - loss: 2.0875 - regression_loss: 1.5332 - classification_loss: 0.5543\n",
      "157/500 [========>.....................] - ETA: 5:49 - loss: 2.0908 - regression_loss: 1.5367 - classification_loss: 0.5541\n",
      "158/500 [========>.....................] - ETA: 5:48 - loss: 2.0956 - regression_loss: 1.5395 - classification_loss: 0.5561\n",
      "159/500 [========>.....................] - ETA: 5:47 - loss: 2.0997 - regression_loss: 1.5428 - classification_loss: 0.5570\n",
      "160/500 [========>.....................] - ETA: 5:46 - loss: 2.1076 - regression_loss: 1.5479 - classification_loss: 0.5597\n",
      "161/500 [========>.....................] - ETA: 5:45 - loss: 2.1082 - regression_loss: 1.5467 - classification_loss: 0.5614\n",
      "162/500 [========>.....................] - ETA: 5:44 - loss: 2.1023 - regression_loss: 1.5427 - classification_loss: 0.5596\n",
      "163/500 [========>.....................] - ETA: 5:43 - loss: 2.1048 - regression_loss: 1.5446 - classification_loss: 0.5602\n",
      "164/500 [========>.....................] - ETA: 5:42 - loss: 2.1045 - regression_loss: 1.5446 - classification_loss: 0.5599\n",
      "165/500 [========>.....................] - ETA: 5:41 - loss: 2.1041 - regression_loss: 1.5442 - classification_loss: 0.5599\n",
      "166/500 [========>.....................] - ETA: 5:40 - loss: 2.1037 - regression_loss: 1.5439 - classification_loss: 0.5598\n",
      "167/500 [=========>....................] - ETA: 5:39 - loss: 2.0998 - regression_loss: 1.5412 - classification_loss: 0.5586\n",
      "168/500 [=========>....................] - ETA: 5:38 - loss: 2.0980 - regression_loss: 1.5402 - classification_loss: 0.5579\n",
      "169/500 [=========>....................] - ETA: 5:37 - loss: 2.0962 - regression_loss: 1.5391 - classification_loss: 0.5571\n",
      "170/500 [=========>....................] - ETA: 5:36 - loss: 2.0988 - regression_loss: 1.5415 - classification_loss: 0.5573\n",
      "171/500 [=========>....................] - ETA: 5:35 - loss: 2.0972 - regression_loss: 1.5403 - classification_loss: 0.5569\n",
      "172/500 [=========>....................] - ETA: 5:34 - loss: 2.0980 - regression_loss: 1.5413 - classification_loss: 0.5567\n",
      "173/500 [=========>....................] - ETA: 5:33 - loss: 2.1028 - regression_loss: 1.5450 - classification_loss: 0.5579\n",
      "174/500 [=========>....................] - ETA: 5:32 - loss: 2.1002 - regression_loss: 1.5439 - classification_loss: 0.5564\n",
      "175/500 [=========>....................] - ETA: 5:31 - loss: 2.0954 - regression_loss: 1.5406 - classification_loss: 0.5548\n",
      "176/500 [=========>....................] - ETA: 5:30 - loss: 2.0956 - regression_loss: 1.5407 - classification_loss: 0.5548\n",
      "177/500 [=========>....................] - ETA: 5:29 - loss: 2.0945 - regression_loss: 1.5405 - classification_loss: 0.5540\n",
      "178/500 [=========>....................] - ETA: 5:28 - loss: 2.0965 - regression_loss: 1.5420 - classification_loss: 0.5546\n",
      "179/500 [=========>....................] - ETA: 5:27 - loss: 2.0951 - regression_loss: 1.5403 - classification_loss: 0.5548\n",
      "180/500 [=========>....................] - ETA: 5:25 - loss: 2.0926 - regression_loss: 1.5381 - classification_loss: 0.5546\n",
      "181/500 [=========>....................] - ETA: 5:24 - loss: 2.0921 - regression_loss: 1.5378 - classification_loss: 0.5542\n",
      "182/500 [=========>....................] - ETA: 5:23 - loss: 2.0905 - regression_loss: 1.5368 - classification_loss: 0.5536\n",
      "183/500 [=========>....................] - ETA: 5:22 - loss: 2.0870 - regression_loss: 1.5346 - classification_loss: 0.5524\n",
      "184/500 [==========>...................] - ETA: 5:21 - loss: 2.0863 - regression_loss: 1.5341 - classification_loss: 0.5522\n",
      "185/500 [==========>...................] - ETA: 5:20 - loss: 2.0853 - regression_loss: 1.5335 - classification_loss: 0.5518\n",
      "186/500 [==========>...................] - ETA: 5:19 - loss: 2.0813 - regression_loss: 1.5305 - classification_loss: 0.5508\n",
      "187/500 [==========>...................] - ETA: 5:18 - loss: 2.0815 - regression_loss: 1.5310 - classification_loss: 0.5506\n",
      "188/500 [==========>...................] - ETA: 5:17 - loss: 2.0822 - regression_loss: 1.5305 - classification_loss: 0.5517\n",
      "189/500 [==========>...................] - ETA: 5:16 - loss: 2.0864 - regression_loss: 1.5344 - classification_loss: 0.5520\n",
      "190/500 [==========>...................] - ETA: 5:16 - loss: 2.0883 - regression_loss: 1.5354 - classification_loss: 0.5528\n",
      "191/500 [==========>...................] - ETA: 5:15 - loss: 2.0837 - regression_loss: 1.5319 - classification_loss: 0.5517\n",
      "192/500 [==========>...................] - ETA: 5:14 - loss: 2.0817 - regression_loss: 1.5306 - classification_loss: 0.5511\n",
      "193/500 [==========>...................] - ETA: 5:13 - loss: 2.0807 - regression_loss: 1.5301 - classification_loss: 0.5506\n",
      "194/500 [==========>...................] - ETA: 5:12 - loss: 2.0793 - regression_loss: 1.5292 - classification_loss: 0.5502\n",
      "195/500 [==========>...................] - ETA: 5:11 - loss: 2.0837 - regression_loss: 1.5315 - classification_loss: 0.5521\n",
      "196/500 [==========>...................] - ETA: 5:10 - loss: 2.0847 - regression_loss: 1.5320 - classification_loss: 0.5527\n",
      "197/500 [==========>...................] - ETA: 5:09 - loss: 2.0845 - regression_loss: 1.5325 - classification_loss: 0.5520\n",
      "198/500 [==========>...................] - ETA: 5:07 - loss: 2.0867 - regression_loss: 1.5341 - classification_loss: 0.5526\n",
      "199/500 [==========>...................] - ETA: 5:07 - loss: 2.0821 - regression_loss: 1.5300 - classification_loss: 0.5521\n",
      "200/500 [===========>..................] - ETA: 5:06 - loss: 2.0820 - regression_loss: 1.5295 - classification_loss: 0.5526\n",
      "201/500 [===========>..................] - ETA: 5:05 - loss: 2.0838 - regression_loss: 1.5306 - classification_loss: 0.5531\n",
      "202/500 [===========>..................] - ETA: 5:04 - loss: 2.0847 - regression_loss: 1.5313 - classification_loss: 0.5534\n",
      "203/500 [===========>..................] - ETA: 5:03 - loss: 2.0841 - regression_loss: 1.5317 - classification_loss: 0.5525\n",
      "204/500 [===========>..................] - ETA: 5:02 - loss: 2.0834 - regression_loss: 1.5315 - classification_loss: 0.5519\n",
      "205/500 [===========>..................] - ETA: 5:01 - loss: 2.0820 - regression_loss: 1.5309 - classification_loss: 0.5511\n",
      "206/500 [===========>..................] - ETA: 5:00 - loss: 2.0840 - regression_loss: 1.5319 - classification_loss: 0.5521\n",
      "207/500 [===========>..................] - ETA: 4:59 - loss: 2.0857 - regression_loss: 1.5332 - classification_loss: 0.5525\n",
      "208/500 [===========>..................] - ETA: 4:58 - loss: 2.0839 - regression_loss: 1.5318 - classification_loss: 0.5521\n",
      "209/500 [===========>..................] - ETA: 4:57 - loss: 2.0839 - regression_loss: 1.5326 - classification_loss: 0.5512\n",
      "210/500 [===========>..................] - ETA: 4:56 - loss: 2.0823 - regression_loss: 1.5319 - classification_loss: 0.5504\n",
      "211/500 [===========>..................] - ETA: 4:55 - loss: 2.0799 - regression_loss: 1.5300 - classification_loss: 0.5499\n",
      "212/500 [===========>..................] - ETA: 4:54 - loss: 2.0810 - regression_loss: 1.5309 - classification_loss: 0.5501\n",
      "213/500 [===========>..................] - ETA: 4:52 - loss: 2.0778 - regression_loss: 1.5289 - classification_loss: 0.5488\n",
      "214/500 [===========>..................] - ETA: 4:51 - loss: 2.0800 - regression_loss: 1.5313 - classification_loss: 0.5487\n",
      "215/500 [===========>..................] - ETA: 4:50 - loss: 2.0809 - regression_loss: 1.5320 - classification_loss: 0.5489\n",
      "216/500 [===========>..................] - ETA: 4:49 - loss: 2.0804 - regression_loss: 1.5319 - classification_loss: 0.5484\n",
      "217/500 [============>.................] - ETA: 4:48 - loss: 2.0845 - regression_loss: 1.5346 - classification_loss: 0.5499\n",
      "218/500 [============>.................] - ETA: 4:47 - loss: 2.0836 - regression_loss: 1.5336 - classification_loss: 0.5499\n",
      "219/500 [============>.................] - ETA: 4:46 - loss: 2.0827 - regression_loss: 1.5333 - classification_loss: 0.5494\n",
      "220/500 [============>.................] - ETA: 4:45 - loss: 2.0828 - regression_loss: 1.5340 - classification_loss: 0.5488\n",
      "221/500 [============>.................] - ETA: 4:44 - loss: 2.0828 - regression_loss: 1.5343 - classification_loss: 0.5485\n",
      "222/500 [============>.................] - ETA: 4:43 - loss: 2.0840 - regression_loss: 1.5358 - classification_loss: 0.5482\n",
      "223/500 [============>.................] - ETA: 4:42 - loss: 2.0840 - regression_loss: 1.5367 - classification_loss: 0.5473\n",
      "224/500 [============>.................] - ETA: 4:41 - loss: 2.0827 - regression_loss: 1.5360 - classification_loss: 0.5467\n",
      "225/500 [============>.................] - ETA: 4:40 - loss: 2.0841 - regression_loss: 1.5375 - classification_loss: 0.5466\n",
      "226/500 [============>.................] - ETA: 4:39 - loss: 2.0888 - regression_loss: 1.5410 - classification_loss: 0.5478\n",
      "227/500 [============>.................] - ETA: 4:38 - loss: 2.0861 - regression_loss: 1.5393 - classification_loss: 0.5468\n",
      "228/500 [============>.................] - ETA: 4:37 - loss: 2.0825 - regression_loss: 1.5363 - classification_loss: 0.5463\n",
      "229/500 [============>.................] - ETA: 4:36 - loss: 2.0861 - regression_loss: 1.5389 - classification_loss: 0.5472\n",
      "230/500 [============>.................] - ETA: 4:35 - loss: 2.0847 - regression_loss: 1.5376 - classification_loss: 0.5470\n",
      "231/500 [============>.................] - ETA: 4:34 - loss: 2.0872 - regression_loss: 1.5395 - classification_loss: 0.5477\n",
      "232/500 [============>.................] - ETA: 4:33 - loss: 2.0878 - regression_loss: 1.5397 - classification_loss: 0.5481\n",
      "233/500 [============>.................] - ETA: 4:32 - loss: 2.0883 - regression_loss: 1.5394 - classification_loss: 0.5489\n",
      "234/500 [=============>................] - ETA: 4:31 - loss: 2.0908 - regression_loss: 1.5414 - classification_loss: 0.5494\n",
      "235/500 [=============>................] - ETA: 4:30 - loss: 2.0920 - regression_loss: 1.5428 - classification_loss: 0.5493\n",
      "236/500 [=============>................] - ETA: 4:29 - loss: 2.0907 - regression_loss: 1.5413 - classification_loss: 0.5494\n",
      "237/500 [=============>................] - ETA: 4:28 - loss: 2.0877 - regression_loss: 1.5391 - classification_loss: 0.5486\n",
      "238/500 [=============>................] - ETA: 4:27 - loss: 2.0898 - regression_loss: 1.5397 - classification_loss: 0.5501\n",
      "239/500 [=============>................] - ETA: 4:26 - loss: 2.0909 - regression_loss: 1.5402 - classification_loss: 0.5508\n",
      "240/500 [=============>................] - ETA: 4:25 - loss: 2.0924 - regression_loss: 1.5414 - classification_loss: 0.5510\n",
      "241/500 [=============>................] - ETA: 4:24 - loss: 2.0891 - regression_loss: 1.5390 - classification_loss: 0.5501\n",
      "242/500 [=============>................] - ETA: 4:23 - loss: 2.0907 - regression_loss: 1.5403 - classification_loss: 0.5504\n",
      "243/500 [=============>................] - ETA: 4:22 - loss: 2.0936 - regression_loss: 1.5428 - classification_loss: 0.5508\n",
      "244/500 [=============>................] - ETA: 4:21 - loss: 2.0933 - regression_loss: 1.5419 - classification_loss: 0.5514\n",
      "245/500 [=============>................] - ETA: 4:20 - loss: 2.0921 - regression_loss: 1.5409 - classification_loss: 0.5513\n",
      "246/500 [=============>................] - ETA: 4:19 - loss: 2.0921 - regression_loss: 1.5405 - classification_loss: 0.5517\n",
      "247/500 [=============>................] - ETA: 4:18 - loss: 2.0918 - regression_loss: 1.5404 - classification_loss: 0.5514\n",
      "248/500 [=============>................] - ETA: 4:17 - loss: 2.0916 - regression_loss: 1.5409 - classification_loss: 0.5507\n",
      "249/500 [=============>................] - ETA: 4:16 - loss: 2.0876 - regression_loss: 1.5377 - classification_loss: 0.5499\n",
      "250/500 [==============>...............] - ETA: 4:15 - loss: 2.0863 - regression_loss: 1.5366 - classification_loss: 0.5497\n",
      "251/500 [==============>...............] - ETA: 4:14 - loss: 2.0876 - regression_loss: 1.5378 - classification_loss: 0.5497\n",
      "252/500 [==============>...............] - ETA: 4:13 - loss: 2.0843 - regression_loss: 1.5354 - classification_loss: 0.5489\n",
      "253/500 [==============>...............] - ETA: 4:12 - loss: 2.0851 - regression_loss: 1.5354 - classification_loss: 0.5497\n",
      "254/500 [==============>...............] - ETA: 4:11 - loss: 2.0851 - regression_loss: 1.5361 - classification_loss: 0.5491\n",
      "255/500 [==============>...............] - ETA: 4:10 - loss: 2.0840 - regression_loss: 1.5350 - classification_loss: 0.5490\n",
      "256/500 [==============>...............] - ETA: 4:09 - loss: 2.0849 - regression_loss: 1.5343 - classification_loss: 0.5506\n",
      "257/500 [==============>...............] - ETA: 4:08 - loss: 2.0876 - regression_loss: 1.5361 - classification_loss: 0.5514\n",
      "258/500 [==============>...............] - ETA: 4:07 - loss: 2.0860 - regression_loss: 1.5350 - classification_loss: 0.5510\n",
      "259/500 [==============>...............] - ETA: 4:06 - loss: 2.0843 - regression_loss: 1.5341 - classification_loss: 0.5503\n",
      "260/500 [==============>...............] - ETA: 4:05 - loss: 2.0875 - regression_loss: 1.5356 - classification_loss: 0.5519\n",
      "261/500 [==============>...............] - ETA: 4:04 - loss: 2.0917 - regression_loss: 1.5382 - classification_loss: 0.5535\n",
      "262/500 [==============>...............] - ETA: 4:05 - loss: 2.0895 - regression_loss: 1.5365 - classification_loss: 0.5530\n",
      "263/500 [==============>...............] - ETA: 4:04 - loss: 2.0896 - regression_loss: 1.5369 - classification_loss: 0.5526\n",
      "264/500 [==============>...............] - ETA: 4:03 - loss: 2.0863 - regression_loss: 1.5345 - classification_loss: 0.5518\n",
      "265/500 [==============>...............] - ETA: 4:02 - loss: 2.0865 - regression_loss: 1.5348 - classification_loss: 0.5518\n",
      "266/500 [==============>...............] - ETA: 4:01 - loss: 2.0857 - regression_loss: 1.5338 - classification_loss: 0.5519\n",
      "267/500 [===============>..............] - ETA: 4:00 - loss: 2.0853 - regression_loss: 1.5335 - classification_loss: 0.5519\n",
      "268/500 [===============>..............] - ETA: 3:59 - loss: 2.0856 - regression_loss: 1.5340 - classification_loss: 0.5516\n",
      "269/500 [===============>..............] - ETA: 3:58 - loss: 2.0884 - regression_loss: 1.5357 - classification_loss: 0.5527\n",
      "270/500 [===============>..............] - ETA: 3:57 - loss: 2.0876 - regression_loss: 1.5355 - classification_loss: 0.5521\n",
      "271/500 [===============>..............] - ETA: 3:56 - loss: 2.0848 - regression_loss: 1.5334 - classification_loss: 0.5514\n",
      "272/500 [===============>..............] - ETA: 3:55 - loss: 2.0859 - regression_loss: 1.5343 - classification_loss: 0.5516\n",
      "273/500 [===============>..............] - ETA: 3:54 - loss: 2.0861 - regression_loss: 1.5345 - classification_loss: 0.5516\n",
      "274/500 [===============>..............] - ETA: 3:53 - loss: 2.0868 - regression_loss: 1.5352 - classification_loss: 0.5517\n",
      "275/500 [===============>..............] - ETA: 3:52 - loss: 2.0849 - regression_loss: 1.5339 - classification_loss: 0.5510\n",
      "276/500 [===============>..............] - ETA: 3:51 - loss: 2.0838 - regression_loss: 1.5333 - classification_loss: 0.5504\n",
      "277/500 [===============>..............] - ETA: 3:49 - loss: 2.0825 - regression_loss: 1.5317 - classification_loss: 0.5508\n",
      "278/500 [===============>..............] - ETA: 3:49 - loss: 2.0818 - regression_loss: 1.5309 - classification_loss: 0.5509\n",
      "279/500 [===============>..............] - ETA: 3:48 - loss: 2.0797 - regression_loss: 1.5295 - classification_loss: 0.5502\n",
      "280/500 [===============>..............] - ETA: 3:46 - loss: 2.0769 - regression_loss: 1.5277 - classification_loss: 0.5492\n",
      "281/500 [===============>..............] - ETA: 3:45 - loss: 2.0761 - regression_loss: 1.5270 - classification_loss: 0.5491\n",
      "282/500 [===============>..............] - ETA: 3:44 - loss: 2.0774 - regression_loss: 1.5283 - classification_loss: 0.5490\n",
      "283/500 [===============>..............] - ETA: 3:43 - loss: 2.0753 - regression_loss: 1.5269 - classification_loss: 0.5483\n",
      "284/500 [================>.............] - ETA: 3:42 - loss: 2.0722 - regression_loss: 1.5246 - classification_loss: 0.5476\n",
      "285/500 [================>.............] - ETA: 3:41 - loss: 2.0715 - regression_loss: 1.5242 - classification_loss: 0.5473\n",
      "286/500 [================>.............] - ETA: 3:40 - loss: 2.0716 - regression_loss: 1.5244 - classification_loss: 0.5471\n",
      "287/500 [================>.............] - ETA: 3:39 - loss: 2.0699 - regression_loss: 1.5230 - classification_loss: 0.5469\n",
      "288/500 [================>.............] - ETA: 3:38 - loss: 2.0693 - regression_loss: 1.5225 - classification_loss: 0.5468\n",
      "289/500 [================>.............] - ETA: 3:37 - loss: 2.0653 - regression_loss: 1.5193 - classification_loss: 0.5460\n",
      "290/500 [================>.............] - ETA: 3:36 - loss: 2.0645 - regression_loss: 1.5187 - classification_loss: 0.5458\n",
      "291/500 [================>.............] - ETA: 3:35 - loss: 2.0629 - regression_loss: 1.5177 - classification_loss: 0.5452\n",
      "292/500 [================>.............] - ETA: 3:34 - loss: 2.0628 - regression_loss: 1.5173 - classification_loss: 0.5455\n",
      "293/500 [================>.............] - ETA: 3:33 - loss: 2.0644 - regression_loss: 1.5189 - classification_loss: 0.5454\n",
      "294/500 [================>.............] - ETA: 3:32 - loss: 2.0619 - regression_loss: 1.5173 - classification_loss: 0.5447\n",
      "295/500 [================>.............] - ETA: 3:31 - loss: 2.0634 - regression_loss: 1.5185 - classification_loss: 0.5449\n",
      "296/500 [================>.............] - ETA: 3:30 - loss: 2.0655 - regression_loss: 1.5205 - classification_loss: 0.5450\n",
      "297/500 [================>.............] - ETA: 3:29 - loss: 2.0650 - regression_loss: 1.5203 - classification_loss: 0.5447\n",
      "298/500 [================>.............] - ETA: 3:27 - loss: 2.0662 - regression_loss: 1.5209 - classification_loss: 0.5453\n",
      "299/500 [================>.............] - ETA: 3:28 - loss: 2.0661 - regression_loss: 1.5209 - classification_loss: 0.5452\n",
      "300/500 [=================>............] - ETA: 3:27 - loss: 2.0671 - regression_loss: 1.5213 - classification_loss: 0.5458\n",
      "301/500 [=================>............] - ETA: 3:26 - loss: 2.0678 - regression_loss: 1.5209 - classification_loss: 0.5469\n",
      "302/500 [=================>............] - ETA: 3:25 - loss: 2.0648 - regression_loss: 1.5189 - classification_loss: 0.5458\n",
      "303/500 [=================>............] - ETA: 3:24 - loss: 2.0646 - regression_loss: 1.5190 - classification_loss: 0.5456\n",
      "304/500 [=================>............] - ETA: 3:23 - loss: 2.0624 - regression_loss: 1.5177 - classification_loss: 0.5448\n",
      "305/500 [=================>............] - ETA: 3:22 - loss: 2.0607 - regression_loss: 1.5167 - classification_loss: 0.5440\n",
      "306/500 [=================>............] - ETA: 3:21 - loss: 2.0610 - regression_loss: 1.5170 - classification_loss: 0.5440\n",
      "307/500 [=================>............] - ETA: 3:19 - loss: 2.0609 - regression_loss: 1.5169 - classification_loss: 0.5440\n",
      "308/500 [=================>............] - ETA: 3:18 - loss: 2.0626 - regression_loss: 1.5185 - classification_loss: 0.5441\n",
      "309/500 [=================>............] - ETA: 3:17 - loss: 2.0650 - regression_loss: 1.5201 - classification_loss: 0.5449\n",
      "310/500 [=================>............] - ETA: 3:16 - loss: 2.0639 - regression_loss: 1.5189 - classification_loss: 0.5450\n",
      "311/500 [=================>............] - ETA: 3:15 - loss: 2.0643 - regression_loss: 1.5188 - classification_loss: 0.5455\n",
      "312/500 [=================>............] - ETA: 3:14 - loss: 2.0638 - regression_loss: 1.5186 - classification_loss: 0.5452\n",
      "313/500 [=================>............] - ETA: 3:13 - loss: 2.0632 - regression_loss: 1.5182 - classification_loss: 0.5449\n",
      "314/500 [=================>............] - ETA: 3:12 - loss: 2.0622 - regression_loss: 1.5176 - classification_loss: 0.5446\n",
      "315/500 [=================>............] - ETA: 3:11 - loss: 2.0599 - regression_loss: 1.5160 - classification_loss: 0.5439\n",
      "316/500 [=================>............] - ETA: 3:10 - loss: 2.0568 - regression_loss: 1.5140 - classification_loss: 0.5428\n",
      "317/500 [==================>...........] - ETA: 3:09 - loss: 2.0591 - regression_loss: 1.5159 - classification_loss: 0.5432\n",
      "318/500 [==================>...........] - ETA: 3:08 - loss: 2.0578 - regression_loss: 1.5149 - classification_loss: 0.5429\n",
      "319/500 [==================>...........] - ETA: 3:07 - loss: 2.0562 - regression_loss: 1.5136 - classification_loss: 0.5426\n",
      "320/500 [==================>...........] - ETA: 3:06 - loss: 2.0549 - regression_loss: 1.5124 - classification_loss: 0.5426\n",
      "321/500 [==================>...........] - ETA: 3:05 - loss: 2.0544 - regression_loss: 1.5118 - classification_loss: 0.5426\n",
      "322/500 [==================>...........] - ETA: 3:04 - loss: 2.0555 - regression_loss: 1.5129 - classification_loss: 0.5426\n",
      "323/500 [==================>...........] - ETA: 3:03 - loss: 2.0542 - regression_loss: 1.5117 - classification_loss: 0.5425\n",
      "324/500 [==================>...........] - ETA: 3:02 - loss: 2.0546 - regression_loss: 1.5122 - classification_loss: 0.5424\n",
      "325/500 [==================>...........] - ETA: 3:01 - loss: 2.0577 - regression_loss: 1.5145 - classification_loss: 0.5431\n",
      "326/500 [==================>...........] - ETA: 3:00 - loss: 2.0581 - regression_loss: 1.5151 - classification_loss: 0.5430\n",
      "327/500 [==================>...........] - ETA: 2:59 - loss: 2.0607 - regression_loss: 1.5172 - classification_loss: 0.5435\n",
      "328/500 [==================>...........] - ETA: 2:58 - loss: 2.0632 - regression_loss: 1.5197 - classification_loss: 0.5435\n",
      "329/500 [==================>...........] - ETA: 2:57 - loss: 2.0643 - regression_loss: 1.5204 - classification_loss: 0.5439\n",
      "330/500 [==================>...........] - ETA: 2:56 - loss: 2.0648 - regression_loss: 1.5212 - classification_loss: 0.5436\n",
      "331/500 [==================>...........] - ETA: 2:55 - loss: 2.0624 - regression_loss: 1.5193 - classification_loss: 0.5431\n",
      "332/500 [==================>...........] - ETA: 2:54 - loss: 2.0617 - regression_loss: 1.5191 - classification_loss: 0.5426\n",
      "333/500 [==================>...........] - ETA: 2:53 - loss: 2.0615 - regression_loss: 1.5186 - classification_loss: 0.5429\n",
      "334/500 [===================>..........] - ETA: 2:52 - loss: 2.0592 - regression_loss: 1.5166 - classification_loss: 0.5426\n",
      "335/500 [===================>..........] - ETA: 2:50 - loss: 2.0586 - regression_loss: 1.5164 - classification_loss: 0.5422\n",
      "336/500 [===================>..........] - ETA: 2:49 - loss: 2.0562 - regression_loss: 1.5145 - classification_loss: 0.5417\n",
      "337/500 [===================>..........] - ETA: 2:48 - loss: 2.0546 - regression_loss: 1.5130 - classification_loss: 0.5416\n",
      "338/500 [===================>..........] - ETA: 2:47 - loss: 2.0518 - regression_loss: 1.5109 - classification_loss: 0.5409\n",
      "339/500 [===================>..........] - ETA: 2:46 - loss: 2.0496 - regression_loss: 1.5092 - classification_loss: 0.5403\n",
      "340/500 [===================>..........] - ETA: 2:45 - loss: 2.0490 - regression_loss: 1.5087 - classification_loss: 0.5403\n",
      "341/500 [===================>..........] - ETA: 2:44 - loss: 2.0502 - regression_loss: 1.5096 - classification_loss: 0.5405\n",
      "342/500 [===================>..........] - ETA: 2:43 - loss: 2.0516 - regression_loss: 1.5107 - classification_loss: 0.5409\n",
      "343/500 [===================>..........] - ETA: 2:42 - loss: 2.0516 - regression_loss: 1.5106 - classification_loss: 0.5411\n",
      "344/500 [===================>..........] - ETA: 2:41 - loss: 2.0516 - regression_loss: 1.5102 - classification_loss: 0.5415\n",
      "345/500 [===================>..........] - ETA: 2:40 - loss: 2.0507 - regression_loss: 1.5095 - classification_loss: 0.5412\n",
      "346/500 [===================>..........] - ETA: 2:39 - loss: 2.0516 - regression_loss: 1.5104 - classification_loss: 0.5412\n",
      "347/500 [===================>..........] - ETA: 2:38 - loss: 2.0539 - regression_loss: 1.5125 - classification_loss: 0.5413\n",
      "348/500 [===================>..........] - ETA: 2:37 - loss: 2.0550 - regression_loss: 1.5140 - classification_loss: 0.5410\n",
      "349/500 [===================>..........] - ETA: 2:36 - loss: 2.0562 - regression_loss: 1.5152 - classification_loss: 0.5410\n",
      "350/500 [====================>.........] - ETA: 2:35 - loss: 2.0588 - regression_loss: 1.5165 - classification_loss: 0.5424\n",
      "351/500 [====================>.........] - ETA: 2:34 - loss: 2.0600 - regression_loss: 1.5172 - classification_loss: 0.5428\n",
      "352/500 [====================>.........] - ETA: 2:33 - loss: 2.0587 - regression_loss: 1.5162 - classification_loss: 0.5426\n",
      "353/500 [====================>.........] - ETA: 2:32 - loss: 2.0568 - regression_loss: 1.5147 - classification_loss: 0.5421\n",
      "354/500 [====================>.........] - ETA: 2:31 - loss: 2.0564 - regression_loss: 1.5141 - classification_loss: 0.5422\n",
      "355/500 [====================>.........] - ETA: 2:30 - loss: 2.0589 - regression_loss: 1.5160 - classification_loss: 0.5429\n",
      "356/500 [====================>.........] - ETA: 2:29 - loss: 2.0594 - regression_loss: 1.5168 - classification_loss: 0.5426\n",
      "357/500 [====================>.........] - ETA: 2:28 - loss: 2.0592 - regression_loss: 1.5168 - classification_loss: 0.5424\n",
      "358/500 [====================>.........] - ETA: 2:27 - loss: 2.0575 - regression_loss: 1.5155 - classification_loss: 0.5420\n",
      "359/500 [====================>.........] - ETA: 2:26 - loss: 2.0570 - regression_loss: 1.5154 - classification_loss: 0.5415\n",
      "360/500 [====================>.........] - ETA: 2:25 - loss: 2.0572 - regression_loss: 1.5157 - classification_loss: 0.5415\n",
      "361/500 [====================>.........] - ETA: 2:23 - loss: 2.0575 - regression_loss: 1.5162 - classification_loss: 0.5413\n",
      "362/500 [====================>.........] - ETA: 2:22 - loss: 2.0572 - regression_loss: 1.5154 - classification_loss: 0.5417\n",
      "363/500 [====================>.........] - ETA: 2:21 - loss: 2.0554 - regression_loss: 1.5139 - classification_loss: 0.5415\n",
      "364/500 [====================>.........] - ETA: 2:20 - loss: 2.0536 - regression_loss: 1.5124 - classification_loss: 0.5412\n",
      "365/500 [====================>.........] - ETA: 2:19 - loss: 2.0528 - regression_loss: 1.5121 - classification_loss: 0.5407\n",
      "366/500 [====================>.........] - ETA: 2:18 - loss: 2.0512 - regression_loss: 1.5108 - classification_loss: 0.5404\n",
      "367/500 [=====================>........] - ETA: 2:17 - loss: 2.0523 - regression_loss: 1.5111 - classification_loss: 0.5412\n",
      "368/500 [=====================>........] - ETA: 2:16 - loss: 2.0507 - regression_loss: 1.5100 - classification_loss: 0.5407\n",
      "369/500 [=====================>........] - ETA: 2:15 - loss: 2.0491 - regression_loss: 1.5088 - classification_loss: 0.5403\n",
      "370/500 [=====================>........] - ETA: 2:14 - loss: 2.0479 - regression_loss: 1.5078 - classification_loss: 0.5401\n",
      "371/500 [=====================>........] - ETA: 2:13 - loss: 2.0461 - regression_loss: 1.5063 - classification_loss: 0.5398\n",
      "372/500 [=====================>........] - ETA: 2:12 - loss: 2.0469 - regression_loss: 1.5071 - classification_loss: 0.5398\n",
      "373/500 [=====================>........] - ETA: 2:11 - loss: 2.0509 - regression_loss: 1.5099 - classification_loss: 0.5410\n",
      "374/500 [=====================>........] - ETA: 2:10 - loss: 2.0526 - regression_loss: 1.5112 - classification_loss: 0.5413\n",
      "375/500 [=====================>........] - ETA: 2:09 - loss: 2.0510 - regression_loss: 1.5100 - classification_loss: 0.5410\n",
      "376/500 [=====================>........] - ETA: 2:08 - loss: 2.0538 - regression_loss: 1.5118 - classification_loss: 0.5421\n",
      "377/500 [=====================>........] - ETA: 2:07 - loss: 2.0534 - regression_loss: 1.5115 - classification_loss: 0.5419\n",
      "378/500 [=====================>........] - ETA: 2:06 - loss: 2.0531 - regression_loss: 1.5116 - classification_loss: 0.5415\n",
      "379/500 [=====================>........] - ETA: 2:05 - loss: 2.0558 - regression_loss: 1.5137 - classification_loss: 0.5421\n",
      "380/500 [=====================>........] - ETA: 2:04 - loss: 2.0533 - regression_loss: 1.5119 - classification_loss: 0.5414\n",
      "381/500 [=====================>........] - ETA: 2:03 - loss: 2.0531 - regression_loss: 1.5118 - classification_loss: 0.5413\n",
      "382/500 [=====================>........] - ETA: 2:02 - loss: 2.0506 - regression_loss: 1.5100 - classification_loss: 0.5406\n",
      "383/500 [=====================>........] - ETA: 2:01 - loss: 2.0512 - regression_loss: 1.5104 - classification_loss: 0.5408\n",
      "384/500 [======================>.......] - ETA: 2:00 - loss: 2.0510 - regression_loss: 1.5101 - classification_loss: 0.5410\n",
      "385/500 [======================>.......] - ETA: 1:59 - loss: 2.0501 - regression_loss: 1.5095 - classification_loss: 0.5407\n",
      "386/500 [======================>.......] - ETA: 1:58 - loss: 2.0506 - regression_loss: 1.5097 - classification_loss: 0.5410\n",
      "387/500 [======================>.......] - ETA: 1:57 - loss: 2.0512 - regression_loss: 1.5101 - classification_loss: 0.5411\n",
      "388/500 [======================>.......] - ETA: 1:56 - loss: 2.0526 - regression_loss: 1.5111 - classification_loss: 0.5415\n",
      "389/500 [======================>.......] - ETA: 1:55 - loss: 2.0530 - regression_loss: 1.5113 - classification_loss: 0.5417\n",
      "390/500 [======================>.......] - ETA: 1:54 - loss: 2.0546 - regression_loss: 1.5118 - classification_loss: 0.5428\n",
      "391/500 [======================>.......] - ETA: 1:52 - loss: 2.0532 - regression_loss: 1.5108 - classification_loss: 0.5424\n",
      "392/500 [======================>.......] - ETA: 1:51 - loss: 2.0517 - regression_loss: 1.5097 - classification_loss: 0.5420\n",
      "393/500 [======================>.......] - ETA: 1:50 - loss: 2.0541 - regression_loss: 1.5113 - classification_loss: 0.5428\n",
      "394/500 [======================>.......] - ETA: 1:49 - loss: 2.0529 - regression_loss: 1.5104 - classification_loss: 0.5425\n",
      "395/500 [======================>.......] - ETA: 1:49 - loss: 2.0533 - regression_loss: 1.5107 - classification_loss: 0.5426\n",
      "396/500 [======================>.......] - ETA: 1:48 - loss: 2.0515 - regression_loss: 1.5097 - classification_loss: 0.5418\n",
      "397/500 [======================>.......] - ETA: 1:47 - loss: 2.0539 - regression_loss: 1.5115 - classification_loss: 0.5424\n",
      "398/500 [======================>.......] - ETA: 1:46 - loss: 2.0519 - regression_loss: 1.5100 - classification_loss: 0.5420\n",
      "399/500 [======================>.......] - ETA: 1:45 - loss: 2.0500 - regression_loss: 1.5086 - classification_loss: 0.5414\n",
      "400/500 [=======================>......] - ETA: 1:44 - loss: 2.0484 - regression_loss: 1.5075 - classification_loss: 0.5408\n",
      "401/500 [=======================>......] - ETA: 1:43 - loss: 2.0480 - regression_loss: 1.5075 - classification_loss: 0.5404\n",
      "402/500 [=======================>......] - ETA: 1:42 - loss: 2.0473 - regression_loss: 1.5070 - classification_loss: 0.5404\n",
      "403/500 [=======================>......] - ETA: 1:41 - loss: 2.0457 - regression_loss: 1.5058 - classification_loss: 0.5399\n",
      "404/500 [=======================>......] - ETA: 1:40 - loss: 2.0462 - regression_loss: 1.5062 - classification_loss: 0.5399\n",
      "405/500 [=======================>......] - ETA: 1:38 - loss: 2.0463 - regression_loss: 1.5063 - classification_loss: 0.5400\n",
      "406/500 [=======================>......] - ETA: 1:37 - loss: 2.0461 - regression_loss: 1.5063 - classification_loss: 0.5398\n",
      "407/500 [=======================>......] - ETA: 1:36 - loss: 2.0455 - regression_loss: 1.5061 - classification_loss: 0.5395\n",
      "408/500 [=======================>......] - ETA: 1:35 - loss: 2.0460 - regression_loss: 1.5060 - classification_loss: 0.5400\n",
      "409/500 [=======================>......] - ETA: 1:34 - loss: 2.0457 - regression_loss: 1.5058 - classification_loss: 0.5398\n",
      "410/500 [=======================>......] - ETA: 1:33 - loss: 2.0452 - regression_loss: 1.5055 - classification_loss: 0.5397\n",
      "411/500 [=======================>......] - ETA: 1:32 - loss: 2.0448 - regression_loss: 1.5052 - classification_loss: 0.5396\n",
      "412/500 [=======================>......] - ETA: 1:31 - loss: 2.0448 - regression_loss: 1.5051 - classification_loss: 0.5397\n",
      "413/500 [=======================>......] - ETA: 1:30 - loss: 2.0453 - regression_loss: 1.5059 - classification_loss: 0.5394\n",
      "414/500 [=======================>......] - ETA: 1:29 - loss: 2.0441 - regression_loss: 1.5050 - classification_loss: 0.5391\n",
      "415/500 [=======================>......] - ETA: 1:28 - loss: 2.0456 - regression_loss: 1.5059 - classification_loss: 0.5397\n",
      "416/500 [=======================>......] - ETA: 1:27 - loss: 2.0457 - regression_loss: 1.5062 - classification_loss: 0.5395\n",
      "417/500 [========================>.....] - ETA: 1:26 - loss: 2.0448 - regression_loss: 1.5054 - classification_loss: 0.5394\n",
      "418/500 [========================>.....] - ETA: 1:25 - loss: 2.0437 - regression_loss: 1.5042 - classification_loss: 0.5396\n",
      "419/500 [========================>.....] - ETA: 1:24 - loss: 2.0435 - regression_loss: 1.5036 - classification_loss: 0.5398\n",
      "420/500 [========================>.....] - ETA: 1:23 - loss: 2.0419 - regression_loss: 1.5024 - classification_loss: 0.5395\n",
      "421/500 [========================>.....] - ETA: 1:22 - loss: 2.0403 - regression_loss: 1.5013 - classification_loss: 0.5389\n",
      "422/500 [========================>.....] - ETA: 1:21 - loss: 2.0425 - regression_loss: 1.5032 - classification_loss: 0.5393\n",
      "423/500 [========================>.....] - ETA: 1:20 - loss: 2.0410 - regression_loss: 1.5019 - classification_loss: 0.5391\n",
      "424/500 [========================>.....] - ETA: 1:19 - loss: 2.0412 - regression_loss: 1.5020 - classification_loss: 0.5392\n",
      "425/500 [========================>.....] - ETA: 1:18 - loss: 2.0404 - regression_loss: 1.5016 - classification_loss: 0.5389\n",
      "426/500 [========================>.....] - ETA: 1:16 - loss: 2.0391 - regression_loss: 1.5006 - classification_loss: 0.5385\n",
      "427/500 [========================>.....] - ETA: 1:15 - loss: 2.0390 - regression_loss: 1.5009 - classification_loss: 0.5381\n",
      "428/500 [========================>.....] - ETA: 1:14 - loss: 2.0387 - regression_loss: 1.5005 - classification_loss: 0.5382\n",
      "429/500 [========================>.....] - ETA: 1:13 - loss: 2.0389 - regression_loss: 1.5006 - classification_loss: 0.5382\n",
      "430/500 [========================>.....] - ETA: 1:12 - loss: 2.0380 - regression_loss: 1.5001 - classification_loss: 0.5379\n",
      "431/500 [========================>.....] - ETA: 1:11 - loss: 2.0367 - regression_loss: 1.4991 - classification_loss: 0.5376\n",
      "432/500 [========================>.....] - ETA: 1:10 - loss: 2.0347 - regression_loss: 1.4977 - classification_loss: 0.5370\n",
      "433/500 [========================>.....] - ETA: 1:09 - loss: 2.0325 - regression_loss: 1.4959 - classification_loss: 0.5365\n",
      "434/500 [=========================>....] - ETA: 1:08 - loss: 2.0323 - regression_loss: 1.4959 - classification_loss: 0.5364\n",
      "435/500 [=========================>....] - ETA: 1:07 - loss: 2.0311 - regression_loss: 1.4953 - classification_loss: 0.5358\n",
      "436/500 [=========================>....] - ETA: 1:06 - loss: 2.0302 - regression_loss: 1.4945 - classification_loss: 0.5357\n",
      "437/500 [=========================>....] - ETA: 1:05 - loss: 2.0285 - regression_loss: 1.4935 - classification_loss: 0.5350\n",
      "438/500 [=========================>....] - ETA: 1:04 - loss: 2.0287 - regression_loss: 1.4939 - classification_loss: 0.5349\n",
      "439/500 [=========================>....] - ETA: 1:03 - loss: 2.0256 - regression_loss: 1.4915 - classification_loss: 0.5341\n",
      "440/500 [=========================>....] - ETA: 1:02 - loss: 2.0248 - regression_loss: 1.4912 - classification_loss: 0.5336\n",
      "441/500 [=========================>....] - ETA: 1:01 - loss: 2.0260 - regression_loss: 1.4921 - classification_loss: 0.5339\n",
      "442/500 [=========================>....] - ETA: 1:00 - loss: 2.0249 - regression_loss: 1.4914 - classification_loss: 0.5334\n",
      "443/500 [=========================>....] - ETA: 59s - loss: 2.0246 - regression_loss: 1.4912 - classification_loss: 0.5334 \n",
      "444/500 [=========================>....] - ETA: 58s - loss: 2.0257 - regression_loss: 1.4922 - classification_loss: 0.5335\n",
      "445/500 [=========================>....] - ETA: 57s - loss: 2.0274 - regression_loss: 1.4936 - classification_loss: 0.5338\n",
      "446/500 [=========================>....] - ETA: 56s - loss: 2.0254 - regression_loss: 1.4922 - classification_loss: 0.5332\n",
      "447/500 [=========================>....] - ETA: 55s - loss: 2.0252 - regression_loss: 1.4924 - classification_loss: 0.5328\n",
      "448/500 [=========================>....] - ETA: 54s - loss: 2.0243 - regression_loss: 1.4918 - classification_loss: 0.5326\n",
      "449/500 [=========================>....] - ETA: 53s - loss: 2.0246 - regression_loss: 1.4918 - classification_loss: 0.5328\n",
      "450/500 [==========================>...] - ETA: 51s - loss: 2.0231 - regression_loss: 1.4905 - classification_loss: 0.5326\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 2.0234 - regression_loss: 1.4909 - classification_loss: 0.5325\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 2.0248 - regression_loss: 1.4919 - classification_loss: 0.5329\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 2.0237 - regression_loss: 1.4910 - classification_loss: 0.5327\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 2.0230 - regression_loss: 1.4907 - classification_loss: 0.5324\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 2.0220 - regression_loss: 1.4898 - classification_loss: 0.5322\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 2.0227 - regression_loss: 1.4906 - classification_loss: 0.5321\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 2.0228 - regression_loss: 1.4905 - classification_loss: 0.5324\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 2.0225 - regression_loss: 1.4900 - classification_loss: 0.5325\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 2.0211 - regression_loss: 1.4891 - classification_loss: 0.5320\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 2.0218 - regression_loss: 1.4895 - classification_loss: 0.5323\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 2.0212 - regression_loss: 1.4894 - classification_loss: 0.5318\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 2.0202 - regression_loss: 1.4888 - classification_loss: 0.5314\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 2.0200 - regression_loss: 1.4889 - classification_loss: 0.5311\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 2.0177 - regression_loss: 1.4872 - classification_loss: 0.5306\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 2.0181 - regression_loss: 1.4875 - classification_loss: 0.5306\n",
      "466/500 [==========================>...] - ETA: 35s - loss: 2.0173 - regression_loss: 1.4869 - classification_loss: 0.5304\n",
      "467/500 [===========================>..] - ETA: 34s - loss: 2.0166 - regression_loss: 1.4864 - classification_loss: 0.5302\n",
      "468/500 [===========================>..] - ETA: 33s - loss: 2.0170 - regression_loss: 1.4866 - classification_loss: 0.5304\n",
      "469/500 [===========================>..] - ETA: 32s - loss: 2.0162 - regression_loss: 1.4861 - classification_loss: 0.5301\n",
      "470/500 [===========================>..] - ETA: 31s - loss: 2.0173 - regression_loss: 1.4868 - classification_loss: 0.5305\n",
      "471/500 [===========================>..] - ETA: 30s - loss: 2.0185 - regression_loss: 1.4875 - classification_loss: 0.5310\n",
      "472/500 [===========================>..] - ETA: 29s - loss: 2.0174 - regression_loss: 1.4868 - classification_loss: 0.5306\n",
      "473/500 [===========================>..] - ETA: 28s - loss: 2.0162 - regression_loss: 1.4861 - classification_loss: 0.5301\n",
      "474/500 [===========================>..] - ETA: 27s - loss: 2.0151 - regression_loss: 1.4853 - classification_loss: 0.5298\n",
      "475/500 [===========================>..] - ETA: 25s - loss: 2.0147 - regression_loss: 1.4853 - classification_loss: 0.5294\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 2.0146 - regression_loss: 1.4853 - classification_loss: 0.5294\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 2.0154 - regression_loss: 1.4857 - classification_loss: 0.5297\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 2.0158 - regression_loss: 1.4863 - classification_loss: 0.5295\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 2.0158 - regression_loss: 1.4861 - classification_loss: 0.5297\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 2.0160 - regression_loss: 1.4860 - classification_loss: 0.5300\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 2.0147 - regression_loss: 1.4851 - classification_loss: 0.5297\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 2.0150 - regression_loss: 1.4854 - classification_loss: 0.5296\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 2.0154 - regression_loss: 1.4858 - classification_loss: 0.5296\n",
      "484/500 [============================>.] - ETA: 16s - loss: 2.0140 - regression_loss: 1.4848 - classification_loss: 0.5292\n",
      "485/500 [============================>.] - ETA: 15s - loss: 2.0157 - regression_loss: 1.4859 - classification_loss: 0.5298\n",
      "486/500 [============================>.] - ETA: 14s - loss: 2.0155 - regression_loss: 1.4856 - classification_loss: 0.5299\n",
      "487/500 [============================>.] - ETA: 13s - loss: 2.0159 - regression_loss: 1.4861 - classification_loss: 0.5298\n",
      "488/500 [============================>.] - ETA: 12s - loss: 2.0162 - regression_loss: 1.4864 - classification_loss: 0.5297\n",
      "489/500 [============================>.] - ETA: 11s - loss: 2.0154 - regression_loss: 1.4860 - classification_loss: 0.5294\n",
      "490/500 [============================>.] - ETA: 10s - loss: 2.0172 - regression_loss: 1.4872 - classification_loss: 0.5301\n",
      "491/500 [============================>.] - ETA: 9s - loss: 2.0158 - regression_loss: 1.4861 - classification_loss: 0.5297 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 2.0169 - regression_loss: 1.4869 - classification_loss: 0.5300\n",
      "493/500 [============================>.] - ETA: 7s - loss: 2.0171 - regression_loss: 1.4873 - classification_loss: 0.5298\n",
      "494/500 [============================>.] - ETA: 6s - loss: 2.0165 - regression_loss: 1.4868 - classification_loss: 0.5297\n",
      "495/500 [============================>.] - ETA: 5s - loss: 2.0166 - regression_loss: 1.4870 - classification_loss: 0.5296\n",
      "496/500 [============================>.] - ETA: 4s - loss: 2.0157 - regression_loss: 1.4865 - classification_loss: 0.5292\n",
      "497/500 [============================>.] - ETA: 3s - loss: 2.0149 - regression_loss: 1.4860 - classification_loss: 0.5289\n",
      "498/500 [============================>.] - ETA: 2s - loss: 2.0164 - regression_loss: 1.4871 - classification_loss: 0.5293\n",
      "499/500 [============================>.] - ETA: 1s - loss: 2.0153 - regression_loss: 1.4862 - classification_loss: 0.5291\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.0155 - regression_loss: 1.4862 - classification_loss: 0.5293\n",
      "Epoch 00007: saving model to ./snapshots\\resnet50_csv_07.h5\n",
      "\n",
      "500/500 [==============================] - 519s 1s/step - loss: 2.0155 - regression_loss: 1.4862 - classification_loss: 0.5293\n",
      "Epoch 8/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 2.1314 - regression_loss: 1.5183 - classification_loss: 0.6131\n",
      "  2/500 [..............................] - ETA: 4:04 - loss: 1.7053 - regression_loss: 1.2423 - classification_loss: 0.4630\n",
      "  3/500 [..............................] - ETA: 5:26 - loss: 1.5369 - regression_loss: 1.1277 - classification_loss: 0.4092\n",
      "  4/500 [..............................] - ETA: 6:07 - loss: 1.6774 - regression_loss: 1.2304 - classification_loss: 0.4471\n",
      "  5/500 [..............................] - ETA: 6:14 - loss: 1.6401 - regression_loss: 1.2138 - classification_loss: 0.4263\n",
      "  6/500 [..............................] - ETA: 6:51 - loss: 1.7290 - regression_loss: 1.2945 - classification_loss: 0.4345\n",
      "  7/500 [..............................] - ETA: 7:01 - loss: 1.6750 - regression_loss: 1.2652 - classification_loss: 0.4098\n",
      "  8/500 [..............................] - ETA: 7:07 - loss: 1.7505 - regression_loss: 1.3228 - classification_loss: 0.4278\n",
      "  9/500 [..............................] - ETA: 7:18 - loss: 1.7688 - regression_loss: 1.3240 - classification_loss: 0.4447\n",
      " 10/500 [..............................] - ETA: 7:10 - loss: 1.7485 - regression_loss: 1.2864 - classification_loss: 0.4621\n",
      " 11/500 [..............................] - ETA: 7:24 - loss: 1.7467 - regression_loss: 1.2754 - classification_loss: 0.4712\n",
      " 12/500 [..............................] - ETA: 7:30 - loss: 1.7934 - regression_loss: 1.3209 - classification_loss: 0.4725\n",
      " 13/500 [..............................] - ETA: 7:31 - loss: 1.8672 - regression_loss: 1.3613 - classification_loss: 0.5059\n",
      " 14/500 [..............................] - ETA: 7:32 - loss: 1.8721 - regression_loss: 1.3602 - classification_loss: 0.5119\n",
      " 15/500 [..............................] - ETA: 7:36 - loss: 1.8684 - regression_loss: 1.3476 - classification_loss: 0.5208\n",
      " 16/500 [..............................] - ETA: 7:39 - loss: 1.8598 - regression_loss: 1.3499 - classification_loss: 0.5098\n",
      " 17/500 [>.............................] - ETA: 7:43 - loss: 1.8777 - regression_loss: 1.3589 - classification_loss: 0.5188\n",
      " 18/500 [>.............................] - ETA: 7:45 - loss: 1.8750 - regression_loss: 1.3643 - classification_loss: 0.5107\n",
      " 19/500 [>.............................] - ETA: 7:50 - loss: 1.8706 - regression_loss: 1.3692 - classification_loss: 0.5014\n",
      " 20/500 [>.............................] - ETA: 7:52 - loss: 1.8811 - regression_loss: 1.3739 - classification_loss: 0.5073\n",
      " 21/500 [>.............................] - ETA: 7:54 - loss: 1.8804 - regression_loss: 1.3762 - classification_loss: 0.5042\n",
      " 22/500 [>.............................] - ETA: 7:53 - loss: 1.8707 - regression_loss: 1.3720 - classification_loss: 0.4987\n",
      " 23/500 [>.............................] - ETA: 7:48 - loss: 1.8336 - regression_loss: 1.3427 - classification_loss: 0.4909\n",
      " 24/500 [>.............................] - ETA: 7:52 - loss: 1.8613 - regression_loss: 1.3564 - classification_loss: 0.5049\n",
      " 25/500 [>.............................] - ETA: 7:51 - loss: 1.8792 - regression_loss: 1.3776 - classification_loss: 0.5016\n",
      " 26/500 [>.............................] - ETA: 7:52 - loss: 1.8691 - regression_loss: 1.3746 - classification_loss: 0.4945\n",
      " 27/500 [>.............................] - ETA: 7:59 - loss: 1.8957 - regression_loss: 1.3917 - classification_loss: 0.5040\n",
      " 28/500 [>.............................] - ETA: 7:57 - loss: 1.8873 - regression_loss: 1.3846 - classification_loss: 0.5028\n",
      " 29/500 [>.............................] - ETA: 7:57 - loss: 1.8995 - regression_loss: 1.3910 - classification_loss: 0.5084\n",
      " 30/500 [>.............................] - ETA: 7:57 - loss: 1.9127 - regression_loss: 1.4049 - classification_loss: 0.5078\n",
      " 31/500 [>.............................] - ETA: 7:52 - loss: 1.9052 - regression_loss: 1.3976 - classification_loss: 0.5076\n",
      " 32/500 [>.............................] - ETA: 7:53 - loss: 1.9091 - regression_loss: 1.3993 - classification_loss: 0.5098\n",
      " 33/500 [>.............................] - ETA: 7:51 - loss: 1.8845 - regression_loss: 1.3827 - classification_loss: 0.5018\n",
      " 34/500 [=>............................] - ETA: 7:51 - loss: 1.8669 - regression_loss: 1.3694 - classification_loss: 0.4975\n",
      " 35/500 [=>............................] - ETA: 7:51 - loss: 1.8837 - regression_loss: 1.3818 - classification_loss: 0.5019\n",
      " 36/500 [=>............................] - ETA: 7:50 - loss: 1.8793 - regression_loss: 1.3763 - classification_loss: 0.5030\n",
      " 37/500 [=>............................] - ETA: 7:49 - loss: 1.9104 - regression_loss: 1.3996 - classification_loss: 0.5108\n",
      " 38/500 [=>............................] - ETA: 7:45 - loss: 1.9114 - regression_loss: 1.4009 - classification_loss: 0.5105\n",
      " 39/500 [=>............................] - ETA: 7:47 - loss: 1.9194 - regression_loss: 1.4062 - classification_loss: 0.5131\n",
      " 40/500 [=>............................] - ETA: 7:46 - loss: 1.9506 - regression_loss: 1.4291 - classification_loss: 0.5216\n",
      " 41/500 [=>............................] - ETA: 7:46 - loss: 1.9548 - regression_loss: 1.4321 - classification_loss: 0.5227\n",
      " 42/500 [=>............................] - ETA: 7:45 - loss: 1.9471 - regression_loss: 1.4263 - classification_loss: 0.5208\n",
      " 43/500 [=>............................] - ETA: 7:44 - loss: 1.9367 - regression_loss: 1.4180 - classification_loss: 0.5187\n",
      " 44/500 [=>............................] - ETA: 7:43 - loss: 1.9463 - regression_loss: 1.4220 - classification_loss: 0.5242\n",
      " 45/500 [=>............................] - ETA: 7:43 - loss: 1.9521 - regression_loss: 1.4236 - classification_loss: 0.5286\n",
      " 46/500 [=>............................] - ETA: 7:43 - loss: 1.9584 - regression_loss: 1.4312 - classification_loss: 0.5272\n",
      " 47/500 [=>............................] - ETA: 7:57 - loss: 1.9508 - regression_loss: 1.4246 - classification_loss: 0.5262\n",
      " 48/500 [=>............................] - ETA: 7:56 - loss: 1.9430 - regression_loss: 1.4170 - classification_loss: 0.5260\n",
      " 49/500 [=>............................] - ETA: 7:54 - loss: 1.9385 - regression_loss: 1.4081 - classification_loss: 0.5304\n",
      " 50/500 [==>...........................] - ETA: 7:53 - loss: 1.9466 - regression_loss: 1.4137 - classification_loss: 0.5329\n",
      " 51/500 [==>...........................] - ETA: 7:52 - loss: 1.9290 - regression_loss: 1.3999 - classification_loss: 0.5291\n",
      " 52/500 [==>...........................] - ETA: 7:51 - loss: 1.9277 - regression_loss: 1.4017 - classification_loss: 0.5260\n",
      " 53/500 [==>...........................] - ETA: 7:50 - loss: 1.9369 - regression_loss: 1.4089 - classification_loss: 0.5280\n",
      " 54/500 [==>...........................] - ETA: 7:48 - loss: 1.9312 - regression_loss: 1.4055 - classification_loss: 0.5257\n",
      " 55/500 [==>...........................] - ETA: 7:47 - loss: 1.9421 - regression_loss: 1.4131 - classification_loss: 0.5290\n",
      " 56/500 [==>...........................] - ETA: 7:47 - loss: 1.9353 - regression_loss: 1.4101 - classification_loss: 0.5252\n",
      " 57/500 [==>...........................] - ETA: 7:45 - loss: 1.9263 - regression_loss: 1.3979 - classification_loss: 0.5284\n",
      " 58/500 [==>...........................] - ETA: 7:44 - loss: 1.9475 - regression_loss: 1.4118 - classification_loss: 0.5357\n",
      " 59/500 [==>...........................] - ETA: 7:43 - loss: 1.9420 - regression_loss: 1.4087 - classification_loss: 0.5333\n",
      " 60/500 [==>...........................] - ETA: 7:41 - loss: 1.9479 - regression_loss: 1.4150 - classification_loss: 0.5329\n",
      " 61/500 [==>...........................] - ETA: 7:40 - loss: 1.9611 - regression_loss: 1.4259 - classification_loss: 0.5352\n",
      " 62/500 [==>...........................] - ETA: 7:38 - loss: 1.9471 - regression_loss: 1.4169 - classification_loss: 0.5301\n",
      " 63/500 [==>...........................] - ETA: 7:37 - loss: 1.9482 - regression_loss: 1.4196 - classification_loss: 0.5286\n",
      " 64/500 [==>...........................] - ETA: 7:36 - loss: 1.9520 - regression_loss: 1.4234 - classification_loss: 0.5286\n",
      " 65/500 [==>...........................] - ETA: 7:38 - loss: 1.9616 - regression_loss: 1.4308 - classification_loss: 0.5308\n",
      " 66/500 [==>...........................] - ETA: 7:36 - loss: 1.9482 - regression_loss: 1.4200 - classification_loss: 0.5282\n",
      " 67/500 [===>..........................] - ETA: 7:35 - loss: 1.9497 - regression_loss: 1.4235 - classification_loss: 0.5261\n",
      " 68/500 [===>..........................] - ETA: 7:34 - loss: 1.9466 - regression_loss: 1.4226 - classification_loss: 0.5240\n",
      " 69/500 [===>..........................] - ETA: 7:48 - loss: 1.9523 - regression_loss: 1.4295 - classification_loss: 0.5228\n",
      " 70/500 [===>..........................] - ETA: 7:46 - loss: 1.9632 - regression_loss: 1.4373 - classification_loss: 0.5258\n",
      " 71/500 [===>..........................] - ETA: 7:45 - loss: 1.9663 - regression_loss: 1.4398 - classification_loss: 0.5265\n",
      " 72/500 [===>..........................] - ETA: 7:44 - loss: 1.9715 - regression_loss: 1.4463 - classification_loss: 0.5252\n",
      " 73/500 [===>..........................] - ETA: 7:43 - loss: 1.9844 - regression_loss: 1.4540 - classification_loss: 0.5304\n",
      " 74/500 [===>..........................] - ETA: 7:41 - loss: 1.9932 - regression_loss: 1.4628 - classification_loss: 0.5304\n",
      " 75/500 [===>..........................] - ETA: 7:40 - loss: 1.9956 - regression_loss: 1.4622 - classification_loss: 0.5334\n",
      " 76/500 [===>..........................] - ETA: 7:40 - loss: 1.9981 - regression_loss: 1.4658 - classification_loss: 0.5323\n",
      " 77/500 [===>..........................] - ETA: 7:47 - loss: 1.9997 - regression_loss: 1.4655 - classification_loss: 0.5343\n",
      " 78/500 [===>..........................] - ETA: 7:45 - loss: 1.9970 - regression_loss: 1.4656 - classification_loss: 0.5314\n",
      " 79/500 [===>..........................] - ETA: 7:43 - loss: 1.9911 - regression_loss: 1.4619 - classification_loss: 0.5291\n",
      " 80/500 [===>..........................] - ETA: 7:42 - loss: 1.9831 - regression_loss: 1.4550 - classification_loss: 0.5281\n",
      " 81/500 [===>..........................] - ETA: 7:41 - loss: 1.9835 - regression_loss: 1.4572 - classification_loss: 0.5263\n",
      " 82/500 [===>..........................] - ETA: 7:39 - loss: 1.9895 - regression_loss: 1.4621 - classification_loss: 0.5274\n",
      " 83/500 [===>..........................] - ETA: 7:39 - loss: 1.9899 - regression_loss: 1.4630 - classification_loss: 0.5269\n",
      " 84/500 [====>.........................] - ETA: 7:38 - loss: 2.0020 - regression_loss: 1.4706 - classification_loss: 0.5314\n",
      " 85/500 [====>.........................] - ETA: 7:36 - loss: 1.9994 - regression_loss: 1.4684 - classification_loss: 0.5310\n",
      " 86/500 [====>.........................] - ETA: 7:35 - loss: 2.0057 - regression_loss: 1.4725 - classification_loss: 0.5332\n",
      " 87/500 [====>.........................] - ETA: 7:33 - loss: 2.0092 - regression_loss: 1.4735 - classification_loss: 0.5357\n",
      " 88/500 [====>.........................] - ETA: 7:31 - loss: 2.0039 - regression_loss: 1.4698 - classification_loss: 0.5341\n",
      " 89/500 [====>.........................] - ETA: 7:31 - loss: 2.0122 - regression_loss: 1.4751 - classification_loss: 0.5371\n",
      " 90/500 [====>.........................] - ETA: 7:33 - loss: 2.0240 - regression_loss: 1.4829 - classification_loss: 0.5412\n",
      " 91/500 [====>.........................] - ETA: 7:32 - loss: 2.0168 - regression_loss: 1.4763 - classification_loss: 0.5404\n",
      " 92/500 [====>.........................] - ETA: 7:31 - loss: 2.0140 - regression_loss: 1.4758 - classification_loss: 0.5383\n",
      " 93/500 [====>.........................] - ETA: 7:30 - loss: 2.0150 - regression_loss: 1.4767 - classification_loss: 0.5383\n",
      " 94/500 [====>.........................] - ETA: 7:28 - loss: 2.0063 - regression_loss: 1.4710 - classification_loss: 0.5354\n",
      " 95/500 [====>.........................] - ETA: 7:27 - loss: 2.0224 - regression_loss: 1.4829 - classification_loss: 0.5395\n",
      " 96/500 [====>.........................] - ETA: 7:26 - loss: 2.0211 - regression_loss: 1.4833 - classification_loss: 0.5378\n",
      " 97/500 [====>.........................] - ETA: 7:24 - loss: 2.0241 - regression_loss: 1.4863 - classification_loss: 0.5378\n",
      " 98/500 [====>.........................] - ETA: 7:22 - loss: 2.0261 - regression_loss: 1.4879 - classification_loss: 0.5381\n",
      " 99/500 [====>.........................] - ETA: 7:21 - loss: 2.0210 - regression_loss: 1.4848 - classification_loss: 0.5362\n",
      "100/500 [=====>........................] - ETA: 7:20 - loss: 2.0279 - regression_loss: 1.4894 - classification_loss: 0.5386\n",
      "101/500 [=====>........................] - ETA: 7:19 - loss: 2.0251 - regression_loss: 1.4875 - classification_loss: 0.5377\n",
      "102/500 [=====>........................] - ETA: 7:18 - loss: 2.0308 - regression_loss: 1.4916 - classification_loss: 0.5392\n",
      "103/500 [=====>........................] - ETA: 7:16 - loss: 2.0313 - regression_loss: 1.4914 - classification_loss: 0.5399\n",
      "104/500 [=====>........................] - ETA: 7:15 - loss: 2.0261 - regression_loss: 1.4883 - classification_loss: 0.5379\n",
      "105/500 [=====>........................] - ETA: 7:13 - loss: 2.0207 - regression_loss: 1.4850 - classification_loss: 0.5357\n",
      "106/500 [=====>........................] - ETA: 7:12 - loss: 2.0157 - regression_loss: 1.4827 - classification_loss: 0.5330\n",
      "107/500 [=====>........................] - ETA: 7:10 - loss: 2.0143 - regression_loss: 1.4820 - classification_loss: 0.5322\n",
      "108/500 [=====>........................] - ETA: 7:09 - loss: 2.0180 - regression_loss: 1.4845 - classification_loss: 0.5336\n",
      "109/500 [=====>........................] - ETA: 7:08 - loss: 2.0208 - regression_loss: 1.4873 - classification_loss: 0.5335\n",
      "110/500 [=====>........................] - ETA: 7:07 - loss: 2.0183 - regression_loss: 1.4859 - classification_loss: 0.5324\n",
      "111/500 [=====>........................] - ETA: 7:06 - loss: 2.0182 - regression_loss: 1.4853 - classification_loss: 0.5329\n",
      "112/500 [=====>........................] - ETA: 7:05 - loss: 2.0226 - regression_loss: 1.4895 - classification_loss: 0.5331\n",
      "113/500 [=====>........................] - ETA: 7:03 - loss: 2.0226 - regression_loss: 1.4880 - classification_loss: 0.5346\n",
      "114/500 [=====>........................] - ETA: 7:01 - loss: 2.0218 - regression_loss: 1.4872 - classification_loss: 0.5345\n",
      "115/500 [=====>........................] - ETA: 7:00 - loss: 2.0175 - regression_loss: 1.4847 - classification_loss: 0.5328\n",
      "116/500 [=====>........................] - ETA: 6:59 - loss: 2.0181 - regression_loss: 1.4854 - classification_loss: 0.5327\n",
      "117/500 [======>.......................] - ETA: 6:58 - loss: 2.0302 - regression_loss: 1.4930 - classification_loss: 0.5371\n",
      "118/500 [======>.......................] - ETA: 6:57 - loss: 2.0249 - regression_loss: 1.4898 - classification_loss: 0.5351\n",
      "119/500 [======>.......................] - ETA: 6:56 - loss: 2.0178 - regression_loss: 1.4848 - classification_loss: 0.5330\n",
      "120/500 [======>.......................] - ETA: 6:54 - loss: 2.0189 - regression_loss: 1.4861 - classification_loss: 0.5328\n",
      "121/500 [======>.......................] - ETA: 6:53 - loss: 2.0122 - regression_loss: 1.4807 - classification_loss: 0.5315\n",
      "122/500 [======>.......................] - ETA: 6:52 - loss: 2.0120 - regression_loss: 1.4806 - classification_loss: 0.5314\n",
      "123/500 [======>.......................] - ETA: 6:50 - loss: 2.0163 - regression_loss: 1.4838 - classification_loss: 0.5325\n",
      "124/500 [======>.......................] - ETA: 6:49 - loss: 2.0110 - regression_loss: 1.4800 - classification_loss: 0.5311\n",
      "125/500 [======>.......................] - ETA: 6:47 - loss: 2.0105 - regression_loss: 1.4798 - classification_loss: 0.5307\n",
      "126/500 [======>.......................] - ETA: 6:46 - loss: 2.0043 - regression_loss: 1.4758 - classification_loss: 0.5284\n",
      "127/500 [======>.......................] - ETA: 6:45 - loss: 2.0026 - regression_loss: 1.4740 - classification_loss: 0.5286\n",
      "128/500 [======>.......................] - ETA: 6:44 - loss: 2.0014 - regression_loss: 1.4737 - classification_loss: 0.5277\n",
      "129/500 [======>.......................] - ETA: 6:43 - loss: 2.0111 - regression_loss: 1.4800 - classification_loss: 0.5311\n",
      "130/500 [======>.......................] - ETA: 6:42 - loss: 2.0115 - regression_loss: 1.4787 - classification_loss: 0.5328\n",
      "131/500 [======>.......................] - ETA: 6:40 - loss: 2.0112 - regression_loss: 1.4785 - classification_loss: 0.5327\n",
      "132/500 [======>.......................] - ETA: 6:39 - loss: 2.0094 - regression_loss: 1.4782 - classification_loss: 0.5312\n",
      "133/500 [======>.......................] - ETA: 6:37 - loss: 2.0054 - regression_loss: 1.4753 - classification_loss: 0.5301\n",
      "134/500 [=======>......................] - ETA: 6:36 - loss: 2.0046 - regression_loss: 1.4734 - classification_loss: 0.5312\n",
      "135/500 [=======>......................] - ETA: 6:34 - loss: 2.0065 - regression_loss: 1.4751 - classification_loss: 0.5314\n",
      "136/500 [=======>......................] - ETA: 6:34 - loss: 2.0070 - regression_loss: 1.4751 - classification_loss: 0.5319\n",
      "137/500 [=======>......................] - ETA: 6:32 - loss: 2.0068 - regression_loss: 1.4741 - classification_loss: 0.5327\n",
      "138/500 [=======>......................] - ETA: 6:31 - loss: 2.0028 - regression_loss: 1.4711 - classification_loss: 0.5317\n",
      "139/500 [=======>......................] - ETA: 6:30 - loss: 2.0065 - regression_loss: 1.4729 - classification_loss: 0.5337\n",
      "140/500 [=======>......................] - ETA: 6:29 - loss: 2.0107 - regression_loss: 1.4760 - classification_loss: 0.5348\n",
      "141/500 [=======>......................] - ETA: 6:27 - loss: 2.0087 - regression_loss: 1.4744 - classification_loss: 0.5343\n",
      "142/500 [=======>......................] - ETA: 6:26 - loss: 2.0129 - regression_loss: 1.4778 - classification_loss: 0.5351\n",
      "143/500 [=======>......................] - ETA: 6:25 - loss: 2.0145 - regression_loss: 1.4783 - classification_loss: 0.5362\n",
      "144/500 [=======>......................] - ETA: 6:24 - loss: 2.0092 - regression_loss: 1.4744 - classification_loss: 0.5348\n",
      "145/500 [=======>......................] - ETA: 6:22 - loss: 2.0123 - regression_loss: 1.4761 - classification_loss: 0.5362\n",
      "146/500 [=======>......................] - ETA: 6:21 - loss: 2.0145 - regression_loss: 1.4776 - classification_loss: 0.5369\n",
      "147/500 [=======>......................] - ETA: 6:20 - loss: 2.0212 - regression_loss: 1.4821 - classification_loss: 0.5390\n",
      "148/500 [=======>......................] - ETA: 6:19 - loss: 2.0199 - regression_loss: 1.4813 - classification_loss: 0.5386\n",
      "149/500 [=======>......................] - ETA: 6:18 - loss: 2.0200 - regression_loss: 1.4813 - classification_loss: 0.5387\n",
      "150/500 [========>.....................] - ETA: 6:17 - loss: 2.0183 - regression_loss: 1.4799 - classification_loss: 0.5383\n",
      "151/500 [========>.....................] - ETA: 6:15 - loss: 2.0178 - regression_loss: 1.4805 - classification_loss: 0.5372\n",
      "152/500 [========>.....................] - ETA: 6:14 - loss: 2.0163 - regression_loss: 1.4797 - classification_loss: 0.5367\n",
      "153/500 [========>.....................] - ETA: 6:13 - loss: 2.0141 - regression_loss: 1.4781 - classification_loss: 0.5360\n",
      "154/500 [========>.....................] - ETA: 6:12 - loss: 2.0142 - regression_loss: 1.4779 - classification_loss: 0.5362\n",
      "155/500 [========>.....................] - ETA: 6:11 - loss: 2.0126 - regression_loss: 1.4766 - classification_loss: 0.5360\n",
      "156/500 [========>.....................] - ETA: 6:10 - loss: 2.0158 - regression_loss: 1.4790 - classification_loss: 0.5368\n",
      "157/500 [========>.....................] - ETA: 6:08 - loss: 2.0129 - regression_loss: 1.4770 - classification_loss: 0.5359\n",
      "158/500 [========>.....................] - ETA: 6:07 - loss: 2.0108 - regression_loss: 1.4752 - classification_loss: 0.5356\n",
      "159/500 [========>.....................] - ETA: 6:06 - loss: 2.0100 - regression_loss: 1.4745 - classification_loss: 0.5355\n",
      "160/500 [========>.....................] - ETA: 6:05 - loss: 2.0053 - regression_loss: 1.4711 - classification_loss: 0.5343\n",
      "161/500 [========>.....................] - ETA: 6:04 - loss: 1.9995 - regression_loss: 1.4666 - classification_loss: 0.5330\n",
      "162/500 [========>.....................] - ETA: 6:03 - loss: 2.0009 - regression_loss: 1.4675 - classification_loss: 0.5334\n",
      "163/500 [========>.....................] - ETA: 6:02 - loss: 1.9970 - regression_loss: 1.4642 - classification_loss: 0.5327\n",
      "164/500 [========>.....................] - ETA: 6:01 - loss: 2.0006 - regression_loss: 1.4675 - classification_loss: 0.5331\n",
      "165/500 [========>.....................] - ETA: 5:59 - loss: 1.9994 - regression_loss: 1.4677 - classification_loss: 0.5317\n",
      "166/500 [========>.....................] - ETA: 5:58 - loss: 2.0050 - regression_loss: 1.4719 - classification_loss: 0.5332\n",
      "167/500 [=========>....................] - ETA: 5:57 - loss: 2.0118 - regression_loss: 1.4762 - classification_loss: 0.5356\n",
      "168/500 [=========>....................] - ETA: 5:56 - loss: 2.0097 - regression_loss: 1.4746 - classification_loss: 0.5351\n",
      "169/500 [=========>....................] - ETA: 5:55 - loss: 2.0062 - regression_loss: 1.4718 - classification_loss: 0.5344\n",
      "170/500 [=========>....................] - ETA: 5:54 - loss: 2.0068 - regression_loss: 1.4729 - classification_loss: 0.5340\n",
      "171/500 [=========>....................] - ETA: 5:52 - loss: 2.0020 - regression_loss: 1.4689 - classification_loss: 0.5331\n",
      "172/500 [=========>....................] - ETA: 5:51 - loss: 2.0061 - regression_loss: 1.4726 - classification_loss: 0.5335\n",
      "173/500 [=========>....................] - ETA: 5:50 - loss: 2.0032 - regression_loss: 1.4697 - classification_loss: 0.5335\n",
      "174/500 [=========>....................] - ETA: 5:49 - loss: 2.0066 - regression_loss: 1.4727 - classification_loss: 0.5339\n",
      "175/500 [=========>....................] - ETA: 5:48 - loss: 2.0055 - regression_loss: 1.4722 - classification_loss: 0.5332\n",
      "176/500 [=========>....................] - ETA: 5:46 - loss: 2.0065 - regression_loss: 1.4729 - classification_loss: 0.5336\n",
      "177/500 [=========>....................] - ETA: 5:46 - loss: 2.0022 - regression_loss: 1.4700 - classification_loss: 0.5322\n",
      "178/500 [=========>....................] - ETA: 5:44 - loss: 2.0015 - regression_loss: 1.4701 - classification_loss: 0.5314\n",
      "179/500 [=========>....................] - ETA: 5:43 - loss: 2.0019 - regression_loss: 1.4708 - classification_loss: 0.5311\n",
      "180/500 [=========>....................] - ETA: 5:42 - loss: 2.0028 - regression_loss: 1.4710 - classification_loss: 0.5318\n",
      "181/500 [=========>....................] - ETA: 5:41 - loss: 2.0002 - regression_loss: 1.4696 - classification_loss: 0.5306\n",
      "182/500 [=========>....................] - ETA: 5:40 - loss: 2.0049 - regression_loss: 1.4738 - classification_loss: 0.5311\n",
      "183/500 [=========>....................] - ETA: 5:39 - loss: 2.0048 - regression_loss: 1.4738 - classification_loss: 0.5310\n",
      "184/500 [==========>...................] - ETA: 5:38 - loss: 2.0063 - regression_loss: 1.4750 - classification_loss: 0.5314\n",
      "185/500 [==========>...................] - ETA: 5:36 - loss: 2.0050 - regression_loss: 1.4744 - classification_loss: 0.5306\n",
      "186/500 [==========>...................] - ETA: 5:35 - loss: 2.0033 - regression_loss: 1.4727 - classification_loss: 0.5306\n",
      "187/500 [==========>...................] - ETA: 5:34 - loss: 2.0020 - regression_loss: 1.4716 - classification_loss: 0.5304\n",
      "188/500 [==========>...................] - ETA: 5:33 - loss: 2.0061 - regression_loss: 1.4744 - classification_loss: 0.5317\n",
      "189/500 [==========>...................] - ETA: 5:32 - loss: 2.0107 - regression_loss: 1.4780 - classification_loss: 0.5326\n",
      "190/500 [==========>...................] - ETA: 5:31 - loss: 2.0101 - regression_loss: 1.4776 - classification_loss: 0.5326\n",
      "191/500 [==========>...................] - ETA: 5:30 - loss: 2.0130 - regression_loss: 1.4800 - classification_loss: 0.5330\n",
      "192/500 [==========>...................] - ETA: 5:29 - loss: 2.0121 - regression_loss: 1.4794 - classification_loss: 0.5327\n",
      "193/500 [==========>...................] - ETA: 5:28 - loss: 2.0091 - regression_loss: 1.4771 - classification_loss: 0.5319\n",
      "194/500 [==========>...................] - ETA: 5:27 - loss: 2.0098 - regression_loss: 1.4781 - classification_loss: 0.5317\n",
      "195/500 [==========>...................] - ETA: 5:25 - loss: 2.0099 - regression_loss: 1.4782 - classification_loss: 0.5317\n",
      "196/500 [==========>...................] - ETA: 5:24 - loss: 2.0137 - regression_loss: 1.4816 - classification_loss: 0.5322\n",
      "197/500 [==========>...................] - ETA: 5:23 - loss: 2.0098 - regression_loss: 1.4778 - classification_loss: 0.5319\n",
      "198/500 [==========>...................] - ETA: 5:22 - loss: 2.0115 - regression_loss: 1.4788 - classification_loss: 0.5327\n",
      "199/500 [==========>...................] - ETA: 5:21 - loss: 2.0082 - regression_loss: 1.4761 - classification_loss: 0.5321\n",
      "200/500 [===========>..................] - ETA: 5:20 - loss: 2.0109 - regression_loss: 1.4786 - classification_loss: 0.5323\n",
      "201/500 [===========>..................] - ETA: 5:19 - loss: 2.0130 - regression_loss: 1.4805 - classification_loss: 0.5326\n",
      "202/500 [===========>..................] - ETA: 5:17 - loss: 2.0135 - regression_loss: 1.4811 - classification_loss: 0.5324\n",
      "203/500 [===========>..................] - ETA: 5:16 - loss: 2.0125 - regression_loss: 1.4802 - classification_loss: 0.5323\n",
      "204/500 [===========>..................] - ETA: 5:15 - loss: 2.0140 - regression_loss: 1.4809 - classification_loss: 0.5330\n",
      "205/500 [===========>..................] - ETA: 5:14 - loss: 2.0134 - regression_loss: 1.4803 - classification_loss: 0.5331\n",
      "206/500 [===========>..................] - ETA: 5:13 - loss: 2.0146 - regression_loss: 1.4812 - classification_loss: 0.5333\n",
      "207/500 [===========>..................] - ETA: 5:12 - loss: 2.0158 - regression_loss: 1.4818 - classification_loss: 0.5339\n",
      "208/500 [===========>..................] - ETA: 5:11 - loss: 2.0153 - regression_loss: 1.4821 - classification_loss: 0.5332\n",
      "209/500 [===========>..................] - ETA: 5:10 - loss: 2.0122 - regression_loss: 1.4800 - classification_loss: 0.5323\n",
      "210/500 [===========>..................] - ETA: 5:09 - loss: 2.0107 - regression_loss: 1.4788 - classification_loss: 0.5319\n",
      "211/500 [===========>..................] - ETA: 5:08 - loss: 2.0133 - regression_loss: 1.4794 - classification_loss: 0.5339\n",
      "212/500 [===========>..................] - ETA: 5:07 - loss: 2.0152 - regression_loss: 1.4805 - classification_loss: 0.5347\n",
      "213/500 [===========>..................] - ETA: 5:06 - loss: 2.0127 - regression_loss: 1.4792 - classification_loss: 0.5335\n",
      "214/500 [===========>..................] - ETA: 5:05 - loss: 2.0130 - regression_loss: 1.4793 - classification_loss: 0.5337\n",
      "215/500 [===========>..................] - ETA: 5:03 - loss: 2.0123 - regression_loss: 1.4795 - classification_loss: 0.5327\n",
      "216/500 [===========>..................] - ETA: 5:02 - loss: 2.0110 - regression_loss: 1.4791 - classification_loss: 0.5320\n",
      "217/500 [============>.................] - ETA: 5:01 - loss: 2.0108 - regression_loss: 1.4789 - classification_loss: 0.5320\n",
      "218/500 [============>.................] - ETA: 5:00 - loss: 2.0089 - regression_loss: 1.4769 - classification_loss: 0.5321\n",
      "219/500 [============>.................] - ETA: 4:59 - loss: 2.0102 - regression_loss: 1.4778 - classification_loss: 0.5324\n",
      "220/500 [============>.................] - ETA: 4:58 - loss: 2.0095 - regression_loss: 1.4773 - classification_loss: 0.5322\n",
      "221/500 [============>.................] - ETA: 4:57 - loss: 2.0096 - regression_loss: 1.4774 - classification_loss: 0.5321\n",
      "222/500 [============>.................] - ETA: 4:56 - loss: 2.0144 - regression_loss: 1.4808 - classification_loss: 0.5336\n",
      "223/500 [============>.................] - ETA: 4:54 - loss: 2.0137 - regression_loss: 1.4802 - classification_loss: 0.5334\n",
      "224/500 [============>.................] - ETA: 4:53 - loss: 2.0125 - regression_loss: 1.4783 - classification_loss: 0.5342\n",
      "225/500 [============>.................] - ETA: 4:52 - loss: 2.0097 - regression_loss: 1.4765 - classification_loss: 0.5332\n",
      "226/500 [============>.................] - ETA: 4:51 - loss: 2.0107 - regression_loss: 1.4776 - classification_loss: 0.5331\n",
      "227/500 [============>.................] - ETA: 4:49 - loss: 2.0082 - regression_loss: 1.4761 - classification_loss: 0.5321\n",
      "228/500 [============>.................] - ETA: 4:48 - loss: 2.0062 - regression_loss: 1.4744 - classification_loss: 0.5318\n",
      "229/500 [============>.................] - ETA: 4:47 - loss: 2.0043 - regression_loss: 1.4728 - classification_loss: 0.5315\n",
      "230/500 [============>.................] - ETA: 4:46 - loss: 2.0039 - regression_loss: 1.4731 - classification_loss: 0.5308\n",
      "231/500 [============>.................] - ETA: 4:45 - loss: 2.0024 - regression_loss: 1.4719 - classification_loss: 0.5305\n",
      "232/500 [============>.................] - ETA: 4:44 - loss: 2.0058 - regression_loss: 1.4749 - classification_loss: 0.5309\n",
      "233/500 [============>.................] - ETA: 4:43 - loss: 2.0108 - regression_loss: 1.4786 - classification_loss: 0.5323\n",
      "234/500 [=============>................] - ETA: 4:42 - loss: 2.0108 - regression_loss: 1.4791 - classification_loss: 0.5316\n",
      "235/500 [=============>................] - ETA: 4:41 - loss: 2.0087 - regression_loss: 1.4775 - classification_loss: 0.5312\n",
      "236/500 [=============>................] - ETA: 4:40 - loss: 2.0064 - regression_loss: 1.4754 - classification_loss: 0.5310\n",
      "237/500 [=============>................] - ETA: 4:39 - loss: 2.0087 - regression_loss: 1.4768 - classification_loss: 0.5319\n",
      "238/500 [=============>................] - ETA: 4:38 - loss: 2.0108 - regression_loss: 1.4784 - classification_loss: 0.5324\n",
      "239/500 [=============>................] - ETA: 4:37 - loss: 2.0128 - regression_loss: 1.4799 - classification_loss: 0.5329\n",
      "240/500 [=============>................] - ETA: 4:36 - loss: 2.0118 - regression_loss: 1.4792 - classification_loss: 0.5326\n",
      "241/500 [=============>................] - ETA: 4:35 - loss: 2.0120 - regression_loss: 1.4789 - classification_loss: 0.5331\n",
      "242/500 [=============>................] - ETA: 4:34 - loss: 2.0137 - regression_loss: 1.4800 - classification_loss: 0.5337\n",
      "243/500 [=============>................] - ETA: 4:32 - loss: 2.0128 - regression_loss: 1.4794 - classification_loss: 0.5334\n",
      "244/500 [=============>................] - ETA: 4:31 - loss: 2.0105 - regression_loss: 1.4775 - classification_loss: 0.5330\n",
      "245/500 [=============>................] - ETA: 4:30 - loss: 2.0108 - regression_loss: 1.4772 - classification_loss: 0.5336\n",
      "246/500 [=============>................] - ETA: 4:29 - loss: 2.0073 - regression_loss: 1.4745 - classification_loss: 0.5329\n",
      "247/500 [=============>................] - ETA: 4:28 - loss: 2.0085 - regression_loss: 1.4754 - classification_loss: 0.5331\n",
      "248/500 [=============>................] - ETA: 4:27 - loss: 2.0058 - regression_loss: 1.4730 - classification_loss: 0.5328\n",
      "249/500 [=============>................] - ETA: 4:26 - loss: 2.0094 - regression_loss: 1.4754 - classification_loss: 0.5340\n",
      "250/500 [==============>...............] - ETA: 4:25 - loss: 2.0075 - regression_loss: 1.4744 - classification_loss: 0.5331\n",
      "251/500 [==============>...............] - ETA: 4:24 - loss: 2.0083 - regression_loss: 1.4751 - classification_loss: 0.5331\n",
      "252/500 [==============>...............] - ETA: 4:23 - loss: 2.0090 - regression_loss: 1.4756 - classification_loss: 0.5334\n",
      "253/500 [==============>...............] - ETA: 4:21 - loss: 2.0060 - regression_loss: 1.4737 - classification_loss: 0.5323\n",
      "254/500 [==============>...............] - ETA: 4:20 - loss: 2.0072 - regression_loss: 1.4746 - classification_loss: 0.5326\n",
      "255/500 [==============>...............] - ETA: 4:19 - loss: 2.0049 - regression_loss: 1.4731 - classification_loss: 0.5319\n",
      "256/500 [==============>...............] - ETA: 4:18 - loss: 2.0047 - regression_loss: 1.4731 - classification_loss: 0.5316\n",
      "257/500 [==============>...............] - ETA: 4:17 - loss: 2.0066 - regression_loss: 1.4740 - classification_loss: 0.5326\n",
      "258/500 [==============>...............] - ETA: 4:16 - loss: 2.0059 - regression_loss: 1.4740 - classification_loss: 0.5319\n",
      "259/500 [==============>...............] - ETA: 4:15 - loss: 2.0031 - regression_loss: 1.4721 - classification_loss: 0.5310\n",
      "260/500 [==============>...............] - ETA: 4:15 - loss: 2.0031 - regression_loss: 1.4717 - classification_loss: 0.5314\n",
      "261/500 [==============>...............] - ETA: 4:14 - loss: 2.0042 - regression_loss: 1.4725 - classification_loss: 0.5317\n",
      "262/500 [==============>...............] - ETA: 4:13 - loss: 2.0048 - regression_loss: 1.4726 - classification_loss: 0.5322\n",
      "263/500 [==============>...............] - ETA: 4:11 - loss: 2.0044 - regression_loss: 1.4722 - classification_loss: 0.5322\n",
      "264/500 [==============>...............] - ETA: 4:10 - loss: 2.0035 - regression_loss: 1.4718 - classification_loss: 0.5317\n",
      "265/500 [==============>...............] - ETA: 4:09 - loss: 2.0069 - regression_loss: 1.4743 - classification_loss: 0.5326\n",
      "266/500 [==============>...............] - ETA: 4:08 - loss: 2.0121 - regression_loss: 1.4782 - classification_loss: 0.5339\n",
      "267/500 [===============>..............] - ETA: 4:07 - loss: 2.0160 - regression_loss: 1.4801 - classification_loss: 0.5359\n",
      "268/500 [===============>..............] - ETA: 4:06 - loss: 2.0204 - regression_loss: 1.4829 - classification_loss: 0.5375\n",
      "269/500 [===============>..............] - ETA: 4:05 - loss: 2.0201 - regression_loss: 1.4831 - classification_loss: 0.5370\n",
      "270/500 [===============>..............] - ETA: 4:04 - loss: 2.0217 - regression_loss: 1.4845 - classification_loss: 0.5372\n",
      "271/500 [===============>..............] - ETA: 4:03 - loss: 2.0201 - regression_loss: 1.4835 - classification_loss: 0.5366\n",
      "272/500 [===============>..............] - ETA: 4:01 - loss: 2.0192 - regression_loss: 1.4817 - classification_loss: 0.5375\n",
      "273/500 [===============>..............] - ETA: 4:00 - loss: 2.0204 - regression_loss: 1.4829 - classification_loss: 0.5375\n",
      "274/500 [===============>..............] - ETA: 3:59 - loss: 2.0194 - regression_loss: 1.4821 - classification_loss: 0.5373\n",
      "275/500 [===============>..............] - ETA: 3:58 - loss: 2.0172 - regression_loss: 1.4805 - classification_loss: 0.5367\n",
      "276/500 [===============>..............] - ETA: 3:57 - loss: 2.0181 - regression_loss: 1.4816 - classification_loss: 0.5365\n",
      "277/500 [===============>..............] - ETA: 3:56 - loss: 2.0170 - regression_loss: 1.4812 - classification_loss: 0.5358\n",
      "278/500 [===============>..............] - ETA: 3:55 - loss: 2.0163 - regression_loss: 1.4812 - classification_loss: 0.5351\n",
      "279/500 [===============>..............] - ETA: 3:54 - loss: 2.0166 - regression_loss: 1.4814 - classification_loss: 0.5352\n",
      "280/500 [===============>..............] - ETA: 3:53 - loss: 2.0162 - regression_loss: 1.4812 - classification_loss: 0.5350\n",
      "281/500 [===============>..............] - ETA: 3:52 - loss: 2.0168 - regression_loss: 1.4820 - classification_loss: 0.5348\n",
      "282/500 [===============>..............] - ETA: 3:51 - loss: 2.0139 - regression_loss: 1.4797 - classification_loss: 0.5342\n",
      "283/500 [===============>..............] - ETA: 3:50 - loss: 2.0153 - regression_loss: 1.4808 - classification_loss: 0.5345\n",
      "284/500 [================>.............] - ETA: 3:48 - loss: 2.0146 - regression_loss: 1.4804 - classification_loss: 0.5342\n",
      "285/500 [================>.............] - ETA: 3:47 - loss: 2.0161 - regression_loss: 1.4813 - classification_loss: 0.5349\n",
      "286/500 [================>.............] - ETA: 3:46 - loss: 2.0173 - regression_loss: 1.4820 - classification_loss: 0.5353\n",
      "287/500 [================>.............] - ETA: 3:45 - loss: 2.0160 - regression_loss: 1.4808 - classification_loss: 0.5352\n",
      "288/500 [================>.............] - ETA: 3:44 - loss: 2.0206 - regression_loss: 1.4839 - classification_loss: 0.5367\n",
      "289/500 [================>.............] - ETA: 3:43 - loss: 2.0202 - regression_loss: 1.4834 - classification_loss: 0.5369\n",
      "290/500 [================>.............] - ETA: 3:42 - loss: 2.0201 - regression_loss: 1.4835 - classification_loss: 0.5366\n",
      "291/500 [================>.............] - ETA: 3:41 - loss: 2.0244 - regression_loss: 1.4863 - classification_loss: 0.5381\n",
      "292/500 [================>.............] - ETA: 3:40 - loss: 2.0234 - regression_loss: 1.4850 - classification_loss: 0.5384\n",
      "293/500 [================>.............] - ETA: 3:39 - loss: 2.0233 - regression_loss: 1.4853 - classification_loss: 0.5381\n",
      "294/500 [================>.............] - ETA: 3:38 - loss: 2.0247 - regression_loss: 1.4863 - classification_loss: 0.5383\n",
      "295/500 [================>.............] - ETA: 3:37 - loss: 2.0235 - regression_loss: 1.4852 - classification_loss: 0.5383\n",
      "296/500 [================>.............] - ETA: 3:35 - loss: 2.0212 - regression_loss: 1.4836 - classification_loss: 0.5375\n",
      "297/500 [================>.............] - ETA: 3:34 - loss: 2.0203 - regression_loss: 1.4834 - classification_loss: 0.5369\n",
      "298/500 [================>.............] - ETA: 3:33 - loss: 2.0248 - regression_loss: 1.4868 - classification_loss: 0.5380\n",
      "299/500 [================>.............] - ETA: 3:32 - loss: 2.0234 - regression_loss: 1.4858 - classification_loss: 0.5376\n",
      "300/500 [=================>............] - ETA: 3:31 - loss: 2.0243 - regression_loss: 1.4857 - classification_loss: 0.5386\n",
      "301/500 [=================>............] - ETA: 3:30 - loss: 2.0218 - regression_loss: 1.4838 - classification_loss: 0.5380\n",
      "302/500 [=================>............] - ETA: 3:29 - loss: 2.0225 - regression_loss: 1.4842 - classification_loss: 0.5383\n",
      "303/500 [=================>............] - ETA: 3:28 - loss: 2.0250 - regression_loss: 1.4865 - classification_loss: 0.5386\n",
      "304/500 [=================>............] - ETA: 3:27 - loss: 2.0273 - regression_loss: 1.4880 - classification_loss: 0.5394\n",
      "305/500 [=================>............] - ETA: 3:26 - loss: 2.0283 - regression_loss: 1.4889 - classification_loss: 0.5394\n",
      "306/500 [=================>............] - ETA: 3:25 - loss: 2.0299 - regression_loss: 1.4896 - classification_loss: 0.5403\n",
      "307/500 [=================>............] - ETA: 3:24 - loss: 2.0288 - regression_loss: 1.4889 - classification_loss: 0.5399\n",
      "308/500 [=================>............] - ETA: 3:23 - loss: 2.0325 - regression_loss: 1.4916 - classification_loss: 0.5410\n",
      "309/500 [=================>............] - ETA: 3:22 - loss: 2.0344 - regression_loss: 1.4927 - classification_loss: 0.5416\n",
      "310/500 [=================>............] - ETA: 3:21 - loss: 2.0345 - regression_loss: 1.4929 - classification_loss: 0.5416\n",
      "311/500 [=================>............] - ETA: 3:20 - loss: 2.0339 - regression_loss: 1.4921 - classification_loss: 0.5418\n",
      "312/500 [=================>............] - ETA: 3:19 - loss: 2.0360 - regression_loss: 1.4938 - classification_loss: 0.5422\n",
      "313/500 [=================>............] - ETA: 3:17 - loss: 2.0360 - regression_loss: 1.4940 - classification_loss: 0.5421\n",
      "314/500 [=================>............] - ETA: 3:16 - loss: 2.0354 - regression_loss: 1.4937 - classification_loss: 0.5417\n",
      "315/500 [=================>............] - ETA: 3:15 - loss: 2.0353 - regression_loss: 1.4938 - classification_loss: 0.5415\n",
      "316/500 [=================>............] - ETA: 3:14 - loss: 2.0365 - regression_loss: 1.4943 - classification_loss: 0.5422\n",
      "317/500 [==================>...........] - ETA: 3:13 - loss: 2.0341 - regression_loss: 1.4922 - classification_loss: 0.5419\n",
      "318/500 [==================>...........] - ETA: 3:12 - loss: 2.0353 - regression_loss: 1.4931 - classification_loss: 0.5422\n",
      "319/500 [==================>...........] - ETA: 3:11 - loss: 2.0358 - regression_loss: 1.4934 - classification_loss: 0.5424\n",
      "320/500 [==================>...........] - ETA: 3:10 - loss: 2.0353 - regression_loss: 1.4932 - classification_loss: 0.5420\n",
      "321/500 [==================>...........] - ETA: 3:09 - loss: 2.0328 - regression_loss: 1.4914 - classification_loss: 0.5413\n",
      "322/500 [==================>...........] - ETA: 3:08 - loss: 2.0308 - regression_loss: 1.4895 - classification_loss: 0.5412\n",
      "323/500 [==================>...........] - ETA: 3:07 - loss: 2.0293 - regression_loss: 1.4882 - classification_loss: 0.5411\n",
      "324/500 [==================>...........] - ETA: 3:06 - loss: 2.0265 - regression_loss: 1.4859 - classification_loss: 0.5406\n",
      "325/500 [==================>...........] - ETA: 3:05 - loss: 2.0274 - regression_loss: 1.4870 - classification_loss: 0.5405\n",
      "326/500 [==================>...........] - ETA: 3:04 - loss: 2.0285 - regression_loss: 1.4878 - classification_loss: 0.5407\n",
      "327/500 [==================>...........] - ETA: 3:02 - loss: 2.0282 - regression_loss: 1.4877 - classification_loss: 0.5404\n",
      "328/500 [==================>...........] - ETA: 3:01 - loss: 2.0266 - regression_loss: 1.4866 - classification_loss: 0.5400\n",
      "329/500 [==================>...........] - ETA: 3:00 - loss: 2.0323 - regression_loss: 1.4909 - classification_loss: 0.5414\n",
      "330/500 [==================>...........] - ETA: 2:59 - loss: 2.0329 - regression_loss: 1.4914 - classification_loss: 0.5415\n",
      "331/500 [==================>...........] - ETA: 2:58 - loss: 2.0354 - regression_loss: 1.4937 - classification_loss: 0.5418\n",
      "332/500 [==================>...........] - ETA: 2:57 - loss: 2.0353 - regression_loss: 1.4937 - classification_loss: 0.5416\n",
      "333/500 [==================>...........] - ETA: 2:56 - loss: 2.0352 - regression_loss: 1.4939 - classification_loss: 0.5413\n",
      "334/500 [===================>..........] - ETA: 2:55 - loss: 2.0339 - regression_loss: 1.4931 - classification_loss: 0.5408\n",
      "335/500 [===================>..........] - ETA: 2:54 - loss: 2.0327 - regression_loss: 1.4922 - classification_loss: 0.5405\n",
      "336/500 [===================>..........] - ETA: 2:53 - loss: 2.0303 - regression_loss: 1.4904 - classification_loss: 0.5399\n",
      "337/500 [===================>..........] - ETA: 2:52 - loss: 2.0323 - regression_loss: 1.4924 - classification_loss: 0.5400\n",
      "338/500 [===================>..........] - ETA: 2:51 - loss: 2.0360 - regression_loss: 1.4949 - classification_loss: 0.5412\n",
      "339/500 [===================>..........] - ETA: 2:50 - loss: 2.0361 - regression_loss: 1.4948 - classification_loss: 0.5413\n",
      "340/500 [===================>..........] - ETA: 2:49 - loss: 2.0357 - regression_loss: 1.4945 - classification_loss: 0.5412\n",
      "341/500 [===================>..........] - ETA: 2:47 - loss: 2.0361 - regression_loss: 1.4944 - classification_loss: 0.5417\n",
      "342/500 [===================>..........] - ETA: 2:46 - loss: 2.0352 - regression_loss: 1.4937 - classification_loss: 0.5414\n",
      "343/500 [===================>..........] - ETA: 2:45 - loss: 2.0354 - regression_loss: 1.4936 - classification_loss: 0.5418\n",
      "344/500 [===================>..........] - ETA: 2:44 - loss: 2.0337 - regression_loss: 1.4926 - classification_loss: 0.5411\n",
      "345/500 [===================>..........] - ETA: 2:43 - loss: 2.0336 - regression_loss: 1.4923 - classification_loss: 0.5413\n",
      "346/500 [===================>..........] - ETA: 2:42 - loss: 2.0346 - regression_loss: 1.4931 - classification_loss: 0.5414\n",
      "347/500 [===================>..........] - ETA: 2:41 - loss: 2.0365 - regression_loss: 1.4954 - classification_loss: 0.5411\n",
      "348/500 [===================>..........] - ETA: 2:40 - loss: 2.0372 - regression_loss: 1.4964 - classification_loss: 0.5409\n",
      "349/500 [===================>..........] - ETA: 2:39 - loss: 2.0360 - regression_loss: 1.4958 - classification_loss: 0.5402\n",
      "350/500 [====================>.........] - ETA: 2:38 - loss: 2.0365 - regression_loss: 1.4962 - classification_loss: 0.5403\n",
      "351/500 [====================>.........] - ETA: 2:37 - loss: 2.0344 - regression_loss: 1.4948 - classification_loss: 0.5396\n",
      "352/500 [====================>.........] - ETA: 2:36 - loss: 2.0355 - regression_loss: 1.4956 - classification_loss: 0.5399\n",
      "353/500 [====================>.........] - ETA: 2:35 - loss: 2.0359 - regression_loss: 1.4957 - classification_loss: 0.5402\n",
      "354/500 [====================>.........] - ETA: 2:34 - loss: 2.0384 - regression_loss: 1.4973 - classification_loss: 0.5412\n",
      "355/500 [====================>.........] - ETA: 2:33 - loss: 2.0384 - regression_loss: 1.4973 - classification_loss: 0.5411\n",
      "356/500 [====================>.........] - ETA: 2:31 - loss: 2.0368 - regression_loss: 1.4964 - classification_loss: 0.5404\n",
      "357/500 [====================>.........] - ETA: 2:30 - loss: 2.0374 - regression_loss: 1.4968 - classification_loss: 0.5405\n",
      "358/500 [====================>.........] - ETA: 2:30 - loss: 2.0376 - regression_loss: 1.4974 - classification_loss: 0.5402\n",
      "359/500 [====================>.........] - ETA: 2:29 - loss: 2.0404 - regression_loss: 1.4999 - classification_loss: 0.5405\n",
      "360/500 [====================>.........] - ETA: 2:28 - loss: 2.0385 - regression_loss: 1.4983 - classification_loss: 0.5402\n",
      "361/500 [====================>.........] - ETA: 2:27 - loss: 2.0383 - regression_loss: 1.4980 - classification_loss: 0.5403\n",
      "362/500 [====================>.........] - ETA: 2:26 - loss: 2.0366 - regression_loss: 1.4966 - classification_loss: 0.5399\n",
      "363/500 [====================>.........] - ETA: 2:25 - loss: 2.0387 - regression_loss: 1.4983 - classification_loss: 0.5404\n",
      "364/500 [====================>.........] - ETA: 2:24 - loss: 2.0370 - regression_loss: 1.4970 - classification_loss: 0.5401\n",
      "365/500 [====================>.........] - ETA: 2:23 - loss: 2.0364 - regression_loss: 1.4965 - classification_loss: 0.5399\n",
      "366/500 [====================>.........] - ETA: 2:22 - loss: 2.0365 - regression_loss: 1.4966 - classification_loss: 0.5399\n",
      "367/500 [=====================>........] - ETA: 2:20 - loss: 2.0382 - regression_loss: 1.4981 - classification_loss: 0.5401\n",
      "368/500 [=====================>........] - ETA: 2:19 - loss: 2.0362 - regression_loss: 1.4966 - classification_loss: 0.5396\n",
      "369/500 [=====================>........] - ETA: 2:18 - loss: 2.0351 - regression_loss: 1.4958 - classification_loss: 0.5394\n",
      "370/500 [=====================>........] - ETA: 2:17 - loss: 2.0349 - regression_loss: 1.4952 - classification_loss: 0.5397\n",
      "371/500 [=====================>........] - ETA: 2:16 - loss: 2.0346 - regression_loss: 1.4951 - classification_loss: 0.5395\n",
      "372/500 [=====================>........] - ETA: 2:15 - loss: 2.0370 - regression_loss: 1.4973 - classification_loss: 0.5397\n",
      "373/500 [=====================>........] - ETA: 2:14 - loss: 2.0372 - regression_loss: 1.4975 - classification_loss: 0.5397\n",
      "374/500 [=====================>........] - ETA: 2:13 - loss: 2.0359 - regression_loss: 1.4966 - classification_loss: 0.5393\n",
      "375/500 [=====================>........] - ETA: 2:12 - loss: 2.0353 - regression_loss: 1.4962 - classification_loss: 0.5390\n",
      "376/500 [=====================>........] - ETA: 2:11 - loss: 2.0353 - regression_loss: 1.4961 - classification_loss: 0.5392\n",
      "377/500 [=====================>........] - ETA: 2:10 - loss: 2.0334 - regression_loss: 1.4946 - classification_loss: 0.5388\n",
      "378/500 [=====================>........] - ETA: 2:09 - loss: 2.0325 - regression_loss: 1.4939 - classification_loss: 0.5385\n",
      "379/500 [=====================>........] - ETA: 2:08 - loss: 2.0314 - regression_loss: 1.4930 - classification_loss: 0.5385\n",
      "380/500 [=====================>........] - ETA: 2:07 - loss: 2.0306 - regression_loss: 1.4926 - classification_loss: 0.5380\n",
      "381/500 [=====================>........] - ETA: 2:06 - loss: 2.0283 - regression_loss: 1.4911 - classification_loss: 0.5373\n",
      "382/500 [=====================>........] - ETA: 2:05 - loss: 2.0269 - regression_loss: 1.4900 - classification_loss: 0.5368\n",
      "383/500 [=====================>........] - ETA: 2:03 - loss: 2.0267 - regression_loss: 1.4896 - classification_loss: 0.5371\n",
      "384/500 [======================>.......] - ETA: 2:02 - loss: 2.0259 - regression_loss: 1.4888 - classification_loss: 0.5371\n",
      "385/500 [======================>.......] - ETA: 2:01 - loss: 2.0260 - regression_loss: 1.4890 - classification_loss: 0.5370\n",
      "386/500 [======================>.......] - ETA: 2:00 - loss: 2.0250 - regression_loss: 1.4882 - classification_loss: 0.5368\n",
      "387/500 [======================>.......] - ETA: 1:59 - loss: 2.0237 - regression_loss: 1.4875 - classification_loss: 0.5362\n",
      "388/500 [======================>.......] - ETA: 1:58 - loss: 2.0222 - regression_loss: 1.4866 - classification_loss: 0.5356\n",
      "389/500 [======================>.......] - ETA: 1:57 - loss: 2.0209 - regression_loss: 1.4854 - classification_loss: 0.5354\n",
      "390/500 [======================>.......] - ETA: 1:56 - loss: 2.0225 - regression_loss: 1.4868 - classification_loss: 0.5358\n",
      "391/500 [======================>.......] - ETA: 1:55 - loss: 2.0239 - regression_loss: 1.4878 - classification_loss: 0.5361\n",
      "392/500 [======================>.......] - ETA: 1:54 - loss: 2.0240 - regression_loss: 1.4883 - classification_loss: 0.5357\n",
      "393/500 [======================>.......] - ETA: 1:53 - loss: 2.0232 - regression_loss: 1.4880 - classification_loss: 0.5352\n",
      "394/500 [======================>.......] - ETA: 1:52 - loss: 2.0211 - regression_loss: 1.4866 - classification_loss: 0.5345\n",
      "395/500 [======================>.......] - ETA: 1:51 - loss: 2.0200 - regression_loss: 1.4860 - classification_loss: 0.5340\n",
      "396/500 [======================>.......] - ETA: 1:50 - loss: 2.0182 - regression_loss: 1.4846 - classification_loss: 0.5336\n",
      "397/500 [======================>.......] - ETA: 1:48 - loss: 2.0157 - regression_loss: 1.4827 - classification_loss: 0.5330\n",
      "398/500 [======================>.......] - ETA: 1:47 - loss: 2.0166 - regression_loss: 1.4838 - classification_loss: 0.5328\n",
      "399/500 [======================>.......] - ETA: 1:46 - loss: 2.0168 - regression_loss: 1.4837 - classification_loss: 0.5331\n",
      "400/500 [=======================>......] - ETA: 1:45 - loss: 2.0153 - regression_loss: 1.4829 - classification_loss: 0.5324\n",
      "401/500 [=======================>......] - ETA: 1:44 - loss: 2.0138 - regression_loss: 1.4818 - classification_loss: 0.5320\n",
      "402/500 [=======================>......] - ETA: 1:43 - loss: 2.0139 - regression_loss: 1.4819 - classification_loss: 0.5319\n",
      "403/500 [=======================>......] - ETA: 1:42 - loss: 2.0112 - regression_loss: 1.4802 - classification_loss: 0.5310\n",
      "404/500 [=======================>......] - ETA: 1:41 - loss: 2.0116 - regression_loss: 1.4808 - classification_loss: 0.5309\n",
      "405/500 [=======================>......] - ETA: 1:40 - loss: 2.0101 - regression_loss: 1.4795 - classification_loss: 0.5305\n",
      "406/500 [=======================>......] - ETA: 1:39 - loss: 2.0096 - regression_loss: 1.4793 - classification_loss: 0.5303\n",
      "407/500 [=======================>......] - ETA: 1:38 - loss: 2.0080 - regression_loss: 1.4778 - classification_loss: 0.5302\n",
      "408/500 [=======================>......] - ETA: 1:37 - loss: 2.0091 - regression_loss: 1.4785 - classification_loss: 0.5306\n",
      "409/500 [=======================>......] - ETA: 1:36 - loss: 2.0099 - regression_loss: 1.4794 - classification_loss: 0.5305\n",
      "410/500 [=======================>......] - ETA: 1:35 - loss: 2.0110 - regression_loss: 1.4804 - classification_loss: 0.5306\n",
      "411/500 [=======================>......] - ETA: 1:34 - loss: 2.0119 - regression_loss: 1.4809 - classification_loss: 0.5310\n",
      "412/500 [=======================>......] - ETA: 1:32 - loss: 2.0126 - regression_loss: 1.4812 - classification_loss: 0.5314\n",
      "413/500 [=======================>......] - ETA: 1:31 - loss: 2.0125 - regression_loss: 1.4811 - classification_loss: 0.5313\n",
      "414/500 [=======================>......] - ETA: 1:30 - loss: 2.0116 - regression_loss: 1.4806 - classification_loss: 0.5311\n",
      "415/500 [=======================>......] - ETA: 1:29 - loss: 2.0117 - regression_loss: 1.4807 - classification_loss: 0.5310\n",
      "416/500 [=======================>......] - ETA: 1:28 - loss: 2.0105 - regression_loss: 1.4796 - classification_loss: 0.5309\n",
      "417/500 [========================>.....] - ETA: 1:27 - loss: 2.0098 - regression_loss: 1.4792 - classification_loss: 0.5307\n",
      "418/500 [========================>.....] - ETA: 1:26 - loss: 2.0089 - regression_loss: 1.4786 - classification_loss: 0.5303\n",
      "419/500 [========================>.....] - ETA: 1:25 - loss: 2.0087 - regression_loss: 1.4784 - classification_loss: 0.5303\n",
      "420/500 [========================>.....] - ETA: 1:24 - loss: 2.0083 - regression_loss: 1.4777 - classification_loss: 0.5306\n",
      "421/500 [========================>.....] - ETA: 1:23 - loss: 2.0095 - regression_loss: 1.4785 - classification_loss: 0.5310\n",
      "422/500 [========================>.....] - ETA: 1:22 - loss: 2.0080 - regression_loss: 1.4770 - classification_loss: 0.5310\n",
      "423/500 [========================>.....] - ETA: 1:21 - loss: 2.0094 - regression_loss: 1.4783 - classification_loss: 0.5310\n",
      "424/500 [========================>.....] - ETA: 1:20 - loss: 2.0077 - regression_loss: 1.4771 - classification_loss: 0.5305\n",
      "425/500 [========================>.....] - ETA: 1:19 - loss: 2.0086 - regression_loss: 1.4779 - classification_loss: 0.5307\n",
      "426/500 [========================>.....] - ETA: 1:18 - loss: 2.0111 - regression_loss: 1.4792 - classification_loss: 0.5319\n",
      "427/500 [========================>.....] - ETA: 1:17 - loss: 2.0090 - regression_loss: 1.4775 - classification_loss: 0.5316\n",
      "428/500 [========================>.....] - ETA: 1:16 - loss: 2.0107 - regression_loss: 1.4784 - classification_loss: 0.5322\n",
      "429/500 [========================>.....] - ETA: 1:15 - loss: 2.0115 - regression_loss: 1.4788 - classification_loss: 0.5327\n",
      "430/500 [========================>.....] - ETA: 1:13 - loss: 2.0113 - regression_loss: 1.4790 - classification_loss: 0.5323\n",
      "431/500 [========================>.....] - ETA: 1:12 - loss: 2.0124 - regression_loss: 1.4798 - classification_loss: 0.5327\n",
      "432/500 [========================>.....] - ETA: 1:11 - loss: 2.0117 - regression_loss: 1.4793 - classification_loss: 0.5325\n",
      "433/500 [========================>.....] - ETA: 1:10 - loss: 2.0116 - regression_loss: 1.4794 - classification_loss: 0.5323\n",
      "434/500 [=========================>....] - ETA: 1:09 - loss: 2.0121 - regression_loss: 1.4798 - classification_loss: 0.5323\n",
      "435/500 [=========================>....] - ETA: 1:08 - loss: 2.0141 - regression_loss: 1.4811 - classification_loss: 0.5329\n",
      "436/500 [=========================>....] - ETA: 1:07 - loss: 2.0151 - regression_loss: 1.4818 - classification_loss: 0.5333\n",
      "437/500 [=========================>....] - ETA: 1:06 - loss: 2.0156 - regression_loss: 1.4823 - classification_loss: 0.5334\n",
      "438/500 [=========================>....] - ETA: 1:05 - loss: 2.0174 - regression_loss: 1.4835 - classification_loss: 0.5339\n",
      "439/500 [=========================>....] - ETA: 1:04 - loss: 2.0197 - regression_loss: 1.4856 - classification_loss: 0.5341\n",
      "440/500 [=========================>....] - ETA: 1:03 - loss: 2.0186 - regression_loss: 1.4849 - classification_loss: 0.5337\n",
      "441/500 [=========================>....] - ETA: 1:02 - loss: 2.0177 - regression_loss: 1.4845 - classification_loss: 0.5332\n",
      "442/500 [=========================>....] - ETA: 1:01 - loss: 2.0168 - regression_loss: 1.4838 - classification_loss: 0.5330\n",
      "443/500 [=========================>....] - ETA: 1:00 - loss: 2.0181 - regression_loss: 1.4847 - classification_loss: 0.5333\n",
      "444/500 [=========================>....] - ETA: 59s - loss: 2.0201 - regression_loss: 1.4860 - classification_loss: 0.5341 \n",
      "445/500 [=========================>....] - ETA: 58s - loss: 2.0210 - regression_loss: 1.4867 - classification_loss: 0.5343\n",
      "446/500 [=========================>....] - ETA: 57s - loss: 2.0206 - regression_loss: 1.4865 - classification_loss: 0.5341\n",
      "447/500 [=========================>....] - ETA: 55s - loss: 2.0196 - regression_loss: 1.4859 - classification_loss: 0.5337\n",
      "448/500 [=========================>....] - ETA: 54s - loss: 2.0211 - regression_loss: 1.4868 - classification_loss: 0.5343\n",
      "449/500 [=========================>....] - ETA: 53s - loss: 2.0217 - regression_loss: 1.4873 - classification_loss: 0.5343\n",
      "450/500 [==========================>...] - ETA: 52s - loss: 2.0224 - regression_loss: 1.4878 - classification_loss: 0.5346\n",
      "451/500 [==========================>...] - ETA: 51s - loss: 2.0220 - regression_loss: 1.4876 - classification_loss: 0.5344\n",
      "452/500 [==========================>...] - ETA: 50s - loss: 2.0205 - regression_loss: 1.4865 - classification_loss: 0.5340\n",
      "453/500 [==========================>...] - ETA: 49s - loss: 2.0225 - regression_loss: 1.4880 - classification_loss: 0.5345\n",
      "454/500 [==========================>...] - ETA: 48s - loss: 2.0219 - regression_loss: 1.4878 - classification_loss: 0.5341\n",
      "455/500 [==========================>...] - ETA: 47s - loss: 2.0227 - regression_loss: 1.4885 - classification_loss: 0.5342\n",
      "456/500 [==========================>...] - ETA: 46s - loss: 2.0231 - regression_loss: 1.4888 - classification_loss: 0.5343\n",
      "457/500 [==========================>...] - ETA: 45s - loss: 2.0237 - regression_loss: 1.4890 - classification_loss: 0.5346\n",
      "458/500 [==========================>...] - ETA: 44s - loss: 2.0232 - regression_loss: 1.4884 - classification_loss: 0.5348\n",
      "459/500 [==========================>...] - ETA: 43s - loss: 2.0219 - regression_loss: 1.4875 - classification_loss: 0.5344\n",
      "460/500 [==========================>...] - ETA: 42s - loss: 2.0228 - regression_loss: 1.4883 - classification_loss: 0.5345\n",
      "461/500 [==========================>...] - ETA: 41s - loss: 2.0216 - regression_loss: 1.4874 - classification_loss: 0.5343\n",
      "462/500 [==========================>...] - ETA: 40s - loss: 2.0216 - regression_loss: 1.4873 - classification_loss: 0.5342\n",
      "463/500 [==========================>...] - ETA: 39s - loss: 2.0216 - regression_loss: 1.4875 - classification_loss: 0.5341\n",
      "464/500 [==========================>...] - ETA: 38s - loss: 2.0211 - regression_loss: 1.4870 - classification_loss: 0.5340\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 2.0211 - regression_loss: 1.4872 - classification_loss: 0.5339\n",
      "466/500 [==========================>...] - ETA: 35s - loss: 2.0211 - regression_loss: 1.4872 - classification_loss: 0.5338\n",
      "467/500 [===========================>..] - ETA: 34s - loss: 2.0207 - regression_loss: 1.4872 - classification_loss: 0.5336\n",
      "468/500 [===========================>..] - ETA: 33s - loss: 2.0218 - regression_loss: 1.4883 - classification_loss: 0.5335\n",
      "469/500 [===========================>..] - ETA: 32s - loss: 2.0227 - regression_loss: 1.4892 - classification_loss: 0.5335\n",
      "470/500 [===========================>..] - ETA: 31s - loss: 2.0233 - regression_loss: 1.4897 - classification_loss: 0.5336\n",
      "471/500 [===========================>..] - ETA: 30s - loss: 2.0229 - regression_loss: 1.4893 - classification_loss: 0.5336\n",
      "472/500 [===========================>..] - ETA: 29s - loss: 2.0233 - regression_loss: 1.4896 - classification_loss: 0.5337\n",
      "473/500 [===========================>..] - ETA: 28s - loss: 2.0245 - regression_loss: 1.4906 - classification_loss: 0.5339\n",
      "474/500 [===========================>..] - ETA: 27s - loss: 2.0254 - regression_loss: 1.4911 - classification_loss: 0.5343\n",
      "475/500 [===========================>..] - ETA: 26s - loss: 2.0247 - regression_loss: 1.4907 - classification_loss: 0.5341\n",
      "476/500 [===========================>..] - ETA: 25s - loss: 2.0232 - regression_loss: 1.4894 - classification_loss: 0.5339\n",
      "477/500 [===========================>..] - ETA: 24s - loss: 2.0230 - regression_loss: 1.4894 - classification_loss: 0.5337\n",
      "478/500 [===========================>..] - ETA: 23s - loss: 2.0236 - regression_loss: 1.4899 - classification_loss: 0.5338\n",
      "479/500 [===========================>..] - ETA: 22s - loss: 2.0230 - regression_loss: 1.4894 - classification_loss: 0.5336\n",
      "480/500 [===========================>..] - ETA: 21s - loss: 2.0214 - regression_loss: 1.4882 - classification_loss: 0.5332\n",
      "481/500 [===========================>..] - ETA: 20s - loss: 2.0211 - regression_loss: 1.4883 - classification_loss: 0.5328\n",
      "482/500 [===========================>..] - ETA: 19s - loss: 2.0211 - regression_loss: 1.4883 - classification_loss: 0.5329\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 2.0198 - regression_loss: 1.4873 - classification_loss: 0.5324\n",
      "484/500 [============================>.] - ETA: 16s - loss: 2.0202 - regression_loss: 1.4880 - classification_loss: 0.5322\n",
      "485/500 [============================>.] - ETA: 15s - loss: 2.0188 - regression_loss: 1.4866 - classification_loss: 0.5323\n",
      "486/500 [============================>.] - ETA: 14s - loss: 2.0194 - regression_loss: 1.4873 - classification_loss: 0.5321\n",
      "487/500 [============================>.] - ETA: 13s - loss: 2.0200 - regression_loss: 1.4878 - classification_loss: 0.5322\n",
      "488/500 [============================>.] - ETA: 12s - loss: 2.0202 - regression_loss: 1.4877 - classification_loss: 0.5325\n",
      "489/500 [============================>.] - ETA: 11s - loss: 2.0220 - regression_loss: 1.4892 - classification_loss: 0.5328\n",
      "490/500 [============================>.] - ETA: 10s - loss: 2.0223 - regression_loss: 1.4894 - classification_loss: 0.5329\n",
      "491/500 [============================>.] - ETA: 9s - loss: 2.0234 - regression_loss: 1.4904 - classification_loss: 0.5329 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 2.0250 - regression_loss: 1.4913 - classification_loss: 0.5337\n",
      "493/500 [============================>.] - ETA: 7s - loss: 2.0238 - regression_loss: 1.4905 - classification_loss: 0.5333\n",
      "494/500 [============================>.] - ETA: 6s - loss: 2.0232 - regression_loss: 1.4900 - classification_loss: 0.5332\n",
      "495/500 [============================>.] - ETA: 5s - loss: 2.0223 - regression_loss: 1.4894 - classification_loss: 0.5329\n",
      "496/500 [============================>.] - ETA: 4s - loss: 2.0235 - regression_loss: 1.4905 - classification_loss: 0.5331\n",
      "497/500 [============================>.] - ETA: 3s - loss: 2.0231 - regression_loss: 1.4903 - classification_loss: 0.5328\n",
      "498/500 [============================>.] - ETA: 2s - loss: 2.0220 - regression_loss: 1.4896 - classification_loss: 0.5324\n",
      "499/500 [============================>.] - ETA: 1s - loss: 2.0230 - regression_loss: 1.4903 - classification_loss: 0.5328\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.0218 - regression_loss: 1.4893 - classification_loss: 0.5325\n",
      "Epoch 00008: saving model to ./snapshots\\resnet50_csv_08.h5\n",
      "\n",
      "500/500 [==============================] - 528s 1s/step - loss: 2.0218 - regression_loss: 1.4893 - classification_loss: 0.5325\n",
      "Epoch 9/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 1.5398 - regression_loss: 1.1936 - classification_loss: 0.3463\n",
      "  2/500 [..............................] - ETA: 4:09 - loss: 1.7697 - regression_loss: 1.3840 - classification_loss: 0.3857\n",
      "  3/500 [..............................] - ETA: 5:31 - loss: 1.7342 - regression_loss: 1.3640 - classification_loss: 0.3703\n",
      "  4/500 [..............................] - ETA: 6:23 - loss: 1.6210 - regression_loss: 1.2624 - classification_loss: 0.3586\n",
      "  5/500 [..............................] - ETA: 6:44 - loss: 1.9200 - regression_loss: 1.4418 - classification_loss: 0.4782\n",
      "  6/500 [..............................] - ETA: 7:06 - loss: 1.8004 - regression_loss: 1.3459 - classification_loss: 0.4544\n",
      "  7/500 [..............................] - ETA: 7:21 - loss: 1.7165 - regression_loss: 1.2797 - classification_loss: 0.4368\n",
      "  8/500 [..............................] - ETA: 7:26 - loss: 1.7061 - regression_loss: 1.2691 - classification_loss: 0.4371\n",
      "  9/500 [..............................] - ETA: 7:30 - loss: 1.7266 - regression_loss: 1.2935 - classification_loss: 0.4331\n",
      " 10/500 [..............................] - ETA: 7:33 - loss: 1.6841 - regression_loss: 1.2534 - classification_loss: 0.4307\n",
      " 11/500 [..............................] - ETA: 7:35 - loss: 1.7900 - regression_loss: 1.3280 - classification_loss: 0.4620\n",
      " 12/500 [..............................] - ETA: 7:36 - loss: 1.7614 - regression_loss: 1.3030 - classification_loss: 0.4584\n",
      " 13/500 [..............................] - ETA: 7:35 - loss: 1.8004 - regression_loss: 1.3378 - classification_loss: 0.4626\n",
      " 14/500 [..............................] - ETA: 7:40 - loss: 1.8843 - regression_loss: 1.4047 - classification_loss: 0.4796\n",
      " 15/500 [..............................] - ETA: 7:44 - loss: 1.8475 - regression_loss: 1.3815 - classification_loss: 0.4660\n",
      " 16/500 [..............................] - ETA: 7:45 - loss: 1.8610 - regression_loss: 1.3897 - classification_loss: 0.4713\n",
      " 17/500 [>.............................] - ETA: 7:38 - loss: 1.8485 - regression_loss: 1.3738 - classification_loss: 0.4748\n",
      " 18/500 [>.............................] - ETA: 7:43 - loss: 1.8357 - regression_loss: 1.3628 - classification_loss: 0.4729\n",
      " 19/500 [>.............................] - ETA: 7:43 - loss: 1.8781 - regression_loss: 1.3855 - classification_loss: 0.4926\n",
      " 20/500 [>.............................] - ETA: 7:45 - loss: 1.9059 - regression_loss: 1.4131 - classification_loss: 0.4928\n",
      " 21/500 [>.............................] - ETA: 7:44 - loss: 1.9129 - regression_loss: 1.4176 - classification_loss: 0.4953\n",
      " 22/500 [>.............................] - ETA: 7:44 - loss: 1.9242 - regression_loss: 1.4256 - classification_loss: 0.4986\n",
      " 23/500 [>.............................] - ETA: 7:46 - loss: 1.9134 - regression_loss: 1.4124 - classification_loss: 0.5010\n",
      " 24/500 [>.............................] - ETA: 7:42 - loss: 1.9088 - regression_loss: 1.4112 - classification_loss: 0.4976\n",
      " 25/500 [>.............................] - ETA: 7:43 - loss: 1.8935 - regression_loss: 1.3990 - classification_loss: 0.4945\n",
      " 26/500 [>.............................] - ETA: 7:41 - loss: 1.9072 - regression_loss: 1.4079 - classification_loss: 0.4993\n",
      " 27/500 [>.............................] - ETA: 7:41 - loss: 1.9259 - regression_loss: 1.4236 - classification_loss: 0.5023\n",
      " 28/500 [>.............................] - ETA: 7:44 - loss: 1.9530 - regression_loss: 1.4367 - classification_loss: 0.5163\n",
      " 29/500 [>.............................] - ETA: 7:43 - loss: 1.9556 - regression_loss: 1.4394 - classification_loss: 0.5162\n",
      " 30/500 [>.............................] - ETA: 7:44 - loss: 1.9723 - regression_loss: 1.4522 - classification_loss: 0.5201\n",
      " 31/500 [>.............................] - ETA: 7:43 - loss: 1.9811 - regression_loss: 1.4579 - classification_loss: 0.5232\n",
      " 32/500 [>.............................] - ETA: 7:42 - loss: 1.9675 - regression_loss: 1.4490 - classification_loss: 0.5185\n",
      " 33/500 [>.............................] - ETA: 7:41 - loss: 1.9783 - regression_loss: 1.4562 - classification_loss: 0.5221\n",
      " 34/500 [=>............................] - ETA: 7:42 - loss: 1.9999 - regression_loss: 1.4742 - classification_loss: 0.5257\n",
      " 35/500 [=>............................] - ETA: 7:41 - loss: 2.0170 - regression_loss: 1.4838 - classification_loss: 0.5331\n",
      " 36/500 [=>............................] - ETA: 7:41 - loss: 2.0077 - regression_loss: 1.4787 - classification_loss: 0.5289\n",
      " 37/500 [=>............................] - ETA: 7:41 - loss: 1.9864 - regression_loss: 1.4638 - classification_loss: 0.5227\n",
      " 38/500 [=>............................] - ETA: 7:40 - loss: 1.9768 - regression_loss: 1.4592 - classification_loss: 0.5176\n",
      " 39/500 [=>............................] - ETA: 7:42 - loss: 1.9855 - regression_loss: 1.4680 - classification_loss: 0.5175\n",
      " 40/500 [=>............................] - ETA: 7:40 - loss: 2.0027 - regression_loss: 1.4783 - classification_loss: 0.5243\n",
      " 41/500 [=>............................] - ETA: 7:39 - loss: 1.9989 - regression_loss: 1.4726 - classification_loss: 0.5263\n",
      " 42/500 [=>............................] - ETA: 7:38 - loss: 2.0043 - regression_loss: 1.4797 - classification_loss: 0.5246\n",
      " 43/500 [=>............................] - ETA: 7:38 - loss: 1.9997 - regression_loss: 1.4766 - classification_loss: 0.5231\n",
      " 44/500 [=>............................] - ETA: 7:35 - loss: 1.9854 - regression_loss: 1.4664 - classification_loss: 0.5190\n",
      " 45/500 [=>............................] - ETA: 7:37 - loss: 2.0095 - regression_loss: 1.4829 - classification_loss: 0.5266\n",
      " 46/500 [=>............................] - ETA: 7:37 - loss: 2.0291 - regression_loss: 1.4997 - classification_loss: 0.5294\n",
      " 47/500 [=>............................] - ETA: 7:37 - loss: 2.0257 - regression_loss: 1.4964 - classification_loss: 0.5293\n",
      " 48/500 [=>............................] - ETA: 7:36 - loss: 2.0160 - regression_loss: 1.4900 - classification_loss: 0.5260\n",
      " 49/500 [=>............................] - ETA: 7:35 - loss: 2.0285 - regression_loss: 1.4951 - classification_loss: 0.5334\n",
      " 50/500 [==>...........................] - ETA: 7:34 - loss: 2.0172 - regression_loss: 1.4871 - classification_loss: 0.5302\n",
      " 51/500 [==>...........................] - ETA: 7:34 - loss: 2.0171 - regression_loss: 1.4872 - classification_loss: 0.5299\n",
      " 52/500 [==>...........................] - ETA: 7:34 - loss: 2.0210 - regression_loss: 1.4924 - classification_loss: 0.5286\n",
      " 53/500 [==>...........................] - ETA: 7:32 - loss: 2.0289 - regression_loss: 1.4975 - classification_loss: 0.5313\n",
      " 54/500 [==>...........................] - ETA: 7:31 - loss: 2.0282 - regression_loss: 1.4983 - classification_loss: 0.5299\n",
      " 55/500 [==>...........................] - ETA: 7:31 - loss: 2.0246 - regression_loss: 1.4947 - classification_loss: 0.5299\n",
      " 56/500 [==>...........................] - ETA: 7:31 - loss: 2.0154 - regression_loss: 1.4889 - classification_loss: 0.5265\n",
      " 57/500 [==>...........................] - ETA: 7:30 - loss: 2.0075 - regression_loss: 1.4804 - classification_loss: 0.5271\n",
      " 58/500 [==>...........................] - ETA: 7:30 - loss: 2.0219 - regression_loss: 1.4922 - classification_loss: 0.5297\n",
      " 59/500 [==>...........................] - ETA: 7:29 - loss: 2.0206 - regression_loss: 1.4905 - classification_loss: 0.5301\n",
      " 60/500 [==>...........................] - ETA: 7:29 - loss: 2.0226 - regression_loss: 1.4888 - classification_loss: 0.5337\n",
      " 61/500 [==>...........................] - ETA: 7:28 - loss: 2.0186 - regression_loss: 1.4868 - classification_loss: 0.5318\n",
      " 62/500 [==>...........................] - ETA: 7:28 - loss: 2.0224 - regression_loss: 1.4885 - classification_loss: 0.5339\n",
      " 63/500 [==>...........................] - ETA: 7:27 - loss: 2.0236 - regression_loss: 1.4918 - classification_loss: 0.5318\n",
      " 64/500 [==>...........................] - ETA: 7:27 - loss: 2.0284 - regression_loss: 1.4952 - classification_loss: 0.5332\n",
      " 65/500 [==>...........................] - ETA: 7:25 - loss: 2.0212 - regression_loss: 1.4907 - classification_loss: 0.5305\n",
      " 66/500 [==>...........................] - ETA: 7:24 - loss: 2.0158 - regression_loss: 1.4877 - classification_loss: 0.5281\n",
      " 67/500 [===>..........................] - ETA: 7:23 - loss: 2.0166 - regression_loss: 1.4910 - classification_loss: 0.5256\n",
      " 68/500 [===>..........................] - ETA: 7:22 - loss: 2.0294 - regression_loss: 1.5026 - classification_loss: 0.5268\n",
      " 69/500 [===>..........................] - ETA: 7:21 - loss: 2.0393 - regression_loss: 1.5083 - classification_loss: 0.5310\n",
      " 70/500 [===>..........................] - ETA: 7:20 - loss: 2.0400 - regression_loss: 1.5120 - classification_loss: 0.5280\n",
      " 71/500 [===>..........................] - ETA: 7:18 - loss: 2.0366 - regression_loss: 1.5091 - classification_loss: 0.5276\n",
      " 72/500 [===>..........................] - ETA: 7:18 - loss: 2.0378 - regression_loss: 1.5100 - classification_loss: 0.5278\n",
      " 73/500 [===>..........................] - ETA: 7:17 - loss: 2.0414 - regression_loss: 1.5139 - classification_loss: 0.5275\n",
      " 74/500 [===>..........................] - ETA: 7:17 - loss: 2.0323 - regression_loss: 1.5076 - classification_loss: 0.5248\n",
      " 75/500 [===>..........................] - ETA: 7:15 - loss: 2.0276 - regression_loss: 1.5044 - classification_loss: 0.5232\n",
      " 76/500 [===>..........................] - ETA: 7:14 - loss: 2.0326 - regression_loss: 1.5094 - classification_loss: 0.5231\n",
      " 77/500 [===>..........................] - ETA: 7:13 - loss: 2.0321 - regression_loss: 1.5088 - classification_loss: 0.5233\n",
      " 78/500 [===>..........................] - ETA: 7:12 - loss: 2.0254 - regression_loss: 1.5036 - classification_loss: 0.5218\n",
      " 79/500 [===>..........................] - ETA: 7:10 - loss: 2.0158 - regression_loss: 1.4961 - classification_loss: 0.5197\n",
      " 80/500 [===>..........................] - ETA: 7:10 - loss: 2.0101 - regression_loss: 1.4909 - classification_loss: 0.5193\n",
      " 81/500 [===>..........................] - ETA: 7:08 - loss: 2.0005 - regression_loss: 1.4831 - classification_loss: 0.5174\n",
      " 82/500 [===>..........................] - ETA: 7:07 - loss: 1.9974 - regression_loss: 1.4817 - classification_loss: 0.5157\n",
      " 83/500 [===>..........................] - ETA: 7:06 - loss: 2.0013 - regression_loss: 1.4833 - classification_loss: 0.5180\n",
      " 84/500 [====>.........................] - ETA: 7:05 - loss: 2.0044 - regression_loss: 1.4878 - classification_loss: 0.5165\n",
      " 85/500 [====>.........................] - ETA: 7:04 - loss: 2.0032 - regression_loss: 1.4871 - classification_loss: 0.5161\n",
      " 86/500 [====>.........................] - ETA: 7:03 - loss: 2.0097 - regression_loss: 1.4911 - classification_loss: 0.5187\n",
      " 87/500 [====>.........................] - ETA: 7:02 - loss: 2.0015 - regression_loss: 1.4845 - classification_loss: 0.5170\n",
      " 88/500 [====>.........................] - ETA: 7:01 - loss: 2.0115 - regression_loss: 1.4915 - classification_loss: 0.5200\n",
      " 89/500 [====>.........................] - ETA: 7:01 - loss: 2.0113 - regression_loss: 1.4906 - classification_loss: 0.5206\n",
      " 90/500 [====>.........................] - ETA: 7:00 - loss: 2.0203 - regression_loss: 1.4955 - classification_loss: 0.5248\n",
      " 91/500 [====>.........................] - ETA: 6:58 - loss: 2.0148 - regression_loss: 1.4931 - classification_loss: 0.5217\n",
      " 92/500 [====>.........................] - ETA: 6:58 - loss: 2.0217 - regression_loss: 1.4975 - classification_loss: 0.5242\n",
      " 93/500 [====>.........................] - ETA: 7:00 - loss: 2.0240 - regression_loss: 1.4984 - classification_loss: 0.5256\n",
      " 94/500 [====>.........................] - ETA: 6:59 - loss: 2.0144 - regression_loss: 1.4913 - classification_loss: 0.5231\n",
      " 95/500 [====>.........................] - ETA: 6:58 - loss: 2.0098 - regression_loss: 1.4874 - classification_loss: 0.5224\n",
      " 96/500 [====>.........................] - ETA: 6:57 - loss: 2.0060 - regression_loss: 1.4844 - classification_loss: 0.5216\n",
      " 97/500 [====>.........................] - ETA: 6:56 - loss: 2.0052 - regression_loss: 1.4846 - classification_loss: 0.5205\n",
      " 98/500 [====>.........................] - ETA: 6:55 - loss: 2.0134 - regression_loss: 1.4912 - classification_loss: 0.5223\n",
      " 99/500 [====>.........................] - ETA: 6:54 - loss: 2.0174 - regression_loss: 1.4961 - classification_loss: 0.5213\n",
      "100/500 [=====>........................] - ETA: 6:52 - loss: 2.0140 - regression_loss: 1.4945 - classification_loss: 0.5195\n",
      "101/500 [=====>........................] - ETA: 6:51 - loss: 2.0204 - regression_loss: 1.4979 - classification_loss: 0.5225\n",
      "102/500 [=====>........................] - ETA: 6:50 - loss: 2.0108 - regression_loss: 1.4915 - classification_loss: 0.5193\n",
      "103/500 [=====>........................] - ETA: 6:49 - loss: 2.0194 - regression_loss: 1.4990 - classification_loss: 0.5204\n",
      "104/500 [=====>........................] - ETA: 6:48 - loss: 2.0170 - regression_loss: 1.4981 - classification_loss: 0.5189\n",
      "105/500 [=====>........................] - ETA: 6:47 - loss: 2.0200 - regression_loss: 1.4994 - classification_loss: 0.5206\n",
      "106/500 [=====>........................] - ETA: 6:46 - loss: 2.0300 - regression_loss: 1.5053 - classification_loss: 0.5246\n",
      "107/500 [=====>........................] - ETA: 6:45 - loss: 2.0287 - regression_loss: 1.5039 - classification_loss: 0.5248\n",
      "108/500 [=====>........................] - ETA: 6:44 - loss: 2.0281 - regression_loss: 1.5042 - classification_loss: 0.5239\n",
      "109/500 [=====>........................] - ETA: 6:43 - loss: 2.0234 - regression_loss: 1.5000 - classification_loss: 0.5233\n",
      "110/500 [=====>........................] - ETA: 6:42 - loss: 2.0218 - regression_loss: 1.4980 - classification_loss: 0.5238\n",
      "111/500 [=====>........................] - ETA: 6:41 - loss: 2.0216 - regression_loss: 1.4986 - classification_loss: 0.5230\n",
      "112/500 [=====>........................] - ETA: 6:40 - loss: 2.0199 - regression_loss: 1.4982 - classification_loss: 0.5217\n",
      "113/500 [=====>........................] - ETA: 6:39 - loss: 2.0145 - regression_loss: 1.4940 - classification_loss: 0.5205\n",
      "114/500 [=====>........................] - ETA: 6:38 - loss: 2.0162 - regression_loss: 1.4954 - classification_loss: 0.5208\n",
      "115/500 [=====>........................] - ETA: 6:38 - loss: 2.0118 - regression_loss: 1.4915 - classification_loss: 0.5203\n",
      "116/500 [=====>........................] - ETA: 6:37 - loss: 2.0173 - regression_loss: 1.4963 - classification_loss: 0.5210\n",
      "117/500 [======>.......................] - ETA: 6:36 - loss: 2.0205 - regression_loss: 1.5001 - classification_loss: 0.5204\n",
      "118/500 [======>.......................] - ETA: 6:39 - loss: 2.0207 - regression_loss: 1.5009 - classification_loss: 0.5198\n",
      "119/500 [======>.......................] - ETA: 6:38 - loss: 2.0191 - regression_loss: 1.5001 - classification_loss: 0.5190\n",
      "120/500 [======>.......................] - ETA: 6:37 - loss: 2.0186 - regression_loss: 1.5006 - classification_loss: 0.5179\n",
      "121/500 [======>.......................] - ETA: 6:36 - loss: 2.0163 - regression_loss: 1.4998 - classification_loss: 0.5165\n",
      "122/500 [======>.......................] - ETA: 6:35 - loss: 2.0137 - regression_loss: 1.4970 - classification_loss: 0.5167\n",
      "123/500 [======>.......................] - ETA: 6:34 - loss: 2.0086 - regression_loss: 1.4934 - classification_loss: 0.5152\n",
      "124/500 [======>.......................] - ETA: 6:32 - loss: 2.0072 - regression_loss: 1.4921 - classification_loss: 0.5152\n",
      "125/500 [======>.......................] - ETA: 6:31 - loss: 2.0058 - regression_loss: 1.4912 - classification_loss: 0.5146\n",
      "126/500 [======>.......................] - ETA: 6:30 - loss: 2.0057 - regression_loss: 1.4909 - classification_loss: 0.5149\n",
      "127/500 [======>.......................] - ETA: 6:29 - loss: 2.0022 - regression_loss: 1.4887 - classification_loss: 0.5135\n",
      "128/500 [======>.......................] - ETA: 6:28 - loss: 2.0015 - regression_loss: 1.4889 - classification_loss: 0.5126\n",
      "129/500 [======>.......................] - ETA: 6:26 - loss: 2.0013 - regression_loss: 1.4893 - classification_loss: 0.5119\n",
      "130/500 [======>.......................] - ETA: 6:26 - loss: 2.0032 - regression_loss: 1.4903 - classification_loss: 0.5129\n",
      "131/500 [======>.......................] - ETA: 6:25 - loss: 1.9967 - regression_loss: 1.4845 - classification_loss: 0.5122\n",
      "132/500 [======>.......................] - ETA: 6:24 - loss: 1.9966 - regression_loss: 1.4852 - classification_loss: 0.5115\n",
      "133/500 [======>.......................] - ETA: 6:23 - loss: 1.9971 - regression_loss: 1.4862 - classification_loss: 0.5109\n",
      "134/500 [=======>......................] - ETA: 6:22 - loss: 2.0018 - regression_loss: 1.4891 - classification_loss: 0.5127\n",
      "135/500 [=======>......................] - ETA: 6:21 - loss: 2.0063 - regression_loss: 1.4921 - classification_loss: 0.5143\n",
      "136/500 [=======>......................] - ETA: 6:20 - loss: 2.0147 - regression_loss: 1.4970 - classification_loss: 0.5177\n",
      "137/500 [=======>......................] - ETA: 6:19 - loss: 2.0131 - regression_loss: 1.4967 - classification_loss: 0.5164\n",
      "138/500 [=======>......................] - ETA: 6:18 - loss: 2.0089 - regression_loss: 1.4938 - classification_loss: 0.5151\n",
      "139/500 [=======>......................] - ETA: 6:17 - loss: 2.0122 - regression_loss: 1.4958 - classification_loss: 0.5164\n",
      "140/500 [=======>......................] - ETA: 6:16 - loss: 2.0110 - regression_loss: 1.4952 - classification_loss: 0.5158\n",
      "141/500 [=======>......................] - ETA: 6:15 - loss: 2.0117 - regression_loss: 1.4956 - classification_loss: 0.5160\n",
      "142/500 [=======>......................] - ETA: 6:13 - loss: 2.0104 - regression_loss: 1.4950 - classification_loss: 0.5155\n",
      "143/500 [=======>......................] - ETA: 6:12 - loss: 2.0103 - regression_loss: 1.4944 - classification_loss: 0.5158\n",
      "144/500 [=======>......................] - ETA: 6:12 - loss: 2.0103 - regression_loss: 1.4929 - classification_loss: 0.5174\n",
      "145/500 [=======>......................] - ETA: 6:11 - loss: 2.0064 - regression_loss: 1.4904 - classification_loss: 0.5161\n",
      "146/500 [=======>......................] - ETA: 6:10 - loss: 2.0075 - regression_loss: 1.4916 - classification_loss: 0.5158\n",
      "147/500 [=======>......................] - ETA: 6:08 - loss: 2.0129 - regression_loss: 1.4949 - classification_loss: 0.5180\n",
      "148/500 [=======>......................] - ETA: 6:07 - loss: 2.0150 - regression_loss: 1.4963 - classification_loss: 0.5187\n",
      "149/500 [=======>......................] - ETA: 6:06 - loss: 2.0089 - regression_loss: 1.4916 - classification_loss: 0.5173\n",
      "150/500 [========>.....................] - ETA: 6:05 - loss: 2.0158 - regression_loss: 1.4955 - classification_loss: 0.5203\n",
      "151/500 [========>.....................] - ETA: 6:04 - loss: 2.0233 - regression_loss: 1.5019 - classification_loss: 0.5214\n",
      "152/500 [========>.....................] - ETA: 6:03 - loss: 2.0281 - regression_loss: 1.5047 - classification_loss: 0.5233\n",
      "153/500 [========>.....................] - ETA: 6:02 - loss: 2.0330 - regression_loss: 1.5087 - classification_loss: 0.5243\n",
      "154/500 [========>.....................] - ETA: 6:01 - loss: 2.0321 - regression_loss: 1.5080 - classification_loss: 0.5241\n",
      "155/500 [========>.....................] - ETA: 5:59 - loss: 2.0308 - regression_loss: 1.5067 - classification_loss: 0.5241\n",
      "156/500 [========>.....................] - ETA: 5:59 - loss: 2.0277 - regression_loss: 1.5047 - classification_loss: 0.5231\n",
      "157/500 [========>.....................] - ETA: 5:58 - loss: 2.0285 - regression_loss: 1.5051 - classification_loss: 0.5233\n",
      "158/500 [========>.....................] - ETA: 5:57 - loss: 2.0322 - regression_loss: 1.5081 - classification_loss: 0.5241\n",
      "159/500 [========>.....................] - ETA: 5:55 - loss: 2.0300 - regression_loss: 1.5064 - classification_loss: 0.5236\n",
      "160/500 [========>.....................] - ETA: 5:54 - loss: 2.0291 - regression_loss: 1.5052 - classification_loss: 0.5239\n",
      "161/500 [========>.....................] - ETA: 5:57 - loss: 2.0269 - regression_loss: 1.5039 - classification_loss: 0.5230\n",
      "162/500 [========>.....................] - ETA: 5:56 - loss: 2.0251 - regression_loss: 1.5019 - classification_loss: 0.5233\n",
      "163/500 [========>.....................] - ETA: 5:55 - loss: 2.0265 - regression_loss: 1.5031 - classification_loss: 0.5234\n",
      "164/500 [========>.....................] - ETA: 5:54 - loss: 2.0219 - regression_loss: 1.4997 - classification_loss: 0.5222\n",
      "165/500 [========>.....................] - ETA: 5:53 - loss: 2.0232 - regression_loss: 1.5012 - classification_loss: 0.5220\n",
      "166/500 [========>.....................] - ETA: 5:52 - loss: 2.0237 - regression_loss: 1.5016 - classification_loss: 0.5221\n",
      "167/500 [=========>....................] - ETA: 5:51 - loss: 2.0188 - regression_loss: 1.4980 - classification_loss: 0.5208\n",
      "168/500 [=========>....................] - ETA: 5:49 - loss: 2.0209 - regression_loss: 1.4998 - classification_loss: 0.5211\n",
      "169/500 [=========>....................] - ETA: 5:49 - loss: 2.0221 - regression_loss: 1.5001 - classification_loss: 0.5220\n",
      "170/500 [=========>....................] - ETA: 5:47 - loss: 2.0238 - regression_loss: 1.5008 - classification_loss: 0.5230\n",
      "171/500 [=========>....................] - ETA: 5:46 - loss: 2.0210 - regression_loss: 1.4990 - classification_loss: 0.5220\n",
      "172/500 [=========>....................] - ETA: 5:45 - loss: 2.0192 - regression_loss: 1.4969 - classification_loss: 0.5223\n",
      "173/500 [=========>....................] - ETA: 5:44 - loss: 2.0182 - regression_loss: 1.4962 - classification_loss: 0.5220\n",
      "174/500 [=========>....................] - ETA: 5:43 - loss: 2.0131 - regression_loss: 1.4920 - classification_loss: 0.5211\n",
      "175/500 [=========>....................] - ETA: 5:42 - loss: 2.0126 - regression_loss: 1.4917 - classification_loss: 0.5209\n",
      "176/500 [=========>....................] - ETA: 5:41 - loss: 2.0188 - regression_loss: 1.4965 - classification_loss: 0.5223\n",
      "177/500 [=========>....................] - ETA: 5:40 - loss: 2.0163 - regression_loss: 1.4943 - classification_loss: 0.5220\n",
      "178/500 [=========>....................] - ETA: 5:38 - loss: 2.0143 - regression_loss: 1.4931 - classification_loss: 0.5211\n",
      "179/500 [=========>....................] - ETA: 5:37 - loss: 2.0183 - regression_loss: 1.4962 - classification_loss: 0.5221\n",
      "180/500 [=========>....................] - ETA: 5:36 - loss: 2.0237 - regression_loss: 1.5005 - classification_loss: 0.5232\n",
      "181/500 [=========>....................] - ETA: 5:35 - loss: 2.0233 - regression_loss: 1.5000 - classification_loss: 0.5233\n",
      "182/500 [=========>....................] - ETA: 5:34 - loss: 2.0254 - regression_loss: 1.5014 - classification_loss: 0.5240\n",
      "183/500 [=========>....................] - ETA: 5:33 - loss: 2.0288 - regression_loss: 1.5037 - classification_loss: 0.5251\n",
      "184/500 [==========>...................] - ETA: 5:32 - loss: 2.0294 - regression_loss: 1.5043 - classification_loss: 0.5251\n",
      "185/500 [==========>...................] - ETA: 5:31 - loss: 2.0255 - regression_loss: 1.5013 - classification_loss: 0.5243\n",
      "186/500 [==========>...................] - ETA: 5:30 - loss: 2.0231 - regression_loss: 1.4999 - classification_loss: 0.5232\n",
      "187/500 [==========>...................] - ETA: 5:29 - loss: 2.0220 - regression_loss: 1.4991 - classification_loss: 0.5228\n",
      "188/500 [==========>...................] - ETA: 5:28 - loss: 2.0169 - regression_loss: 1.4949 - classification_loss: 0.5220\n",
      "189/500 [==========>...................] - ETA: 5:26 - loss: 2.0187 - regression_loss: 1.4970 - classification_loss: 0.5217\n",
      "190/500 [==========>...................] - ETA: 5:25 - loss: 2.0188 - regression_loss: 1.4975 - classification_loss: 0.5213\n",
      "191/500 [==========>...................] - ETA: 5:24 - loss: 2.0188 - regression_loss: 1.4966 - classification_loss: 0.5222\n",
      "192/500 [==========>...................] - ETA: 5:23 - loss: 2.0173 - regression_loss: 1.4954 - classification_loss: 0.5219\n",
      "193/500 [==========>...................] - ETA: 5:22 - loss: 2.0189 - regression_loss: 1.4956 - classification_loss: 0.5233\n",
      "194/500 [==========>...................] - ETA: 5:21 - loss: 2.0193 - regression_loss: 1.4959 - classification_loss: 0.5234\n",
      "195/500 [==========>...................] - ETA: 5:20 - loss: 2.0194 - regression_loss: 1.4962 - classification_loss: 0.5233\n",
      "196/500 [==========>...................] - ETA: 5:19 - loss: 2.0180 - regression_loss: 1.4950 - classification_loss: 0.5230\n",
      "197/500 [==========>...................] - ETA: 5:18 - loss: 2.0176 - regression_loss: 1.4949 - classification_loss: 0.5227\n",
      "198/500 [==========>...................] - ETA: 5:17 - loss: 2.0195 - regression_loss: 1.4974 - classification_loss: 0.5221\n",
      "199/500 [==========>...................] - ETA: 5:15 - loss: 2.0221 - regression_loss: 1.5003 - classification_loss: 0.5219\n",
      "200/500 [===========>..................] - ETA: 5:14 - loss: 2.0212 - regression_loss: 1.5000 - classification_loss: 0.5212\n",
      "201/500 [===========>..................] - ETA: 5:13 - loss: 2.0193 - regression_loss: 1.4993 - classification_loss: 0.5200\n",
      "202/500 [===========>..................] - ETA: 5:12 - loss: 2.0232 - regression_loss: 1.5026 - classification_loss: 0.5207\n",
      "203/500 [===========>..................] - ETA: 5:11 - loss: 2.0226 - regression_loss: 1.5030 - classification_loss: 0.5196\n",
      "204/500 [===========>..................] - ETA: 5:10 - loss: 2.0235 - regression_loss: 1.5039 - classification_loss: 0.5196\n",
      "205/500 [===========>..................] - ETA: 5:09 - loss: 2.0240 - regression_loss: 1.5045 - classification_loss: 0.5194\n",
      "206/500 [===========>..................] - ETA: 5:08 - loss: 2.0227 - regression_loss: 1.5037 - classification_loss: 0.5190\n",
      "207/500 [===========>..................] - ETA: 5:07 - loss: 2.0210 - regression_loss: 1.5021 - classification_loss: 0.5189\n",
      "208/500 [===========>..................] - ETA: 5:06 - loss: 2.0206 - regression_loss: 1.5018 - classification_loss: 0.5188\n",
      "209/500 [===========>..................] - ETA: 5:05 - loss: 2.0192 - regression_loss: 1.5006 - classification_loss: 0.5186\n",
      "210/500 [===========>..................] - ETA: 5:03 - loss: 2.0166 - regression_loss: 1.4986 - classification_loss: 0.5180\n",
      "211/500 [===========>..................] - ETA: 5:02 - loss: 2.0171 - regression_loss: 1.4998 - classification_loss: 0.5173\n",
      "212/500 [===========>..................] - ETA: 5:01 - loss: 2.0195 - regression_loss: 1.5018 - classification_loss: 0.5178\n",
      "213/500 [===========>..................] - ETA: 5:00 - loss: 2.0151 - regression_loss: 1.4979 - classification_loss: 0.5172\n",
      "214/500 [===========>..................] - ETA: 4:59 - loss: 2.0169 - regression_loss: 1.4995 - classification_loss: 0.5174\n",
      "215/500 [===========>..................] - ETA: 4:58 - loss: 2.0193 - regression_loss: 1.5012 - classification_loss: 0.5181\n",
      "216/500 [===========>..................] - ETA: 4:57 - loss: 2.0185 - regression_loss: 1.5007 - classification_loss: 0.5178\n",
      "217/500 [============>.................] - ETA: 4:56 - loss: 2.0198 - regression_loss: 1.5015 - classification_loss: 0.5183\n",
      "218/500 [============>.................] - ETA: 4:55 - loss: 2.0213 - regression_loss: 1.5023 - classification_loss: 0.5189\n",
      "219/500 [============>.................] - ETA: 4:54 - loss: 2.0207 - regression_loss: 1.5021 - classification_loss: 0.5186\n",
      "220/500 [============>.................] - ETA: 4:53 - loss: 2.0248 - regression_loss: 1.5051 - classification_loss: 0.5197\n",
      "221/500 [============>.................] - ETA: 4:51 - loss: 2.0260 - regression_loss: 1.5059 - classification_loss: 0.5201\n",
      "222/500 [============>.................] - ETA: 4:50 - loss: 2.0254 - regression_loss: 1.5059 - classification_loss: 0.5194\n",
      "223/500 [============>.................] - ETA: 4:50 - loss: 2.0250 - regression_loss: 1.5056 - classification_loss: 0.5194\n",
      "224/500 [============>.................] - ETA: 4:48 - loss: 2.0257 - regression_loss: 1.5056 - classification_loss: 0.5201\n",
      "225/500 [============>.................] - ETA: 4:47 - loss: 2.0213 - regression_loss: 1.5024 - classification_loss: 0.5188\n",
      "226/500 [============>.................] - ETA: 4:46 - loss: 2.0202 - regression_loss: 1.5014 - classification_loss: 0.5188\n",
      "227/500 [============>.................] - ETA: 4:45 - loss: 2.0184 - regression_loss: 1.5003 - classification_loss: 0.5182\n",
      "228/500 [============>.................] - ETA: 4:44 - loss: 2.0172 - regression_loss: 1.4994 - classification_loss: 0.5178\n",
      "229/500 [============>.................] - ETA: 4:43 - loss: 2.0156 - regression_loss: 1.4981 - classification_loss: 0.5175\n",
      "230/500 [============>.................] - ETA: 4:42 - loss: 2.0133 - regression_loss: 1.4965 - classification_loss: 0.5167\n",
      "231/500 [============>.................] - ETA: 4:41 - loss: 2.0114 - regression_loss: 1.4950 - classification_loss: 0.5164\n",
      "232/500 [============>.................] - ETA: 4:40 - loss: 2.0119 - regression_loss: 1.4954 - classification_loss: 0.5165\n",
      "233/500 [============>.................] - ETA: 4:38 - loss: 2.0127 - regression_loss: 1.4962 - classification_loss: 0.5165\n",
      "234/500 [=============>................] - ETA: 4:37 - loss: 2.0105 - regression_loss: 1.4946 - classification_loss: 0.5159\n",
      "235/500 [=============>................] - ETA: 4:36 - loss: 2.0168 - regression_loss: 1.4993 - classification_loss: 0.5175\n",
      "236/500 [=============>................] - ETA: 4:35 - loss: 2.0194 - regression_loss: 1.5020 - classification_loss: 0.5174\n",
      "237/500 [=============>................] - ETA: 4:34 - loss: 2.0194 - regression_loss: 1.5018 - classification_loss: 0.5177\n",
      "238/500 [=============>................] - ETA: 4:33 - loss: 2.0215 - regression_loss: 1.5027 - classification_loss: 0.5189\n",
      "239/500 [=============>................] - ETA: 4:32 - loss: 2.0207 - regression_loss: 1.5022 - classification_loss: 0.5185\n",
      "240/500 [=============>................] - ETA: 4:31 - loss: 2.0163 - regression_loss: 1.4984 - classification_loss: 0.5178\n",
      "241/500 [=============>................] - ETA: 4:30 - loss: 2.0175 - regression_loss: 1.4988 - classification_loss: 0.5187\n",
      "242/500 [=============>................] - ETA: 4:29 - loss: 2.0205 - regression_loss: 1.5006 - classification_loss: 0.5199\n",
      "243/500 [=============>................] - ETA: 4:28 - loss: 2.0202 - regression_loss: 1.5006 - classification_loss: 0.5196\n",
      "244/500 [=============>................] - ETA: 4:27 - loss: 2.0234 - regression_loss: 1.5024 - classification_loss: 0.5210\n",
      "245/500 [=============>................] - ETA: 4:25 - loss: 2.0231 - regression_loss: 1.5029 - classification_loss: 0.5203\n",
      "246/500 [=============>................] - ETA: 4:24 - loss: 2.0236 - regression_loss: 1.5033 - classification_loss: 0.5203\n",
      "247/500 [=============>................] - ETA: 4:23 - loss: 2.0230 - regression_loss: 1.5031 - classification_loss: 0.5198\n",
      "248/500 [=============>................] - ETA: 4:22 - loss: 2.0221 - regression_loss: 1.5025 - classification_loss: 0.5196\n",
      "249/500 [=============>................] - ETA: 4:21 - loss: 2.0222 - regression_loss: 1.5026 - classification_loss: 0.5196\n",
      "250/500 [==============>...............] - ETA: 4:20 - loss: 2.0218 - regression_loss: 1.5022 - classification_loss: 0.5196\n",
      "251/500 [==============>...............] - ETA: 4:19 - loss: 2.0218 - regression_loss: 1.5020 - classification_loss: 0.5198\n",
      "252/500 [==============>...............] - ETA: 4:18 - loss: 2.0238 - regression_loss: 1.5039 - classification_loss: 0.5199\n",
      "253/500 [==============>...............] - ETA: 4:17 - loss: 2.0249 - regression_loss: 1.5049 - classification_loss: 0.5200\n",
      "254/500 [==============>...............] - ETA: 4:16 - loss: 2.0235 - regression_loss: 1.5041 - classification_loss: 0.5194\n",
      "255/500 [==============>...............] - ETA: 4:15 - loss: 2.0229 - regression_loss: 1.5038 - classification_loss: 0.5191\n",
      "256/500 [==============>...............] - ETA: 4:14 - loss: 2.0213 - regression_loss: 1.5025 - classification_loss: 0.5189\n",
      "257/500 [==============>...............] - ETA: 4:13 - loss: 2.0206 - regression_loss: 1.5020 - classification_loss: 0.5186\n",
      "258/500 [==============>...............] - ETA: 4:12 - loss: 2.0172 - regression_loss: 1.4996 - classification_loss: 0.5176\n",
      "259/500 [==============>...............] - ETA: 4:11 - loss: 2.0144 - regression_loss: 1.4978 - classification_loss: 0.5166\n",
      "260/500 [==============>...............] - ETA: 4:10 - loss: 2.0125 - regression_loss: 1.4965 - classification_loss: 0.5160\n",
      "261/500 [==============>...............] - ETA: 4:09 - loss: 2.0113 - regression_loss: 1.4952 - classification_loss: 0.5161\n",
      "262/500 [==============>...............] - ETA: 4:08 - loss: 2.0090 - regression_loss: 1.4934 - classification_loss: 0.5155\n",
      "263/500 [==============>...............] - ETA: 4:07 - loss: 2.0080 - regression_loss: 1.4928 - classification_loss: 0.5152\n",
      "264/500 [==============>...............] - ETA: 4:06 - loss: 2.0119 - regression_loss: 1.4953 - classification_loss: 0.5165\n",
      "265/500 [==============>...............] - ETA: 4:05 - loss: 2.0098 - regression_loss: 1.4936 - classification_loss: 0.5161\n",
      "266/500 [==============>...............] - ETA: 4:03 - loss: 2.0091 - regression_loss: 1.4932 - classification_loss: 0.5159\n",
      "267/500 [===============>..............] - ETA: 4:02 - loss: 2.0143 - regression_loss: 1.4964 - classification_loss: 0.5180\n",
      "268/500 [===============>..............] - ETA: 4:01 - loss: 2.0118 - regression_loss: 1.4941 - classification_loss: 0.5177\n",
      "269/500 [===============>..............] - ETA: 4:00 - loss: 2.0106 - regression_loss: 1.4932 - classification_loss: 0.5174\n",
      "270/500 [===============>..............] - ETA: 3:59 - loss: 2.0069 - regression_loss: 1.4903 - classification_loss: 0.5166\n",
      "271/500 [===============>..............] - ETA: 3:58 - loss: 2.0066 - regression_loss: 1.4899 - classification_loss: 0.5166\n",
      "272/500 [===============>..............] - ETA: 3:57 - loss: 2.0077 - regression_loss: 1.4902 - classification_loss: 0.5175\n",
      "273/500 [===============>..............] - ETA: 3:56 - loss: 2.0083 - regression_loss: 1.4907 - classification_loss: 0.5176\n",
      "274/500 [===============>..............] - ETA: 3:55 - loss: 2.0063 - regression_loss: 1.4893 - classification_loss: 0.5170\n",
      "275/500 [===============>..............] - ETA: 3:54 - loss: 2.0045 - regression_loss: 1.4880 - classification_loss: 0.5165\n",
      "276/500 [===============>..............] - ETA: 3:56 - loss: 2.0080 - regression_loss: 1.4894 - classification_loss: 0.5186\n",
      "277/500 [===============>..............] - ETA: 3:55 - loss: 2.0090 - regression_loss: 1.4903 - classification_loss: 0.5187\n",
      "278/500 [===============>..............] - ETA: 3:54 - loss: 2.0090 - regression_loss: 1.4908 - classification_loss: 0.5182\n",
      "279/500 [===============>..............] - ETA: 3:53 - loss: 2.0102 - regression_loss: 1.4917 - classification_loss: 0.5185\n",
      "280/500 [===============>..............] - ETA: 3:52 - loss: 2.0109 - regression_loss: 1.4922 - classification_loss: 0.5187\n",
      "281/500 [===============>..............] - ETA: 3:51 - loss: 2.0094 - regression_loss: 1.4913 - classification_loss: 0.5181\n",
      "282/500 [===============>..............] - ETA: 3:50 - loss: 2.0093 - regression_loss: 1.4915 - classification_loss: 0.5178\n",
      "283/500 [===============>..............] - ETA: 3:48 - loss: 2.0114 - regression_loss: 1.4931 - classification_loss: 0.5182\n",
      "284/500 [================>.............] - ETA: 3:47 - loss: 2.0094 - regression_loss: 1.4923 - classification_loss: 0.5172\n",
      "285/500 [================>.............] - ETA: 3:46 - loss: 2.0057 - regression_loss: 1.4893 - classification_loss: 0.5164\n",
      "286/500 [================>.............] - ETA: 3:45 - loss: 2.0077 - regression_loss: 1.4908 - classification_loss: 0.5169\n",
      "287/500 [================>.............] - ETA: 3:44 - loss: 2.0089 - regression_loss: 1.4917 - classification_loss: 0.5172\n",
      "288/500 [================>.............] - ETA: 3:43 - loss: 2.0079 - regression_loss: 1.4907 - classification_loss: 0.5172\n",
      "289/500 [================>.............] - ETA: 3:42 - loss: 2.0071 - regression_loss: 1.4891 - classification_loss: 0.5180\n",
      "290/500 [================>.............] - ETA: 3:41 - loss: 2.0084 - regression_loss: 1.4894 - classification_loss: 0.5190\n",
      "291/500 [================>.............] - ETA: 3:40 - loss: 2.0078 - regression_loss: 1.4891 - classification_loss: 0.5187\n",
      "292/500 [================>.............] - ETA: 3:39 - loss: 2.0085 - regression_loss: 1.4896 - classification_loss: 0.5189\n",
      "293/500 [================>.............] - ETA: 3:38 - loss: 2.0061 - regression_loss: 1.4879 - classification_loss: 0.5182\n",
      "294/500 [================>.............] - ETA: 3:37 - loss: 2.0059 - regression_loss: 1.4881 - classification_loss: 0.5178\n",
      "295/500 [================>.............] - ETA: 3:36 - loss: 2.0039 - regression_loss: 1.4863 - classification_loss: 0.5176\n",
      "296/500 [================>.............] - ETA: 3:35 - loss: 2.0009 - regression_loss: 1.4842 - classification_loss: 0.5166\n",
      "297/500 [================>.............] - ETA: 3:33 - loss: 2.0021 - regression_loss: 1.4850 - classification_loss: 0.5171\n",
      "298/500 [================>.............] - ETA: 3:32 - loss: 2.0024 - regression_loss: 1.4856 - classification_loss: 0.5168\n",
      "299/500 [================>.............] - ETA: 3:31 - loss: 2.0015 - regression_loss: 1.4851 - classification_loss: 0.5164\n",
      "300/500 [=================>............] - ETA: 3:30 - loss: 2.0009 - regression_loss: 1.4845 - classification_loss: 0.5164\n",
      "301/500 [=================>............] - ETA: 3:29 - loss: 2.0011 - regression_loss: 1.4847 - classification_loss: 0.5164\n",
      "302/500 [=================>............] - ETA: 3:28 - loss: 2.0013 - regression_loss: 1.4849 - classification_loss: 0.5164\n",
      "303/500 [=================>............] - ETA: 3:27 - loss: 2.0017 - regression_loss: 1.4848 - classification_loss: 0.5169\n",
      "304/500 [=================>............] - ETA: 3:26 - loss: 2.0041 - regression_loss: 1.4863 - classification_loss: 0.5178\n",
      "305/500 [=================>............] - ETA: 3:25 - loss: 2.0031 - regression_loss: 1.4858 - classification_loss: 0.5174\n",
      "306/500 [=================>............] - ETA: 3:24 - loss: 2.0044 - regression_loss: 1.4868 - classification_loss: 0.5176\n",
      "307/500 [=================>............] - ETA: 3:23 - loss: 2.0080 - regression_loss: 1.4899 - classification_loss: 0.5181\n",
      "308/500 [=================>............] - ETA: 3:22 - loss: 2.0086 - regression_loss: 1.4909 - classification_loss: 0.5177\n",
      "309/500 [=================>............] - ETA: 3:21 - loss: 2.0124 - regression_loss: 1.4933 - classification_loss: 0.5191\n",
      "310/500 [=================>............] - ETA: 3:19 - loss: 2.0144 - regression_loss: 1.4952 - classification_loss: 0.5192\n",
      "311/500 [=================>............] - ETA: 3:18 - loss: 2.0138 - regression_loss: 1.4945 - classification_loss: 0.5193\n",
      "312/500 [=================>............] - ETA: 3:17 - loss: 2.0140 - regression_loss: 1.4948 - classification_loss: 0.5193\n",
      "313/500 [=================>............] - ETA: 3:16 - loss: 2.0143 - regression_loss: 1.4948 - classification_loss: 0.5195\n",
      "314/500 [=================>............] - ETA: 3:15 - loss: 2.0168 - regression_loss: 1.4966 - classification_loss: 0.5202\n",
      "315/500 [=================>............] - ETA: 3:14 - loss: 2.0178 - regression_loss: 1.4976 - classification_loss: 0.5202\n",
      "316/500 [=================>............] - ETA: 3:13 - loss: 2.0179 - regression_loss: 1.4979 - classification_loss: 0.5199\n",
      "317/500 [==================>...........] - ETA: 3:12 - loss: 2.0172 - regression_loss: 1.4974 - classification_loss: 0.5199\n",
      "318/500 [==================>...........] - ETA: 3:12 - loss: 2.0209 - regression_loss: 1.4997 - classification_loss: 0.5212\n",
      "319/500 [==================>...........] - ETA: 3:11 - loss: 2.0216 - regression_loss: 1.5001 - classification_loss: 0.5214\n",
      "320/500 [==================>...........] - ETA: 3:10 - loss: 2.0234 - regression_loss: 1.5022 - classification_loss: 0.5211\n",
      "321/500 [==================>...........] - ETA: 3:09 - loss: 2.0246 - regression_loss: 1.5032 - classification_loss: 0.5213\n",
      "322/500 [==================>...........] - ETA: 3:07 - loss: 2.0245 - regression_loss: 1.5036 - classification_loss: 0.5209\n",
      "323/500 [==================>...........] - ETA: 3:06 - loss: 2.0243 - regression_loss: 1.5038 - classification_loss: 0.5205\n",
      "324/500 [==================>...........] - ETA: 3:05 - loss: 2.0240 - regression_loss: 1.5039 - classification_loss: 0.5201\n",
      "325/500 [==================>...........] - ETA: 3:04 - loss: 2.0232 - regression_loss: 1.5035 - classification_loss: 0.5197\n",
      "326/500 [==================>...........] - ETA: 3:03 - loss: 2.0227 - regression_loss: 1.5031 - classification_loss: 0.5196\n",
      "327/500 [==================>...........] - ETA: 3:02 - loss: 2.0199 - regression_loss: 1.5010 - classification_loss: 0.5189\n",
      "328/500 [==================>...........] - ETA: 3:01 - loss: 2.0208 - regression_loss: 1.5020 - classification_loss: 0.5189\n",
      "329/500 [==================>...........] - ETA: 3:00 - loss: 2.0230 - regression_loss: 1.5037 - classification_loss: 0.5193\n",
      "330/500 [==================>...........] - ETA: 2:59 - loss: 2.0239 - regression_loss: 1.5048 - classification_loss: 0.5191\n",
      "331/500 [==================>...........] - ETA: 2:58 - loss: 2.0264 - regression_loss: 1.5065 - classification_loss: 0.5199\n",
      "332/500 [==================>...........] - ETA: 2:57 - loss: 2.0244 - regression_loss: 1.5050 - classification_loss: 0.5194\n",
      "333/500 [==================>...........] - ETA: 2:56 - loss: 2.0249 - regression_loss: 1.5059 - classification_loss: 0.5190\n",
      "334/500 [===================>..........] - ETA: 2:55 - loss: 2.0268 - regression_loss: 1.5071 - classification_loss: 0.5197\n",
      "335/500 [===================>..........] - ETA: 2:54 - loss: 2.0244 - regression_loss: 1.5054 - classification_loss: 0.5191\n",
      "336/500 [===================>..........] - ETA: 2:52 - loss: 2.0248 - regression_loss: 1.5058 - classification_loss: 0.5191\n",
      "337/500 [===================>..........] - ETA: 2:51 - loss: 2.0242 - regression_loss: 1.5055 - classification_loss: 0.5187\n",
      "338/500 [===================>..........] - ETA: 2:50 - loss: 2.0254 - regression_loss: 1.5069 - classification_loss: 0.5185\n",
      "339/500 [===================>..........] - ETA: 2:49 - loss: 2.0242 - regression_loss: 1.5062 - classification_loss: 0.5180\n",
      "340/500 [===================>..........] - ETA: 2:48 - loss: 2.0252 - regression_loss: 1.5074 - classification_loss: 0.5179\n",
      "341/500 [===================>..........] - ETA: 2:47 - loss: 2.0258 - regression_loss: 1.5075 - classification_loss: 0.5183\n",
      "342/500 [===================>..........] - ETA: 2:46 - loss: 2.0255 - regression_loss: 1.5073 - classification_loss: 0.5182\n",
      "343/500 [===================>..........] - ETA: 2:45 - loss: 2.0220 - regression_loss: 1.5048 - classification_loss: 0.5172\n",
      "344/500 [===================>..........] - ETA: 2:44 - loss: 2.0213 - regression_loss: 1.5042 - classification_loss: 0.5171\n",
      "345/500 [===================>..........] - ETA: 2:43 - loss: 2.0214 - regression_loss: 1.5042 - classification_loss: 0.5171\n",
      "346/500 [===================>..........] - ETA: 2:42 - loss: 2.0209 - regression_loss: 1.5038 - classification_loss: 0.5171\n",
      "347/500 [===================>..........] - ETA: 2:41 - loss: 2.0208 - regression_loss: 1.5035 - classification_loss: 0.5173\n",
      "348/500 [===================>..........] - ETA: 2:40 - loss: 2.0197 - regression_loss: 1.5028 - classification_loss: 0.5169\n",
      "349/500 [===================>..........] - ETA: 2:39 - loss: 2.0190 - regression_loss: 1.5023 - classification_loss: 0.5167\n",
      "350/500 [====================>.........] - ETA: 2:38 - loss: 2.0179 - regression_loss: 1.5014 - classification_loss: 0.5165\n",
      "351/500 [====================>.........] - ETA: 2:37 - loss: 2.0184 - regression_loss: 1.5012 - classification_loss: 0.5172\n",
      "352/500 [====================>.........] - ETA: 2:36 - loss: 2.0185 - regression_loss: 1.5013 - classification_loss: 0.5171\n",
      "353/500 [====================>.........] - ETA: 2:34 - loss: 2.0181 - regression_loss: 1.5009 - classification_loss: 0.5171\n",
      "354/500 [====================>.........] - ETA: 2:33 - loss: 2.0164 - regression_loss: 1.4996 - classification_loss: 0.5168\n",
      "355/500 [====================>.........] - ETA: 2:32 - loss: 2.0156 - regression_loss: 1.4994 - classification_loss: 0.5162\n",
      "356/500 [====================>.........] - ETA: 2:31 - loss: 2.0191 - regression_loss: 1.5016 - classification_loss: 0.5176\n",
      "357/500 [====================>.........] - ETA: 2:30 - loss: 2.0210 - regression_loss: 1.5026 - classification_loss: 0.5184\n",
      "358/500 [====================>.........] - ETA: 2:29 - loss: 2.0238 - regression_loss: 1.5049 - classification_loss: 0.5189\n",
      "359/500 [====================>.........] - ETA: 2:28 - loss: 2.0233 - regression_loss: 1.5041 - classification_loss: 0.5192\n",
      "360/500 [====================>.........] - ETA: 2:27 - loss: 2.0215 - regression_loss: 1.5027 - classification_loss: 0.5187\n",
      "361/500 [====================>.........] - ETA: 2:26 - loss: 2.0208 - regression_loss: 1.5018 - classification_loss: 0.5190\n",
      "362/500 [====================>.........] - ETA: 2:25 - loss: 2.0191 - regression_loss: 1.5004 - classification_loss: 0.5188\n",
      "363/500 [====================>.........] - ETA: 2:24 - loss: 2.0196 - regression_loss: 1.5010 - classification_loss: 0.5185\n",
      "364/500 [====================>.........] - ETA: 2:23 - loss: 2.0200 - regression_loss: 1.5013 - classification_loss: 0.5187\n",
      "365/500 [====================>.........] - ETA: 2:22 - loss: 2.0204 - regression_loss: 1.5015 - classification_loss: 0.5189\n",
      "366/500 [====================>.........] - ETA: 2:20 - loss: 2.0204 - regression_loss: 1.5012 - classification_loss: 0.5192\n",
      "367/500 [=====================>........] - ETA: 2:19 - loss: 2.0183 - regression_loss: 1.4999 - classification_loss: 0.5184\n",
      "368/500 [=====================>........] - ETA: 2:18 - loss: 2.0166 - regression_loss: 1.4983 - classification_loss: 0.5182\n",
      "369/500 [=====================>........] - ETA: 2:17 - loss: 2.0170 - regression_loss: 1.4989 - classification_loss: 0.5181\n",
      "370/500 [=====================>........] - ETA: 2:16 - loss: 2.0150 - regression_loss: 1.4975 - classification_loss: 0.5175\n",
      "371/500 [=====================>........] - ETA: 2:15 - loss: 2.0165 - regression_loss: 1.4991 - classification_loss: 0.5174\n",
      "372/500 [=====================>........] - ETA: 2:14 - loss: 2.0153 - regression_loss: 1.4983 - classification_loss: 0.5170\n",
      "373/500 [=====================>........] - ETA: 2:13 - loss: 2.0182 - regression_loss: 1.4999 - classification_loss: 0.5183\n",
      "374/500 [=====================>........] - ETA: 2:12 - loss: 2.0170 - regression_loss: 1.4993 - classification_loss: 0.5177\n",
      "375/500 [=====================>........] - ETA: 2:11 - loss: 2.0175 - regression_loss: 1.4996 - classification_loss: 0.5179\n",
      "376/500 [=====================>........] - ETA: 2:10 - loss: 2.0178 - regression_loss: 1.4997 - classification_loss: 0.5180\n",
      "377/500 [=====================>........] - ETA: 2:09 - loss: 2.0206 - regression_loss: 1.5012 - classification_loss: 0.5194\n",
      "378/500 [=====================>........] - ETA: 2:08 - loss: 2.0193 - regression_loss: 1.5002 - classification_loss: 0.5191\n",
      "379/500 [=====================>........] - ETA: 2:07 - loss: 2.0189 - regression_loss: 1.5000 - classification_loss: 0.5189\n",
      "380/500 [=====================>........] - ETA: 2:06 - loss: 2.0211 - regression_loss: 1.5018 - classification_loss: 0.5193\n",
      "381/500 [=====================>........] - ETA: 2:05 - loss: 2.0210 - regression_loss: 1.5019 - classification_loss: 0.5192\n",
      "382/500 [=====================>........] - ETA: 2:04 - loss: 2.0228 - regression_loss: 1.5034 - classification_loss: 0.5194\n",
      "383/500 [=====================>........] - ETA: 2:03 - loss: 2.0238 - regression_loss: 1.5037 - classification_loss: 0.5201\n",
      "384/500 [======================>.......] - ETA: 2:01 - loss: 2.0232 - regression_loss: 1.5032 - classification_loss: 0.5200\n",
      "385/500 [======================>.......] - ETA: 2:00 - loss: 2.0234 - regression_loss: 1.5035 - classification_loss: 0.5199\n",
      "386/500 [======================>.......] - ETA: 1:59 - loss: 2.0211 - regression_loss: 1.5015 - classification_loss: 0.5196\n",
      "387/500 [======================>.......] - ETA: 1:58 - loss: 2.0216 - regression_loss: 1.5019 - classification_loss: 0.5197\n",
      "388/500 [======================>.......] - ETA: 1:57 - loss: 2.0249 - regression_loss: 1.5040 - classification_loss: 0.5209\n",
      "389/500 [======================>.......] - ETA: 1:56 - loss: 2.0256 - regression_loss: 1.5045 - classification_loss: 0.5211\n",
      "390/500 [======================>.......] - ETA: 1:55 - loss: 2.0255 - regression_loss: 1.5045 - classification_loss: 0.5211\n",
      "391/500 [======================>.......] - ETA: 1:54 - loss: 2.0236 - regression_loss: 1.5032 - classification_loss: 0.5204\n",
      "392/500 [======================>.......] - ETA: 1:53 - loss: 2.0232 - regression_loss: 1.5028 - classification_loss: 0.5204\n",
      "393/500 [======================>.......] - ETA: 1:52 - loss: 2.0223 - regression_loss: 1.5023 - classification_loss: 0.5200\n",
      "394/500 [======================>.......] - ETA: 1:51 - loss: 2.0214 - regression_loss: 1.5016 - classification_loss: 0.5198\n",
      "395/500 [======================>.......] - ETA: 1:50 - loss: 2.0222 - regression_loss: 1.5020 - classification_loss: 0.5203\n",
      "396/500 [======================>.......] - ETA: 1:49 - loss: 2.0211 - regression_loss: 1.5015 - classification_loss: 0.5196\n",
      "397/500 [======================>.......] - ETA: 1:48 - loss: 2.0191 - regression_loss: 1.5000 - classification_loss: 0.5191\n",
      "398/500 [======================>.......] - ETA: 1:47 - loss: 2.0169 - regression_loss: 1.4984 - classification_loss: 0.5185\n",
      "399/500 [======================>.......] - ETA: 1:46 - loss: 2.0165 - regression_loss: 1.4980 - classification_loss: 0.5185\n",
      "400/500 [=======================>......] - ETA: 1:44 - loss: 2.0148 - regression_loss: 1.4966 - classification_loss: 0.5182\n",
      "401/500 [=======================>......] - ETA: 1:43 - loss: 2.0140 - regression_loss: 1.4959 - classification_loss: 0.5181\n",
      "402/500 [=======================>......] - ETA: 1:42 - loss: 2.0132 - regression_loss: 1.4951 - classification_loss: 0.5180\n",
      "403/500 [=======================>......] - ETA: 1:41 - loss: 2.0118 - regression_loss: 1.4944 - classification_loss: 0.5175\n",
      "404/500 [=======================>......] - ETA: 1:40 - loss: 2.0118 - regression_loss: 1.4942 - classification_loss: 0.5176\n",
      "405/500 [=======================>......] - ETA: 1:39 - loss: 2.0108 - regression_loss: 1.4936 - classification_loss: 0.5172\n",
      "406/500 [=======================>......] - ETA: 1:38 - loss: 2.0097 - regression_loss: 1.4930 - classification_loss: 0.5166\n",
      "407/500 [=======================>......] - ETA: 1:37 - loss: 2.0084 - regression_loss: 1.4920 - classification_loss: 0.5163\n",
      "408/500 [=======================>......] - ETA: 1:36 - loss: 2.0096 - regression_loss: 1.4927 - classification_loss: 0.5168\n",
      "409/500 [=======================>......] - ETA: 1:35 - loss: 2.0117 - regression_loss: 1.4941 - classification_loss: 0.5176\n",
      "410/500 [=======================>......] - ETA: 1:34 - loss: 2.0115 - regression_loss: 1.4940 - classification_loss: 0.5175\n",
      "411/500 [=======================>......] - ETA: 1:33 - loss: 2.0132 - regression_loss: 1.4954 - classification_loss: 0.5177\n",
      "412/500 [=======================>......] - ETA: 1:32 - loss: 2.0138 - regression_loss: 1.4960 - classification_loss: 0.5177\n",
      "413/500 [=======================>......] - ETA: 1:31 - loss: 2.0134 - regression_loss: 1.4959 - classification_loss: 0.5176\n",
      "414/500 [=======================>......] - ETA: 1:30 - loss: 2.0130 - regression_loss: 1.4957 - classification_loss: 0.5173\n",
      "415/500 [=======================>......] - ETA: 1:29 - loss: 2.0141 - regression_loss: 1.4966 - classification_loss: 0.5175\n",
      "416/500 [=======================>......] - ETA: 1:28 - loss: 2.0150 - regression_loss: 1.4973 - classification_loss: 0.5177\n",
      "417/500 [========================>.....] - ETA: 1:27 - loss: 2.0155 - regression_loss: 1.4974 - classification_loss: 0.5180\n",
      "418/500 [========================>.....] - ETA: 1:25 - loss: 2.0161 - regression_loss: 1.4975 - classification_loss: 0.5186\n",
      "419/500 [========================>.....] - ETA: 1:24 - loss: 2.0154 - regression_loss: 1.4968 - classification_loss: 0.5186\n",
      "420/500 [========================>.....] - ETA: 1:23 - loss: 2.0181 - regression_loss: 1.4989 - classification_loss: 0.5192\n",
      "421/500 [========================>.....] - ETA: 1:22 - loss: 2.0157 - regression_loss: 1.4971 - classification_loss: 0.5186\n",
      "422/500 [========================>.....] - ETA: 1:21 - loss: 2.0164 - regression_loss: 1.4976 - classification_loss: 0.5188\n",
      "423/500 [========================>.....] - ETA: 1:20 - loss: 2.0145 - regression_loss: 1.4963 - classification_loss: 0.5182\n",
      "424/500 [========================>.....] - ETA: 1:19 - loss: 2.0146 - regression_loss: 1.4967 - classification_loss: 0.5179\n",
      "425/500 [========================>.....] - ETA: 1:18 - loss: 2.0162 - regression_loss: 1.4976 - classification_loss: 0.5186\n",
      "426/500 [========================>.....] - ETA: 1:17 - loss: 2.0143 - regression_loss: 1.4959 - classification_loss: 0.5183\n",
      "427/500 [========================>.....] - ETA: 1:16 - loss: 2.0141 - regression_loss: 1.4960 - classification_loss: 0.5180\n",
      "428/500 [========================>.....] - ETA: 1:15 - loss: 2.0161 - regression_loss: 1.4974 - classification_loss: 0.5186\n",
      "429/500 [========================>.....] - ETA: 1:14 - loss: 2.0138 - regression_loss: 1.4957 - classification_loss: 0.5181\n",
      "430/500 [========================>.....] - ETA: 1:13 - loss: 2.0139 - regression_loss: 1.4958 - classification_loss: 0.5181\n",
      "431/500 [========================>.....] - ETA: 1:12 - loss: 2.0151 - regression_loss: 1.4969 - classification_loss: 0.5183\n",
      "432/500 [========================>.....] - ETA: 1:11 - loss: 2.0144 - regression_loss: 1.4964 - classification_loss: 0.5180\n",
      "433/500 [========================>.....] - ETA: 1:10 - loss: 2.0161 - regression_loss: 1.4969 - classification_loss: 0.5192\n",
      "434/500 [=========================>....] - ETA: 1:09 - loss: 2.0144 - regression_loss: 1.4958 - classification_loss: 0.5187\n",
      "435/500 [=========================>....] - ETA: 1:08 - loss: 2.0146 - regression_loss: 1.4958 - classification_loss: 0.5188\n",
      "436/500 [=========================>....] - ETA: 1:06 - loss: 2.0123 - regression_loss: 1.4941 - classification_loss: 0.5182\n",
      "437/500 [=========================>....] - ETA: 1:05 - loss: 2.0128 - regression_loss: 1.4941 - classification_loss: 0.5188\n",
      "438/500 [=========================>....] - ETA: 1:04 - loss: 2.0129 - regression_loss: 1.4941 - classification_loss: 0.5188\n",
      "439/500 [=========================>....] - ETA: 1:03 - loss: 2.0115 - regression_loss: 1.4929 - classification_loss: 0.5186\n",
      "440/500 [=========================>....] - ETA: 1:02 - loss: 2.0110 - regression_loss: 1.4925 - classification_loss: 0.5184\n",
      "441/500 [=========================>....] - ETA: 1:01 - loss: 2.0099 - regression_loss: 1.4917 - classification_loss: 0.5182\n",
      "442/500 [=========================>....] - ETA: 1:00 - loss: 2.0089 - regression_loss: 1.4910 - classification_loss: 0.5179\n",
      "443/500 [=========================>....] - ETA: 59s - loss: 2.0084 - regression_loss: 1.4905 - classification_loss: 0.5179 \n",
      "444/500 [=========================>....] - ETA: 58s - loss: 2.0089 - regression_loss: 1.4907 - classification_loss: 0.5183\n",
      "445/500 [=========================>....] - ETA: 57s - loss: 2.0096 - regression_loss: 1.4911 - classification_loss: 0.5185\n",
      "446/500 [=========================>....] - ETA: 56s - loss: 2.0104 - regression_loss: 1.4917 - classification_loss: 0.5187\n",
      "447/500 [=========================>....] - ETA: 55s - loss: 2.0099 - regression_loss: 1.4913 - classification_loss: 0.5186\n",
      "448/500 [=========================>....] - ETA: 54s - loss: 2.0094 - regression_loss: 1.4909 - classification_loss: 0.5184\n",
      "449/500 [=========================>....] - ETA: 53s - loss: 2.0091 - regression_loss: 1.4908 - classification_loss: 0.5184\n",
      "450/500 [==========================>...] - ETA: 52s - loss: 2.0093 - regression_loss: 1.4910 - classification_loss: 0.5183\n",
      "451/500 [==========================>...] - ETA: 51s - loss: 2.0115 - regression_loss: 1.4921 - classification_loss: 0.5194\n",
      "452/500 [==========================>...] - ETA: 50s - loss: 2.0116 - regression_loss: 1.4920 - classification_loss: 0.5196\n",
      "453/500 [==========================>...] - ETA: 49s - loss: 2.0133 - regression_loss: 1.4930 - classification_loss: 0.5203\n",
      "454/500 [==========================>...] - ETA: 48s - loss: 2.0139 - regression_loss: 1.4932 - classification_loss: 0.5207\n",
      "455/500 [==========================>...] - ETA: 47s - loss: 2.0139 - regression_loss: 1.4931 - classification_loss: 0.5208\n",
      "456/500 [==========================>...] - ETA: 46s - loss: 2.0133 - regression_loss: 1.4924 - classification_loss: 0.5208\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 2.0122 - regression_loss: 1.4913 - classification_loss: 0.5208\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 2.0124 - regression_loss: 1.4913 - classification_loss: 0.5211\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 2.0124 - regression_loss: 1.4912 - classification_loss: 0.5212\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 2.0123 - regression_loss: 1.4912 - classification_loss: 0.5211\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 2.0107 - regression_loss: 1.4899 - classification_loss: 0.5208\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 2.0093 - regression_loss: 1.4886 - classification_loss: 0.5207\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 2.0094 - regression_loss: 1.4884 - classification_loss: 0.5210\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 2.0093 - regression_loss: 1.4881 - classification_loss: 0.5212\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 2.0076 - regression_loss: 1.4864 - classification_loss: 0.5213\n",
      "466/500 [==========================>...] - ETA: 35s - loss: 2.0075 - regression_loss: 1.4860 - classification_loss: 0.5215\n",
      "467/500 [===========================>..] - ETA: 34s - loss: 2.0067 - regression_loss: 1.4855 - classification_loss: 0.5212\n",
      "468/500 [===========================>..] - ETA: 33s - loss: 2.0084 - regression_loss: 1.4869 - classification_loss: 0.5216\n",
      "469/500 [===========================>..] - ETA: 32s - loss: 2.0073 - regression_loss: 1.4860 - classification_loss: 0.5213\n",
      "470/500 [===========================>..] - ETA: 31s - loss: 2.0059 - regression_loss: 1.4850 - classification_loss: 0.5209\n",
      "471/500 [===========================>..] - ETA: 30s - loss: 2.0049 - regression_loss: 1.4843 - classification_loss: 0.5207\n",
      "472/500 [===========================>..] - ETA: 29s - loss: 2.0062 - regression_loss: 1.4854 - classification_loss: 0.5207\n",
      "473/500 [===========================>..] - ETA: 28s - loss: 2.0049 - regression_loss: 1.4845 - classification_loss: 0.5204\n",
      "474/500 [===========================>..] - ETA: 27s - loss: 2.0048 - regression_loss: 1.4843 - classification_loss: 0.5205\n",
      "475/500 [===========================>..] - ETA: 26s - loss: 2.0031 - regression_loss: 1.4830 - classification_loss: 0.5202\n",
      "476/500 [===========================>..] - ETA: 25s - loss: 2.0033 - regression_loss: 1.4831 - classification_loss: 0.5202\n",
      "477/500 [===========================>..] - ETA: 24s - loss: 2.0018 - regression_loss: 1.4820 - classification_loss: 0.5199\n",
      "478/500 [===========================>..] - ETA: 23s - loss: 2.0003 - regression_loss: 1.4807 - classification_loss: 0.5196\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.9989 - regression_loss: 1.4795 - classification_loss: 0.5194\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.9996 - regression_loss: 1.4799 - classification_loss: 0.5196\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 2.0009 - regression_loss: 1.4811 - classification_loss: 0.5198\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 2.0017 - regression_loss: 1.4819 - classification_loss: 0.5199\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 2.0010 - regression_loss: 1.4815 - classification_loss: 0.5194\n",
      "484/500 [============================>.] - ETA: 16s - loss: 2.0025 - regression_loss: 1.4824 - classification_loss: 0.5201\n",
      "485/500 [============================>.] - ETA: 15s - loss: 2.0045 - regression_loss: 1.4831 - classification_loss: 0.5213\n",
      "486/500 [============================>.] - ETA: 14s - loss: 2.0026 - regression_loss: 1.4818 - classification_loss: 0.5208\n",
      "487/500 [============================>.] - ETA: 13s - loss: 2.0039 - regression_loss: 1.4827 - classification_loss: 0.5212\n",
      "488/500 [============================>.] - ETA: 12s - loss: 2.0027 - regression_loss: 1.4818 - classification_loss: 0.5209\n",
      "489/500 [============================>.] - ETA: 11s - loss: 2.0031 - regression_loss: 1.4822 - classification_loss: 0.5208\n",
      "490/500 [============================>.] - ETA: 10s - loss: 2.0025 - regression_loss: 1.4821 - classification_loss: 0.5204\n",
      "491/500 [============================>.] - ETA: 9s - loss: 2.0047 - regression_loss: 1.4835 - classification_loss: 0.5212 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 2.0039 - regression_loss: 1.4830 - classification_loss: 0.5209\n",
      "493/500 [============================>.] - ETA: 7s - loss: 2.0048 - regression_loss: 1.4840 - classification_loss: 0.5208\n",
      "494/500 [============================>.] - ETA: 6s - loss: 2.0062 - regression_loss: 1.4847 - classification_loss: 0.5216\n",
      "495/500 [============================>.] - ETA: 5s - loss: 2.0055 - regression_loss: 1.4839 - classification_loss: 0.5216\n",
      "496/500 [============================>.] - ETA: 4s - loss: 2.0038 - regression_loss: 1.4827 - classification_loss: 0.5211\n",
      "497/500 [============================>.] - ETA: 3s - loss: 2.0019 - regression_loss: 1.4811 - classification_loss: 0.5207\n",
      "498/500 [============================>.] - ETA: 2s - loss: 2.0020 - regression_loss: 1.4818 - classification_loss: 0.5202\n",
      "499/500 [============================>.] - ETA: 1s - loss: 2.0026 - regression_loss: 1.4822 - classification_loss: 0.5204\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.0019 - regression_loss: 1.4818 - classification_loss: 0.5200\n",
      "Epoch 00009: saving model to ./snapshots\\resnet50_csv_09.h5\n",
      "\n",
      "500/500 [==============================] - 523s 1s/step - loss: 2.0019 - regression_loss: 1.4818 - classification_loss: 0.5200\n",
      "Epoch 10/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 1.6678 - regression_loss: 1.2922 - classification_loss: 0.3756\n",
      "  2/500 [..............................] - ETA: 3:33 - loss: 2.1274 - regression_loss: 1.6906 - classification_loss: 0.4368\n",
      "  3/500 [..............................] - ETA: 5:08 - loss: 1.9025 - regression_loss: 1.5066 - classification_loss: 0.3959\n",
      "  4/500 [..............................] - ETA: 6:09 - loss: 1.7800 - regression_loss: 1.3868 - classification_loss: 0.3932\n",
      "  5/500 [..............................] - ETA: 6:46 - loss: 1.7513 - regression_loss: 1.3337 - classification_loss: 0.4176\n",
      "  6/500 [..............................] - ETA: 7:06 - loss: 1.7167 - regression_loss: 1.2943 - classification_loss: 0.4224\n",
      "  7/500 [..............................] - ETA: 7:13 - loss: 1.7301 - regression_loss: 1.3034 - classification_loss: 0.4267\n",
      "  8/500 [..............................] - ETA: 7:19 - loss: 1.7625 - regression_loss: 1.3271 - classification_loss: 0.4354\n",
      "  9/500 [..............................] - ETA: 7:23 - loss: 1.7829 - regression_loss: 1.3450 - classification_loss: 0.4379\n",
      " 10/500 [..............................] - ETA: 7:25 - loss: 1.8130 - regression_loss: 1.3665 - classification_loss: 0.4465\n",
      " 11/500 [..............................] - ETA: 7:32 - loss: 1.8018 - regression_loss: 1.3458 - classification_loss: 0.4560\n",
      " 12/500 [..............................] - ETA: 7:38 - loss: 1.7922 - regression_loss: 1.3356 - classification_loss: 0.4566\n",
      " 13/500 [..............................] - ETA: 7:39 - loss: 1.8436 - regression_loss: 1.3741 - classification_loss: 0.4695\n",
      " 14/500 [..............................] - ETA: 7:42 - loss: 1.8292 - regression_loss: 1.3586 - classification_loss: 0.4706\n",
      " 15/500 [..............................] - ETA: 7:42 - loss: 1.8155 - regression_loss: 1.3545 - classification_loss: 0.4610\n",
      " 16/500 [..............................] - ETA: 7:42 - loss: 1.8127 - regression_loss: 1.3473 - classification_loss: 0.4655\n",
      " 17/500 [>.............................] - ETA: 7:46 - loss: 1.8029 - regression_loss: 1.3342 - classification_loss: 0.4687\n",
      " 18/500 [>.............................] - ETA: 7:48 - loss: 1.8152 - regression_loss: 1.3491 - classification_loss: 0.4662\n",
      " 19/500 [>.............................] - ETA: 7:50 - loss: 1.8664 - regression_loss: 1.3719 - classification_loss: 0.4945\n",
      " 20/500 [>.............................] - ETA: 7:49 - loss: 1.9238 - regression_loss: 1.4151 - classification_loss: 0.5087\n",
      " 21/500 [>.............................] - ETA: 7:51 - loss: 1.9299 - regression_loss: 1.4252 - classification_loss: 0.5047\n",
      " 22/500 [>.............................] - ETA: 7:50 - loss: 1.9199 - regression_loss: 1.4204 - classification_loss: 0.4995\n",
      " 23/500 [>.............................] - ETA: 7:49 - loss: 1.8861 - regression_loss: 1.3922 - classification_loss: 0.4939\n",
      " 24/500 [>.............................] - ETA: 7:50 - loss: 1.9141 - regression_loss: 1.4142 - classification_loss: 0.4999\n",
      " 25/500 [>.............................] - ETA: 7:51 - loss: 1.9659 - regression_loss: 1.4492 - classification_loss: 0.5167\n",
      " 26/500 [>.............................] - ETA: 7:52 - loss: 1.9683 - regression_loss: 1.4513 - classification_loss: 0.5170\n",
      " 27/500 [>.............................] - ETA: 7:53 - loss: 1.9329 - regression_loss: 1.4227 - classification_loss: 0.5102\n",
      " 28/500 [>.............................] - ETA: 7:53 - loss: 1.9660 - regression_loss: 1.4348 - classification_loss: 0.5312\n",
      " 29/500 [>.............................] - ETA: 7:53 - loss: 1.9643 - regression_loss: 1.4373 - classification_loss: 0.5271\n",
      " 30/500 [>.............................] - ETA: 7:52 - loss: 1.9630 - regression_loss: 1.4322 - classification_loss: 0.5308\n",
      " 31/500 [>.............................] - ETA: 7:51 - loss: 1.9702 - regression_loss: 1.4370 - classification_loss: 0.5331\n",
      " 32/500 [>.............................] - ETA: 7:50 - loss: 1.9672 - regression_loss: 1.4353 - classification_loss: 0.5319\n",
      " 33/500 [>.............................] - ETA: 7:46 - loss: 1.9403 - regression_loss: 1.4146 - classification_loss: 0.5257\n",
      " 34/500 [=>............................] - ETA: 7:49 - loss: 1.9463 - regression_loss: 1.4190 - classification_loss: 0.5273\n",
      " 35/500 [=>............................] - ETA: 7:47 - loss: 1.9518 - regression_loss: 1.4191 - classification_loss: 0.5327\n",
      " 36/500 [=>............................] - ETA: 7:46 - loss: 1.9539 - regression_loss: 1.4207 - classification_loss: 0.5332\n",
      " 37/500 [=>............................] - ETA: 7:46 - loss: 1.9566 - regression_loss: 1.4228 - classification_loss: 0.5338\n",
      " 38/500 [=>............................] - ETA: 7:45 - loss: 1.9705 - regression_loss: 1.4337 - classification_loss: 0.5368\n",
      " 39/500 [=>............................] - ETA: 7:44 - loss: 1.9904 - regression_loss: 1.4495 - classification_loss: 0.5409\n",
      " 40/500 [=>............................] - ETA: 7:43 - loss: 2.0075 - regression_loss: 1.4633 - classification_loss: 0.5443\n",
      " 41/500 [=>............................] - ETA: 7:42 - loss: 2.0144 - regression_loss: 1.4684 - classification_loss: 0.5460\n",
      " 42/500 [=>............................] - ETA: 7:42 - loss: 2.0071 - regression_loss: 1.4636 - classification_loss: 0.5435\n",
      " 43/500 [=>............................] - ETA: 7:42 - loss: 2.0038 - regression_loss: 1.4649 - classification_loss: 0.5389\n",
      " 44/500 [=>............................] - ETA: 7:41 - loss: 1.9917 - regression_loss: 1.4574 - classification_loss: 0.5342\n",
      " 45/500 [=>............................] - ETA: 7:41 - loss: 1.9862 - regression_loss: 1.4569 - classification_loss: 0.5294\n",
      " 46/500 [=>............................] - ETA: 7:41 - loss: 1.9782 - regression_loss: 1.4511 - classification_loss: 0.5271\n",
      " 47/500 [=>............................] - ETA: 7:40 - loss: 1.9743 - regression_loss: 1.4463 - classification_loss: 0.5281\n",
      " 48/500 [=>............................] - ETA: 7:39 - loss: 1.9731 - regression_loss: 1.4433 - classification_loss: 0.5298\n",
      " 49/500 [=>............................] - ETA: 7:39 - loss: 1.9642 - regression_loss: 1.4388 - classification_loss: 0.5254\n",
      " 50/500 [==>...........................] - ETA: 7:38 - loss: 1.9641 - regression_loss: 1.4405 - classification_loss: 0.5236\n",
      " 51/500 [==>...........................] - ETA: 7:38 - loss: 1.9654 - regression_loss: 1.4423 - classification_loss: 0.5231\n",
      " 52/500 [==>...........................] - ETA: 7:34 - loss: 1.9549 - regression_loss: 1.4371 - classification_loss: 0.5178\n",
      " 53/500 [==>...........................] - ETA: 7:34 - loss: 1.9547 - regression_loss: 1.4380 - classification_loss: 0.5167\n",
      " 54/500 [==>...........................] - ETA: 7:53 - loss: 1.9382 - regression_loss: 1.4248 - classification_loss: 0.5134\n",
      " 55/500 [==>...........................] - ETA: 7:49 - loss: 1.9414 - regression_loss: 1.4277 - classification_loss: 0.5138\n",
      " 56/500 [==>...........................] - ETA: 7:49 - loss: 1.9299 - regression_loss: 1.4202 - classification_loss: 0.5097\n",
      " 57/500 [==>...........................] - ETA: 7:48 - loss: 1.9258 - regression_loss: 1.4176 - classification_loss: 0.5081\n",
      " 58/500 [==>...........................] - ETA: 7:47 - loss: 1.9270 - regression_loss: 1.4169 - classification_loss: 0.5101\n",
      " 59/500 [==>...........................] - ETA: 7:46 - loss: 1.9274 - regression_loss: 1.4186 - classification_loss: 0.5088\n",
      " 60/500 [==>...........................] - ETA: 7:44 - loss: 1.9161 - regression_loss: 1.4109 - classification_loss: 0.5052\n",
      " 61/500 [==>...........................] - ETA: 7:42 - loss: 1.9228 - regression_loss: 1.4151 - classification_loss: 0.5076\n",
      " 62/500 [==>...........................] - ETA: 7:41 - loss: 1.9192 - regression_loss: 1.4141 - classification_loss: 0.5051\n",
      " 63/500 [==>...........................] - ETA: 7:39 - loss: 1.9078 - regression_loss: 1.4055 - classification_loss: 0.5023\n",
      " 64/500 [==>...........................] - ETA: 7:39 - loss: 1.9050 - regression_loss: 1.4040 - classification_loss: 0.5009\n",
      " 65/500 [==>...........................] - ETA: 7:36 - loss: 1.9143 - regression_loss: 1.4097 - classification_loss: 0.5045\n",
      " 66/500 [==>...........................] - ETA: 7:35 - loss: 1.9087 - regression_loss: 1.4077 - classification_loss: 0.5010\n",
      " 67/500 [===>..........................] - ETA: 7:34 - loss: 1.9075 - regression_loss: 1.4063 - classification_loss: 0.5012\n",
      " 68/500 [===>..........................] - ETA: 7:32 - loss: 1.9161 - regression_loss: 1.4158 - classification_loss: 0.5003\n",
      " 69/500 [===>..........................] - ETA: 7:29 - loss: 1.9202 - regression_loss: 1.4180 - classification_loss: 0.5022\n",
      " 70/500 [===>..........................] - ETA: 7:29 - loss: 1.9102 - regression_loss: 1.4114 - classification_loss: 0.4988\n",
      " 71/500 [===>..........................] - ETA: 7:28 - loss: 1.9140 - regression_loss: 1.4144 - classification_loss: 0.4996\n",
      " 72/500 [===>..........................] - ETA: 7:26 - loss: 1.9149 - regression_loss: 1.4154 - classification_loss: 0.4995\n",
      " 73/500 [===>..........................] - ETA: 7:24 - loss: 1.9272 - regression_loss: 1.4260 - classification_loss: 0.5012\n",
      " 74/500 [===>..........................] - ETA: 7:23 - loss: 1.9257 - regression_loss: 1.4243 - classification_loss: 0.5014\n",
      " 75/500 [===>..........................] - ETA: 7:21 - loss: 1.9206 - regression_loss: 1.4205 - classification_loss: 0.5001\n",
      " 76/500 [===>..........................] - ETA: 7:20 - loss: 1.9225 - regression_loss: 1.4216 - classification_loss: 0.5009\n",
      " 77/500 [===>..........................] - ETA: 7:19 - loss: 1.9214 - regression_loss: 1.4212 - classification_loss: 0.5003\n",
      " 78/500 [===>..........................] - ETA: 7:19 - loss: 1.9314 - regression_loss: 1.4278 - classification_loss: 0.5036\n",
      " 79/500 [===>..........................] - ETA: 7:17 - loss: 1.9368 - regression_loss: 1.4343 - classification_loss: 0.5025\n",
      " 80/500 [===>..........................] - ETA: 7:16 - loss: 1.9351 - regression_loss: 1.4350 - classification_loss: 0.5001\n",
      " 81/500 [===>..........................] - ETA: 7:15 - loss: 1.9406 - regression_loss: 1.4406 - classification_loss: 0.5000\n",
      " 82/500 [===>..........................] - ETA: 7:14 - loss: 1.9538 - regression_loss: 1.4531 - classification_loss: 0.5007\n",
      " 83/500 [===>..........................] - ETA: 7:12 - loss: 1.9516 - regression_loss: 1.4528 - classification_loss: 0.4987\n",
      " 84/500 [====>.........................] - ETA: 7:11 - loss: 1.9522 - regression_loss: 1.4530 - classification_loss: 0.4992\n",
      " 85/500 [====>.........................] - ETA: 7:11 - loss: 1.9508 - regression_loss: 1.4527 - classification_loss: 0.4981\n",
      " 86/500 [====>.........................] - ETA: 7:09 - loss: 1.9419 - regression_loss: 1.4465 - classification_loss: 0.4954\n",
      " 87/500 [====>.........................] - ETA: 7:08 - loss: 1.9444 - regression_loss: 1.4473 - classification_loss: 0.4972\n",
      " 88/500 [====>.........................] - ETA: 7:07 - loss: 1.9524 - regression_loss: 1.4523 - classification_loss: 0.5001\n",
      " 89/500 [====>.........................] - ETA: 7:06 - loss: 1.9517 - regression_loss: 1.4532 - classification_loss: 0.4985\n",
      " 90/500 [====>.........................] - ETA: 7:05 - loss: 1.9577 - regression_loss: 1.4559 - classification_loss: 0.5019\n",
      " 91/500 [====>.........................] - ETA: 7:02 - loss: 1.9538 - regression_loss: 1.4524 - classification_loss: 0.5014\n",
      " 92/500 [====>.........................] - ETA: 7:03 - loss: 1.9500 - regression_loss: 1.4499 - classification_loss: 0.5001\n",
      " 93/500 [====>.........................] - ETA: 7:01 - loss: 1.9421 - regression_loss: 1.4440 - classification_loss: 0.4981\n",
      " 94/500 [====>.........................] - ETA: 7:00 - loss: 1.9436 - regression_loss: 1.4443 - classification_loss: 0.4993\n",
      " 95/500 [====>.........................] - ETA: 6:59 - loss: 1.9519 - regression_loss: 1.4501 - classification_loss: 0.5018\n",
      " 96/500 [====>.........................] - ETA: 6:58 - loss: 1.9571 - regression_loss: 1.4528 - classification_loss: 0.5043\n",
      " 97/500 [====>.........................] - ETA: 6:57 - loss: 1.9560 - regression_loss: 1.4530 - classification_loss: 0.5029\n",
      " 98/500 [====>.........................] - ETA: 6:56 - loss: 1.9575 - regression_loss: 1.4533 - classification_loss: 0.5043\n",
      " 99/500 [====>.........................] - ETA: 6:54 - loss: 1.9579 - regression_loss: 1.4542 - classification_loss: 0.5037\n",
      "100/500 [=====>........................] - ETA: 6:53 - loss: 1.9647 - regression_loss: 1.4593 - classification_loss: 0.5054\n",
      "101/500 [=====>........................] - ETA: 6:52 - loss: 1.9714 - regression_loss: 1.4652 - classification_loss: 0.5062\n",
      "102/500 [=====>........................] - ETA: 6:51 - loss: 1.9819 - regression_loss: 1.4724 - classification_loss: 0.5095\n",
      "103/500 [=====>........................] - ETA: 6:50 - loss: 1.9749 - regression_loss: 1.4676 - classification_loss: 0.5073\n",
      "104/500 [=====>........................] - ETA: 6:49 - loss: 1.9795 - regression_loss: 1.4701 - classification_loss: 0.5094\n",
      "105/500 [=====>........................] - ETA: 6:48 - loss: 1.9702 - regression_loss: 1.4631 - classification_loss: 0.5071\n",
      "106/500 [=====>........................] - ETA: 6:47 - loss: 1.9671 - regression_loss: 1.4613 - classification_loss: 0.5059\n",
      "107/500 [=====>........................] - ETA: 6:45 - loss: 1.9639 - regression_loss: 1.4595 - classification_loss: 0.5044\n",
      "108/500 [=====>........................] - ETA: 6:45 - loss: 1.9734 - regression_loss: 1.4666 - classification_loss: 0.5068\n",
      "109/500 [=====>........................] - ETA: 6:43 - loss: 1.9718 - regression_loss: 1.4658 - classification_loss: 0.5060\n",
      "110/500 [=====>........................] - ETA: 6:42 - loss: 1.9723 - regression_loss: 1.4659 - classification_loss: 0.5064\n",
      "111/500 [=====>........................] - ETA: 6:42 - loss: 1.9712 - regression_loss: 1.4665 - classification_loss: 0.5047\n",
      "112/500 [=====>........................] - ETA: 6:40 - loss: 1.9707 - regression_loss: 1.4665 - classification_loss: 0.5043\n",
      "113/500 [=====>........................] - ETA: 6:39 - loss: 1.9788 - regression_loss: 1.4735 - classification_loss: 0.5053\n",
      "114/500 [=====>........................] - ETA: 6:38 - loss: 1.9807 - regression_loss: 1.4752 - classification_loss: 0.5055\n",
      "115/500 [=====>........................] - ETA: 6:37 - loss: 1.9772 - regression_loss: 1.4714 - classification_loss: 0.5058\n",
      "116/500 [=====>........................] - ETA: 6:36 - loss: 1.9764 - regression_loss: 1.4720 - classification_loss: 0.5044\n",
      "117/500 [======>.......................] - ETA: 6:35 - loss: 1.9770 - regression_loss: 1.4725 - classification_loss: 0.5044\n",
      "118/500 [======>.......................] - ETA: 6:34 - loss: 1.9748 - regression_loss: 1.4707 - classification_loss: 0.5041\n",
      "119/500 [======>.......................] - ETA: 6:33 - loss: 1.9669 - regression_loss: 1.4634 - classification_loss: 0.5034\n",
      "120/500 [======>.......................] - ETA: 6:32 - loss: 1.9652 - regression_loss: 1.4624 - classification_loss: 0.5028\n",
      "121/500 [======>.......................] - ETA: 6:31 - loss: 1.9689 - regression_loss: 1.4652 - classification_loss: 0.5037\n",
      "122/500 [======>.......................] - ETA: 6:30 - loss: 1.9719 - regression_loss: 1.4647 - classification_loss: 0.5072\n",
      "123/500 [======>.......................] - ETA: 6:28 - loss: 1.9673 - regression_loss: 1.4614 - classification_loss: 0.5059\n",
      "124/500 [======>.......................] - ETA: 6:27 - loss: 1.9688 - regression_loss: 1.4624 - classification_loss: 0.5064\n",
      "125/500 [======>.......................] - ETA: 6:26 - loss: 1.9709 - regression_loss: 1.4640 - classification_loss: 0.5069\n",
      "126/500 [======>.......................] - ETA: 6:25 - loss: 1.9720 - regression_loss: 1.4651 - classification_loss: 0.5069\n",
      "127/500 [======>.......................] - ETA: 6:23 - loss: 1.9720 - regression_loss: 1.4641 - classification_loss: 0.5079\n",
      "128/500 [======>.......................] - ETA: 6:23 - loss: 1.9757 - regression_loss: 1.4671 - classification_loss: 0.5086\n",
      "129/500 [======>.......................] - ETA: 6:22 - loss: 1.9739 - regression_loss: 1.4652 - classification_loss: 0.5087\n",
      "130/500 [======>.......................] - ETA: 6:21 - loss: 1.9723 - regression_loss: 1.4636 - classification_loss: 0.5087\n",
      "131/500 [======>.......................] - ETA: 6:20 - loss: 1.9701 - regression_loss: 1.4618 - classification_loss: 0.5083\n",
      "132/500 [======>.......................] - ETA: 6:19 - loss: 1.9637 - regression_loss: 1.4575 - classification_loss: 0.5063\n",
      "133/500 [======>.......................] - ETA: 6:18 - loss: 1.9626 - regression_loss: 1.4576 - classification_loss: 0.5049\n",
      "134/500 [=======>......................] - ETA: 6:16 - loss: 1.9676 - regression_loss: 1.4611 - classification_loss: 0.5065\n",
      "135/500 [=======>......................] - ETA: 6:15 - loss: 1.9662 - regression_loss: 1.4606 - classification_loss: 0.5056\n",
      "136/500 [=======>......................] - ETA: 6:14 - loss: 1.9667 - regression_loss: 1.4622 - classification_loss: 0.5046\n",
      "137/500 [=======>......................] - ETA: 6:13 - loss: 1.9651 - regression_loss: 1.4612 - classification_loss: 0.5039\n",
      "138/500 [=======>......................] - ETA: 6:12 - loss: 1.9673 - regression_loss: 1.4633 - classification_loss: 0.5041\n",
      "139/500 [=======>......................] - ETA: 6:11 - loss: 1.9666 - regression_loss: 1.4634 - classification_loss: 0.5032\n",
      "140/500 [=======>......................] - ETA: 6:10 - loss: 1.9611 - regression_loss: 1.4597 - classification_loss: 0.5014\n",
      "141/500 [=======>......................] - ETA: 6:09 - loss: 1.9571 - regression_loss: 1.4565 - classification_loss: 0.5006\n",
      "142/500 [=======>......................] - ETA: 6:08 - loss: 1.9570 - regression_loss: 1.4564 - classification_loss: 0.5006\n",
      "143/500 [=======>......................] - ETA: 6:07 - loss: 1.9597 - regression_loss: 1.4574 - classification_loss: 0.5023\n",
      "144/500 [=======>......................] - ETA: 6:06 - loss: 1.9591 - regression_loss: 1.4559 - classification_loss: 0.5032\n",
      "145/500 [=======>......................] - ETA: 6:05 - loss: 1.9578 - regression_loss: 1.4552 - classification_loss: 0.5026\n",
      "146/500 [=======>......................] - ETA: 6:04 - loss: 1.9523 - regression_loss: 1.4510 - classification_loss: 0.5012\n",
      "147/500 [=======>......................] - ETA: 6:03 - loss: 1.9502 - regression_loss: 1.4491 - classification_loss: 0.5011\n",
      "148/500 [=======>......................] - ETA: 6:02 - loss: 1.9531 - regression_loss: 1.4515 - classification_loss: 0.5016\n",
      "149/500 [=======>......................] - ETA: 6:00 - loss: 1.9482 - regression_loss: 1.4478 - classification_loss: 0.5004\n",
      "150/500 [========>.....................] - ETA: 5:59 - loss: 1.9515 - regression_loss: 1.4500 - classification_loss: 0.5015\n",
      "151/500 [========>.....................] - ETA: 5:58 - loss: 1.9507 - regression_loss: 1.4504 - classification_loss: 0.5003\n",
      "152/500 [========>.....................] - ETA: 5:57 - loss: 1.9455 - regression_loss: 1.4465 - classification_loss: 0.4990\n",
      "153/500 [========>.....................] - ETA: 5:56 - loss: 1.9420 - regression_loss: 1.4439 - classification_loss: 0.4981\n",
      "154/500 [========>.....................] - ETA: 5:55 - loss: 1.9397 - regression_loss: 1.4424 - classification_loss: 0.4973\n",
      "155/500 [========>.....................] - ETA: 5:54 - loss: 1.9417 - regression_loss: 1.4432 - classification_loss: 0.4985\n",
      "156/500 [========>.....................] - ETA: 5:53 - loss: 1.9411 - regression_loss: 1.4433 - classification_loss: 0.4979\n",
      "157/500 [========>.....................] - ETA: 5:52 - loss: 1.9451 - regression_loss: 1.4456 - classification_loss: 0.4995\n",
      "158/500 [========>.....................] - ETA: 5:52 - loss: 1.9430 - regression_loss: 1.4433 - classification_loss: 0.4997\n",
      "159/500 [========>.....................] - ETA: 5:51 - loss: 1.9420 - regression_loss: 1.4430 - classification_loss: 0.4991\n",
      "160/500 [========>.....................] - ETA: 5:50 - loss: 1.9424 - regression_loss: 1.4430 - classification_loss: 0.4994\n",
      "161/500 [========>.....................] - ETA: 5:48 - loss: 1.9454 - regression_loss: 1.4455 - classification_loss: 0.4999\n",
      "162/500 [========>.....................] - ETA: 5:47 - loss: 1.9418 - regression_loss: 1.4433 - classification_loss: 0.4985\n",
      "163/500 [========>.....................] - ETA: 5:47 - loss: 1.9492 - regression_loss: 1.4476 - classification_loss: 0.5016\n",
      "164/500 [========>.....................] - ETA: 5:45 - loss: 1.9472 - regression_loss: 1.4460 - classification_loss: 0.5012\n",
      "165/500 [========>.....................] - ETA: 5:45 - loss: 1.9476 - regression_loss: 1.4470 - classification_loss: 0.5006\n",
      "166/500 [========>.....................] - ETA: 5:44 - loss: 1.9437 - regression_loss: 1.4439 - classification_loss: 0.4998\n",
      "167/500 [=========>....................] - ETA: 5:42 - loss: 1.9407 - regression_loss: 1.4420 - classification_loss: 0.4986\n",
      "168/500 [=========>....................] - ETA: 5:41 - loss: 1.9378 - regression_loss: 1.4398 - classification_loss: 0.4980\n",
      "169/500 [=========>....................] - ETA: 5:40 - loss: 1.9422 - regression_loss: 1.4434 - classification_loss: 0.4988\n",
      "170/500 [=========>....................] - ETA: 5:39 - loss: 1.9431 - regression_loss: 1.4437 - classification_loss: 0.4994\n",
      "171/500 [=========>....................] - ETA: 5:38 - loss: 1.9506 - regression_loss: 1.4488 - classification_loss: 0.5018\n",
      "172/500 [=========>....................] - ETA: 5:38 - loss: 1.9455 - regression_loss: 1.4450 - classification_loss: 0.5005\n",
      "173/500 [=========>....................] - ETA: 5:36 - loss: 1.9465 - regression_loss: 1.4451 - classification_loss: 0.5013\n",
      "174/500 [=========>....................] - ETA: 5:35 - loss: 1.9489 - regression_loss: 1.4472 - classification_loss: 0.5017\n",
      "175/500 [=========>....................] - ETA: 5:34 - loss: 1.9469 - regression_loss: 1.4462 - classification_loss: 0.5007\n",
      "176/500 [=========>....................] - ETA: 5:33 - loss: 1.9465 - regression_loss: 1.4458 - classification_loss: 0.5007\n",
      "177/500 [=========>....................] - ETA: 5:32 - loss: 1.9450 - regression_loss: 1.4451 - classification_loss: 0.5000\n",
      "178/500 [=========>....................] - ETA: 5:31 - loss: 1.9459 - regression_loss: 1.4457 - classification_loss: 0.5001\n",
      "179/500 [=========>....................] - ETA: 5:30 - loss: 1.9464 - regression_loss: 1.4467 - classification_loss: 0.4997\n",
      "180/500 [=========>....................] - ETA: 5:29 - loss: 1.9431 - regression_loss: 1.4433 - classification_loss: 0.4998\n",
      "181/500 [=========>....................] - ETA: 5:28 - loss: 1.9455 - regression_loss: 1.4449 - classification_loss: 0.5006\n",
      "182/500 [=========>....................] - ETA: 5:27 - loss: 1.9505 - regression_loss: 1.4466 - classification_loss: 0.5039\n",
      "183/500 [=========>....................] - ETA: 5:26 - loss: 1.9503 - regression_loss: 1.4464 - classification_loss: 0.5038\n",
      "184/500 [==========>...................] - ETA: 5:25 - loss: 1.9471 - regression_loss: 1.4431 - classification_loss: 0.5041\n",
      "185/500 [==========>...................] - ETA: 5:24 - loss: 1.9466 - regression_loss: 1.4417 - classification_loss: 0.5049\n",
      "186/500 [==========>...................] - ETA: 5:23 - loss: 1.9487 - regression_loss: 1.4435 - classification_loss: 0.5052\n",
      "187/500 [==========>...................] - ETA: 5:22 - loss: 1.9468 - regression_loss: 1.4419 - classification_loss: 0.5049\n",
      "188/500 [==========>...................] - ETA: 5:21 - loss: 1.9431 - regression_loss: 1.4390 - classification_loss: 0.5041\n",
      "189/500 [==========>...................] - ETA: 5:20 - loss: 1.9437 - regression_loss: 1.4397 - classification_loss: 0.5040\n",
      "190/500 [==========>...................] - ETA: 5:19 - loss: 1.9435 - regression_loss: 1.4391 - classification_loss: 0.5044\n",
      "191/500 [==========>...................] - ETA: 5:17 - loss: 1.9460 - regression_loss: 1.4398 - classification_loss: 0.5062\n",
      "192/500 [==========>...................] - ETA: 5:16 - loss: 1.9474 - regression_loss: 1.4413 - classification_loss: 0.5061\n",
      "193/500 [==========>...................] - ETA: 5:15 - loss: 1.9510 - regression_loss: 1.4446 - classification_loss: 0.5064\n",
      "194/500 [==========>...................] - ETA: 5:14 - loss: 1.9505 - regression_loss: 1.4435 - classification_loss: 0.5071\n",
      "195/500 [==========>...................] - ETA: 5:13 - loss: 1.9540 - regression_loss: 1.4457 - classification_loss: 0.5084\n",
      "196/500 [==========>...................] - ETA: 5:12 - loss: 1.9525 - regression_loss: 1.4446 - classification_loss: 0.5079\n",
      "197/500 [==========>...................] - ETA: 5:11 - loss: 1.9516 - regression_loss: 1.4437 - classification_loss: 0.5078\n",
      "198/500 [==========>...................] - ETA: 5:10 - loss: 1.9566 - regression_loss: 1.4474 - classification_loss: 0.5092\n",
      "199/500 [==========>...................] - ETA: 5:09 - loss: 1.9517 - regression_loss: 1.4437 - classification_loss: 0.5080\n",
      "200/500 [===========>..................] - ETA: 5:08 - loss: 1.9507 - regression_loss: 1.4429 - classification_loss: 0.5077\n",
      "201/500 [===========>..................] - ETA: 5:07 - loss: 1.9515 - regression_loss: 1.4433 - classification_loss: 0.5082\n",
      "202/500 [===========>..................] - ETA: 5:06 - loss: 1.9545 - regression_loss: 1.4452 - classification_loss: 0.5093\n",
      "203/500 [===========>..................] - ETA: 5:05 - loss: 1.9564 - regression_loss: 1.4459 - classification_loss: 0.5105\n",
      "204/500 [===========>..................] - ETA: 5:04 - loss: 1.9575 - regression_loss: 1.4458 - classification_loss: 0.5116\n",
      "205/500 [===========>..................] - ETA: 5:03 - loss: 1.9606 - regression_loss: 1.4490 - classification_loss: 0.5116\n",
      "206/500 [===========>..................] - ETA: 5:02 - loss: 1.9614 - regression_loss: 1.4494 - classification_loss: 0.5120\n",
      "207/500 [===========>..................] - ETA: 5:01 - loss: 1.9579 - regression_loss: 1.4470 - classification_loss: 0.5108\n",
      "208/500 [===========>..................] - ETA: 5:00 - loss: 1.9599 - regression_loss: 1.4484 - classification_loss: 0.5114\n",
      "209/500 [===========>..................] - ETA: 4:59 - loss: 1.9573 - regression_loss: 1.4471 - classification_loss: 0.5102\n",
      "210/500 [===========>..................] - ETA: 4:58 - loss: 1.9548 - regression_loss: 1.4451 - classification_loss: 0.5098\n",
      "211/500 [===========>..................] - ETA: 4:57 - loss: 1.9538 - regression_loss: 1.4447 - classification_loss: 0.5091\n",
      "212/500 [===========>..................] - ETA: 4:56 - loss: 1.9544 - regression_loss: 1.4456 - classification_loss: 0.5088\n",
      "213/500 [===========>..................] - ETA: 4:55 - loss: 1.9524 - regression_loss: 1.4442 - classification_loss: 0.5083\n",
      "214/500 [===========>..................] - ETA: 4:54 - loss: 1.9543 - regression_loss: 1.4456 - classification_loss: 0.5087\n",
      "215/500 [===========>..................] - ETA: 4:53 - loss: 1.9575 - regression_loss: 1.4482 - classification_loss: 0.5093\n",
      "216/500 [===========>..................] - ETA: 4:52 - loss: 1.9583 - regression_loss: 1.4490 - classification_loss: 0.5093\n",
      "217/500 [============>.................] - ETA: 4:51 - loss: 1.9560 - regression_loss: 1.4476 - classification_loss: 0.5084\n",
      "218/500 [============>.................] - ETA: 4:50 - loss: 1.9504 - regression_loss: 1.4433 - classification_loss: 0.5071\n",
      "219/500 [============>.................] - ETA: 4:49 - loss: 1.9482 - regression_loss: 1.4421 - classification_loss: 0.5062\n",
      "220/500 [============>.................] - ETA: 4:48 - loss: 1.9477 - regression_loss: 1.4422 - classification_loss: 0.5055\n",
      "221/500 [============>.................] - ETA: 4:47 - loss: 1.9486 - regression_loss: 1.4426 - classification_loss: 0.5060\n",
      "222/500 [============>.................] - ETA: 4:46 - loss: 1.9485 - regression_loss: 1.4427 - classification_loss: 0.5058\n",
      "223/500 [============>.................] - ETA: 4:45 - loss: 1.9468 - regression_loss: 1.4416 - classification_loss: 0.5052\n",
      "224/500 [============>.................] - ETA: 4:44 - loss: 1.9459 - regression_loss: 1.4411 - classification_loss: 0.5048\n",
      "225/500 [============>.................] - ETA: 4:43 - loss: 1.9461 - regression_loss: 1.4411 - classification_loss: 0.5050\n",
      "226/500 [============>.................] - ETA: 4:42 - loss: 1.9488 - regression_loss: 1.4429 - classification_loss: 0.5060\n",
      "227/500 [============>.................] - ETA: 4:41 - loss: 1.9478 - regression_loss: 1.4424 - classification_loss: 0.5054\n",
      "228/500 [============>.................] - ETA: 4:40 - loss: 1.9513 - regression_loss: 1.4449 - classification_loss: 0.5064\n",
      "229/500 [============>.................] - ETA: 4:39 - loss: 1.9479 - regression_loss: 1.4421 - classification_loss: 0.5059\n",
      "230/500 [============>.................] - ETA: 4:38 - loss: 1.9443 - regression_loss: 1.4392 - classification_loss: 0.5051\n",
      "231/500 [============>.................] - ETA: 4:37 - loss: 1.9470 - regression_loss: 1.4408 - classification_loss: 0.5062\n",
      "232/500 [============>.................] - ETA: 4:36 - loss: 1.9464 - regression_loss: 1.4401 - classification_loss: 0.5063\n",
      "233/500 [============>.................] - ETA: 4:35 - loss: 1.9481 - regression_loss: 1.4411 - classification_loss: 0.5070\n",
      "234/500 [=============>................] - ETA: 4:34 - loss: 1.9503 - regression_loss: 1.4428 - classification_loss: 0.5074\n",
      "235/500 [=============>................] - ETA: 4:33 - loss: 1.9510 - regression_loss: 1.4434 - classification_loss: 0.5076\n",
      "236/500 [=============>................] - ETA: 4:32 - loss: 1.9510 - regression_loss: 1.4437 - classification_loss: 0.5073\n",
      "237/500 [=============>................] - ETA: 4:31 - loss: 1.9464 - regression_loss: 1.4400 - classification_loss: 0.5063\n",
      "238/500 [=============>................] - ETA: 4:30 - loss: 1.9450 - regression_loss: 1.4392 - classification_loss: 0.5058\n",
      "239/500 [=============>................] - ETA: 4:29 - loss: 1.9458 - regression_loss: 1.4393 - classification_loss: 0.5065\n",
      "240/500 [=============>................] - ETA: 4:28 - loss: 1.9469 - regression_loss: 1.4403 - classification_loss: 0.5066\n",
      "241/500 [=============>................] - ETA: 4:26 - loss: 1.9471 - regression_loss: 1.4408 - classification_loss: 0.5063\n",
      "242/500 [=============>................] - ETA: 4:25 - loss: 1.9452 - regression_loss: 1.4390 - classification_loss: 0.5062\n",
      "243/500 [=============>................] - ETA: 4:24 - loss: 1.9478 - regression_loss: 1.4414 - classification_loss: 0.5064\n",
      "244/500 [=============>................] - ETA: 4:23 - loss: 1.9473 - regression_loss: 1.4415 - classification_loss: 0.5058\n",
      "245/500 [=============>................] - ETA: 4:22 - loss: 1.9460 - regression_loss: 1.4409 - classification_loss: 0.5050\n",
      "246/500 [=============>................] - ETA: 4:21 - loss: 1.9438 - regression_loss: 1.4394 - classification_loss: 0.5045\n",
      "247/500 [=============>................] - ETA: 4:20 - loss: 1.9451 - regression_loss: 1.4404 - classification_loss: 0.5047\n",
      "248/500 [=============>................] - ETA: 4:19 - loss: 1.9443 - regression_loss: 1.4401 - classification_loss: 0.5042\n",
      "249/500 [=============>................] - ETA: 4:18 - loss: 1.9418 - regression_loss: 1.4380 - classification_loss: 0.5038\n",
      "250/500 [==============>...............] - ETA: 4:17 - loss: 1.9398 - regression_loss: 1.4366 - classification_loss: 0.5032\n",
      "251/500 [==============>...............] - ETA: 4:16 - loss: 1.9409 - regression_loss: 1.4373 - classification_loss: 0.5036\n",
      "252/500 [==============>...............] - ETA: 4:15 - loss: 1.9419 - regression_loss: 1.4384 - classification_loss: 0.5035\n",
      "253/500 [==============>...............] - ETA: 4:14 - loss: 1.9420 - regression_loss: 1.4384 - classification_loss: 0.5036\n",
      "254/500 [==============>...............] - ETA: 4:13 - loss: 1.9431 - regression_loss: 1.4397 - classification_loss: 0.5034\n",
      "255/500 [==============>...............] - ETA: 4:12 - loss: 1.9434 - regression_loss: 1.4399 - classification_loss: 0.5035\n",
      "256/500 [==============>...............] - ETA: 4:11 - loss: 1.9403 - regression_loss: 1.4376 - classification_loss: 0.5027\n",
      "257/500 [==============>...............] - ETA: 4:10 - loss: 1.9429 - regression_loss: 1.4393 - classification_loss: 0.5036\n",
      "258/500 [==============>...............] - ETA: 4:09 - loss: 1.9446 - regression_loss: 1.4406 - classification_loss: 0.5039\n",
      "259/500 [==============>...............] - ETA: 4:08 - loss: 1.9441 - regression_loss: 1.4408 - classification_loss: 0.5033\n",
      "260/500 [==============>...............] - ETA: 4:07 - loss: 1.9411 - regression_loss: 1.4385 - classification_loss: 0.5026\n",
      "261/500 [==============>...............] - ETA: 4:06 - loss: 1.9396 - regression_loss: 1.4378 - classification_loss: 0.5018\n",
      "262/500 [==============>...............] - ETA: 4:04 - loss: 1.9411 - regression_loss: 1.4389 - classification_loss: 0.5022\n",
      "263/500 [==============>...............] - ETA: 4:03 - loss: 1.9423 - regression_loss: 1.4398 - classification_loss: 0.5024\n",
      "264/500 [==============>...............] - ETA: 4:02 - loss: 1.9423 - regression_loss: 1.4397 - classification_loss: 0.5026\n",
      "265/500 [==============>...............] - ETA: 4:01 - loss: 1.9444 - regression_loss: 1.4416 - classification_loss: 0.5029\n",
      "266/500 [==============>...............] - ETA: 4:00 - loss: 1.9414 - regression_loss: 1.4391 - classification_loss: 0.5023\n",
      "267/500 [===============>..............] - ETA: 3:59 - loss: 1.9402 - regression_loss: 1.4384 - classification_loss: 0.5018\n",
      "268/500 [===============>..............] - ETA: 3:58 - loss: 1.9408 - regression_loss: 1.4392 - classification_loss: 0.5016\n",
      "269/500 [===============>..............] - ETA: 3:57 - loss: 1.9398 - regression_loss: 1.4383 - classification_loss: 0.5014\n",
      "270/500 [===============>..............] - ETA: 3:56 - loss: 1.9404 - regression_loss: 1.4388 - classification_loss: 0.5016\n",
      "271/500 [===============>..............] - ETA: 3:55 - loss: 1.9391 - regression_loss: 1.4381 - classification_loss: 0.5010\n",
      "272/500 [===============>..............] - ETA: 3:54 - loss: 1.9401 - regression_loss: 1.4393 - classification_loss: 0.5008\n",
      "273/500 [===============>..............] - ETA: 3:53 - loss: 1.9399 - regression_loss: 1.4394 - classification_loss: 0.5005\n",
      "274/500 [===============>..............] - ETA: 3:52 - loss: 1.9381 - regression_loss: 1.4377 - classification_loss: 0.5004\n",
      "275/500 [===============>..............] - ETA: 3:51 - loss: 1.9380 - regression_loss: 1.4379 - classification_loss: 0.5001\n",
      "276/500 [===============>..............] - ETA: 3:50 - loss: 1.9375 - regression_loss: 1.4374 - classification_loss: 0.5001\n",
      "277/500 [===============>..............] - ETA: 3:49 - loss: 1.9409 - regression_loss: 1.4400 - classification_loss: 0.5009\n",
      "278/500 [===============>..............] - ETA: 3:48 - loss: 1.9422 - regression_loss: 1.4406 - classification_loss: 0.5016\n",
      "279/500 [===============>..............] - ETA: 3:47 - loss: 1.9404 - regression_loss: 1.4394 - classification_loss: 0.5010\n",
      "280/500 [===============>..............] - ETA: 3:46 - loss: 1.9383 - regression_loss: 1.4374 - classification_loss: 0.5009\n",
      "281/500 [===============>..............] - ETA: 3:45 - loss: 1.9391 - regression_loss: 1.4378 - classification_loss: 0.5013\n",
      "282/500 [===============>..............] - ETA: 3:44 - loss: 1.9394 - regression_loss: 1.4378 - classification_loss: 0.5016\n",
      "283/500 [===============>..............] - ETA: 3:43 - loss: 1.9356 - regression_loss: 1.4348 - classification_loss: 0.5009\n",
      "284/500 [================>.............] - ETA: 3:42 - loss: 1.9351 - regression_loss: 1.4349 - classification_loss: 0.5002\n",
      "285/500 [================>.............] - ETA: 3:41 - loss: 1.9338 - regression_loss: 1.4335 - classification_loss: 0.5003\n",
      "286/500 [================>.............] - ETA: 3:40 - loss: 1.9369 - regression_loss: 1.4361 - classification_loss: 0.5008\n",
      "287/500 [================>.............] - ETA: 3:39 - loss: 1.9356 - regression_loss: 1.4353 - classification_loss: 0.5003\n",
      "288/500 [================>.............] - ETA: 3:38 - loss: 1.9337 - regression_loss: 1.4337 - classification_loss: 0.5000\n",
      "289/500 [================>.............] - ETA: 3:37 - loss: 1.9352 - regression_loss: 1.4353 - classification_loss: 0.4999\n",
      "290/500 [================>.............] - ETA: 3:36 - loss: 1.9363 - regression_loss: 1.4358 - classification_loss: 0.5005\n",
      "291/500 [================>.............] - ETA: 3:35 - loss: 1.9404 - regression_loss: 1.4382 - classification_loss: 0.5022\n",
      "292/500 [================>.............] - ETA: 3:34 - loss: 1.9429 - regression_loss: 1.4396 - classification_loss: 0.5033\n",
      "293/500 [================>.............] - ETA: 3:33 - loss: 1.9435 - regression_loss: 1.4403 - classification_loss: 0.5033\n",
      "294/500 [================>.............] - ETA: 3:32 - loss: 1.9436 - regression_loss: 1.4403 - classification_loss: 0.5033\n",
      "295/500 [================>.............] - ETA: 3:31 - loss: 1.9436 - regression_loss: 1.4402 - classification_loss: 0.5034\n",
      "296/500 [================>.............] - ETA: 3:30 - loss: 1.9442 - regression_loss: 1.4408 - classification_loss: 0.5034\n",
      "297/500 [================>.............] - ETA: 3:29 - loss: 1.9434 - regression_loss: 1.4402 - classification_loss: 0.5031\n",
      "298/500 [================>.............] - ETA: 3:28 - loss: 1.9422 - regression_loss: 1.4396 - classification_loss: 0.5026\n",
      "299/500 [================>.............] - ETA: 3:26 - loss: 1.9441 - regression_loss: 1.4405 - classification_loss: 0.5036\n",
      "300/500 [=================>............] - ETA: 3:25 - loss: 1.9427 - regression_loss: 1.4397 - classification_loss: 0.5030\n",
      "301/500 [=================>............] - ETA: 3:24 - loss: 1.9428 - regression_loss: 1.4403 - classification_loss: 0.5025\n",
      "302/500 [=================>............] - ETA: 3:23 - loss: 1.9420 - regression_loss: 1.4395 - classification_loss: 0.5025\n",
      "303/500 [=================>............] - ETA: 3:22 - loss: 1.9410 - regression_loss: 1.4389 - classification_loss: 0.5022\n",
      "304/500 [=================>............] - ETA: 3:21 - loss: 1.9385 - regression_loss: 1.4374 - classification_loss: 0.5012\n",
      "305/500 [=================>............] - ETA: 3:20 - loss: 1.9394 - regression_loss: 1.4384 - classification_loss: 0.5011\n",
      "306/500 [=================>............] - ETA: 3:19 - loss: 1.9387 - regression_loss: 1.4379 - classification_loss: 0.5007\n",
      "307/500 [=================>............] - ETA: 3:18 - loss: 1.9396 - regression_loss: 1.4389 - classification_loss: 0.5007\n",
      "308/500 [=================>............] - ETA: 3:17 - loss: 1.9405 - regression_loss: 1.4398 - classification_loss: 0.5008\n",
      "309/500 [=================>............] - ETA: 3:16 - loss: 1.9380 - regression_loss: 1.4379 - classification_loss: 0.5001\n",
      "310/500 [=================>............] - ETA: 3:15 - loss: 1.9381 - regression_loss: 1.4381 - classification_loss: 0.5000\n",
      "311/500 [=================>............] - ETA: 3:14 - loss: 1.9354 - regression_loss: 1.4360 - classification_loss: 0.4993\n",
      "312/500 [=================>............] - ETA: 3:13 - loss: 1.9345 - regression_loss: 1.4353 - classification_loss: 0.4992\n",
      "313/500 [=================>............] - ETA: 3:12 - loss: 1.9363 - regression_loss: 1.4363 - classification_loss: 0.5001\n",
      "314/500 [=================>............] - ETA: 3:11 - loss: 1.9365 - regression_loss: 1.4370 - classification_loss: 0.4995\n",
      "315/500 [=================>............] - ETA: 3:10 - loss: 1.9357 - regression_loss: 1.4362 - classification_loss: 0.4996\n",
      "316/500 [=================>............] - ETA: 3:09 - loss: 1.9370 - regression_loss: 1.4366 - classification_loss: 0.5005\n",
      "317/500 [==================>...........] - ETA: 3:08 - loss: 1.9360 - regression_loss: 1.4360 - classification_loss: 0.5000\n",
      "318/500 [==================>...........] - ETA: 3:07 - loss: 1.9367 - regression_loss: 1.4369 - classification_loss: 0.4999\n",
      "319/500 [==================>...........] - ETA: 3:06 - loss: 1.9379 - regression_loss: 1.4380 - classification_loss: 0.4999\n",
      "320/500 [==================>...........] - ETA: 3:05 - loss: 1.9377 - regression_loss: 1.4383 - classification_loss: 0.4995\n",
      "321/500 [==================>...........] - ETA: 3:04 - loss: 1.9393 - regression_loss: 1.4397 - classification_loss: 0.4996\n",
      "322/500 [==================>...........] - ETA: 3:03 - loss: 1.9386 - regression_loss: 1.4389 - classification_loss: 0.4997\n",
      "323/500 [==================>...........] - ETA: 3:02 - loss: 1.9392 - regression_loss: 1.4385 - classification_loss: 0.5007\n",
      "324/500 [==================>...........] - ETA: 3:01 - loss: 1.9401 - regression_loss: 1.4390 - classification_loss: 0.5010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/500 [==================>...........] - ETA: 3:00 - loss: 1.9415 - regression_loss: 1.4404 - classification_loss: 0.5011\n",
      "326/500 [==================>...........] - ETA: 2:59 - loss: 1.9404 - regression_loss: 1.4397 - classification_loss: 0.5007\n",
      "327/500 [==================>...........] - ETA: 2:58 - loss: 1.9387 - regression_loss: 1.4384 - classification_loss: 0.5003\n",
      "328/500 [==================>...........] - ETA: 2:57 - loss: 1.9383 - regression_loss: 1.4379 - classification_loss: 0.5004\n",
      "329/500 [==================>...........] - ETA: 2:56 - loss: 1.9393 - regression_loss: 1.4386 - classification_loss: 0.5007\n",
      "330/500 [==================>...........] - ETA: 2:55 - loss: 1.9382 - regression_loss: 1.4376 - classification_loss: 0.5006\n",
      "331/500 [==================>...........] - ETA: 2:54 - loss: 1.9422 - regression_loss: 1.4406 - classification_loss: 0.5016\n",
      "332/500 [==================>...........] - ETA: 2:53 - loss: 1.9413 - regression_loss: 1.4402 - classification_loss: 0.5011\n",
      "333/500 [==================>...........] - ETA: 2:52 - loss: 1.9391 - regression_loss: 1.4387 - classification_loss: 0.5004\n",
      "334/500 [===================>..........] - ETA: 2:51 - loss: 1.9438 - regression_loss: 1.4419 - classification_loss: 0.5018\n",
      "335/500 [===================>..........] - ETA: 2:50 - loss: 1.9434 - regression_loss: 1.4414 - classification_loss: 0.5020\n",
      "336/500 [===================>..........] - ETA: 2:49 - loss: 1.9411 - regression_loss: 1.4397 - classification_loss: 0.5014\n",
      "337/500 [===================>..........] - ETA: 2:47 - loss: 1.9414 - regression_loss: 1.4401 - classification_loss: 0.5013\n",
      "338/500 [===================>..........] - ETA: 2:46 - loss: 1.9400 - regression_loss: 1.4391 - classification_loss: 0.5009\n",
      "339/500 [===================>..........] - ETA: 2:45 - loss: 1.9403 - regression_loss: 1.4398 - classification_loss: 0.5005\n",
      "340/500 [===================>..........] - ETA: 2:44 - loss: 1.9406 - regression_loss: 1.4404 - classification_loss: 0.5001\n",
      "341/500 [===================>..........] - ETA: 2:43 - loss: 1.9409 - regression_loss: 1.4406 - classification_loss: 0.5002\n",
      "342/500 [===================>..........] - ETA: 2:42 - loss: 1.9381 - regression_loss: 1.4387 - classification_loss: 0.4994\n",
      "343/500 [===================>..........] - ETA: 2:41 - loss: 1.9369 - regression_loss: 1.4380 - classification_loss: 0.4989\n",
      "344/500 [===================>..........] - ETA: 2:40 - loss: 1.9375 - regression_loss: 1.4389 - classification_loss: 0.4987\n",
      "345/500 [===================>..........] - ETA: 2:39 - loss: 1.9407 - regression_loss: 1.4410 - classification_loss: 0.4997\n",
      "346/500 [===================>..........] - ETA: 2:38 - loss: 1.9403 - regression_loss: 1.4407 - classification_loss: 0.4996\n",
      "347/500 [===================>..........] - ETA: 2:37 - loss: 1.9391 - regression_loss: 1.4397 - classification_loss: 0.4994\n",
      "348/500 [===================>..........] - ETA: 2:36 - loss: 1.9380 - regression_loss: 1.4389 - classification_loss: 0.4991\n",
      "349/500 [===================>..........] - ETA: 2:35 - loss: 1.9405 - regression_loss: 1.4402 - classification_loss: 0.5003\n",
      "350/500 [====================>.........] - ETA: 2:34 - loss: 1.9382 - regression_loss: 1.4386 - classification_loss: 0.4996\n",
      "351/500 [====================>.........] - ETA: 2:33 - loss: 1.9379 - regression_loss: 1.4384 - classification_loss: 0.4995\n",
      "352/500 [====================>.........] - ETA: 2:32 - loss: 1.9410 - regression_loss: 1.4402 - classification_loss: 0.5008\n",
      "353/500 [====================>.........] - ETA: 2:31 - loss: 1.9433 - regression_loss: 1.4413 - classification_loss: 0.5020\n",
      "354/500 [====================>.........] - ETA: 2:30 - loss: 1.9445 - regression_loss: 1.4418 - classification_loss: 0.5027\n",
      "355/500 [====================>.........] - ETA: 2:29 - loss: 1.9454 - regression_loss: 1.4424 - classification_loss: 0.5030\n",
      "356/500 [====================>.........] - ETA: 2:28 - loss: 1.9461 - regression_loss: 1.4428 - classification_loss: 0.5033\n",
      "357/500 [====================>.........] - ETA: 2:27 - loss: 1.9463 - regression_loss: 1.4433 - classification_loss: 0.5030\n",
      "358/500 [====================>.........] - ETA: 2:26 - loss: 1.9450 - regression_loss: 1.4427 - classification_loss: 0.5024\n",
      "359/500 [====================>.........] - ETA: 2:25 - loss: 1.9458 - regression_loss: 1.4430 - classification_loss: 0.5028\n",
      "360/500 [====================>.........] - ETA: 2:24 - loss: 1.9461 - regression_loss: 1.4429 - classification_loss: 0.5033\n",
      "361/500 [====================>.........] - ETA: 2:23 - loss: 1.9476 - regression_loss: 1.4439 - classification_loss: 0.5037\n",
      "362/500 [====================>.........] - ETA: 2:22 - loss: 1.9474 - regression_loss: 1.4440 - classification_loss: 0.5033\n",
      "363/500 [====================>.........] - ETA: 2:21 - loss: 1.9506 - regression_loss: 1.4464 - classification_loss: 0.5041\n",
      "364/500 [====================>.........] - ETA: 2:20 - loss: 1.9506 - regression_loss: 1.4464 - classification_loss: 0.5042\n",
      "365/500 [====================>.........] - ETA: 2:19 - loss: 1.9543 - regression_loss: 1.4490 - classification_loss: 0.5052\n",
      "366/500 [====================>.........] - ETA: 2:18 - loss: 1.9561 - regression_loss: 1.4506 - classification_loss: 0.5055\n",
      "367/500 [=====================>........] - ETA: 2:17 - loss: 1.9546 - regression_loss: 1.4498 - classification_loss: 0.5049\n",
      "368/500 [=====================>........] - ETA: 2:15 - loss: 1.9542 - regression_loss: 1.4489 - classification_loss: 0.5053\n",
      "369/500 [=====================>........] - ETA: 2:14 - loss: 1.9534 - regression_loss: 1.4478 - classification_loss: 0.5056\n",
      "370/500 [=====================>........] - ETA: 2:13 - loss: 1.9541 - regression_loss: 1.4481 - classification_loss: 0.5060\n",
      "371/500 [=====================>........] - ETA: 2:12 - loss: 1.9536 - regression_loss: 1.4477 - classification_loss: 0.5059\n",
      "372/500 [=====================>........] - ETA: 2:11 - loss: 1.9543 - regression_loss: 1.4484 - classification_loss: 0.5059\n",
      "373/500 [=====================>........] - ETA: 2:10 - loss: 1.9550 - regression_loss: 1.4488 - classification_loss: 0.5062\n",
      "374/500 [=====================>........] - ETA: 2:09 - loss: 1.9546 - regression_loss: 1.4486 - classification_loss: 0.5060\n",
      "375/500 [=====================>........] - ETA: 2:08 - loss: 1.9538 - regression_loss: 1.4480 - classification_loss: 0.5058\n",
      "376/500 [=====================>........] - ETA: 2:07 - loss: 1.9542 - regression_loss: 1.4485 - classification_loss: 0.5056\n",
      "377/500 [=====================>........] - ETA: 2:06 - loss: 1.9565 - regression_loss: 1.4510 - classification_loss: 0.5055\n",
      "378/500 [=====================>........] - ETA: 2:05 - loss: 1.9578 - regression_loss: 1.4518 - classification_loss: 0.5060\n",
      "379/500 [=====================>........] - ETA: 2:04 - loss: 1.9588 - regression_loss: 1.4527 - classification_loss: 0.5062\n",
      "380/500 [=====================>........] - ETA: 2:03 - loss: 1.9591 - regression_loss: 1.4528 - classification_loss: 0.5064\n",
      "381/500 [=====================>........] - ETA: 2:02 - loss: 1.9601 - regression_loss: 1.4538 - classification_loss: 0.5063\n",
      "382/500 [=====================>........] - ETA: 2:01 - loss: 1.9607 - regression_loss: 1.4544 - classification_loss: 0.5063\n",
      "383/500 [=====================>........] - ETA: 2:00 - loss: 1.9606 - regression_loss: 1.4547 - classification_loss: 0.5059\n",
      "384/500 [======================>.......] - ETA: 1:59 - loss: 1.9602 - regression_loss: 1.4544 - classification_loss: 0.5058\n",
      "385/500 [======================>.......] - ETA: 1:58 - loss: 1.9605 - regression_loss: 1.4546 - classification_loss: 0.5059\n",
      "386/500 [======================>.......] - ETA: 1:57 - loss: 1.9578 - regression_loss: 1.4526 - classification_loss: 0.5052\n",
      "387/500 [======================>.......] - ETA: 1:56 - loss: 1.9553 - regression_loss: 1.4507 - classification_loss: 0.5046\n",
      "388/500 [======================>.......] - ETA: 1:55 - loss: 1.9556 - regression_loss: 1.4511 - classification_loss: 0.5045\n",
      "389/500 [======================>.......] - ETA: 1:54 - loss: 1.9567 - regression_loss: 1.4519 - classification_loss: 0.5047\n",
      "390/500 [======================>.......] - ETA: 1:53 - loss: 1.9561 - regression_loss: 1.4512 - classification_loss: 0.5048\n",
      "391/500 [======================>.......] - ETA: 1:52 - loss: 1.9551 - regression_loss: 1.4504 - classification_loss: 0.5047\n",
      "392/500 [======================>.......] - ETA: 1:51 - loss: 1.9553 - regression_loss: 1.4504 - classification_loss: 0.5049\n",
      "393/500 [======================>.......] - ETA: 1:50 - loss: 1.9538 - regression_loss: 1.4495 - classification_loss: 0.5043\n",
      "394/500 [======================>.......] - ETA: 1:49 - loss: 1.9544 - regression_loss: 1.4496 - classification_loss: 0.5048\n",
      "395/500 [======================>.......] - ETA: 1:48 - loss: 1.9536 - regression_loss: 1.4494 - classification_loss: 0.5042\n",
      "396/500 [======================>.......] - ETA: 1:47 - loss: 1.9513 - regression_loss: 1.4475 - classification_loss: 0.5038\n",
      "397/500 [======================>.......] - ETA: 1:46 - loss: 1.9513 - regression_loss: 1.4477 - classification_loss: 0.5036\n",
      "398/500 [======================>.......] - ETA: 1:45 - loss: 1.9510 - regression_loss: 1.4478 - classification_loss: 0.5032\n",
      "399/500 [======================>.......] - ETA: 1:44 - loss: 1.9504 - regression_loss: 1.4475 - classification_loss: 0.5028\n",
      "400/500 [=======================>......] - ETA: 1:43 - loss: 1.9489 - regression_loss: 1.4468 - classification_loss: 0.5021\n",
      "401/500 [=======================>......] - ETA: 1:42 - loss: 1.9496 - regression_loss: 1.4475 - classification_loss: 0.5022\n",
      "402/500 [=======================>......] - ETA: 1:41 - loss: 1.9502 - regression_loss: 1.4477 - classification_loss: 0.5025\n",
      "403/500 [=======================>......] - ETA: 1:40 - loss: 1.9504 - regression_loss: 1.4476 - classification_loss: 0.5028\n",
      "404/500 [=======================>......] - ETA: 1:39 - loss: 1.9514 - regression_loss: 1.4482 - classification_loss: 0.5033\n",
      "405/500 [=======================>......] - ETA: 1:37 - loss: 1.9501 - regression_loss: 1.4473 - classification_loss: 0.5028\n",
      "406/500 [=======================>......] - ETA: 1:36 - loss: 1.9485 - regression_loss: 1.4462 - classification_loss: 0.5023\n",
      "407/500 [=======================>......] - ETA: 1:35 - loss: 1.9504 - regression_loss: 1.4479 - classification_loss: 0.5025\n",
      "408/500 [=======================>......] - ETA: 1:34 - loss: 1.9503 - regression_loss: 1.4480 - classification_loss: 0.5023\n",
      "409/500 [=======================>......] - ETA: 1:33 - loss: 1.9509 - regression_loss: 1.4484 - classification_loss: 0.5025\n",
      "410/500 [=======================>......] - ETA: 1:32 - loss: 1.9517 - regression_loss: 1.4486 - classification_loss: 0.5030\n",
      "411/500 [=======================>......] - ETA: 1:31 - loss: 1.9513 - regression_loss: 1.4482 - classification_loss: 0.5030\n",
      "412/500 [=======================>......] - ETA: 1:30 - loss: 1.9501 - regression_loss: 1.4475 - classification_loss: 0.5026\n",
      "413/500 [=======================>......] - ETA: 1:29 - loss: 1.9510 - regression_loss: 1.4482 - classification_loss: 0.5028\n",
      "414/500 [=======================>......] - ETA: 1:28 - loss: 1.9492 - regression_loss: 1.4471 - classification_loss: 0.5022\n",
      "415/500 [=======================>......] - ETA: 1:27 - loss: 1.9492 - regression_loss: 1.4473 - classification_loss: 0.5019\n",
      "416/500 [=======================>......] - ETA: 1:26 - loss: 1.9474 - regression_loss: 1.4459 - classification_loss: 0.5015\n",
      "417/500 [========================>.....] - ETA: 1:25 - loss: 1.9499 - regression_loss: 1.4474 - classification_loss: 0.5025\n",
      "418/500 [========================>.....] - ETA: 1:24 - loss: 1.9504 - regression_loss: 1.4477 - classification_loss: 0.5028\n",
      "419/500 [========================>.....] - ETA: 1:23 - loss: 1.9504 - regression_loss: 1.4473 - classification_loss: 0.5030\n",
      "420/500 [========================>.....] - ETA: 1:22 - loss: 1.9522 - regression_loss: 1.4483 - classification_loss: 0.5039\n",
      "421/500 [========================>.....] - ETA: 1:21 - loss: 1.9507 - regression_loss: 1.4472 - classification_loss: 0.5035\n",
      "422/500 [========================>.....] - ETA: 1:20 - loss: 1.9491 - regression_loss: 1.4461 - classification_loss: 0.5030\n",
      "423/500 [========================>.....] - ETA: 1:19 - loss: 1.9490 - regression_loss: 1.4461 - classification_loss: 0.5029\n",
      "424/500 [========================>.....] - ETA: 1:18 - loss: 1.9489 - regression_loss: 1.4462 - classification_loss: 0.5027\n",
      "425/500 [========================>.....] - ETA: 1:17 - loss: 1.9478 - regression_loss: 1.4455 - classification_loss: 0.5023\n",
      "426/500 [========================>.....] - ETA: 1:16 - loss: 1.9488 - regression_loss: 1.4458 - classification_loss: 0.5029\n",
      "427/500 [========================>.....] - ETA: 1:15 - loss: 1.9500 - regression_loss: 1.4467 - classification_loss: 0.5033\n",
      "428/500 [========================>.....] - ETA: 1:14 - loss: 1.9482 - regression_loss: 1.4452 - classification_loss: 0.5029\n",
      "429/500 [========================>.....] - ETA: 1:13 - loss: 1.9510 - regression_loss: 1.4471 - classification_loss: 0.5040\n",
      "430/500 [========================>.....] - ETA: 1:12 - loss: 1.9498 - regression_loss: 1.4462 - classification_loss: 0.5036\n",
      "431/500 [========================>.....] - ETA: 1:11 - loss: 1.9503 - regression_loss: 1.4463 - classification_loss: 0.5039\n",
      "432/500 [========================>.....] - ETA: 1:10 - loss: 1.9506 - regression_loss: 1.4464 - classification_loss: 0.5042\n",
      "433/500 [========================>.....] - ETA: 1:09 - loss: 1.9503 - regression_loss: 1.4463 - classification_loss: 0.5040\n",
      "434/500 [=========================>....] - ETA: 1:08 - loss: 1.9547 - regression_loss: 1.4497 - classification_loss: 0.5051\n",
      "435/500 [=========================>....] - ETA: 1:06 - loss: 1.9543 - regression_loss: 1.4494 - classification_loss: 0.5049\n",
      "436/500 [=========================>....] - ETA: 1:05 - loss: 1.9528 - regression_loss: 1.4486 - classification_loss: 0.5042\n",
      "437/500 [=========================>....] - ETA: 1:04 - loss: 1.9536 - regression_loss: 1.4493 - classification_loss: 0.5044\n",
      "438/500 [=========================>....] - ETA: 1:03 - loss: 1.9539 - regression_loss: 1.4492 - classification_loss: 0.5047\n",
      "439/500 [=========================>....] - ETA: 1:02 - loss: 1.9540 - regression_loss: 1.4495 - classification_loss: 0.5045\n",
      "440/500 [=========================>....] - ETA: 1:01 - loss: 1.9537 - regression_loss: 1.4495 - classification_loss: 0.5042\n",
      "441/500 [=========================>....] - ETA: 1:00 - loss: 1.9543 - regression_loss: 1.4495 - classification_loss: 0.5049\n",
      "442/500 [=========================>....] - ETA: 59s - loss: 1.9520 - regression_loss: 1.4476 - classification_loss: 0.5044 \n",
      "443/500 [=========================>....] - ETA: 58s - loss: 1.9526 - regression_loss: 1.4481 - classification_loss: 0.5044\n",
      "444/500 [=========================>....] - ETA: 57s - loss: 1.9520 - regression_loss: 1.4476 - classification_loss: 0.5044\n",
      "445/500 [=========================>....] - ETA: 56s - loss: 1.9520 - regression_loss: 1.4476 - classification_loss: 0.5044\n",
      "446/500 [=========================>....] - ETA: 55s - loss: 1.9507 - regression_loss: 1.4469 - classification_loss: 0.5038\n",
      "447/500 [=========================>....] - ETA: 54s - loss: 1.9491 - regression_loss: 1.4456 - classification_loss: 0.5035\n",
      "448/500 [=========================>....] - ETA: 53s - loss: 1.9518 - regression_loss: 1.4476 - classification_loss: 0.5042\n",
      "449/500 [=========================>....] - ETA: 52s - loss: 1.9524 - regression_loss: 1.4481 - classification_loss: 0.5043\n",
      "450/500 [==========================>...] - ETA: 51s - loss: 1.9554 - regression_loss: 1.4500 - classification_loss: 0.5054\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 1.9573 - regression_loss: 1.4515 - classification_loss: 0.5058\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 1.9591 - regression_loss: 1.4526 - classification_loss: 0.5065\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.9597 - regression_loss: 1.4533 - classification_loss: 0.5065\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.9603 - regression_loss: 1.4535 - classification_loss: 0.5068\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.9607 - regression_loss: 1.4537 - classification_loss: 0.5070\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.9609 - regression_loss: 1.4539 - classification_loss: 0.5070\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.9637 - regression_loss: 1.4558 - classification_loss: 0.5079\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.9633 - regression_loss: 1.4557 - classification_loss: 0.5076\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.9650 - regression_loss: 1.4570 - classification_loss: 0.5080\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.9653 - regression_loss: 1.4570 - classification_loss: 0.5083\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 1.9660 - regression_loss: 1.4574 - classification_loss: 0.5085\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 1.9652 - regression_loss: 1.4567 - classification_loss: 0.5085\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 1.9658 - regression_loss: 1.4573 - classification_loss: 0.5085\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 1.9641 - regression_loss: 1.4562 - classification_loss: 0.5080\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 1.9653 - regression_loss: 1.4570 - classification_loss: 0.5083\n",
      "466/500 [==========================>...] - ETA: 34s - loss: 1.9670 - regression_loss: 1.4582 - classification_loss: 0.5089\n",
      "467/500 [===========================>..] - ETA: 33s - loss: 1.9675 - regression_loss: 1.4586 - classification_loss: 0.5089\n",
      "468/500 [===========================>..] - ETA: 32s - loss: 1.9663 - regression_loss: 1.4578 - classification_loss: 0.5085\n",
      "469/500 [===========================>..] - ETA: 31s - loss: 1.9672 - regression_loss: 1.4585 - classification_loss: 0.5087\n",
      "470/500 [===========================>..] - ETA: 30s - loss: 1.9666 - regression_loss: 1.4580 - classification_loss: 0.5086\n",
      "471/500 [===========================>..] - ETA: 29s - loss: 1.9645 - regression_loss: 1.4563 - classification_loss: 0.5082\n",
      "472/500 [===========================>..] - ETA: 28s - loss: 1.9667 - regression_loss: 1.4579 - classification_loss: 0.5088\n",
      "473/500 [===========================>..] - ETA: 27s - loss: 1.9670 - regression_loss: 1.4580 - classification_loss: 0.5089\n",
      "474/500 [===========================>..] - ETA: 26s - loss: 1.9673 - regression_loss: 1.4583 - classification_loss: 0.5090\n",
      "475/500 [===========================>..] - ETA: 25s - loss: 1.9689 - regression_loss: 1.4591 - classification_loss: 0.5097\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 1.9688 - regression_loss: 1.4591 - classification_loss: 0.5097\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.9693 - regression_loss: 1.4593 - classification_loss: 0.5100\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.9695 - regression_loss: 1.4597 - classification_loss: 0.5098\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.9706 - regression_loss: 1.4607 - classification_loss: 0.5099\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.9696 - regression_loss: 1.4599 - classification_loss: 0.5097\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.9688 - regression_loss: 1.4593 - classification_loss: 0.5096\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.9698 - regression_loss: 1.4596 - classification_loss: 0.5102\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.9696 - regression_loss: 1.4595 - classification_loss: 0.5101\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.9714 - regression_loss: 1.4610 - classification_loss: 0.5104\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.9711 - regression_loss: 1.4607 - classification_loss: 0.5104\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.9723 - regression_loss: 1.4616 - classification_loss: 0.5107\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.9738 - regression_loss: 1.4625 - classification_loss: 0.5112\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.9728 - regression_loss: 1.4617 - classification_loss: 0.5111\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.9725 - regression_loss: 1.4615 - classification_loss: 0.5109\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.9743 - regression_loss: 1.4626 - classification_loss: 0.5117\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.9740 - regression_loss: 1.4626 - classification_loss: 0.5114 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.9744 - regression_loss: 1.4629 - classification_loss: 0.5115\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.9734 - regression_loss: 1.4622 - classification_loss: 0.5112\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.9742 - regression_loss: 1.4624 - classification_loss: 0.5119\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.9765 - regression_loss: 1.4634 - classification_loss: 0.5130\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.9749 - regression_loss: 1.4623 - classification_loss: 0.5126\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.9746 - regression_loss: 1.4622 - classification_loss: 0.5124\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.9731 - regression_loss: 1.4610 - classification_loss: 0.5121\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.9744 - regression_loss: 1.4620 - classification_loss: 0.5123\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9761 - regression_loss: 1.4633 - classification_loss: 0.5128\n",
      "Epoch 00010: saving model to ./snapshots\\resnet50_csv_10.h5\n",
      "\n",
      "500/500 [==============================] - 515s 1s/step - loss: 1.9761 - regression_loss: 1.4633 - classification_loss: 0.5128\n",
      "Epoch 11/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 2.2768 - regression_loss: 1.6682 - classification_loss: 0.6086\n",
      "  2/500 [..............................] - ETA: 4:05 - loss: 2.1969 - regression_loss: 1.6361 - classification_loss: 0.5608\n",
      "  3/500 [..............................] - ETA: 5:28 - loss: 1.9093 - regression_loss: 1.4473 - classification_loss: 0.4621\n",
      "  4/500 [..............................] - ETA: 5:44 - loss: 2.0187 - regression_loss: 1.5491 - classification_loss: 0.4697\n",
      "  5/500 [..............................] - ETA: 6:30 - loss: 2.1193 - regression_loss: 1.6366 - classification_loss: 0.4826\n",
      "  6/500 [..............................] - ETA: 6:45 - loss: 2.1713 - regression_loss: 1.6504 - classification_loss: 0.5208\n",
      "  7/500 [..............................] - ETA: 6:55 - loss: 2.1947 - regression_loss: 1.6793 - classification_loss: 0.5154\n",
      "  8/500 [..............................] - ETA: 6:59 - loss: 2.2279 - regression_loss: 1.6849 - classification_loss: 0.5430\n",
      "  9/500 [..............................] - ETA: 7:06 - loss: 2.1875 - regression_loss: 1.6488 - classification_loss: 0.5387\n",
      " 10/500 [..............................] - ETA: 7:11 - loss: 2.1519 - regression_loss: 1.6250 - classification_loss: 0.5269\n",
      " 11/500 [..............................] - ETA: 7:18 - loss: 2.1437 - regression_loss: 1.6205 - classification_loss: 0.5233\n",
      " 12/500 [..............................] - ETA: 7:12 - loss: 2.1042 - regression_loss: 1.5845 - classification_loss: 0.5198\n",
      " 13/500 [..............................] - ETA: 7:23 - loss: 2.0205 - regression_loss: 1.5245 - classification_loss: 0.4960\n",
      " 14/500 [..............................] - ETA: 7:27 - loss: 2.0447 - regression_loss: 1.5444 - classification_loss: 0.5003\n",
      " 15/500 [..............................] - ETA: 7:33 - loss: 2.0301 - regression_loss: 1.5307 - classification_loss: 0.4994\n",
      " 16/500 [..............................] - ETA: 7:36 - loss: 2.0810 - regression_loss: 1.5545 - classification_loss: 0.5265\n",
      " 17/500 [>.............................] - ETA: 7:40 - loss: 2.1098 - regression_loss: 1.5610 - classification_loss: 0.5488\n",
      " 18/500 [>.............................] - ETA: 7:43 - loss: 2.0836 - regression_loss: 1.5325 - classification_loss: 0.5511\n",
      " 19/500 [>.............................] - ETA: 7:47 - loss: 2.0712 - regression_loss: 1.5171 - classification_loss: 0.5541\n",
      " 20/500 [>.............................] - ETA: 7:49 - loss: 2.0778 - regression_loss: 1.5264 - classification_loss: 0.5514\n",
      " 21/500 [>.............................] - ETA: 7:51 - loss: 2.0957 - regression_loss: 1.5490 - classification_loss: 0.5466\n",
      " 22/500 [>.............................] - ETA: 7:52 - loss: 2.1159 - regression_loss: 1.5646 - classification_loss: 0.5513\n",
      " 23/500 [>.............................] - ETA: 7:51 - loss: 2.0877 - regression_loss: 1.5425 - classification_loss: 0.5452\n",
      " 24/500 [>.............................] - ETA: 7:49 - loss: 2.0560 - regression_loss: 1.5176 - classification_loss: 0.5383\n",
      " 25/500 [>.............................] - ETA: 7:50 - loss: 2.0130 - regression_loss: 1.4822 - classification_loss: 0.5308\n",
      " 26/500 [>.............................] - ETA: 7:51 - loss: 1.9879 - regression_loss: 1.4649 - classification_loss: 0.5230\n",
      " 27/500 [>.............................] - ETA: 7:51 - loss: 1.9919 - regression_loss: 1.4697 - classification_loss: 0.5222\n",
      " 28/500 [>.............................] - ETA: 7:52 - loss: 2.0111 - regression_loss: 1.4790 - classification_loss: 0.5320\n",
      " 29/500 [>.............................] - ETA: 7:50 - loss: 1.9875 - regression_loss: 1.4632 - classification_loss: 0.5243\n",
      " 30/500 [>.............................] - ETA: 7:51 - loss: 1.9933 - regression_loss: 1.4683 - classification_loss: 0.5250\n",
      " 31/500 [>.............................] - ETA: 7:46 - loss: 1.9999 - regression_loss: 1.4739 - classification_loss: 0.5260\n",
      " 32/500 [>.............................] - ETA: 7:48 - loss: 1.9959 - regression_loss: 1.4733 - classification_loss: 0.5226\n",
      " 33/500 [>.............................] - ETA: 7:47 - loss: 1.9668 - regression_loss: 1.4531 - classification_loss: 0.5138\n",
      " 34/500 [=>............................] - ETA: 7:46 - loss: 1.9561 - regression_loss: 1.4496 - classification_loss: 0.5065\n",
      " 35/500 [=>............................] - ETA: 7:45 - loss: 1.9435 - regression_loss: 1.4408 - classification_loss: 0.5027\n",
      " 36/500 [=>............................] - ETA: 7:43 - loss: 1.9588 - regression_loss: 1.4492 - classification_loss: 0.5097\n",
      " 37/500 [=>............................] - ETA: 7:43 - loss: 1.9765 - regression_loss: 1.4615 - classification_loss: 0.5150\n",
      " 38/500 [=>............................] - ETA: 7:42 - loss: 1.9713 - regression_loss: 1.4574 - classification_loss: 0.5138\n",
      " 39/500 [=>............................] - ETA: 7:41 - loss: 1.9690 - regression_loss: 1.4585 - classification_loss: 0.5105\n",
      " 40/500 [=>............................] - ETA: 7:40 - loss: 1.9521 - regression_loss: 1.4469 - classification_loss: 0.5052\n",
      " 41/500 [=>............................] - ETA: 7:40 - loss: 1.9605 - regression_loss: 1.4547 - classification_loss: 0.5057\n",
      " 42/500 [=>............................] - ETA: 7:39 - loss: 1.9392 - regression_loss: 1.4391 - classification_loss: 0.5000\n",
      " 43/500 [=>............................] - ETA: 7:38 - loss: 1.9624 - regression_loss: 1.4555 - classification_loss: 0.5069\n",
      " 44/500 [=>............................] - ETA: 7:36 - loss: 1.9720 - regression_loss: 1.4622 - classification_loss: 0.5098\n",
      " 45/500 [=>............................] - ETA: 7:35 - loss: 1.9594 - regression_loss: 1.4556 - classification_loss: 0.5037\n",
      " 46/500 [=>............................] - ETA: 7:34 - loss: 1.9547 - regression_loss: 1.4519 - classification_loss: 0.5028\n",
      " 47/500 [=>............................] - ETA: 7:34 - loss: 1.9721 - regression_loss: 1.4652 - classification_loss: 0.5070\n",
      " 48/500 [=>............................] - ETA: 7:31 - loss: 1.9668 - regression_loss: 1.4629 - classification_loss: 0.5038\n",
      " 49/500 [=>............................] - ETA: 7:32 - loss: 1.9576 - regression_loss: 1.4560 - classification_loss: 0.5016\n",
      " 50/500 [==>...........................] - ETA: 7:31 - loss: 1.9697 - regression_loss: 1.4641 - classification_loss: 0.5057\n",
      " 51/500 [==>...........................] - ETA: 7:29 - loss: 1.9583 - regression_loss: 1.4550 - classification_loss: 0.5033\n",
      " 52/500 [==>...........................] - ETA: 7:29 - loss: 1.9513 - regression_loss: 1.4500 - classification_loss: 0.5013\n",
      " 53/500 [==>...........................] - ETA: 7:28 - loss: 1.9395 - regression_loss: 1.4416 - classification_loss: 0.4980\n",
      " 54/500 [==>...........................] - ETA: 7:27 - loss: 1.9228 - regression_loss: 1.4306 - classification_loss: 0.4921\n",
      " 55/500 [==>...........................] - ETA: 7:27 - loss: 1.9335 - regression_loss: 1.4406 - classification_loss: 0.4929\n",
      " 56/500 [==>...........................] - ETA: 7:25 - loss: 1.9455 - regression_loss: 1.4447 - classification_loss: 0.5008\n",
      " 57/500 [==>...........................] - ETA: 7:24 - loss: 1.9422 - regression_loss: 1.4436 - classification_loss: 0.4986\n",
      " 58/500 [==>...........................] - ETA: 7:24 - loss: 1.9459 - regression_loss: 1.4448 - classification_loss: 0.5012\n",
      " 59/500 [==>...........................] - ETA: 7:23 - loss: 1.9396 - regression_loss: 1.4405 - classification_loss: 0.4991\n",
      " 60/500 [==>...........................] - ETA: 7:22 - loss: 1.9288 - regression_loss: 1.4342 - classification_loss: 0.4946\n",
      " 61/500 [==>...........................] - ETA: 7:21 - loss: 1.9287 - regression_loss: 1.4345 - classification_loss: 0.4942\n",
      " 62/500 [==>...........................] - ETA: 7:21 - loss: 1.9299 - regression_loss: 1.4358 - classification_loss: 0.4941\n",
      " 63/500 [==>...........................] - ETA: 7:19 - loss: 1.9301 - regression_loss: 1.4335 - classification_loss: 0.4966\n",
      " 64/500 [==>...........................] - ETA: 7:19 - loss: 1.9264 - regression_loss: 1.4322 - classification_loss: 0.4942\n",
      " 65/500 [==>...........................] - ETA: 7:18 - loss: 1.9238 - regression_loss: 1.4321 - classification_loss: 0.4916\n",
      " 66/500 [==>...........................] - ETA: 7:18 - loss: 1.9125 - regression_loss: 1.4247 - classification_loss: 0.4878\n",
      " 67/500 [===>..........................] - ETA: 7:17 - loss: 1.9134 - regression_loss: 1.4257 - classification_loss: 0.4876\n",
      " 68/500 [===>..........................] - ETA: 7:40 - loss: 1.9272 - regression_loss: 1.4323 - classification_loss: 0.4949\n",
      " 69/500 [===>..........................] - ETA: 7:39 - loss: 1.9256 - regression_loss: 1.4319 - classification_loss: 0.4938\n",
      " 70/500 [===>..........................] - ETA: 7:37 - loss: 1.9283 - regression_loss: 1.4314 - classification_loss: 0.4969\n",
      " 71/500 [===>..........................] - ETA: 7:36 - loss: 1.9340 - regression_loss: 1.4354 - classification_loss: 0.4986\n",
      " 72/500 [===>..........................] - ETA: 7:35 - loss: 1.9262 - regression_loss: 1.4297 - classification_loss: 0.4965\n",
      " 73/500 [===>..........................] - ETA: 7:34 - loss: 1.9278 - regression_loss: 1.4313 - classification_loss: 0.4965\n",
      " 74/500 [===>..........................] - ETA: 7:33 - loss: 1.9204 - regression_loss: 1.4260 - classification_loss: 0.4944\n",
      " 75/500 [===>..........................] - ETA: 7:31 - loss: 1.9225 - regression_loss: 1.4275 - classification_loss: 0.4950\n",
      " 76/500 [===>..........................] - ETA: 7:31 - loss: 1.9212 - regression_loss: 1.4251 - classification_loss: 0.4961\n",
      " 77/500 [===>..........................] - ETA: 7:28 - loss: 1.9164 - regression_loss: 1.4223 - classification_loss: 0.4941\n",
      " 78/500 [===>..........................] - ETA: 7:28 - loss: 1.9233 - regression_loss: 1.4272 - classification_loss: 0.4961\n",
      " 79/500 [===>..........................] - ETA: 7:26 - loss: 1.9189 - regression_loss: 1.4249 - classification_loss: 0.4940\n",
      " 80/500 [===>..........................] - ETA: 7:25 - loss: 1.9133 - regression_loss: 1.4214 - classification_loss: 0.4919\n",
      " 81/500 [===>..........................] - ETA: 7:23 - loss: 1.9142 - regression_loss: 1.4215 - classification_loss: 0.4927\n",
      " 82/500 [===>..........................] - ETA: 7:22 - loss: 1.9143 - regression_loss: 1.4225 - classification_loss: 0.4918\n",
      " 83/500 [===>..........................] - ETA: 7:20 - loss: 1.9203 - regression_loss: 1.4255 - classification_loss: 0.4948\n",
      " 84/500 [====>.........................] - ETA: 7:19 - loss: 1.9143 - regression_loss: 1.4222 - classification_loss: 0.4921\n",
      " 85/500 [====>.........................] - ETA: 7:18 - loss: 1.9081 - regression_loss: 1.4166 - classification_loss: 0.4915\n",
      " 86/500 [====>.........................] - ETA: 7:16 - loss: 1.9036 - regression_loss: 1.4136 - classification_loss: 0.4900\n",
      " 87/500 [====>.........................] - ETA: 7:15 - loss: 1.9075 - regression_loss: 1.4154 - classification_loss: 0.4921\n",
      " 88/500 [====>.........................] - ETA: 7:14 - loss: 1.9035 - regression_loss: 1.4131 - classification_loss: 0.4904\n",
      " 89/500 [====>.........................] - ETA: 7:13 - loss: 1.9156 - regression_loss: 1.4216 - classification_loss: 0.4940\n",
      " 90/500 [====>.........................] - ETA: 7:12 - loss: 1.9101 - regression_loss: 1.4181 - classification_loss: 0.4920\n",
      " 91/500 [====>.........................] - ETA: 7:12 - loss: 1.9126 - regression_loss: 1.4203 - classification_loss: 0.4923\n",
      " 92/500 [====>.........................] - ETA: 7:11 - loss: 1.9127 - regression_loss: 1.4200 - classification_loss: 0.4927\n",
      " 93/500 [====>.........................] - ETA: 7:10 - loss: 1.9166 - regression_loss: 1.4227 - classification_loss: 0.4939\n",
      " 94/500 [====>.........................] - ETA: 7:08 - loss: 1.9253 - regression_loss: 1.4295 - classification_loss: 0.4959\n",
      " 95/500 [====>.........................] - ETA: 7:07 - loss: 1.9245 - regression_loss: 1.4289 - classification_loss: 0.4956\n",
      " 96/500 [====>.........................] - ETA: 7:06 - loss: 1.9275 - regression_loss: 1.4320 - classification_loss: 0.4955\n",
      " 97/500 [====>.........................] - ETA: 7:05 - loss: 1.9297 - regression_loss: 1.4347 - classification_loss: 0.4951\n",
      " 98/500 [====>.........................] - ETA: 7:04 - loss: 1.9295 - regression_loss: 1.4343 - classification_loss: 0.4952\n",
      " 99/500 [====>.........................] - ETA: 7:03 - loss: 1.9279 - regression_loss: 1.4326 - classification_loss: 0.4954\n",
      "100/500 [=====>........................] - ETA: 7:02 - loss: 1.9227 - regression_loss: 1.4289 - classification_loss: 0.4939\n",
      "101/500 [=====>........................] - ETA: 7:00 - loss: 1.9245 - regression_loss: 1.4295 - classification_loss: 0.4950\n",
      "102/500 [=====>........................] - ETA: 6:59 - loss: 1.9311 - regression_loss: 1.4343 - classification_loss: 0.4968\n",
      "103/500 [=====>........................] - ETA: 6:57 - loss: 1.9241 - regression_loss: 1.4291 - classification_loss: 0.4950\n",
      "104/500 [=====>........................] - ETA: 6:56 - loss: 1.9201 - regression_loss: 1.4250 - classification_loss: 0.4950\n",
      "105/500 [=====>........................] - ETA: 6:55 - loss: 1.9178 - regression_loss: 1.4222 - classification_loss: 0.4955\n",
      "106/500 [=====>........................] - ETA: 6:54 - loss: 1.9225 - regression_loss: 1.4271 - classification_loss: 0.4954\n",
      "107/500 [=====>........................] - ETA: 6:53 - loss: 1.9206 - regression_loss: 1.4261 - classification_loss: 0.4946\n",
      "108/500 [=====>........................] - ETA: 6:52 - loss: 1.9216 - regression_loss: 1.4265 - classification_loss: 0.4950\n",
      "109/500 [=====>........................] - ETA: 6:50 - loss: 1.9287 - regression_loss: 1.4316 - classification_loss: 0.4971\n",
      "110/500 [=====>........................] - ETA: 6:49 - loss: 1.9269 - regression_loss: 1.4304 - classification_loss: 0.4966\n",
      "111/500 [=====>........................] - ETA: 6:48 - loss: 1.9293 - regression_loss: 1.4315 - classification_loss: 0.4978\n",
      "112/500 [=====>........................] - ETA: 6:47 - loss: 1.9352 - regression_loss: 1.4364 - classification_loss: 0.4989\n",
      "113/500 [=====>........................] - ETA: 6:46 - loss: 1.9329 - regression_loss: 1.4347 - classification_loss: 0.4982\n",
      "114/500 [=====>........................] - ETA: 6:45 - loss: 1.9335 - regression_loss: 1.4354 - classification_loss: 0.4981\n",
      "115/500 [=====>........................] - ETA: 6:43 - loss: 1.9367 - regression_loss: 1.4365 - classification_loss: 0.5002\n",
      "116/500 [=====>........................] - ETA: 6:43 - loss: 1.9320 - regression_loss: 1.4314 - classification_loss: 0.5006\n",
      "117/500 [======>.......................] - ETA: 6:42 - loss: 1.9397 - regression_loss: 1.4370 - classification_loss: 0.5026\n",
      "118/500 [======>.......................] - ETA: 6:41 - loss: 1.9407 - regression_loss: 1.4382 - classification_loss: 0.5025\n",
      "119/500 [======>.......................] - ETA: 6:39 - loss: 1.9390 - regression_loss: 1.4366 - classification_loss: 0.5024\n",
      "120/500 [======>.......................] - ETA: 6:38 - loss: 1.9429 - regression_loss: 1.4401 - classification_loss: 0.5028\n",
      "121/500 [======>.......................] - ETA: 6:37 - loss: 1.9451 - regression_loss: 1.4419 - classification_loss: 0.5032\n",
      "122/500 [======>.......................] - ETA: 6:36 - loss: 1.9519 - regression_loss: 1.4481 - classification_loss: 0.5039\n",
      "123/500 [======>.......................] - ETA: 6:34 - loss: 1.9459 - regression_loss: 1.4443 - classification_loss: 0.5016\n",
      "124/500 [======>.......................] - ETA: 6:34 - loss: 1.9507 - regression_loss: 1.4475 - classification_loss: 0.5032\n",
      "125/500 [======>.......................] - ETA: 6:33 - loss: 1.9573 - regression_loss: 1.4534 - classification_loss: 0.5039\n",
      "126/500 [======>.......................] - ETA: 6:31 - loss: 1.9529 - regression_loss: 1.4503 - classification_loss: 0.5026\n",
      "127/500 [======>.......................] - ETA: 6:30 - loss: 1.9556 - regression_loss: 1.4519 - classification_loss: 0.5037\n",
      "128/500 [======>.......................] - ETA: 6:29 - loss: 1.9529 - regression_loss: 1.4498 - classification_loss: 0.5031\n",
      "129/500 [======>.......................] - ETA: 6:28 - loss: 1.9507 - regression_loss: 1.4490 - classification_loss: 0.5017\n",
      "130/500 [======>.......................] - ETA: 6:27 - loss: 1.9478 - regression_loss: 1.4469 - classification_loss: 0.5009\n",
      "131/500 [======>.......................] - ETA: 6:26 - loss: 1.9480 - regression_loss: 1.4475 - classification_loss: 0.5005\n",
      "132/500 [======>.......................] - ETA: 6:25 - loss: 1.9465 - regression_loss: 1.4478 - classification_loss: 0.4987\n",
      "133/500 [======>.......................] - ETA: 6:24 - loss: 1.9462 - regression_loss: 1.4470 - classification_loss: 0.4992\n",
      "134/500 [=======>......................] - ETA: 6:23 - loss: 1.9495 - regression_loss: 1.4494 - classification_loss: 0.5001\n",
      "135/500 [=======>......................] - ETA: 6:22 - loss: 1.9489 - regression_loss: 1.4496 - classification_loss: 0.4993\n",
      "136/500 [=======>......................] - ETA: 6:21 - loss: 1.9531 - regression_loss: 1.4522 - classification_loss: 0.5009\n",
      "137/500 [=======>......................] - ETA: 6:20 - loss: 1.9541 - regression_loss: 1.4520 - classification_loss: 0.5021\n",
      "138/500 [=======>......................] - ETA: 6:19 - loss: 1.9534 - regression_loss: 1.4508 - classification_loss: 0.5026\n",
      "139/500 [=======>......................] - ETA: 6:18 - loss: 1.9525 - regression_loss: 1.4502 - classification_loss: 0.5023\n",
      "140/500 [=======>......................] - ETA: 6:16 - loss: 1.9515 - regression_loss: 1.4488 - classification_loss: 0.5027\n",
      "141/500 [=======>......................] - ETA: 6:15 - loss: 1.9509 - regression_loss: 1.4475 - classification_loss: 0.5034\n",
      "142/500 [=======>......................] - ETA: 6:14 - loss: 1.9475 - regression_loss: 1.4456 - classification_loss: 0.5019\n",
      "143/500 [=======>......................] - ETA: 6:13 - loss: 1.9456 - regression_loss: 1.4440 - classification_loss: 0.5016\n",
      "144/500 [=======>......................] - ETA: 6:12 - loss: 1.9462 - regression_loss: 1.4449 - classification_loss: 0.5013\n",
      "145/500 [=======>......................] - ETA: 6:11 - loss: 1.9475 - regression_loss: 1.4454 - classification_loss: 0.5021\n",
      "146/500 [=======>......................] - ETA: 6:10 - loss: 1.9511 - regression_loss: 1.4466 - classification_loss: 0.5044\n",
      "147/500 [=======>......................] - ETA: 6:09 - loss: 1.9439 - regression_loss: 1.4414 - classification_loss: 0.5025\n",
      "148/500 [=======>......................] - ETA: 6:08 - loss: 1.9367 - regression_loss: 1.4356 - classification_loss: 0.5011\n",
      "149/500 [=======>......................] - ETA: 6:07 - loss: 1.9404 - regression_loss: 1.4383 - classification_loss: 0.5021\n",
      "150/500 [========>.....................] - ETA: 6:06 - loss: 1.9387 - regression_loss: 1.4380 - classification_loss: 0.5007\n",
      "151/500 [========>.....................] - ETA: 6:05 - loss: 1.9380 - regression_loss: 1.4376 - classification_loss: 0.5005\n",
      "152/500 [========>.....................] - ETA: 6:04 - loss: 1.9362 - regression_loss: 1.4361 - classification_loss: 0.5001\n",
      "153/500 [========>.....................] - ETA: 6:03 - loss: 1.9380 - regression_loss: 1.4373 - classification_loss: 0.5007\n",
      "154/500 [========>.....................] - ETA: 6:02 - loss: 1.9430 - regression_loss: 1.4413 - classification_loss: 0.5017\n",
      "155/500 [========>.....................] - ETA: 6:01 - loss: 1.9392 - regression_loss: 1.4386 - classification_loss: 0.5006\n",
      "156/500 [========>.....................] - ETA: 6:00 - loss: 1.9394 - regression_loss: 1.4398 - classification_loss: 0.4996\n",
      "157/500 [========>.....................] - ETA: 5:59 - loss: 1.9355 - regression_loss: 1.4369 - classification_loss: 0.4986\n",
      "158/500 [========>.....................] - ETA: 5:58 - loss: 1.9337 - regression_loss: 1.4337 - classification_loss: 0.5000\n",
      "159/500 [========>.....................] - ETA: 5:57 - loss: 1.9324 - regression_loss: 1.4331 - classification_loss: 0.4993\n",
      "160/500 [========>.....................] - ETA: 5:56 - loss: 1.9328 - regression_loss: 1.4331 - classification_loss: 0.4997\n",
      "161/500 [========>.....................] - ETA: 5:55 - loss: 1.9294 - regression_loss: 1.4304 - classification_loss: 0.4990\n",
      "162/500 [========>.....................] - ETA: 5:54 - loss: 1.9301 - regression_loss: 1.4303 - classification_loss: 0.4998\n",
      "163/500 [========>.....................] - ETA: 5:53 - loss: 1.9335 - regression_loss: 1.4333 - classification_loss: 0.5002\n",
      "164/500 [========>.....................] - ETA: 5:51 - loss: 1.9368 - regression_loss: 1.4353 - classification_loss: 0.5014\n",
      "165/500 [========>.....................] - ETA: 5:50 - loss: 1.9366 - regression_loss: 1.4362 - classification_loss: 0.5004\n",
      "166/500 [========>.....................] - ETA: 5:49 - loss: 1.9356 - regression_loss: 1.4361 - classification_loss: 0.4995\n",
      "167/500 [=========>....................] - ETA: 5:48 - loss: 1.9331 - regression_loss: 1.4344 - classification_loss: 0.4986\n",
      "168/500 [=========>....................] - ETA: 5:47 - loss: 1.9371 - regression_loss: 1.4383 - classification_loss: 0.4988\n",
      "169/500 [=========>....................] - ETA: 5:46 - loss: 1.9404 - regression_loss: 1.4414 - classification_loss: 0.4991\n",
      "170/500 [=========>....................] - ETA: 5:45 - loss: 1.9405 - regression_loss: 1.4403 - classification_loss: 0.5002\n",
      "171/500 [=========>....................] - ETA: 5:44 - loss: 1.9437 - regression_loss: 1.4423 - classification_loss: 0.5014\n",
      "172/500 [=========>....................] - ETA: 5:43 - loss: 1.9390 - regression_loss: 1.4392 - classification_loss: 0.4998\n",
      "173/500 [=========>....................] - ETA: 5:42 - loss: 1.9384 - regression_loss: 1.4391 - classification_loss: 0.4993\n",
      "174/500 [=========>....................] - ETA: 5:41 - loss: 1.9399 - regression_loss: 1.4399 - classification_loss: 0.5001\n",
      "175/500 [=========>....................] - ETA: 5:40 - loss: 1.9402 - regression_loss: 1.4401 - classification_loss: 0.5001\n",
      "176/500 [=========>....................] - ETA: 5:39 - loss: 1.9428 - regression_loss: 1.4418 - classification_loss: 0.5011\n",
      "177/500 [=========>....................] - ETA: 5:37 - loss: 1.9496 - regression_loss: 1.4466 - classification_loss: 0.5030\n",
      "178/500 [=========>....................] - ETA: 5:36 - loss: 1.9549 - regression_loss: 1.4504 - classification_loss: 0.5045\n",
      "179/500 [=========>....................] - ETA: 5:35 - loss: 1.9574 - regression_loss: 1.4518 - classification_loss: 0.5056\n",
      "180/500 [=========>....................] - ETA: 5:34 - loss: 1.9602 - regression_loss: 1.4532 - classification_loss: 0.5070\n",
      "181/500 [=========>....................] - ETA: 5:33 - loss: 1.9582 - regression_loss: 1.4507 - classification_loss: 0.5075\n",
      "182/500 [=========>....................] - ETA: 5:32 - loss: 1.9574 - regression_loss: 1.4495 - classification_loss: 0.5080\n",
      "183/500 [=========>....................] - ETA: 5:31 - loss: 1.9604 - regression_loss: 1.4515 - classification_loss: 0.5089\n",
      "184/500 [==========>...................] - ETA: 5:30 - loss: 1.9593 - regression_loss: 1.4500 - classification_loss: 0.5093\n",
      "185/500 [==========>...................] - ETA: 5:29 - loss: 1.9588 - regression_loss: 1.4499 - classification_loss: 0.5090\n",
      "186/500 [==========>...................] - ETA: 5:27 - loss: 1.9567 - regression_loss: 1.4485 - classification_loss: 0.5082\n",
      "187/500 [==========>...................] - ETA: 5:26 - loss: 1.9534 - regression_loss: 1.4465 - classification_loss: 0.5069\n",
      "188/500 [==========>...................] - ETA: 5:25 - loss: 1.9554 - regression_loss: 1.4488 - classification_loss: 0.5066\n",
      "189/500 [==========>...................] - ETA: 5:24 - loss: 1.9589 - regression_loss: 1.4522 - classification_loss: 0.5067\n",
      "190/500 [==========>...................] - ETA: 5:23 - loss: 1.9553 - regression_loss: 1.4495 - classification_loss: 0.5058\n",
      "191/500 [==========>...................] - ETA: 5:22 - loss: 1.9546 - regression_loss: 1.4484 - classification_loss: 0.5061\n",
      "192/500 [==========>...................] - ETA: 5:21 - loss: 1.9552 - regression_loss: 1.4489 - classification_loss: 0.5063\n",
      "193/500 [==========>...................] - ETA: 5:20 - loss: 1.9538 - regression_loss: 1.4484 - classification_loss: 0.5054\n",
      "194/500 [==========>...................] - ETA: 5:19 - loss: 1.9539 - regression_loss: 1.4487 - classification_loss: 0.5052\n",
      "195/500 [==========>...................] - ETA: 5:18 - loss: 1.9514 - regression_loss: 1.4467 - classification_loss: 0.5047\n",
      "196/500 [==========>...................] - ETA: 5:16 - loss: 1.9493 - regression_loss: 1.4449 - classification_loss: 0.5044\n",
      "197/500 [==========>...................] - ETA: 5:15 - loss: 1.9461 - regression_loss: 1.4423 - classification_loss: 0.5038\n",
      "198/500 [==========>...................] - ETA: 5:14 - loss: 1.9416 - regression_loss: 1.4390 - classification_loss: 0.5026\n",
      "199/500 [==========>...................] - ETA: 5:13 - loss: 1.9461 - regression_loss: 1.4421 - classification_loss: 0.5040\n",
      "200/500 [===========>..................] - ETA: 5:12 - loss: 1.9424 - regression_loss: 1.4395 - classification_loss: 0.5030\n",
      "201/500 [===========>..................] - ETA: 5:11 - loss: 1.9401 - regression_loss: 1.4381 - classification_loss: 0.5021\n",
      "202/500 [===========>..................] - ETA: 5:10 - loss: 1.9457 - regression_loss: 1.4422 - classification_loss: 0.5036\n",
      "203/500 [===========>..................] - ETA: 5:09 - loss: 1.9463 - regression_loss: 1.4433 - classification_loss: 0.5030\n",
      "204/500 [===========>..................] - ETA: 5:08 - loss: 1.9430 - regression_loss: 1.4414 - classification_loss: 0.5016\n",
      "205/500 [===========>..................] - ETA: 5:07 - loss: 1.9456 - regression_loss: 1.4432 - classification_loss: 0.5024\n",
      "206/500 [===========>..................] - ETA: 5:05 - loss: 1.9426 - regression_loss: 1.4414 - classification_loss: 0.5012\n",
      "207/500 [===========>..................] - ETA: 5:05 - loss: 1.9413 - regression_loss: 1.4405 - classification_loss: 0.5007\n",
      "208/500 [===========>..................] - ETA: 5:04 - loss: 1.9402 - regression_loss: 1.4403 - classification_loss: 0.4999\n",
      "209/500 [===========>..................] - ETA: 5:03 - loss: 1.9410 - regression_loss: 1.4411 - classification_loss: 0.4999\n",
      "210/500 [===========>..................] - ETA: 5:02 - loss: 1.9427 - regression_loss: 1.4421 - classification_loss: 0.5006\n",
      "211/500 [===========>..................] - ETA: 5:00 - loss: 1.9443 - regression_loss: 1.4435 - classification_loss: 0.5008\n",
      "212/500 [===========>..................] - ETA: 4:59 - loss: 1.9440 - regression_loss: 1.4434 - classification_loss: 0.5006\n",
      "213/500 [===========>..................] - ETA: 4:58 - loss: 1.9483 - regression_loss: 1.4461 - classification_loss: 0.5023\n",
      "214/500 [===========>..................] - ETA: 4:57 - loss: 1.9481 - regression_loss: 1.4461 - classification_loss: 0.5021\n",
      "215/500 [===========>..................] - ETA: 4:56 - loss: 1.9476 - regression_loss: 1.4455 - classification_loss: 0.5021\n",
      "216/500 [===========>..................] - ETA: 4:55 - loss: 1.9493 - regression_loss: 1.4469 - classification_loss: 0.5023\n",
      "217/500 [============>.................] - ETA: 4:54 - loss: 1.9447 - regression_loss: 1.4430 - classification_loss: 0.5017\n",
      "218/500 [============>.................] - ETA: 4:53 - loss: 1.9474 - regression_loss: 1.4455 - classification_loss: 0.5019\n",
      "219/500 [============>.................] - ETA: 4:52 - loss: 1.9512 - regression_loss: 1.4487 - classification_loss: 0.5025\n",
      "220/500 [============>.................] - ETA: 4:51 - loss: 1.9509 - regression_loss: 1.4486 - classification_loss: 0.5022\n",
      "221/500 [============>.................] - ETA: 4:49 - loss: 1.9528 - regression_loss: 1.4505 - classification_loss: 0.5023\n",
      "222/500 [============>.................] - ETA: 4:49 - loss: 1.9523 - regression_loss: 1.4503 - classification_loss: 0.5020\n",
      "223/500 [============>.................] - ETA: 4:47 - loss: 1.9540 - regression_loss: 1.4516 - classification_loss: 0.5024\n",
      "224/500 [============>.................] - ETA: 4:46 - loss: 1.9520 - regression_loss: 1.4503 - classification_loss: 0.5017\n",
      "225/500 [============>.................] - ETA: 4:45 - loss: 1.9509 - regression_loss: 1.4497 - classification_loss: 0.5012\n",
      "226/500 [============>.................] - ETA: 4:44 - loss: 1.9495 - regression_loss: 1.4483 - classification_loss: 0.5012\n",
      "227/500 [============>.................] - ETA: 4:43 - loss: 1.9469 - regression_loss: 1.4466 - classification_loss: 0.5003\n",
      "228/500 [============>.................] - ETA: 4:42 - loss: 1.9438 - regression_loss: 1.4445 - classification_loss: 0.4994\n",
      "229/500 [============>.................] - ETA: 4:41 - loss: 1.9453 - regression_loss: 1.4454 - classification_loss: 0.4998\n",
      "230/500 [============>.................] - ETA: 4:40 - loss: 1.9432 - regression_loss: 1.4436 - classification_loss: 0.4996\n",
      "231/500 [============>.................] - ETA: 4:39 - loss: 1.9417 - regression_loss: 1.4427 - classification_loss: 0.4990\n",
      "232/500 [============>.................] - ETA: 4:38 - loss: 1.9403 - regression_loss: 1.4422 - classification_loss: 0.4981\n",
      "233/500 [============>.................] - ETA: 4:37 - loss: 1.9434 - regression_loss: 1.4449 - classification_loss: 0.4985\n",
      "234/500 [=============>................] - ETA: 4:36 - loss: 1.9412 - regression_loss: 1.4432 - classification_loss: 0.4979\n",
      "235/500 [=============>................] - ETA: 4:35 - loss: 1.9413 - regression_loss: 1.4429 - classification_loss: 0.4984\n",
      "236/500 [=============>................] - ETA: 4:33 - loss: 1.9448 - regression_loss: 1.4454 - classification_loss: 0.4993\n",
      "237/500 [=============>................] - ETA: 4:32 - loss: 1.9462 - regression_loss: 1.4464 - classification_loss: 0.4998\n",
      "238/500 [=============>................] - ETA: 4:31 - loss: 1.9500 - regression_loss: 1.4488 - classification_loss: 0.5012\n",
      "239/500 [=============>................] - ETA: 4:30 - loss: 1.9475 - regression_loss: 1.4467 - classification_loss: 0.5008\n",
      "240/500 [=============>................] - ETA: 4:29 - loss: 1.9502 - regression_loss: 1.4490 - classification_loss: 0.5012\n",
      "241/500 [=============>................] - ETA: 4:28 - loss: 1.9496 - regression_loss: 1.4486 - classification_loss: 0.5010\n",
      "242/500 [=============>................] - ETA: 4:27 - loss: 1.9496 - regression_loss: 1.4486 - classification_loss: 0.5010\n",
      "243/500 [=============>................] - ETA: 4:26 - loss: 1.9490 - regression_loss: 1.4485 - classification_loss: 0.5005\n",
      "244/500 [=============>................] - ETA: 4:25 - loss: 1.9490 - regression_loss: 1.4484 - classification_loss: 0.5006\n",
      "245/500 [=============>................] - ETA: 4:24 - loss: 1.9487 - regression_loss: 1.4479 - classification_loss: 0.5008\n",
      "246/500 [=============>................] - ETA: 4:23 - loss: 1.9461 - regression_loss: 1.4458 - classification_loss: 0.5003\n",
      "247/500 [=============>................] - ETA: 4:24 - loss: 1.9458 - regression_loss: 1.4459 - classification_loss: 0.4999\n",
      "248/500 [=============>................] - ETA: 4:23 - loss: 1.9461 - regression_loss: 1.4457 - classification_loss: 0.5004\n",
      "249/500 [=============>................] - ETA: 4:22 - loss: 1.9467 - regression_loss: 1.4463 - classification_loss: 0.5004\n",
      "250/500 [==============>...............] - ETA: 4:21 - loss: 1.9498 - regression_loss: 1.4486 - classification_loss: 0.5012\n",
      "251/500 [==============>...............] - ETA: 4:20 - loss: 1.9484 - regression_loss: 1.4478 - classification_loss: 0.5006\n",
      "252/500 [==============>...............] - ETA: 4:19 - loss: 1.9490 - regression_loss: 1.4474 - classification_loss: 0.5016\n",
      "253/500 [==============>...............] - ETA: 4:18 - loss: 1.9456 - regression_loss: 1.4448 - classification_loss: 0.5007\n",
      "254/500 [==============>...............] - ETA: 4:17 - loss: 1.9464 - regression_loss: 1.4457 - classification_loss: 0.5008\n",
      "255/500 [==============>...............] - ETA: 4:16 - loss: 1.9462 - regression_loss: 1.4459 - classification_loss: 0.5003\n",
      "256/500 [==============>...............] - ETA: 4:15 - loss: 1.9454 - regression_loss: 1.4451 - classification_loss: 0.5002\n",
      "257/500 [==============>...............] - ETA: 4:13 - loss: 1.9488 - regression_loss: 1.4474 - classification_loss: 0.5013\n",
      "258/500 [==============>...............] - ETA: 4:12 - loss: 1.9484 - regression_loss: 1.4466 - classification_loss: 0.5018\n",
      "259/500 [==============>...............] - ETA: 4:11 - loss: 1.9485 - regression_loss: 1.4465 - classification_loss: 0.5021\n",
      "260/500 [==============>...............] - ETA: 4:10 - loss: 1.9493 - regression_loss: 1.4471 - classification_loss: 0.5021\n",
      "261/500 [==============>...............] - ETA: 4:09 - loss: 1.9487 - regression_loss: 1.4465 - classification_loss: 0.5022\n",
      "262/500 [==============>...............] - ETA: 4:08 - loss: 1.9474 - regression_loss: 1.4458 - classification_loss: 0.5016\n",
      "263/500 [==============>...............] - ETA: 4:07 - loss: 1.9482 - regression_loss: 1.4465 - classification_loss: 0.5017\n",
      "264/500 [==============>...............] - ETA: 4:06 - loss: 1.9522 - regression_loss: 1.4490 - classification_loss: 0.5032\n",
      "265/500 [==============>...............] - ETA: 4:05 - loss: 1.9509 - regression_loss: 1.4484 - classification_loss: 0.5025\n",
      "266/500 [==============>...............] - ETA: 4:04 - loss: 1.9543 - regression_loss: 1.4502 - classification_loss: 0.5041\n",
      "267/500 [===============>..............] - ETA: 4:03 - loss: 1.9539 - regression_loss: 1.4502 - classification_loss: 0.5036\n",
      "268/500 [===============>..............] - ETA: 4:02 - loss: 1.9518 - regression_loss: 1.4487 - classification_loss: 0.5031\n",
      "269/500 [===============>..............] - ETA: 4:01 - loss: 1.9494 - regression_loss: 1.4468 - classification_loss: 0.5026\n",
      "270/500 [===============>..............] - ETA: 4:00 - loss: 1.9487 - regression_loss: 1.4465 - classification_loss: 0.5022\n",
      "271/500 [===============>..............] - ETA: 3:59 - loss: 1.9494 - regression_loss: 1.4472 - classification_loss: 0.5023\n",
      "272/500 [===============>..............] - ETA: 3:58 - loss: 1.9490 - regression_loss: 1.4470 - classification_loss: 0.5020\n",
      "273/500 [===============>..............] - ETA: 3:57 - loss: 1.9491 - regression_loss: 1.4468 - classification_loss: 0.5023\n",
      "274/500 [===============>..............] - ETA: 3:56 - loss: 1.9454 - regression_loss: 1.4440 - classification_loss: 0.5015\n",
      "275/500 [===============>..............] - ETA: 3:55 - loss: 1.9442 - regression_loss: 1.4434 - classification_loss: 0.5008\n",
      "276/500 [===============>..............] - ETA: 3:54 - loss: 1.9446 - regression_loss: 1.4438 - classification_loss: 0.5008\n",
      "277/500 [===============>..............] - ETA: 3:53 - loss: 1.9450 - regression_loss: 1.4442 - classification_loss: 0.5008\n",
      "278/500 [===============>..............] - ETA: 3:51 - loss: 1.9433 - regression_loss: 1.4433 - classification_loss: 0.5000\n",
      "279/500 [===============>..............] - ETA: 3:50 - loss: 1.9416 - regression_loss: 1.4424 - classification_loss: 0.4992\n",
      "280/500 [===============>..............] - ETA: 3:49 - loss: 1.9419 - regression_loss: 1.4430 - classification_loss: 0.4990\n",
      "281/500 [===============>..............] - ETA: 3:48 - loss: 1.9448 - regression_loss: 1.4454 - classification_loss: 0.4994\n",
      "282/500 [===============>..............] - ETA: 3:47 - loss: 1.9431 - regression_loss: 1.4441 - classification_loss: 0.4990\n",
      "283/500 [===============>..............] - ETA: 3:46 - loss: 1.9439 - regression_loss: 1.4447 - classification_loss: 0.4993\n",
      "284/500 [================>.............] - ETA: 3:45 - loss: 1.9428 - regression_loss: 1.4438 - classification_loss: 0.4990\n",
      "285/500 [================>.............] - ETA: 3:44 - loss: 1.9403 - regression_loss: 1.4416 - classification_loss: 0.4987\n",
      "286/500 [================>.............] - ETA: 3:43 - loss: 1.9373 - regression_loss: 1.4396 - classification_loss: 0.4977\n",
      "287/500 [================>.............] - ETA: 3:42 - loss: 1.9354 - regression_loss: 1.4383 - classification_loss: 0.4971\n",
      "288/500 [================>.............] - ETA: 3:41 - loss: 1.9337 - regression_loss: 1.4377 - classification_loss: 0.4960\n",
      "289/500 [================>.............] - ETA: 3:40 - loss: 1.9364 - regression_loss: 1.4399 - classification_loss: 0.4964\n",
      "290/500 [================>.............] - ETA: 3:39 - loss: 1.9347 - regression_loss: 1.4390 - classification_loss: 0.4957\n",
      "291/500 [================>.............] - ETA: 3:38 - loss: 1.9319 - regression_loss: 1.4372 - classification_loss: 0.4947\n",
      "292/500 [================>.............] - ETA: 3:37 - loss: 1.9334 - regression_loss: 1.4388 - classification_loss: 0.4946\n",
      "293/500 [================>.............] - ETA: 3:36 - loss: 1.9330 - regression_loss: 1.4385 - classification_loss: 0.4945\n",
      "294/500 [================>.............] - ETA: 3:35 - loss: 1.9348 - regression_loss: 1.4404 - classification_loss: 0.4945\n",
      "295/500 [================>.............] - ETA: 3:33 - loss: 1.9346 - regression_loss: 1.4401 - classification_loss: 0.4945\n",
      "296/500 [================>.............] - ETA: 3:32 - loss: 1.9363 - regression_loss: 1.4406 - classification_loss: 0.4957\n",
      "297/500 [================>.............] - ETA: 3:31 - loss: 1.9382 - regression_loss: 1.4421 - classification_loss: 0.4961\n",
      "298/500 [================>.............] - ETA: 3:30 - loss: 1.9386 - regression_loss: 1.4420 - classification_loss: 0.4967\n",
      "299/500 [================>.............] - ETA: 3:29 - loss: 1.9385 - regression_loss: 1.4419 - classification_loss: 0.4966\n",
      "300/500 [=================>............] - ETA: 3:28 - loss: 1.9369 - regression_loss: 1.4409 - classification_loss: 0.4961\n",
      "301/500 [=================>............] - ETA: 3:27 - loss: 1.9353 - regression_loss: 1.4400 - classification_loss: 0.4953\n",
      "302/500 [=================>............] - ETA: 3:26 - loss: 1.9350 - regression_loss: 1.4401 - classification_loss: 0.4949\n",
      "303/500 [=================>............] - ETA: 3:25 - loss: 1.9353 - regression_loss: 1.4401 - classification_loss: 0.4951\n",
      "304/500 [=================>............] - ETA: 3:24 - loss: 1.9382 - regression_loss: 1.4419 - classification_loss: 0.4962\n",
      "305/500 [=================>............] - ETA: 3:23 - loss: 1.9411 - regression_loss: 1.4445 - classification_loss: 0.4967\n",
      "306/500 [=================>............] - ETA: 3:22 - loss: 1.9427 - regression_loss: 1.4466 - classification_loss: 0.4961\n",
      "307/500 [=================>............] - ETA: 3:21 - loss: 1.9435 - regression_loss: 1.4471 - classification_loss: 0.4964\n",
      "308/500 [=================>............] - ETA: 3:20 - loss: 1.9427 - regression_loss: 1.4465 - classification_loss: 0.4963\n",
      "309/500 [=================>............] - ETA: 3:19 - loss: 1.9426 - regression_loss: 1.4459 - classification_loss: 0.4967\n",
      "310/500 [=================>............] - ETA: 3:18 - loss: 1.9457 - regression_loss: 1.4480 - classification_loss: 0.4977\n",
      "311/500 [=================>............] - ETA: 3:17 - loss: 1.9473 - regression_loss: 1.4492 - classification_loss: 0.4981\n",
      "312/500 [=================>............] - ETA: 3:16 - loss: 1.9449 - regression_loss: 1.4472 - classification_loss: 0.4978\n",
      "313/500 [=================>............] - ETA: 3:15 - loss: 1.9450 - regression_loss: 1.4467 - classification_loss: 0.4983\n",
      "314/500 [=================>............] - ETA: 3:14 - loss: 1.9470 - regression_loss: 1.4480 - classification_loss: 0.4991\n",
      "315/500 [=================>............] - ETA: 3:13 - loss: 1.9448 - regression_loss: 1.4459 - classification_loss: 0.4989\n",
      "316/500 [=================>............] - ETA: 3:12 - loss: 1.9450 - regression_loss: 1.4463 - classification_loss: 0.4987\n",
      "317/500 [==================>...........] - ETA: 3:10 - loss: 1.9469 - regression_loss: 1.4473 - classification_loss: 0.4995\n",
      "318/500 [==================>...........] - ETA: 3:09 - loss: 1.9492 - regression_loss: 1.4493 - classification_loss: 0.4999\n",
      "319/500 [==================>...........] - ETA: 3:08 - loss: 1.9477 - regression_loss: 1.4483 - classification_loss: 0.4995\n",
      "320/500 [==================>...........] - ETA: 3:07 - loss: 1.9470 - regression_loss: 1.4480 - classification_loss: 0.4990\n",
      "321/500 [==================>...........] - ETA: 3:06 - loss: 1.9446 - regression_loss: 1.4461 - classification_loss: 0.4984\n",
      "322/500 [==================>...........] - ETA: 3:05 - loss: 1.9431 - regression_loss: 1.4453 - classification_loss: 0.4978\n",
      "323/500 [==================>...........] - ETA: 3:04 - loss: 1.9424 - regression_loss: 1.4444 - classification_loss: 0.4980\n",
      "324/500 [==================>...........] - ETA: 3:03 - loss: 1.9452 - regression_loss: 1.4460 - classification_loss: 0.4991\n",
      "325/500 [==================>...........] - ETA: 3:02 - loss: 1.9453 - regression_loss: 1.4455 - classification_loss: 0.4998\n",
      "326/500 [==================>...........] - ETA: 3:01 - loss: 1.9465 - regression_loss: 1.4465 - classification_loss: 0.5000\n",
      "327/500 [==================>...........] - ETA: 3:00 - loss: 1.9483 - regression_loss: 1.4471 - classification_loss: 0.5012\n",
      "328/500 [==================>...........] - ETA: 2:59 - loss: 1.9476 - regression_loss: 1.4467 - classification_loss: 0.5009\n",
      "329/500 [==================>...........] - ETA: 2:58 - loss: 1.9454 - regression_loss: 1.4452 - classification_loss: 0.5001\n",
      "330/500 [==================>...........] - ETA: 2:57 - loss: 1.9438 - regression_loss: 1.4438 - classification_loss: 0.5000\n",
      "331/500 [==================>...........] - ETA: 2:56 - loss: 1.9446 - regression_loss: 1.4445 - classification_loss: 0.5000\n",
      "332/500 [==================>...........] - ETA: 2:55 - loss: 1.9441 - regression_loss: 1.4439 - classification_loss: 0.5002\n",
      "333/500 [==================>...........] - ETA: 2:54 - loss: 1.9444 - regression_loss: 1.4439 - classification_loss: 0.5005\n",
      "334/500 [===================>..........] - ETA: 2:53 - loss: 1.9445 - regression_loss: 1.4440 - classification_loss: 0.5004\n",
      "335/500 [===================>..........] - ETA: 2:51 - loss: 1.9430 - regression_loss: 1.4429 - classification_loss: 0.5000\n",
      "336/500 [===================>..........] - ETA: 2:50 - loss: 1.9427 - regression_loss: 1.4428 - classification_loss: 0.4999\n",
      "337/500 [===================>..........] - ETA: 2:49 - loss: 1.9400 - regression_loss: 1.4407 - classification_loss: 0.4993\n",
      "338/500 [===================>..........] - ETA: 2:48 - loss: 1.9398 - regression_loss: 1.4408 - classification_loss: 0.4990\n",
      "339/500 [===================>..........] - ETA: 2:47 - loss: 1.9409 - regression_loss: 1.4418 - classification_loss: 0.4991\n",
      "340/500 [===================>..........] - ETA: 2:46 - loss: 1.9422 - regression_loss: 1.4432 - classification_loss: 0.4989\n",
      "341/500 [===================>..........] - ETA: 2:45 - loss: 1.9427 - regression_loss: 1.4437 - classification_loss: 0.4990\n",
      "342/500 [===================>..........] - ETA: 2:44 - loss: 1.9433 - regression_loss: 1.4438 - classification_loss: 0.4996\n",
      "343/500 [===================>..........] - ETA: 2:43 - loss: 1.9432 - regression_loss: 1.4439 - classification_loss: 0.4993\n",
      "344/500 [===================>..........] - ETA: 2:42 - loss: 1.9430 - regression_loss: 1.4438 - classification_loss: 0.4992\n",
      "345/500 [===================>..........] - ETA: 2:41 - loss: 1.9433 - regression_loss: 1.4443 - classification_loss: 0.4990\n",
      "346/500 [===================>..........] - ETA: 2:40 - loss: 1.9438 - regression_loss: 1.4451 - classification_loss: 0.4987\n",
      "347/500 [===================>..........] - ETA: 2:39 - loss: 1.9437 - regression_loss: 1.4452 - classification_loss: 0.4985\n",
      "348/500 [===================>..........] - ETA: 2:38 - loss: 1.9467 - regression_loss: 1.4465 - classification_loss: 0.5002\n",
      "349/500 [===================>..........] - ETA: 2:37 - loss: 1.9456 - regression_loss: 1.4460 - classification_loss: 0.4996\n",
      "350/500 [====================>.........] - ETA: 2:36 - loss: 1.9456 - regression_loss: 1.4459 - classification_loss: 0.4997\n",
      "351/500 [====================>.........] - ETA: 2:35 - loss: 1.9444 - regression_loss: 1.4453 - classification_loss: 0.4992\n",
      "352/500 [====================>.........] - ETA: 2:34 - loss: 1.9478 - regression_loss: 1.4470 - classification_loss: 0.5008\n",
      "353/500 [====================>.........] - ETA: 2:33 - loss: 1.9466 - regression_loss: 1.4457 - classification_loss: 0.5009\n",
      "354/500 [====================>.........] - ETA: 2:32 - loss: 1.9470 - regression_loss: 1.4467 - classification_loss: 0.5003\n",
      "355/500 [====================>.........] - ETA: 2:31 - loss: 1.9483 - regression_loss: 1.4478 - classification_loss: 0.5005\n",
      "356/500 [====================>.........] - ETA: 2:29 - loss: 1.9478 - regression_loss: 1.4477 - classification_loss: 0.5001\n",
      "357/500 [====================>.........] - ETA: 2:28 - loss: 1.9460 - regression_loss: 1.4463 - classification_loss: 0.4997\n",
      "358/500 [====================>.........] - ETA: 2:27 - loss: 1.9445 - regression_loss: 1.4451 - classification_loss: 0.4994\n",
      "359/500 [====================>.........] - ETA: 2:26 - loss: 1.9450 - regression_loss: 1.4457 - classification_loss: 0.4993\n",
      "360/500 [====================>.........] - ETA: 2:25 - loss: 1.9459 - regression_loss: 1.4461 - classification_loss: 0.4998\n",
      "361/500 [====================>.........] - ETA: 2:24 - loss: 1.9481 - regression_loss: 1.4474 - classification_loss: 0.5007\n",
      "362/500 [====================>.........] - ETA: 2:23 - loss: 1.9476 - regression_loss: 1.4475 - classification_loss: 0.5002\n",
      "363/500 [====================>.........] - ETA: 2:22 - loss: 1.9464 - regression_loss: 1.4466 - classification_loss: 0.4998\n",
      "364/500 [====================>.........] - ETA: 2:21 - loss: 1.9487 - regression_loss: 1.4484 - classification_loss: 0.5003\n",
      "365/500 [====================>.........] - ETA: 2:20 - loss: 1.9484 - regression_loss: 1.4481 - classification_loss: 0.5004\n",
      "366/500 [====================>.........] - ETA: 2:19 - loss: 1.9486 - regression_loss: 1.4485 - classification_loss: 0.5001\n",
      "367/500 [=====================>........] - ETA: 2:18 - loss: 1.9506 - regression_loss: 1.4496 - classification_loss: 0.5010\n",
      "368/500 [=====================>........] - ETA: 2:17 - loss: 1.9519 - regression_loss: 1.4508 - classification_loss: 0.5011\n",
      "369/500 [=====================>........] - ETA: 2:16 - loss: 1.9509 - regression_loss: 1.4497 - classification_loss: 0.5013\n",
      "370/500 [=====================>........] - ETA: 2:15 - loss: 1.9540 - regression_loss: 1.4515 - classification_loss: 0.5025\n",
      "371/500 [=====================>........] - ETA: 2:14 - loss: 1.9541 - regression_loss: 1.4517 - classification_loss: 0.5024\n",
      "372/500 [=====================>........] - ETA: 2:13 - loss: 1.9517 - regression_loss: 1.4497 - classification_loss: 0.5021\n",
      "373/500 [=====================>........] - ETA: 2:12 - loss: 1.9494 - regression_loss: 1.4479 - classification_loss: 0.5015\n",
      "374/500 [=====================>........] - ETA: 2:11 - loss: 1.9475 - regression_loss: 1.4464 - classification_loss: 0.5010\n",
      "375/500 [=====================>........] - ETA: 2:10 - loss: 1.9453 - regression_loss: 1.4449 - classification_loss: 0.5004\n",
      "376/500 [=====================>........] - ETA: 2:09 - loss: 1.9448 - regression_loss: 1.4442 - classification_loss: 0.5006\n",
      "377/500 [=====================>........] - ETA: 2:08 - loss: 1.9433 - regression_loss: 1.4428 - classification_loss: 0.5004\n",
      "378/500 [=====================>........] - ETA: 2:07 - loss: 1.9426 - regression_loss: 1.4426 - classification_loss: 0.5000\n",
      "379/500 [=====================>........] - ETA: 2:05 - loss: 1.9415 - regression_loss: 1.4416 - classification_loss: 0.5000\n",
      "380/500 [=====================>........] - ETA: 2:04 - loss: 1.9426 - regression_loss: 1.4428 - classification_loss: 0.4999\n",
      "381/500 [=====================>........] - ETA: 2:03 - loss: 1.9415 - regression_loss: 1.4422 - classification_loss: 0.4993\n",
      "382/500 [=====================>........] - ETA: 2:02 - loss: 1.9401 - regression_loss: 1.4411 - classification_loss: 0.4990\n",
      "383/500 [=====================>........] - ETA: 2:01 - loss: 1.9389 - regression_loss: 1.4399 - classification_loss: 0.4990\n",
      "384/500 [======================>.......] - ETA: 2:00 - loss: 1.9375 - regression_loss: 1.4387 - classification_loss: 0.4987\n",
      "385/500 [======================>.......] - ETA: 1:59 - loss: 1.9379 - regression_loss: 1.4389 - classification_loss: 0.4990\n",
      "386/500 [======================>.......] - ETA: 1:58 - loss: 1.9373 - regression_loss: 1.4387 - classification_loss: 0.4986\n",
      "387/500 [======================>.......] - ETA: 1:57 - loss: 1.9399 - regression_loss: 1.4403 - classification_loss: 0.4996\n",
      "388/500 [======================>.......] - ETA: 1:56 - loss: 1.9387 - regression_loss: 1.4393 - classification_loss: 0.4994\n",
      "389/500 [======================>.......] - ETA: 1:55 - loss: 1.9397 - regression_loss: 1.4399 - classification_loss: 0.4998\n",
      "390/500 [======================>.......] - ETA: 1:54 - loss: 1.9382 - regression_loss: 1.4387 - classification_loss: 0.4995\n",
      "391/500 [======================>.......] - ETA: 1:53 - loss: 1.9381 - regression_loss: 1.4386 - classification_loss: 0.4995\n",
      "392/500 [======================>.......] - ETA: 1:52 - loss: 1.9398 - regression_loss: 1.4396 - classification_loss: 0.5002\n",
      "393/500 [======================>.......] - ETA: 1:51 - loss: 1.9384 - regression_loss: 1.4388 - classification_loss: 0.4996\n",
      "394/500 [======================>.......] - ETA: 1:50 - loss: 1.9396 - regression_loss: 1.4394 - classification_loss: 0.5001\n",
      "395/500 [======================>.......] - ETA: 1:49 - loss: 1.9383 - regression_loss: 1.4384 - classification_loss: 0.4999\n",
      "396/500 [======================>.......] - ETA: 1:48 - loss: 1.9366 - regression_loss: 1.4373 - classification_loss: 0.4994\n",
      "397/500 [======================>.......] - ETA: 1:47 - loss: 1.9362 - regression_loss: 1.4364 - classification_loss: 0.4998\n",
      "398/500 [======================>.......] - ETA: 1:46 - loss: 1.9341 - regression_loss: 1.4348 - classification_loss: 0.4992\n",
      "399/500 [======================>.......] - ETA: 1:45 - loss: 1.9340 - regression_loss: 1.4344 - classification_loss: 0.4996\n",
      "400/500 [=======================>......] - ETA: 1:44 - loss: 1.9340 - regression_loss: 1.4346 - classification_loss: 0.4994\n",
      "401/500 [=======================>......] - ETA: 1:42 - loss: 1.9353 - regression_loss: 1.4350 - classification_loss: 0.5002\n",
      "402/500 [=======================>......] - ETA: 1:41 - loss: 1.9350 - regression_loss: 1.4351 - classification_loss: 0.4999\n",
      "403/500 [=======================>......] - ETA: 1:40 - loss: 1.9358 - regression_loss: 1.4359 - classification_loss: 0.5000\n",
      "404/500 [=======================>......] - ETA: 1:39 - loss: 1.9361 - regression_loss: 1.4359 - classification_loss: 0.5002\n",
      "405/500 [=======================>......] - ETA: 1:38 - loss: 1.9361 - regression_loss: 1.4362 - classification_loss: 0.4998\n",
      "406/500 [=======================>......] - ETA: 1:37 - loss: 1.9345 - regression_loss: 1.4348 - classification_loss: 0.4996\n",
      "407/500 [=======================>......] - ETA: 1:36 - loss: 1.9334 - regression_loss: 1.4340 - classification_loss: 0.4994\n",
      "408/500 [=======================>......] - ETA: 1:35 - loss: 1.9325 - regression_loss: 1.4334 - classification_loss: 0.4991\n",
      "409/500 [=======================>......] - ETA: 1:34 - loss: 1.9312 - regression_loss: 1.4321 - classification_loss: 0.4991\n",
      "410/500 [=======================>......] - ETA: 1:33 - loss: 1.9309 - regression_loss: 1.4315 - classification_loss: 0.4994\n",
      "411/500 [=======================>......] - ETA: 1:32 - loss: 1.9335 - regression_loss: 1.4336 - classification_loss: 0.4998\n",
      "412/500 [=======================>......] - ETA: 1:32 - loss: 1.9316 - regression_loss: 1.4320 - classification_loss: 0.4996\n",
      "413/500 [=======================>......] - ETA: 1:30 - loss: 1.9308 - regression_loss: 1.4316 - classification_loss: 0.4992\n",
      "414/500 [=======================>......] - ETA: 1:29 - loss: 1.9303 - regression_loss: 1.4313 - classification_loss: 0.4990\n",
      "415/500 [=======================>......] - ETA: 1:28 - loss: 1.9299 - regression_loss: 1.4311 - classification_loss: 0.4987\n",
      "416/500 [=======================>......] - ETA: 1:27 - loss: 1.9292 - regression_loss: 1.4306 - classification_loss: 0.4986\n",
      "417/500 [========================>.....] - ETA: 1:26 - loss: 1.9271 - regression_loss: 1.4290 - classification_loss: 0.4982\n",
      "418/500 [========================>.....] - ETA: 1:25 - loss: 1.9274 - regression_loss: 1.4294 - classification_loss: 0.4980\n",
      "419/500 [========================>.....] - ETA: 1:24 - loss: 1.9272 - regression_loss: 1.4290 - classification_loss: 0.4982\n",
      "420/500 [========================>.....] - ETA: 1:23 - loss: 1.9274 - regression_loss: 1.4293 - classification_loss: 0.4981\n",
      "421/500 [========================>.....] - ETA: 1:22 - loss: 1.9266 - regression_loss: 1.4289 - classification_loss: 0.4977\n",
      "422/500 [========================>.....] - ETA: 1:21 - loss: 1.9275 - regression_loss: 1.4297 - classification_loss: 0.4977\n",
      "423/500 [========================>.....] - ETA: 1:20 - loss: 1.9265 - regression_loss: 1.4290 - classification_loss: 0.4974\n",
      "424/500 [========================>.....] - ETA: 1:19 - loss: 1.9279 - regression_loss: 1.4300 - classification_loss: 0.4979\n",
      "425/500 [========================>.....] - ETA: 1:18 - loss: 1.9301 - regression_loss: 1.4318 - classification_loss: 0.4983\n",
      "426/500 [========================>.....] - ETA: 1:17 - loss: 1.9307 - regression_loss: 1.4323 - classification_loss: 0.4984\n",
      "427/500 [========================>.....] - ETA: 1:16 - loss: 1.9294 - regression_loss: 1.4312 - classification_loss: 0.4982\n",
      "428/500 [========================>.....] - ETA: 1:15 - loss: 1.9296 - regression_loss: 1.4316 - classification_loss: 0.4980\n",
      "429/500 [========================>.....] - ETA: 1:14 - loss: 1.9301 - regression_loss: 1.4322 - classification_loss: 0.4979\n",
      "430/500 [========================>.....] - ETA: 1:13 - loss: 1.9284 - regression_loss: 1.4306 - classification_loss: 0.4978\n",
      "431/500 [========================>.....] - ETA: 1:12 - loss: 1.9293 - regression_loss: 1.4313 - classification_loss: 0.4980\n",
      "432/500 [========================>.....] - ETA: 1:11 - loss: 1.9276 - regression_loss: 1.4302 - classification_loss: 0.4975\n",
      "433/500 [========================>.....] - ETA: 1:09 - loss: 1.9262 - regression_loss: 1.4292 - classification_loss: 0.4970\n",
      "434/500 [=========================>....] - ETA: 1:08 - loss: 1.9258 - regression_loss: 1.4290 - classification_loss: 0.4968\n",
      "435/500 [=========================>....] - ETA: 1:07 - loss: 1.9244 - regression_loss: 1.4279 - classification_loss: 0.4965\n",
      "436/500 [=========================>....] - ETA: 1:06 - loss: 1.9227 - regression_loss: 1.4265 - classification_loss: 0.4963\n",
      "437/500 [=========================>....] - ETA: 1:05 - loss: 1.9239 - regression_loss: 1.4274 - classification_loss: 0.4965\n",
      "438/500 [=========================>....] - ETA: 1:04 - loss: 1.9245 - regression_loss: 1.4282 - classification_loss: 0.4963\n",
      "439/500 [=========================>....] - ETA: 1:03 - loss: 1.9255 - regression_loss: 1.4291 - classification_loss: 0.4963\n",
      "440/500 [=========================>....] - ETA: 1:02 - loss: 1.9253 - regression_loss: 1.4293 - classification_loss: 0.4960\n",
      "441/500 [=========================>....] - ETA: 1:01 - loss: 1.9241 - regression_loss: 1.4285 - classification_loss: 0.4956\n",
      "442/500 [=========================>....] - ETA: 1:00 - loss: 1.9220 - regression_loss: 1.4270 - classification_loss: 0.4951\n",
      "443/500 [=========================>....] - ETA: 59s - loss: 1.9236 - regression_loss: 1.4279 - classification_loss: 0.4957 \n",
      "444/500 [=========================>....] - ETA: 58s - loss: 1.9247 - regression_loss: 1.4285 - classification_loss: 0.4962\n",
      "445/500 [=========================>....] - ETA: 57s - loss: 1.9245 - regression_loss: 1.4285 - classification_loss: 0.4960\n",
      "446/500 [=========================>....] - ETA: 56s - loss: 1.9243 - regression_loss: 1.4283 - classification_loss: 0.4960\n",
      "447/500 [=========================>....] - ETA: 55s - loss: 1.9236 - regression_loss: 1.4278 - classification_loss: 0.4958\n",
      "448/500 [=========================>....] - ETA: 54s - loss: 1.9231 - regression_loss: 1.4273 - classification_loss: 0.4959\n",
      "449/500 [=========================>....] - ETA: 53s - loss: 1.9225 - regression_loss: 1.4268 - classification_loss: 0.4957\n",
      "450/500 [==========================>...] - ETA: 52s - loss: 1.9263 - regression_loss: 1.4283 - classification_loss: 0.4980\n",
      "451/500 [==========================>...] - ETA: 51s - loss: 1.9259 - regression_loss: 1.4279 - classification_loss: 0.4980\n",
      "452/500 [==========================>...] - ETA: 50s - loss: 1.9257 - regression_loss: 1.4275 - classification_loss: 0.4982\n",
      "453/500 [==========================>...] - ETA: 49s - loss: 1.9275 - regression_loss: 1.4288 - classification_loss: 0.4987\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.9285 - regression_loss: 1.4300 - classification_loss: 0.4985\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.9275 - regression_loss: 1.4291 - classification_loss: 0.4983\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.9269 - regression_loss: 1.4285 - classification_loss: 0.4984\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.9286 - regression_loss: 1.4298 - classification_loss: 0.4987\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.9282 - regression_loss: 1.4294 - classification_loss: 0.4988\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.9290 - regression_loss: 1.4300 - classification_loss: 0.4990\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.9284 - regression_loss: 1.4296 - classification_loss: 0.4989\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 1.9272 - regression_loss: 1.4289 - classification_loss: 0.4983\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 1.9254 - regression_loss: 1.4275 - classification_loss: 0.4979\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 1.9262 - regression_loss: 1.4283 - classification_loss: 0.4978\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 1.9270 - regression_loss: 1.4289 - classification_loss: 0.4981\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 1.9286 - regression_loss: 1.4302 - classification_loss: 0.4983\n",
      "466/500 [==========================>...] - ETA: 35s - loss: 1.9280 - regression_loss: 1.4297 - classification_loss: 0.4983\n",
      "467/500 [===========================>..] - ETA: 34s - loss: 1.9270 - regression_loss: 1.4291 - classification_loss: 0.4979\n",
      "468/500 [===========================>..] - ETA: 33s - loss: 1.9267 - regression_loss: 1.4285 - classification_loss: 0.4982\n",
      "469/500 [===========================>..] - ETA: 32s - loss: 1.9271 - regression_loss: 1.4290 - classification_loss: 0.4982\n",
      "470/500 [===========================>..] - ETA: 31s - loss: 1.9278 - regression_loss: 1.4298 - classification_loss: 0.4980\n",
      "471/500 [===========================>..] - ETA: 30s - loss: 1.9286 - regression_loss: 1.4306 - classification_loss: 0.4980\n",
      "472/500 [===========================>..] - ETA: 29s - loss: 1.9291 - regression_loss: 1.4311 - classification_loss: 0.4981\n",
      "473/500 [===========================>..] - ETA: 28s - loss: 1.9301 - regression_loss: 1.4318 - classification_loss: 0.4984\n",
      "474/500 [===========================>..] - ETA: 27s - loss: 1.9324 - regression_loss: 1.4331 - classification_loss: 0.4994\n",
      "475/500 [===========================>..] - ETA: 26s - loss: 1.9314 - regression_loss: 1.4322 - classification_loss: 0.4992\n",
      "476/500 [===========================>..] - ETA: 25s - loss: 1.9324 - regression_loss: 1.4329 - classification_loss: 0.4995\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.9317 - regression_loss: 1.4323 - classification_loss: 0.4995\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.9326 - regression_loss: 1.4328 - classification_loss: 0.4997\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.9323 - regression_loss: 1.4327 - classification_loss: 0.4995\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.9317 - regression_loss: 1.4324 - classification_loss: 0.4994\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.9311 - regression_loss: 1.4319 - classification_loss: 0.4993\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.9315 - regression_loss: 1.4324 - classification_loss: 0.4991\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.9330 - regression_loss: 1.4335 - classification_loss: 0.4995\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.9343 - regression_loss: 1.4345 - classification_loss: 0.4998\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.9339 - regression_loss: 1.4343 - classification_loss: 0.4996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486/500 [============================>.] - ETA: 14s - loss: 1.9352 - regression_loss: 1.4353 - classification_loss: 0.4999\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.9344 - regression_loss: 1.4343 - classification_loss: 0.5000\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.9356 - regression_loss: 1.4352 - classification_loss: 0.5004\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.9366 - regression_loss: 1.4361 - classification_loss: 0.5006\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.9359 - regression_loss: 1.4358 - classification_loss: 0.5001\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.9356 - regression_loss: 1.4356 - classification_loss: 0.5000 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.9349 - regression_loss: 1.4352 - classification_loss: 0.4997\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.9354 - regression_loss: 1.4351 - classification_loss: 0.5003\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.9378 - regression_loss: 1.4368 - classification_loss: 0.5010\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.9389 - regression_loss: 1.4375 - classification_loss: 0.5015\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.9397 - regression_loss: 1.4381 - classification_loss: 0.5016\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.9416 - regression_loss: 1.4399 - classification_loss: 0.5017\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.9429 - regression_loss: 1.4410 - classification_loss: 0.5019\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.9443 - regression_loss: 1.4420 - classification_loss: 0.5023\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9438 - regression_loss: 1.4414 - classification_loss: 0.5024\n",
      "Epoch 00011: saving model to ./snapshots\\resnet50_csv_11.h5\n",
      "\n",
      "500/500 [==============================] - 521s 1s/step - loss: 1.9438 - regression_loss: 1.4414 - classification_loss: 0.5024\n",
      "Epoch 12/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 1.9624 - regression_loss: 1.3609 - classification_loss: 0.6015\n",
      "  2/500 [..............................] - ETA: 5:13 - loss: 2.2632 - regression_loss: 1.6739 - classification_loss: 0.5893\n",
      "  3/500 [..............................] - ETA: 6:27 - loss: 2.2275 - regression_loss: 1.6273 - classification_loss: 0.6002\n",
      "  4/500 [..............................] - ETA: 6:51 - loss: 2.3825 - regression_loss: 1.7924 - classification_loss: 0.5902\n",
      "  5/500 [..............................] - ETA: 7:05 - loss: 2.1994 - regression_loss: 1.5991 - classification_loss: 0.6003\n",
      "  6/500 [..............................] - ETA: 7:15 - loss: 2.0398 - regression_loss: 1.4866 - classification_loss: 0.5532\n",
      "  7/500 [..............................] - ETA: 7:21 - loss: 1.9976 - regression_loss: 1.4446 - classification_loss: 0.5530\n",
      "  8/500 [..............................] - ETA: 7:31 - loss: 2.0689 - regression_loss: 1.4965 - classification_loss: 0.5724\n",
      "  9/500 [..............................] - ETA: 7:21 - loss: 1.9751 - regression_loss: 1.4417 - classification_loss: 0.5334\n",
      " 10/500 [..............................] - ETA: 7:36 - loss: 1.9752 - regression_loss: 1.4518 - classification_loss: 0.5234\n",
      " 11/500 [..............................] - ETA: 7:29 - loss: 1.9693 - regression_loss: 1.4537 - classification_loss: 0.5156\n",
      " 12/500 [..............................] - ETA: 7:40 - loss: 1.9775 - regression_loss: 1.4634 - classification_loss: 0.5141\n",
      " 13/500 [..............................] - ETA: 7:44 - loss: 1.9786 - regression_loss: 1.4584 - classification_loss: 0.5202\n",
      " 14/500 [..............................] - ETA: 7:48 - loss: 1.9314 - regression_loss: 1.4219 - classification_loss: 0.5095\n",
      " 15/500 [..............................] - ETA: 7:50 - loss: 1.9350 - regression_loss: 1.4254 - classification_loss: 0.5096\n",
      " 16/500 [..............................] - ETA: 7:49 - loss: 1.8862 - regression_loss: 1.3851 - classification_loss: 0.5011\n",
      " 17/500 [>.............................] - ETA: 7:51 - loss: 1.8681 - regression_loss: 1.3723 - classification_loss: 0.4959\n",
      " 18/500 [>.............................] - ETA: 7:51 - loss: 1.9016 - regression_loss: 1.3902 - classification_loss: 0.5114\n",
      " 19/500 [>.............................] - ETA: 7:50 - loss: 1.9053 - regression_loss: 1.3970 - classification_loss: 0.5082\n",
      " 20/500 [>.............................] - ETA: 7:51 - loss: 1.9196 - regression_loss: 1.4084 - classification_loss: 0.5112\n",
      " 21/500 [>.............................] - ETA: 7:53 - loss: 1.8929 - regression_loss: 1.3918 - classification_loss: 0.5011\n",
      " 22/500 [>.............................] - ETA: 7:52 - loss: 1.9196 - regression_loss: 1.4152 - classification_loss: 0.5044\n",
      " 23/500 [>.............................] - ETA: 7:53 - loss: 1.9342 - regression_loss: 1.4254 - classification_loss: 0.5088\n",
      " 24/500 [>.............................] - ETA: 7:54 - loss: 1.9382 - regression_loss: 1.4282 - classification_loss: 0.5100\n",
      " 25/500 [>.............................] - ETA: 7:54 - loss: 1.9254 - regression_loss: 1.4115 - classification_loss: 0.5139\n",
      " 26/500 [>.............................] - ETA: 7:53 - loss: 1.9456 - regression_loss: 1.4302 - classification_loss: 0.5154\n",
      " 27/500 [>.............................] - ETA: 7:52 - loss: 1.9136 - regression_loss: 1.4043 - classification_loss: 0.5093\n",
      " 28/500 [>.............................] - ETA: 7:52 - loss: 1.9183 - regression_loss: 1.4034 - classification_loss: 0.5150\n",
      " 29/500 [>.............................] - ETA: 7:53 - loss: 1.9114 - regression_loss: 1.3973 - classification_loss: 0.5141\n",
      " 30/500 [>.............................] - ETA: 7:51 - loss: 1.9178 - regression_loss: 1.4046 - classification_loss: 0.5132\n",
      " 31/500 [>.............................] - ETA: 7:46 - loss: 1.9071 - regression_loss: 1.3947 - classification_loss: 0.5124\n",
      " 32/500 [>.............................] - ETA: 7:47 - loss: 1.8983 - regression_loss: 1.3887 - classification_loss: 0.5096\n",
      " 33/500 [>.............................] - ETA: 7:49 - loss: 1.9070 - regression_loss: 1.3918 - classification_loss: 0.5153\n",
      " 34/500 [=>............................] - ETA: 7:49 - loss: 1.9070 - regression_loss: 1.3947 - classification_loss: 0.5123\n",
      " 35/500 [=>............................] - ETA: 7:49 - loss: 1.9341 - regression_loss: 1.4148 - classification_loss: 0.5193\n",
      " 36/500 [=>............................] - ETA: 7:49 - loss: 1.9676 - regression_loss: 1.4409 - classification_loss: 0.5267\n",
      " 37/500 [=>............................] - ETA: 7:48 - loss: 1.9545 - regression_loss: 1.4316 - classification_loss: 0.5229\n",
      " 38/500 [=>............................] - ETA: 7:46 - loss: 1.9680 - regression_loss: 1.4397 - classification_loss: 0.5283\n",
      " 39/500 [=>............................] - ETA: 7:46 - loss: 1.9756 - regression_loss: 1.4446 - classification_loss: 0.5310\n",
      " 40/500 [=>............................] - ETA: 7:45 - loss: 1.9516 - regression_loss: 1.4277 - classification_loss: 0.5239\n",
      " 41/500 [=>............................] - ETA: 7:44 - loss: 1.9293 - regression_loss: 1.4122 - classification_loss: 0.5171\n",
      " 42/500 [=>............................] - ETA: 7:43 - loss: 1.9482 - regression_loss: 1.4262 - classification_loss: 0.5220\n",
      " 43/500 [=>............................] - ETA: 7:44 - loss: 1.9449 - regression_loss: 1.4175 - classification_loss: 0.5274\n",
      " 44/500 [=>............................] - ETA: 7:44 - loss: 1.9423 - regression_loss: 1.4186 - classification_loss: 0.5237\n",
      " 45/500 [=>............................] - ETA: 7:42 - loss: 1.9466 - regression_loss: 1.4211 - classification_loss: 0.5255\n",
      " 46/500 [=>............................] - ETA: 7:42 - loss: 1.9478 - regression_loss: 1.4231 - classification_loss: 0.5247\n",
      " 47/500 [=>............................] - ETA: 7:42 - loss: 1.9474 - regression_loss: 1.4195 - classification_loss: 0.5279\n",
      " 48/500 [=>............................] - ETA: 7:41 - loss: 1.9364 - regression_loss: 1.4131 - classification_loss: 0.5233\n",
      " 49/500 [=>............................] - ETA: 7:40 - loss: 1.9230 - regression_loss: 1.4049 - classification_loss: 0.5181\n",
      " 50/500 [==>...........................] - ETA: 7:40 - loss: 1.9324 - regression_loss: 1.4133 - classification_loss: 0.5191\n",
      " 51/500 [==>...........................] - ETA: 7:38 - loss: 1.9211 - regression_loss: 1.4050 - classification_loss: 0.5160\n",
      " 52/500 [==>...........................] - ETA: 7:37 - loss: 1.9322 - regression_loss: 1.4166 - classification_loss: 0.5156\n",
      " 53/500 [==>...........................] - ETA: 7:35 - loss: 1.9295 - regression_loss: 1.4175 - classification_loss: 0.5119\n",
      " 54/500 [==>...........................] - ETA: 7:35 - loss: 1.9327 - regression_loss: 1.4201 - classification_loss: 0.5125\n",
      " 55/500 [==>...........................] - ETA: 7:34 - loss: 1.9411 - regression_loss: 1.4264 - classification_loss: 0.5147\n",
      " 56/500 [==>...........................] - ETA: 7:33 - loss: 1.9356 - regression_loss: 1.4222 - classification_loss: 0.5134\n",
      " 57/500 [==>...........................] - ETA: 7:33 - loss: 1.9207 - regression_loss: 1.4099 - classification_loss: 0.5108\n",
      " 58/500 [==>...........................] - ETA: 7:32 - loss: 1.9208 - regression_loss: 1.4121 - classification_loss: 0.5086\n",
      " 59/500 [==>...........................] - ETA: 7:31 - loss: 1.9139 - regression_loss: 1.4076 - classification_loss: 0.5063\n",
      " 60/500 [==>...........................] - ETA: 7:30 - loss: 1.9059 - regression_loss: 1.4030 - classification_loss: 0.5029\n",
      " 61/500 [==>...........................] - ETA: 7:29 - loss: 1.9042 - regression_loss: 1.4020 - classification_loss: 0.5023\n",
      " 62/500 [==>...........................] - ETA: 7:28 - loss: 1.9223 - regression_loss: 1.4131 - classification_loss: 0.5092\n",
      " 63/500 [==>...........................] - ETA: 7:27 - loss: 1.9254 - regression_loss: 1.4148 - classification_loss: 0.5106\n",
      " 64/500 [==>...........................] - ETA: 7:26 - loss: 1.9274 - regression_loss: 1.4143 - classification_loss: 0.5131\n",
      " 65/500 [==>...........................] - ETA: 7:25 - loss: 1.9217 - regression_loss: 1.4107 - classification_loss: 0.5110\n",
      " 66/500 [==>...........................] - ETA: 7:25 - loss: 1.9224 - regression_loss: 1.4107 - classification_loss: 0.5117\n",
      " 67/500 [===>..........................] - ETA: 7:24 - loss: 1.9240 - regression_loss: 1.4130 - classification_loss: 0.5110\n",
      " 68/500 [===>..........................] - ETA: 7:23 - loss: 1.9296 - regression_loss: 1.4170 - classification_loss: 0.5126\n",
      " 69/500 [===>..........................] - ETA: 7:21 - loss: 1.9242 - regression_loss: 1.4119 - classification_loss: 0.5124\n",
      " 70/500 [===>..........................] - ETA: 7:21 - loss: 1.9246 - regression_loss: 1.4125 - classification_loss: 0.5121\n",
      " 71/500 [===>..........................] - ETA: 7:20 - loss: 1.9277 - regression_loss: 1.4162 - classification_loss: 0.5116\n",
      " 72/500 [===>..........................] - ETA: 7:18 - loss: 1.9247 - regression_loss: 1.4140 - classification_loss: 0.5106\n",
      " 73/500 [===>..........................] - ETA: 7:18 - loss: 1.9415 - regression_loss: 1.4252 - classification_loss: 0.5163\n",
      " 74/500 [===>..........................] - ETA: 7:16 - loss: 1.9307 - regression_loss: 1.4178 - classification_loss: 0.5129\n",
      " 75/500 [===>..........................] - ETA: 7:16 - loss: 1.9321 - regression_loss: 1.4192 - classification_loss: 0.5129\n",
      " 76/500 [===>..........................] - ETA: 7:15 - loss: 1.9363 - regression_loss: 1.4244 - classification_loss: 0.5119\n",
      " 77/500 [===>..........................] - ETA: 7:13 - loss: 1.9380 - regression_loss: 1.4256 - classification_loss: 0.5124\n",
      " 78/500 [===>..........................] - ETA: 7:12 - loss: 1.9353 - regression_loss: 1.4248 - classification_loss: 0.5105\n",
      " 79/500 [===>..........................] - ETA: 7:11 - loss: 1.9394 - regression_loss: 1.4295 - classification_loss: 0.5099\n",
      " 80/500 [===>..........................] - ETA: 7:10 - loss: 1.9405 - regression_loss: 1.4303 - classification_loss: 0.5102\n",
      " 81/500 [===>..........................] - ETA: 7:09 - loss: 1.9405 - regression_loss: 1.4306 - classification_loss: 0.5100\n",
      " 82/500 [===>..........................] - ETA: 7:08 - loss: 1.9336 - regression_loss: 1.4261 - classification_loss: 0.5075\n",
      " 83/500 [===>..........................] - ETA: 7:07 - loss: 1.9464 - regression_loss: 1.4366 - classification_loss: 0.5098\n",
      " 84/500 [====>.........................] - ETA: 7:06 - loss: 1.9479 - regression_loss: 1.4392 - classification_loss: 0.5087\n",
      " 85/500 [====>.........................] - ETA: 7:05 - loss: 1.9487 - regression_loss: 1.4414 - classification_loss: 0.5073\n",
      " 86/500 [====>.........................] - ETA: 7:04 - loss: 1.9399 - regression_loss: 1.4354 - classification_loss: 0.5045\n",
      " 87/500 [====>.........................] - ETA: 7:03 - loss: 1.9385 - regression_loss: 1.4354 - classification_loss: 0.5031\n",
      " 88/500 [====>.........................] - ETA: 7:02 - loss: 1.9410 - regression_loss: 1.4390 - classification_loss: 0.5020\n",
      " 89/500 [====>.........................] - ETA: 7:01 - loss: 1.9486 - regression_loss: 1.4446 - classification_loss: 0.5040\n",
      " 90/500 [====>.........................] - ETA: 7:00 - loss: 1.9400 - regression_loss: 1.4389 - classification_loss: 0.5011\n",
      " 91/500 [====>.........................] - ETA: 6:59 - loss: 1.9321 - regression_loss: 1.4320 - classification_loss: 0.5001\n",
      " 92/500 [====>.........................] - ETA: 6:58 - loss: 1.9311 - regression_loss: 1.4306 - classification_loss: 0.5005\n",
      " 93/500 [====>.........................] - ETA: 6:57 - loss: 1.9301 - regression_loss: 1.4295 - classification_loss: 0.5006\n",
      " 94/500 [====>.........................] - ETA: 6:55 - loss: 1.9283 - regression_loss: 1.4280 - classification_loss: 0.5003\n",
      " 95/500 [====>.........................] - ETA: 6:54 - loss: 1.9390 - regression_loss: 1.4330 - classification_loss: 0.5060\n",
      " 96/500 [====>.........................] - ETA: 6:53 - loss: 1.9357 - regression_loss: 1.4308 - classification_loss: 0.5049\n",
      " 97/500 [====>.........................] - ETA: 6:52 - loss: 1.9405 - regression_loss: 1.4356 - classification_loss: 0.5049\n",
      " 98/500 [====>.........................] - ETA: 6:51 - loss: 1.9378 - regression_loss: 1.4338 - classification_loss: 0.5040\n",
      " 99/500 [====>.........................] - ETA: 6:49 - loss: 1.9350 - regression_loss: 1.4312 - classification_loss: 0.5039\n",
      "100/500 [=====>........................] - ETA: 6:48 - loss: 1.9342 - regression_loss: 1.4293 - classification_loss: 0.5049\n",
      "101/500 [=====>........................] - ETA: 6:47 - loss: 1.9332 - regression_loss: 1.4286 - classification_loss: 0.5046\n",
      "102/500 [=====>........................] - ETA: 6:46 - loss: 1.9407 - regression_loss: 1.4343 - classification_loss: 0.5064\n",
      "103/500 [=====>........................] - ETA: 6:45 - loss: 1.9412 - regression_loss: 1.4351 - classification_loss: 0.5061\n",
      "104/500 [=====>........................] - ETA: 6:44 - loss: 1.9414 - regression_loss: 1.4355 - classification_loss: 0.5059\n",
      "105/500 [=====>........................] - ETA: 6:42 - loss: 1.9332 - regression_loss: 1.4290 - classification_loss: 0.5042\n",
      "106/500 [=====>........................] - ETA: 6:42 - loss: 1.9419 - regression_loss: 1.4367 - classification_loss: 0.5053\n",
      "107/500 [=====>........................] - ETA: 6:41 - loss: 1.9497 - regression_loss: 1.4447 - classification_loss: 0.5051\n",
      "108/500 [=====>........................] - ETA: 6:40 - loss: 1.9491 - regression_loss: 1.4452 - classification_loss: 0.5040\n",
      "109/500 [=====>........................] - ETA: 6:39 - loss: 1.9513 - regression_loss: 1.4478 - classification_loss: 0.5035\n",
      "110/500 [=====>........................] - ETA: 6:38 - loss: 1.9513 - regression_loss: 1.4480 - classification_loss: 0.5033\n",
      "111/500 [=====>........................] - ETA: 6:37 - loss: 1.9492 - regression_loss: 1.4472 - classification_loss: 0.5020\n",
      "112/500 [=====>........................] - ETA: 6:36 - loss: 1.9451 - regression_loss: 1.4439 - classification_loss: 0.5012\n",
      "113/500 [=====>........................] - ETA: 6:35 - loss: 1.9494 - regression_loss: 1.4464 - classification_loss: 0.5030\n",
      "114/500 [=====>........................] - ETA: 6:34 - loss: 1.9528 - regression_loss: 1.4495 - classification_loss: 0.5033\n",
      "115/500 [=====>........................] - ETA: 6:33 - loss: 1.9517 - regression_loss: 1.4490 - classification_loss: 0.5027\n",
      "116/500 [=====>........................] - ETA: 6:33 - loss: 1.9493 - regression_loss: 1.4484 - classification_loss: 0.5010\n",
      "117/500 [======>.......................] - ETA: 6:32 - loss: 1.9553 - regression_loss: 1.4515 - classification_loss: 0.5038\n",
      "118/500 [======>.......................] - ETA: 6:31 - loss: 1.9551 - regression_loss: 1.4502 - classification_loss: 0.5049\n",
      "119/500 [======>.......................] - ETA: 6:30 - loss: 1.9559 - regression_loss: 1.4516 - classification_loss: 0.5043\n",
      "120/500 [======>.......................] - ETA: 6:29 - loss: 1.9508 - regression_loss: 1.4477 - classification_loss: 0.5031\n",
      "121/500 [======>.......................] - ETA: 6:27 - loss: 1.9548 - regression_loss: 1.4517 - classification_loss: 0.5031\n",
      "122/500 [======>.......................] - ETA: 6:26 - loss: 1.9527 - regression_loss: 1.4496 - classification_loss: 0.5031\n",
      "123/500 [======>.......................] - ETA: 6:25 - loss: 1.9530 - regression_loss: 1.4503 - classification_loss: 0.5026\n",
      "124/500 [======>.......................] - ETA: 6:25 - loss: 1.9506 - regression_loss: 1.4483 - classification_loss: 0.5023\n",
      "125/500 [======>.......................] - ETA: 6:23 - loss: 1.9471 - regression_loss: 1.4447 - classification_loss: 0.5025\n",
      "126/500 [======>.......................] - ETA: 6:23 - loss: 1.9492 - regression_loss: 1.4459 - classification_loss: 0.5033\n",
      "127/500 [======>.......................] - ETA: 6:22 - loss: 1.9477 - regression_loss: 1.4452 - classification_loss: 0.5025\n",
      "128/500 [======>.......................] - ETA: 6:23 - loss: 1.9567 - regression_loss: 1.4507 - classification_loss: 0.5060\n",
      "129/500 [======>.......................] - ETA: 6:22 - loss: 1.9557 - regression_loss: 1.4505 - classification_loss: 0.5052\n",
      "130/500 [======>.......................] - ETA: 6:22 - loss: 1.9531 - regression_loss: 1.4480 - classification_loss: 0.5051\n",
      "131/500 [======>.......................] - ETA: 6:21 - loss: 1.9544 - regression_loss: 1.4497 - classification_loss: 0.5047\n",
      "132/500 [======>.......................] - ETA: 6:20 - loss: 1.9493 - regression_loss: 1.4461 - classification_loss: 0.5032\n",
      "133/500 [======>.......................] - ETA: 6:18 - loss: 1.9508 - regression_loss: 1.4484 - classification_loss: 0.5024\n",
      "134/500 [=======>......................] - ETA: 6:17 - loss: 1.9556 - regression_loss: 1.4515 - classification_loss: 0.5041\n",
      "135/500 [=======>......................] - ETA: 6:16 - loss: 1.9521 - regression_loss: 1.4501 - classification_loss: 0.5020\n",
      "136/500 [=======>......................] - ETA: 6:15 - loss: 1.9568 - regression_loss: 1.4545 - classification_loss: 0.5023\n",
      "137/500 [=======>......................] - ETA: 6:14 - loss: 1.9563 - regression_loss: 1.4542 - classification_loss: 0.5021\n",
      "138/500 [=======>......................] - ETA: 6:13 - loss: 1.9554 - regression_loss: 1.4530 - classification_loss: 0.5024\n",
      "139/500 [=======>......................] - ETA: 6:12 - loss: 1.9534 - regression_loss: 1.4519 - classification_loss: 0.5016\n",
      "140/500 [=======>......................] - ETA: 6:11 - loss: 1.9506 - regression_loss: 1.4498 - classification_loss: 0.5008\n",
      "141/500 [=======>......................] - ETA: 6:10 - loss: 1.9532 - regression_loss: 1.4520 - classification_loss: 0.5012\n",
      "142/500 [=======>......................] - ETA: 6:08 - loss: 1.9502 - regression_loss: 1.4490 - classification_loss: 0.5012\n",
      "143/500 [=======>......................] - ETA: 6:07 - loss: 1.9534 - regression_loss: 1.4521 - classification_loss: 0.5013\n",
      "144/500 [=======>......................] - ETA: 6:06 - loss: 1.9507 - regression_loss: 1.4507 - classification_loss: 0.5000\n",
      "145/500 [=======>......................] - ETA: 6:05 - loss: 1.9508 - regression_loss: 1.4508 - classification_loss: 0.4999\n",
      "146/500 [=======>......................] - ETA: 6:04 - loss: 1.9543 - regression_loss: 1.4540 - classification_loss: 0.5003\n",
      "147/500 [=======>......................] - ETA: 6:03 - loss: 1.9500 - regression_loss: 1.4508 - classification_loss: 0.4992\n",
      "148/500 [=======>......................] - ETA: 6:02 - loss: 1.9510 - regression_loss: 1.4516 - classification_loss: 0.4994\n",
      "149/500 [=======>......................] - ETA: 6:01 - loss: 1.9525 - regression_loss: 1.4537 - classification_loss: 0.4989\n",
      "150/500 [========>.....................] - ETA: 6:00 - loss: 1.9517 - regression_loss: 1.4533 - classification_loss: 0.4985\n",
      "151/500 [========>.....................] - ETA: 5:59 - loss: 1.9536 - regression_loss: 1.4549 - classification_loss: 0.4986\n",
      "152/500 [========>.....................] - ETA: 5:58 - loss: 1.9574 - regression_loss: 1.4582 - classification_loss: 0.4992\n",
      "153/500 [========>.....................] - ETA: 5:57 - loss: 1.9589 - regression_loss: 1.4594 - classification_loss: 0.4995\n",
      "154/500 [========>.....................] - ETA: 5:55 - loss: 1.9604 - regression_loss: 1.4611 - classification_loss: 0.4993\n",
      "155/500 [========>.....................] - ETA: 5:54 - loss: 1.9609 - regression_loss: 1.4605 - classification_loss: 0.5003\n",
      "156/500 [========>.....................] - ETA: 5:53 - loss: 1.9606 - regression_loss: 1.4587 - classification_loss: 0.5018\n",
      "157/500 [========>.....................] - ETA: 5:52 - loss: 1.9590 - regression_loss: 1.4583 - classification_loss: 0.5008\n",
      "158/500 [========>.....................] - ETA: 5:51 - loss: 1.9544 - regression_loss: 1.4549 - classification_loss: 0.4995\n",
      "159/500 [========>.....................] - ETA: 5:50 - loss: 1.9492 - regression_loss: 1.4504 - classification_loss: 0.4989\n",
      "160/500 [========>.....................] - ETA: 5:49 - loss: 1.9440 - regression_loss: 1.4467 - classification_loss: 0.4974\n",
      "161/500 [========>.....................] - ETA: 5:48 - loss: 1.9412 - regression_loss: 1.4445 - classification_loss: 0.4967\n",
      "162/500 [========>.....................] - ETA: 5:47 - loss: 1.9427 - regression_loss: 1.4463 - classification_loss: 0.4964\n",
      "163/500 [========>.....................] - ETA: 5:46 - loss: 1.9424 - regression_loss: 1.4464 - classification_loss: 0.4960\n",
      "164/500 [========>.....................] - ETA: 5:45 - loss: 1.9407 - regression_loss: 1.4456 - classification_loss: 0.4951\n",
      "165/500 [========>.....................] - ETA: 5:44 - loss: 1.9372 - regression_loss: 1.4432 - classification_loss: 0.4941\n",
      "166/500 [========>.....................] - ETA: 5:43 - loss: 1.9342 - regression_loss: 1.4412 - classification_loss: 0.4930\n",
      "167/500 [=========>....................] - ETA: 5:42 - loss: 1.9324 - regression_loss: 1.4391 - classification_loss: 0.4933\n",
      "168/500 [=========>....................] - ETA: 5:41 - loss: 1.9294 - regression_loss: 1.4370 - classification_loss: 0.4924\n",
      "169/500 [=========>....................] - ETA: 5:39 - loss: 1.9273 - regression_loss: 1.4355 - classification_loss: 0.4918\n",
      "170/500 [=========>....................] - ETA: 5:39 - loss: 1.9286 - regression_loss: 1.4373 - classification_loss: 0.4913\n",
      "171/500 [=========>....................] - ETA: 5:37 - loss: 1.9281 - regression_loss: 1.4362 - classification_loss: 0.4918\n",
      "172/500 [=========>....................] - ETA: 5:37 - loss: 1.9304 - regression_loss: 1.4387 - classification_loss: 0.4917\n",
      "173/500 [=========>....................] - ETA: 5:35 - loss: 1.9355 - regression_loss: 1.4423 - classification_loss: 0.4931\n",
      "174/500 [=========>....................] - ETA: 5:34 - loss: 1.9334 - regression_loss: 1.4407 - classification_loss: 0.4927\n",
      "175/500 [=========>....................] - ETA: 5:33 - loss: 1.9357 - regression_loss: 1.4424 - classification_loss: 0.4933\n",
      "176/500 [=========>....................] - ETA: 5:32 - loss: 1.9416 - regression_loss: 1.4451 - classification_loss: 0.4965\n",
      "177/500 [=========>....................] - ETA: 5:31 - loss: 1.9418 - regression_loss: 1.4453 - classification_loss: 0.4965\n",
      "178/500 [=========>....................] - ETA: 5:30 - loss: 1.9394 - regression_loss: 1.4440 - classification_loss: 0.4955\n",
      "179/500 [=========>....................] - ETA: 5:29 - loss: 1.9429 - regression_loss: 1.4457 - classification_loss: 0.4972\n",
      "180/500 [=========>....................] - ETA: 5:28 - loss: 1.9388 - regression_loss: 1.4428 - classification_loss: 0.4960\n",
      "181/500 [=========>....................] - ETA: 5:27 - loss: 1.9403 - regression_loss: 1.4441 - classification_loss: 0.4962\n",
      "182/500 [=========>....................] - ETA: 5:26 - loss: 1.9415 - regression_loss: 1.4457 - classification_loss: 0.4959\n",
      "183/500 [=========>....................] - ETA: 5:25 - loss: 1.9434 - regression_loss: 1.4452 - classification_loss: 0.4982\n",
      "184/500 [==========>...................] - ETA: 5:24 - loss: 1.9406 - regression_loss: 1.4436 - classification_loss: 0.4971\n",
      "185/500 [==========>...................] - ETA: 5:23 - loss: 1.9393 - regression_loss: 1.4430 - classification_loss: 0.4963\n",
      "186/500 [==========>...................] - ETA: 5:22 - loss: 1.9400 - regression_loss: 1.4432 - classification_loss: 0.4968\n",
      "187/500 [==========>...................] - ETA: 5:21 - loss: 1.9465 - regression_loss: 1.4470 - classification_loss: 0.4994\n",
      "188/500 [==========>...................] - ETA: 5:20 - loss: 1.9439 - regression_loss: 1.4449 - classification_loss: 0.4990\n",
      "189/500 [==========>...................] - ETA: 5:19 - loss: 1.9411 - regression_loss: 1.4430 - classification_loss: 0.4981\n",
      "190/500 [==========>...................] - ETA: 5:18 - loss: 1.9397 - regression_loss: 1.4419 - classification_loss: 0.4978\n",
      "191/500 [==========>...................] - ETA: 5:17 - loss: 1.9386 - regression_loss: 1.4411 - classification_loss: 0.4975\n",
      "192/500 [==========>...................] - ETA: 5:16 - loss: 1.9383 - regression_loss: 1.4408 - classification_loss: 0.4975\n",
      "193/500 [==========>...................] - ETA: 5:15 - loss: 1.9362 - regression_loss: 1.4395 - classification_loss: 0.4967\n",
      "194/500 [==========>...................] - ETA: 5:14 - loss: 1.9317 - regression_loss: 1.4362 - classification_loss: 0.4955\n",
      "195/500 [==========>...................] - ETA: 5:13 - loss: 1.9341 - regression_loss: 1.4379 - classification_loss: 0.4962\n",
      "196/500 [==========>...................] - ETA: 5:12 - loss: 1.9320 - regression_loss: 1.4371 - classification_loss: 0.4949\n",
      "197/500 [==========>...................] - ETA: 5:11 - loss: 1.9336 - regression_loss: 1.4390 - classification_loss: 0.4946\n",
      "198/500 [==========>...................] - ETA: 5:10 - loss: 1.9335 - regression_loss: 1.4392 - classification_loss: 0.4943\n",
      "199/500 [==========>...................] - ETA: 5:09 - loss: 1.9291 - regression_loss: 1.4359 - classification_loss: 0.4932\n",
      "200/500 [===========>..................] - ETA: 5:08 - loss: 1.9280 - regression_loss: 1.4353 - classification_loss: 0.4927\n",
      "201/500 [===========>..................] - ETA: 5:07 - loss: 1.9313 - regression_loss: 1.4373 - classification_loss: 0.4940\n",
      "202/500 [===========>..................] - ETA: 5:05 - loss: 1.9331 - regression_loss: 1.4377 - classification_loss: 0.4954\n",
      "203/500 [===========>..................] - ETA: 5:04 - loss: 1.9319 - regression_loss: 1.4370 - classification_loss: 0.4949\n",
      "204/500 [===========>..................] - ETA: 5:04 - loss: 1.9317 - regression_loss: 1.4370 - classification_loss: 0.4947\n",
      "205/500 [===========>..................] - ETA: 5:03 - loss: 1.9321 - regression_loss: 1.4371 - classification_loss: 0.4950\n",
      "206/500 [===========>..................] - ETA: 5:02 - loss: 1.9312 - regression_loss: 1.4370 - classification_loss: 0.4943\n",
      "207/500 [===========>..................] - ETA: 5:00 - loss: 1.9303 - regression_loss: 1.4366 - classification_loss: 0.4937\n",
      "208/500 [===========>..................] - ETA: 4:59 - loss: 1.9323 - regression_loss: 1.4373 - classification_loss: 0.4951\n",
      "209/500 [===========>..................] - ETA: 4:58 - loss: 1.9279 - regression_loss: 1.4341 - classification_loss: 0.4938\n",
      "210/500 [===========>..................] - ETA: 4:57 - loss: 1.9275 - regression_loss: 1.4337 - classification_loss: 0.4939\n",
      "211/500 [===========>..................] - ETA: 4:56 - loss: 1.9271 - regression_loss: 1.4339 - classification_loss: 0.4932\n",
      "212/500 [===========>..................] - ETA: 4:55 - loss: 1.9235 - regression_loss: 1.4308 - classification_loss: 0.4927\n",
      "213/500 [===========>..................] - ETA: 4:54 - loss: 1.9294 - regression_loss: 1.4356 - classification_loss: 0.4938\n",
      "214/500 [===========>..................] - ETA: 4:53 - loss: 1.9293 - regression_loss: 1.4355 - classification_loss: 0.4938\n",
      "215/500 [===========>..................] - ETA: 4:52 - loss: 1.9291 - regression_loss: 1.4360 - classification_loss: 0.4931\n",
      "216/500 [===========>..................] - ETA: 4:51 - loss: 1.9287 - regression_loss: 1.4364 - classification_loss: 0.4922\n",
      "217/500 [============>.................] - ETA: 4:50 - loss: 1.9306 - regression_loss: 1.4383 - classification_loss: 0.4922\n",
      "218/500 [============>.................] - ETA: 4:49 - loss: 1.9276 - regression_loss: 1.4362 - classification_loss: 0.4914\n",
      "219/500 [============>.................] - ETA: 4:48 - loss: 1.9310 - regression_loss: 1.4382 - classification_loss: 0.4928\n",
      "220/500 [============>.................] - ETA: 4:47 - loss: 1.9308 - regression_loss: 1.4380 - classification_loss: 0.4928\n",
      "221/500 [============>.................] - ETA: 4:46 - loss: 1.9319 - regression_loss: 1.4392 - classification_loss: 0.4927\n",
      "222/500 [============>.................] - ETA: 4:45 - loss: 1.9349 - regression_loss: 1.4419 - classification_loss: 0.4930\n",
      "223/500 [============>.................] - ETA: 4:44 - loss: 1.9348 - regression_loss: 1.4406 - classification_loss: 0.4942\n",
      "224/500 [============>.................] - ETA: 4:43 - loss: 1.9333 - regression_loss: 1.4398 - classification_loss: 0.4935\n",
      "225/500 [============>.................] - ETA: 4:42 - loss: 1.9308 - regression_loss: 1.4384 - classification_loss: 0.4924\n",
      "226/500 [============>.................] - ETA: 4:41 - loss: 1.9276 - regression_loss: 1.4363 - classification_loss: 0.4913\n",
      "227/500 [============>.................] - ETA: 4:40 - loss: 1.9261 - regression_loss: 1.4352 - classification_loss: 0.4909\n",
      "228/500 [============>.................] - ETA: 4:39 - loss: 1.9259 - regression_loss: 1.4352 - classification_loss: 0.4907\n",
      "229/500 [============>.................] - ETA: 4:38 - loss: 1.9322 - regression_loss: 1.4393 - classification_loss: 0.4928\n",
      "230/500 [============>.................] - ETA: 4:37 - loss: 1.9290 - regression_loss: 1.4371 - classification_loss: 0.4919\n",
      "231/500 [============>.................] - ETA: 4:36 - loss: 1.9293 - regression_loss: 1.4372 - classification_loss: 0.4921\n",
      "232/500 [============>.................] - ETA: 4:34 - loss: 1.9305 - regression_loss: 1.4387 - classification_loss: 0.4918\n",
      "233/500 [============>.................] - ETA: 4:33 - loss: 1.9311 - regression_loss: 1.4395 - classification_loss: 0.4916\n",
      "234/500 [=============>................] - ETA: 4:32 - loss: 1.9277 - regression_loss: 1.4367 - classification_loss: 0.4909\n",
      "235/500 [=============>................] - ETA: 4:31 - loss: 1.9247 - regression_loss: 1.4347 - classification_loss: 0.4900\n",
      "236/500 [=============>................] - ETA: 4:30 - loss: 1.9237 - regression_loss: 1.4344 - classification_loss: 0.4894\n",
      "237/500 [=============>................] - ETA: 4:29 - loss: 1.9241 - regression_loss: 1.4344 - classification_loss: 0.4896\n",
      "238/500 [=============>................] - ETA: 4:28 - loss: 1.9298 - regression_loss: 1.4374 - classification_loss: 0.4924\n",
      "239/500 [=============>................] - ETA: 4:27 - loss: 1.9338 - regression_loss: 1.4400 - classification_loss: 0.4937\n",
      "240/500 [=============>................] - ETA: 4:26 - loss: 1.9349 - regression_loss: 1.4418 - classification_loss: 0.4932\n",
      "241/500 [=============>................] - ETA: 4:25 - loss: 1.9406 - regression_loss: 1.4450 - classification_loss: 0.4955\n",
      "242/500 [=============>................] - ETA: 4:24 - loss: 1.9401 - regression_loss: 1.4448 - classification_loss: 0.4953\n",
      "243/500 [=============>................] - ETA: 4:23 - loss: 1.9412 - regression_loss: 1.4457 - classification_loss: 0.4955\n",
      "244/500 [=============>................] - ETA: 4:22 - loss: 1.9423 - regression_loss: 1.4466 - classification_loss: 0.4957\n",
      "245/500 [=============>................] - ETA: 4:21 - loss: 1.9443 - regression_loss: 1.4475 - classification_loss: 0.4968\n",
      "246/500 [=============>................] - ETA: 4:20 - loss: 1.9454 - regression_loss: 1.4487 - classification_loss: 0.4966\n",
      "247/500 [=============>................] - ETA: 4:19 - loss: 1.9439 - regression_loss: 1.4478 - classification_loss: 0.4961\n",
      "248/500 [=============>................] - ETA: 4:18 - loss: 1.9485 - regression_loss: 1.4511 - classification_loss: 0.4975\n",
      "249/500 [=============>................] - ETA: 4:17 - loss: 1.9481 - regression_loss: 1.4508 - classification_loss: 0.4973\n",
      "250/500 [==============>...............] - ETA: 4:16 - loss: 1.9511 - regression_loss: 1.4536 - classification_loss: 0.4975\n",
      "251/500 [==============>...............] - ETA: 4:15 - loss: 1.9499 - regression_loss: 1.4529 - classification_loss: 0.4970\n",
      "252/500 [==============>...............] - ETA: 4:14 - loss: 1.9510 - regression_loss: 1.4538 - classification_loss: 0.4971\n",
      "253/500 [==============>...............] - ETA: 4:13 - loss: 1.9466 - regression_loss: 1.4501 - classification_loss: 0.4965\n",
      "254/500 [==============>...............] - ETA: 4:12 - loss: 1.9454 - regression_loss: 1.4490 - classification_loss: 0.4963\n",
      "255/500 [==============>...............] - ETA: 4:11 - loss: 1.9460 - regression_loss: 1.4496 - classification_loss: 0.4965\n",
      "256/500 [==============>...............] - ETA: 4:10 - loss: 1.9445 - regression_loss: 1.4486 - classification_loss: 0.4959\n",
      "257/500 [==============>...............] - ETA: 4:09 - loss: 1.9450 - regression_loss: 1.4489 - classification_loss: 0.4960\n",
      "258/500 [==============>...............] - ETA: 4:08 - loss: 1.9453 - regression_loss: 1.4489 - classification_loss: 0.4964\n",
      "259/500 [==============>...............] - ETA: 4:07 - loss: 1.9446 - regression_loss: 1.4488 - classification_loss: 0.4958\n",
      "260/500 [==============>...............] - ETA: 4:06 - loss: 1.9458 - regression_loss: 1.4493 - classification_loss: 0.4964\n",
      "261/500 [==============>...............] - ETA: 4:05 - loss: 1.9473 - regression_loss: 1.4509 - classification_loss: 0.4963\n",
      "262/500 [==============>...............] - ETA: 4:04 - loss: 1.9475 - regression_loss: 1.4509 - classification_loss: 0.4965\n",
      "263/500 [==============>...............] - ETA: 4:03 - loss: 1.9462 - regression_loss: 1.4503 - classification_loss: 0.4959\n",
      "264/500 [==============>...............] - ETA: 4:02 - loss: 1.9452 - regression_loss: 1.4495 - classification_loss: 0.4957\n",
      "265/500 [==============>...............] - ETA: 4:01 - loss: 1.9445 - regression_loss: 1.4488 - classification_loss: 0.4957\n",
      "266/500 [==============>...............] - ETA: 4:00 - loss: 1.9412 - regression_loss: 1.4461 - classification_loss: 0.4951\n",
      "267/500 [===============>..............] - ETA: 3:59 - loss: 1.9425 - regression_loss: 1.4468 - classification_loss: 0.4957\n",
      "268/500 [===============>..............] - ETA: 3:58 - loss: 1.9402 - regression_loss: 1.4455 - classification_loss: 0.4948\n",
      "269/500 [===============>..............] - ETA: 3:57 - loss: 1.9406 - regression_loss: 1.4460 - classification_loss: 0.4946\n",
      "270/500 [===============>..............] - ETA: 3:56 - loss: 1.9404 - regression_loss: 1.4462 - classification_loss: 0.4942\n",
      "271/500 [===============>..............] - ETA: 3:55 - loss: 1.9430 - regression_loss: 1.4480 - classification_loss: 0.4950\n",
      "272/500 [===============>..............] - ETA: 3:54 - loss: 1.9456 - regression_loss: 1.4498 - classification_loss: 0.4958\n",
      "273/500 [===============>..............] - ETA: 3:53 - loss: 1.9469 - regression_loss: 1.4511 - classification_loss: 0.4958\n",
      "274/500 [===============>..............] - ETA: 3:52 - loss: 1.9497 - regression_loss: 1.4531 - classification_loss: 0.4965\n",
      "275/500 [===============>..............] - ETA: 3:51 - loss: 1.9516 - regression_loss: 1.4545 - classification_loss: 0.4971\n",
      "276/500 [===============>..............] - ETA: 3:50 - loss: 1.9552 - regression_loss: 1.4565 - classification_loss: 0.4988\n",
      "277/500 [===============>..............] - ETA: 3:49 - loss: 1.9562 - regression_loss: 1.4568 - classification_loss: 0.4994\n",
      "278/500 [===============>..............] - ETA: 3:48 - loss: 1.9562 - regression_loss: 1.4558 - classification_loss: 0.5003\n",
      "279/500 [===============>..............] - ETA: 3:47 - loss: 1.9546 - regression_loss: 1.4542 - classification_loss: 0.5003\n",
      "280/500 [===============>..............] - ETA: 3:46 - loss: 1.9547 - regression_loss: 1.4545 - classification_loss: 0.5002\n",
      "281/500 [===============>..............] - ETA: 3:45 - loss: 1.9540 - regression_loss: 1.4537 - classification_loss: 0.5003\n",
      "282/500 [===============>..............] - ETA: 3:44 - loss: 1.9558 - regression_loss: 1.4548 - classification_loss: 0.5010\n",
      "283/500 [===============>..............] - ETA: 3:43 - loss: 1.9552 - regression_loss: 1.4541 - classification_loss: 0.5011\n",
      "284/500 [================>.............] - ETA: 3:42 - loss: 1.9556 - regression_loss: 1.4544 - classification_loss: 0.5012\n",
      "285/500 [================>.............] - ETA: 3:41 - loss: 1.9574 - regression_loss: 1.4556 - classification_loss: 0.5018\n",
      "286/500 [================>.............] - ETA: 3:40 - loss: 1.9555 - regression_loss: 1.4541 - classification_loss: 0.5014\n",
      "287/500 [================>.............] - ETA: 3:39 - loss: 1.9541 - regression_loss: 1.4532 - classification_loss: 0.5009\n",
      "288/500 [================>.............] - ETA: 3:38 - loss: 1.9550 - regression_loss: 1.4537 - classification_loss: 0.5013\n",
      "289/500 [================>.............] - ETA: 3:37 - loss: 1.9538 - regression_loss: 1.4529 - classification_loss: 0.5008\n",
      "290/500 [================>.............] - ETA: 3:36 - loss: 1.9567 - regression_loss: 1.4548 - classification_loss: 0.5019\n",
      "291/500 [================>.............] - ETA: 3:35 - loss: 1.9568 - regression_loss: 1.4550 - classification_loss: 0.5018\n",
      "292/500 [================>.............] - ETA: 3:34 - loss: 1.9554 - regression_loss: 1.4543 - classification_loss: 0.5011\n",
      "293/500 [================>.............] - ETA: 3:32 - loss: 1.9567 - regression_loss: 1.4556 - classification_loss: 0.5010\n",
      "294/500 [================>.............] - ETA: 3:31 - loss: 1.9609 - regression_loss: 1.4590 - classification_loss: 0.5019\n",
      "295/500 [================>.............] - ETA: 3:30 - loss: 1.9625 - regression_loss: 1.4601 - classification_loss: 0.5024\n",
      "296/500 [================>.............] - ETA: 3:29 - loss: 1.9610 - regression_loss: 1.4587 - classification_loss: 0.5022\n",
      "297/500 [================>.............] - ETA: 3:28 - loss: 1.9600 - regression_loss: 1.4580 - classification_loss: 0.5021\n",
      "298/500 [================>.............] - ETA: 3:27 - loss: 1.9600 - regression_loss: 1.4581 - classification_loss: 0.5020\n",
      "299/500 [================>.............] - ETA: 3:26 - loss: 1.9613 - regression_loss: 1.4596 - classification_loss: 0.5017\n",
      "300/500 [=================>............] - ETA: 3:25 - loss: 1.9609 - regression_loss: 1.4589 - classification_loss: 0.5020\n",
      "301/500 [=================>............] - ETA: 3:24 - loss: 1.9605 - regression_loss: 1.4586 - classification_loss: 0.5019\n",
      "302/500 [=================>............] - ETA: 3:23 - loss: 1.9638 - regression_loss: 1.4601 - classification_loss: 0.5037\n",
      "303/500 [=================>............] - ETA: 3:22 - loss: 1.9647 - regression_loss: 1.4611 - classification_loss: 0.5036\n",
      "304/500 [=================>............] - ETA: 3:21 - loss: 1.9672 - regression_loss: 1.4633 - classification_loss: 0.5039\n",
      "305/500 [=================>............] - ETA: 3:20 - loss: 1.9676 - regression_loss: 1.4639 - classification_loss: 0.5037\n",
      "306/500 [=================>............] - ETA: 3:19 - loss: 1.9660 - regression_loss: 1.4626 - classification_loss: 0.5035\n",
      "307/500 [=================>............] - ETA: 3:18 - loss: 1.9659 - regression_loss: 1.4624 - classification_loss: 0.5036\n",
      "308/500 [=================>............] - ETA: 3:17 - loss: 1.9658 - regression_loss: 1.4624 - classification_loss: 0.5033\n",
      "309/500 [=================>............] - ETA: 3:16 - loss: 1.9688 - regression_loss: 1.4652 - classification_loss: 0.5036\n",
      "310/500 [=================>............] - ETA: 3:15 - loss: 1.9707 - regression_loss: 1.4660 - classification_loss: 0.5046\n",
      "311/500 [=================>............] - ETA: 3:14 - loss: 1.9717 - regression_loss: 1.4666 - classification_loss: 0.5051\n",
      "312/500 [=================>............] - ETA: 3:13 - loss: 1.9725 - regression_loss: 1.4671 - classification_loss: 0.5054\n",
      "313/500 [=================>............] - ETA: 3:12 - loss: 1.9714 - regression_loss: 1.4665 - classification_loss: 0.5049\n",
      "314/500 [=================>............] - ETA: 3:11 - loss: 1.9717 - regression_loss: 1.4670 - classification_loss: 0.5046\n",
      "315/500 [=================>............] - ETA: 3:10 - loss: 1.9717 - regression_loss: 1.4674 - classification_loss: 0.5043\n",
      "316/500 [=================>............] - ETA: 3:09 - loss: 1.9714 - regression_loss: 1.4673 - classification_loss: 0.5041\n",
      "317/500 [==================>...........] - ETA: 3:08 - loss: 1.9704 - regression_loss: 1.4662 - classification_loss: 0.5042\n",
      "318/500 [==================>...........] - ETA: 3:07 - loss: 1.9687 - regression_loss: 1.4647 - classification_loss: 0.5040\n",
      "319/500 [==================>...........] - ETA: 3:06 - loss: 1.9685 - regression_loss: 1.4646 - classification_loss: 0.5039\n",
      "320/500 [==================>...........] - ETA: 3:05 - loss: 1.9686 - regression_loss: 1.4647 - classification_loss: 0.5039\n",
      "321/500 [==================>...........] - ETA: 3:03 - loss: 1.9664 - regression_loss: 1.4632 - classification_loss: 0.5032\n",
      "322/500 [==================>...........] - ETA: 3:02 - loss: 1.9692 - regression_loss: 1.4654 - classification_loss: 0.5039\n",
      "323/500 [==================>...........] - ETA: 3:01 - loss: 1.9678 - regression_loss: 1.4643 - classification_loss: 0.5035\n",
      "324/500 [==================>...........] - ETA: 3:00 - loss: 1.9662 - regression_loss: 1.4631 - classification_loss: 0.5031\n",
      "325/500 [==================>...........] - ETA: 2:59 - loss: 1.9639 - regression_loss: 1.4615 - classification_loss: 0.5024\n",
      "326/500 [==================>...........] - ETA: 2:59 - loss: 1.9621 - regression_loss: 1.4604 - classification_loss: 0.5017\n",
      "327/500 [==================>...........] - ETA: 2:58 - loss: 1.9627 - regression_loss: 1.4611 - classification_loss: 0.5016\n",
      "328/500 [==================>...........] - ETA: 2:57 - loss: 1.9607 - regression_loss: 1.4590 - classification_loss: 0.5017\n",
      "329/500 [==================>...........] - ETA: 2:56 - loss: 1.9605 - regression_loss: 1.4587 - classification_loss: 0.5018\n",
      "330/500 [==================>...........] - ETA: 2:55 - loss: 1.9598 - regression_loss: 1.4582 - classification_loss: 0.5016\n",
      "331/500 [==================>...........] - ETA: 2:53 - loss: 1.9573 - regression_loss: 1.4565 - classification_loss: 0.5008\n",
      "332/500 [==================>...........] - ETA: 2:52 - loss: 1.9581 - regression_loss: 1.4574 - classification_loss: 0.5007\n",
      "333/500 [==================>...........] - ETA: 2:51 - loss: 1.9552 - regression_loss: 1.4553 - classification_loss: 0.4999\n",
      "334/500 [===================>..........] - ETA: 2:50 - loss: 1.9555 - regression_loss: 1.4557 - classification_loss: 0.4998\n",
      "335/500 [===================>..........] - ETA: 2:49 - loss: 1.9545 - regression_loss: 1.4550 - classification_loss: 0.4995\n",
      "336/500 [===================>..........] - ETA: 2:48 - loss: 1.9552 - regression_loss: 1.4553 - classification_loss: 0.4999\n",
      "337/500 [===================>..........] - ETA: 2:47 - loss: 1.9593 - regression_loss: 1.4585 - classification_loss: 0.5008\n",
      "338/500 [===================>..........] - ETA: 2:46 - loss: 1.9599 - regression_loss: 1.4595 - classification_loss: 0.5004\n",
      "339/500 [===================>..........] - ETA: 2:45 - loss: 1.9583 - regression_loss: 1.4583 - classification_loss: 0.5000\n",
      "340/500 [===================>..........] - ETA: 2:44 - loss: 1.9572 - regression_loss: 1.4574 - classification_loss: 0.4998\n",
      "341/500 [===================>..........] - ETA: 2:43 - loss: 1.9571 - regression_loss: 1.4573 - classification_loss: 0.4998\n",
      "342/500 [===================>..........] - ETA: 2:42 - loss: 1.9564 - regression_loss: 1.4570 - classification_loss: 0.4995\n",
      "343/500 [===================>..........] - ETA: 2:41 - loss: 1.9561 - regression_loss: 1.4566 - classification_loss: 0.4995\n",
      "344/500 [===================>..........] - ETA: 2:40 - loss: 1.9560 - regression_loss: 1.4566 - classification_loss: 0.4994\n",
      "345/500 [===================>..........] - ETA: 2:39 - loss: 1.9566 - regression_loss: 1.4571 - classification_loss: 0.4995\n",
      "346/500 [===================>..........] - ETA: 2:38 - loss: 1.9604 - regression_loss: 1.4594 - classification_loss: 0.5010\n",
      "347/500 [===================>..........] - ETA: 2:37 - loss: 1.9619 - regression_loss: 1.4599 - classification_loss: 0.5020\n",
      "348/500 [===================>..........] - ETA: 2:36 - loss: 1.9610 - regression_loss: 1.4591 - classification_loss: 0.5018\n",
      "349/500 [===================>..........] - ETA: 2:35 - loss: 1.9602 - regression_loss: 1.4584 - classification_loss: 0.5018\n",
      "350/500 [====================>.........] - ETA: 2:34 - loss: 1.9583 - regression_loss: 1.4569 - classification_loss: 0.5014\n",
      "351/500 [====================>.........] - ETA: 2:33 - loss: 1.9596 - regression_loss: 1.4579 - classification_loss: 0.5017\n",
      "352/500 [====================>.........] - ETA: 2:32 - loss: 1.9617 - regression_loss: 1.4593 - classification_loss: 0.5024\n",
      "353/500 [====================>.........] - ETA: 2:31 - loss: 1.9612 - regression_loss: 1.4587 - classification_loss: 0.5025\n",
      "354/500 [====================>.........] - ETA: 2:30 - loss: 1.9609 - regression_loss: 1.4579 - classification_loss: 0.5030\n",
      "355/500 [====================>.........] - ETA: 2:29 - loss: 1.9638 - regression_loss: 1.4602 - classification_loss: 0.5036\n",
      "356/500 [====================>.........] - ETA: 2:28 - loss: 1.9658 - regression_loss: 1.4612 - classification_loss: 0.5045\n",
      "357/500 [====================>.........] - ETA: 2:27 - loss: 1.9646 - regression_loss: 1.4604 - classification_loss: 0.5043\n",
      "358/500 [====================>.........] - ETA: 2:26 - loss: 1.9638 - regression_loss: 1.4595 - classification_loss: 0.5043\n",
      "359/500 [====================>.........] - ETA: 2:25 - loss: 1.9634 - regression_loss: 1.4595 - classification_loss: 0.5039\n",
      "360/500 [====================>.........] - ETA: 2:24 - loss: 1.9643 - regression_loss: 1.4603 - classification_loss: 0.5040\n",
      "361/500 [====================>.........] - ETA: 2:23 - loss: 1.9677 - regression_loss: 1.4627 - classification_loss: 0.5050\n",
      "362/500 [====================>.........] - ETA: 2:21 - loss: 1.9694 - regression_loss: 1.4640 - classification_loss: 0.5054\n",
      "363/500 [====================>.........] - ETA: 2:20 - loss: 1.9683 - regression_loss: 1.4633 - classification_loss: 0.5050\n",
      "364/500 [====================>.........] - ETA: 2:19 - loss: 1.9702 - regression_loss: 1.4650 - classification_loss: 0.5052\n",
      "365/500 [====================>.........] - ETA: 2:18 - loss: 1.9719 - regression_loss: 1.4654 - classification_loss: 0.5064\n",
      "366/500 [====================>.........] - ETA: 2:17 - loss: 1.9726 - regression_loss: 1.4662 - classification_loss: 0.5063\n",
      "367/500 [=====================>........] - ETA: 2:16 - loss: 1.9701 - regression_loss: 1.4641 - classification_loss: 0.5060\n",
      "368/500 [=====================>........] - ETA: 2:15 - loss: 1.9701 - regression_loss: 1.4639 - classification_loss: 0.5062\n",
      "369/500 [=====================>........] - ETA: 2:14 - loss: 1.9717 - regression_loss: 1.4652 - classification_loss: 0.5065\n",
      "370/500 [=====================>........] - ETA: 2:13 - loss: 1.9715 - regression_loss: 1.4650 - classification_loss: 0.5064\n",
      "371/500 [=====================>........] - ETA: 2:12 - loss: 1.9710 - regression_loss: 1.4648 - classification_loss: 0.5062\n",
      "372/500 [=====================>........] - ETA: 2:11 - loss: 1.9696 - regression_loss: 1.4637 - classification_loss: 0.5059\n",
      "373/500 [=====================>........] - ETA: 2:10 - loss: 1.9676 - regression_loss: 1.4623 - classification_loss: 0.5053\n",
      "374/500 [=====================>........] - ETA: 2:09 - loss: 1.9658 - regression_loss: 1.4606 - classification_loss: 0.5052\n",
      "375/500 [=====================>........] - ETA: 2:08 - loss: 1.9680 - regression_loss: 1.4624 - classification_loss: 0.5056\n",
      "376/500 [=====================>........] - ETA: 2:07 - loss: 1.9680 - regression_loss: 1.4626 - classification_loss: 0.5053\n",
      "377/500 [=====================>........] - ETA: 2:06 - loss: 1.9686 - regression_loss: 1.4636 - classification_loss: 0.5050\n",
      "378/500 [=====================>........] - ETA: 2:05 - loss: 1.9691 - regression_loss: 1.4640 - classification_loss: 0.5050\n",
      "379/500 [=====================>........] - ETA: 2:04 - loss: 1.9720 - regression_loss: 1.4656 - classification_loss: 0.5064\n",
      "380/500 [=====================>........] - ETA: 2:03 - loss: 1.9705 - regression_loss: 1.4644 - classification_loss: 0.5061\n",
      "381/500 [=====================>........] - ETA: 2:02 - loss: 1.9715 - regression_loss: 1.4655 - classification_loss: 0.5060\n",
      "382/500 [=====================>........] - ETA: 2:01 - loss: 1.9712 - regression_loss: 1.4655 - classification_loss: 0.5057\n",
      "383/500 [=====================>........] - ETA: 2:00 - loss: 1.9721 - regression_loss: 1.4665 - classification_loss: 0.5056\n",
      "384/500 [======================>.......] - ETA: 1:59 - loss: 1.9728 - regression_loss: 1.4668 - classification_loss: 0.5061\n",
      "385/500 [======================>.......] - ETA: 1:58 - loss: 1.9731 - regression_loss: 1.4672 - classification_loss: 0.5059\n",
      "386/500 [======================>.......] - ETA: 1:57 - loss: 1.9730 - regression_loss: 1.4671 - classification_loss: 0.5059\n",
      "387/500 [======================>.......] - ETA: 1:56 - loss: 1.9728 - regression_loss: 1.4671 - classification_loss: 0.5057\n",
      "388/500 [======================>.......] - ETA: 1:55 - loss: 1.9733 - regression_loss: 1.4674 - classification_loss: 0.5059\n",
      "389/500 [======================>.......] - ETA: 1:54 - loss: 1.9722 - regression_loss: 1.4664 - classification_loss: 0.5058\n",
      "390/500 [======================>.......] - ETA: 1:53 - loss: 1.9717 - regression_loss: 1.4661 - classification_loss: 0.5057\n",
      "391/500 [======================>.......] - ETA: 1:52 - loss: 1.9704 - regression_loss: 1.4649 - classification_loss: 0.5055\n",
      "392/500 [======================>.......] - ETA: 1:51 - loss: 1.9701 - regression_loss: 1.4648 - classification_loss: 0.5052\n",
      "393/500 [======================>.......] - ETA: 1:49 - loss: 1.9699 - regression_loss: 1.4644 - classification_loss: 0.5056\n",
      "394/500 [======================>.......] - ETA: 1:48 - loss: 1.9720 - regression_loss: 1.4660 - classification_loss: 0.5060\n",
      "395/500 [======================>.......] - ETA: 1:47 - loss: 1.9723 - regression_loss: 1.4658 - classification_loss: 0.5065\n",
      "396/500 [======================>.......] - ETA: 1:46 - loss: 1.9726 - regression_loss: 1.4663 - classification_loss: 0.5063\n",
      "397/500 [======================>.......] - ETA: 1:45 - loss: 1.9724 - regression_loss: 1.4662 - classification_loss: 0.5062\n",
      "398/500 [======================>.......] - ETA: 1:44 - loss: 1.9747 - regression_loss: 1.4675 - classification_loss: 0.5072\n",
      "399/500 [======================>.......] - ETA: 1:43 - loss: 1.9741 - regression_loss: 1.4670 - classification_loss: 0.5071\n",
      "400/500 [=======================>......] - ETA: 1:42 - loss: 1.9757 - regression_loss: 1.4679 - classification_loss: 0.5077\n",
      "401/500 [=======================>......] - ETA: 1:41 - loss: 1.9765 - regression_loss: 1.4686 - classification_loss: 0.5079\n",
      "402/500 [=======================>......] - ETA: 1:40 - loss: 1.9770 - regression_loss: 1.4695 - classification_loss: 0.5075\n",
      "403/500 [=======================>......] - ETA: 1:39 - loss: 1.9779 - regression_loss: 1.4700 - classification_loss: 0.5079\n",
      "404/500 [=======================>......] - ETA: 1:38 - loss: 1.9771 - regression_loss: 1.4693 - classification_loss: 0.5078\n",
      "405/500 [=======================>......] - ETA: 1:37 - loss: 1.9780 - regression_loss: 1.4696 - classification_loss: 0.5084\n",
      "406/500 [=======================>......] - ETA: 1:36 - loss: 1.9766 - regression_loss: 1.4687 - classification_loss: 0.5079\n",
      "407/500 [=======================>......] - ETA: 1:35 - loss: 1.9754 - regression_loss: 1.4679 - classification_loss: 0.5075\n",
      "408/500 [=======================>......] - ETA: 1:34 - loss: 1.9754 - regression_loss: 1.4677 - classification_loss: 0.5077\n",
      "409/500 [=======================>......] - ETA: 1:33 - loss: 1.9755 - regression_loss: 1.4679 - classification_loss: 0.5077\n",
      "410/500 [=======================>......] - ETA: 1:32 - loss: 1.9747 - regression_loss: 1.4674 - classification_loss: 0.5073\n",
      "411/500 [=======================>......] - ETA: 1:31 - loss: 1.9734 - regression_loss: 1.4661 - classification_loss: 0.5073\n",
      "412/500 [=======================>......] - ETA: 1:30 - loss: 1.9722 - regression_loss: 1.4655 - classification_loss: 0.5067\n",
      "413/500 [=======================>......] - ETA: 1:29 - loss: 1.9721 - regression_loss: 1.4652 - classification_loss: 0.5068\n",
      "414/500 [=======================>......] - ETA: 1:28 - loss: 1.9711 - regression_loss: 1.4647 - classification_loss: 0.5064\n",
      "415/500 [=======================>......] - ETA: 1:27 - loss: 1.9705 - regression_loss: 1.4644 - classification_loss: 0.5062\n",
      "416/500 [=======================>......] - ETA: 1:26 - loss: 1.9718 - regression_loss: 1.4649 - classification_loss: 0.5070\n",
      "417/500 [========================>.....] - ETA: 1:25 - loss: 1.9716 - regression_loss: 1.4649 - classification_loss: 0.5067\n",
      "418/500 [========================>.....] - ETA: 1:24 - loss: 1.9707 - regression_loss: 1.4641 - classification_loss: 0.5066\n",
      "419/500 [========================>.....] - ETA: 1:23 - loss: 1.9683 - regression_loss: 1.4625 - classification_loss: 0.5058\n",
      "420/500 [========================>.....] - ETA: 1:22 - loss: 1.9679 - regression_loss: 1.4621 - classification_loss: 0.5059\n",
      "421/500 [========================>.....] - ETA: 1:21 - loss: 1.9682 - regression_loss: 1.4622 - classification_loss: 0.5060\n",
      "422/500 [========================>.....] - ETA: 1:20 - loss: 1.9691 - regression_loss: 1.4633 - classification_loss: 0.5058\n",
      "423/500 [========================>.....] - ETA: 1:19 - loss: 1.9696 - regression_loss: 1.4639 - classification_loss: 0.5057\n",
      "424/500 [========================>.....] - ETA: 1:18 - loss: 1.9699 - regression_loss: 1.4642 - classification_loss: 0.5057\n",
      "425/500 [========================>.....] - ETA: 1:16 - loss: 1.9683 - regression_loss: 1.4627 - classification_loss: 0.5056\n",
      "426/500 [========================>.....] - ETA: 1:15 - loss: 1.9686 - regression_loss: 1.4632 - classification_loss: 0.5054\n",
      "427/500 [========================>.....] - ETA: 1:14 - loss: 1.9703 - regression_loss: 1.4643 - classification_loss: 0.5060\n",
      "428/500 [========================>.....] - ETA: 1:13 - loss: 1.9711 - regression_loss: 1.4648 - classification_loss: 0.5063\n",
      "429/500 [========================>.....] - ETA: 1:12 - loss: 1.9730 - regression_loss: 1.4663 - classification_loss: 0.5066\n",
      "430/500 [========================>.....] - ETA: 1:11 - loss: 1.9750 - regression_loss: 1.4674 - classification_loss: 0.5076\n",
      "431/500 [========================>.....] - ETA: 1:10 - loss: 1.9748 - regression_loss: 1.4674 - classification_loss: 0.5074\n",
      "432/500 [========================>.....] - ETA: 1:09 - loss: 1.9745 - regression_loss: 1.4675 - classification_loss: 0.5070\n",
      "433/500 [========================>.....] - ETA: 1:08 - loss: 1.9742 - regression_loss: 1.4672 - classification_loss: 0.5070\n",
      "434/500 [=========================>....] - ETA: 1:07 - loss: 1.9735 - regression_loss: 1.4666 - classification_loss: 0.5069\n",
      "435/500 [=========================>....] - ETA: 1:06 - loss: 1.9729 - regression_loss: 1.4662 - classification_loss: 0.5066\n",
      "436/500 [=========================>....] - ETA: 1:05 - loss: 1.9728 - regression_loss: 1.4665 - classification_loss: 0.5063\n",
      "437/500 [=========================>....] - ETA: 1:04 - loss: 1.9751 - regression_loss: 1.4683 - classification_loss: 0.5068\n",
      "438/500 [=========================>....] - ETA: 1:03 - loss: 1.9745 - regression_loss: 1.4680 - classification_loss: 0.5065\n",
      "439/500 [=========================>....] - ETA: 1:02 - loss: 1.9730 - regression_loss: 1.4667 - classification_loss: 0.5063\n",
      "440/500 [=========================>....] - ETA: 1:01 - loss: 1.9732 - regression_loss: 1.4663 - classification_loss: 0.5069\n",
      "441/500 [=========================>....] - ETA: 1:00 - loss: 1.9749 - regression_loss: 1.4675 - classification_loss: 0.5073\n",
      "442/500 [=========================>....] - ETA: 59s - loss: 1.9741 - regression_loss: 1.4668 - classification_loss: 0.5073 \n",
      "443/500 [=========================>....] - ETA: 58s - loss: 1.9743 - regression_loss: 1.4671 - classification_loss: 0.5071\n",
      "444/500 [=========================>....] - ETA: 57s - loss: 1.9740 - regression_loss: 1.4671 - classification_loss: 0.5069\n",
      "445/500 [=========================>....] - ETA: 56s - loss: 1.9742 - regression_loss: 1.4669 - classification_loss: 0.5073\n",
      "446/500 [=========================>....] - ETA: 55s - loss: 1.9724 - regression_loss: 1.4656 - classification_loss: 0.5067\n",
      "447/500 [=========================>....] - ETA: 54s - loss: 1.9717 - regression_loss: 1.4654 - classification_loss: 0.5063\n",
      "448/500 [=========================>....] - ETA: 53s - loss: 1.9709 - regression_loss: 1.4650 - classification_loss: 0.5058\n",
      "449/500 [=========================>....] - ETA: 52s - loss: 1.9703 - regression_loss: 1.4646 - classification_loss: 0.5057\n",
      "450/500 [==========================>...] - ETA: 51s - loss: 1.9706 - regression_loss: 1.4649 - classification_loss: 0.5057\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 1.9708 - regression_loss: 1.4649 - classification_loss: 0.5059\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 1.9710 - regression_loss: 1.4649 - classification_loss: 0.5061\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.9704 - regression_loss: 1.4645 - classification_loss: 0.5059\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.9705 - regression_loss: 1.4643 - classification_loss: 0.5062\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.9704 - regression_loss: 1.4643 - classification_loss: 0.5061\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.9696 - regression_loss: 1.4636 - classification_loss: 0.5060\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.9688 - regression_loss: 1.4631 - classification_loss: 0.5057\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.9679 - regression_loss: 1.4620 - classification_loss: 0.5059\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.9677 - regression_loss: 1.4617 - classification_loss: 0.5061\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.9678 - regression_loss: 1.4617 - classification_loss: 0.5062\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 1.9682 - regression_loss: 1.4619 - classification_loss: 0.5063\n",
      "462/500 [==========================>...] - ETA: 38s - loss: 1.9664 - regression_loss: 1.4604 - classification_loss: 0.5060\n",
      "463/500 [==========================>...] - ETA: 37s - loss: 1.9645 - regression_loss: 1.4590 - classification_loss: 0.5054\n",
      "464/500 [==========================>...] - ETA: 36s - loss: 1.9645 - regression_loss: 1.4592 - classification_loss: 0.5053\n",
      "465/500 [==========================>...] - ETA: 35s - loss: 1.9649 - regression_loss: 1.4596 - classification_loss: 0.5053\n",
      "466/500 [==========================>...] - ETA: 34s - loss: 1.9668 - regression_loss: 1.4603 - classification_loss: 0.5065\n",
      "467/500 [===========================>..] - ETA: 33s - loss: 1.9667 - regression_loss: 1.4603 - classification_loss: 0.5064\n",
      "468/500 [===========================>..] - ETA: 32s - loss: 1.9685 - regression_loss: 1.4616 - classification_loss: 0.5068\n",
      "469/500 [===========================>..] - ETA: 31s - loss: 1.9687 - regression_loss: 1.4622 - classification_loss: 0.5065\n",
      "470/500 [===========================>..] - ETA: 30s - loss: 1.9699 - regression_loss: 1.4631 - classification_loss: 0.5068\n",
      "471/500 [===========================>..] - ETA: 29s - loss: 1.9683 - regression_loss: 1.4621 - classification_loss: 0.5062\n",
      "472/500 [===========================>..] - ETA: 28s - loss: 1.9691 - regression_loss: 1.4626 - classification_loss: 0.5064\n",
      "473/500 [===========================>..] - ETA: 27s - loss: 1.9691 - regression_loss: 1.4626 - classification_loss: 0.5066\n",
      "474/500 [===========================>..] - ETA: 26s - loss: 1.9694 - regression_loss: 1.4628 - classification_loss: 0.5066\n",
      "475/500 [===========================>..] - ETA: 25s - loss: 1.9698 - regression_loss: 1.4633 - classification_loss: 0.5065\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 1.9687 - regression_loss: 1.4624 - classification_loss: 0.5063\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.9700 - regression_loss: 1.4634 - classification_loss: 0.5066\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.9712 - regression_loss: 1.4643 - classification_loss: 0.5069\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.9713 - regression_loss: 1.4643 - classification_loss: 0.5070\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.9705 - regression_loss: 1.4638 - classification_loss: 0.5067\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.9689 - regression_loss: 1.4625 - classification_loss: 0.5064\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.9696 - regression_loss: 1.4630 - classification_loss: 0.5066\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.9690 - regression_loss: 1.4627 - classification_loss: 0.5064\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.9690 - regression_loss: 1.4629 - classification_loss: 0.5061\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.9678 - regression_loss: 1.4620 - classification_loss: 0.5058\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.9674 - regression_loss: 1.4619 - classification_loss: 0.5055\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.9667 - regression_loss: 1.4613 - classification_loss: 0.5053\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.9682 - regression_loss: 1.4626 - classification_loss: 0.5056\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.9690 - regression_loss: 1.4635 - classification_loss: 0.5056\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.9690 - regression_loss: 1.4633 - classification_loss: 0.5057\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.9689 - regression_loss: 1.4632 - classification_loss: 0.5057 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.9688 - regression_loss: 1.4634 - classification_loss: 0.5054\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.9714 - regression_loss: 1.4649 - classification_loss: 0.5065\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.9704 - regression_loss: 1.4644 - classification_loss: 0.5060\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.9684 - regression_loss: 1.4630 - classification_loss: 0.5054\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.9672 - regression_loss: 1.4622 - classification_loss: 0.5050\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.9688 - regression_loss: 1.4631 - classification_loss: 0.5057\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.9696 - regression_loss: 1.4635 - classification_loss: 0.5061\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.9712 - regression_loss: 1.4648 - classification_loss: 0.5064\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9700 - regression_loss: 1.4642 - classification_loss: 0.5058\n",
      "Epoch 00012: saving model to ./snapshots\\resnet50_csv_12.h5\n",
      "\n",
      "500/500 [==============================] - 514s 1s/step - loss: 1.9700 - regression_loss: 1.4642 - classification_loss: 0.5058\n",
      "Epoch 13/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 1.9638 - regression_loss: 1.5086 - classification_loss: 0.4552\n",
      "  2/500 [..............................] - ETA: 4:03 - loss: 1.8414 - regression_loss: 1.4226 - classification_loss: 0.4188\n",
      "  3/500 [..............................] - ETA: 5:41 - loss: 2.1627 - regression_loss: 1.5987 - classification_loss: 0.5640\n",
      "  4/500 [..............................] - ETA: 6:16 - loss: 2.3477 - regression_loss: 1.7230 - classification_loss: 0.6247\n",
      "  5/500 [..............................] - ETA: 6:47 - loss: 2.5229 - regression_loss: 1.8272 - classification_loss: 0.6957\n",
      "  6/500 [..............................] - ETA: 7:06 - loss: 2.3449 - regression_loss: 1.6743 - classification_loss: 0.6706\n",
      "  7/500 [..............................] - ETA: 7:14 - loss: 2.2748 - regression_loss: 1.6440 - classification_loss: 0.6307\n",
      "  8/500 [..............................] - ETA: 7:18 - loss: 2.1636 - regression_loss: 1.5693 - classification_loss: 0.5943\n",
      "  9/500 [..............................] - ETA: 7:21 - loss: 2.1653 - regression_loss: 1.5850 - classification_loss: 0.5803\n",
      " 10/500 [..............................] - ETA: 7:25 - loss: 2.2556 - regression_loss: 1.6469 - classification_loss: 0.6087\n",
      " 11/500 [..............................] - ETA: 7:26 - loss: 2.2790 - regression_loss: 1.6717 - classification_loss: 0.6073\n",
      " 12/500 [..............................] - ETA: 7:28 - loss: 2.2798 - regression_loss: 1.6722 - classification_loss: 0.6076\n",
      " 13/500 [..............................] - ETA: 7:29 - loss: 2.2281 - regression_loss: 1.6376 - classification_loss: 0.5905\n",
      " 14/500 [..............................] - ETA: 7:34 - loss: 2.1496 - regression_loss: 1.5796 - classification_loss: 0.5700\n",
      " 15/500 [..............................] - ETA: 7:28 - loss: 2.1678 - regression_loss: 1.6076 - classification_loss: 0.5602\n",
      " 16/500 [..............................] - ETA: 7:36 - loss: 2.1426 - regression_loss: 1.5862 - classification_loss: 0.5564\n",
      " 17/500 [>.............................] - ETA: 7:39 - loss: 2.1183 - regression_loss: 1.5640 - classification_loss: 0.5543\n",
      " 18/500 [>.............................] - ETA: 7:41 - loss: 2.1326 - regression_loss: 1.5617 - classification_loss: 0.5710\n",
      " 19/500 [>.............................] - ETA: 7:41 - loss: 2.1274 - regression_loss: 1.5635 - classification_loss: 0.5639\n",
      " 20/500 [>.............................] - ETA: 7:40 - loss: 2.1072 - regression_loss: 1.5560 - classification_loss: 0.5512\n",
      " 21/500 [>.............................] - ETA: 7:40 - loss: 2.0886 - regression_loss: 1.5435 - classification_loss: 0.5450\n",
      " 22/500 [>.............................] - ETA: 7:42 - loss: 2.0934 - regression_loss: 1.5507 - classification_loss: 0.5427\n",
      " 23/500 [>.............................] - ETA: 7:41 - loss: 2.0616 - regression_loss: 1.5279 - classification_loss: 0.5338\n",
      " 24/500 [>.............................] - ETA: 7:40 - loss: 2.0566 - regression_loss: 1.5230 - classification_loss: 0.5336\n",
      " 25/500 [>.............................] - ETA: 7:42 - loss: 2.0459 - regression_loss: 1.5118 - classification_loss: 0.5341\n",
      " 26/500 [>.............................] - ETA: 7:41 - loss: 2.0251 - regression_loss: 1.4968 - classification_loss: 0.5283\n",
      " 27/500 [>.............................] - ETA: 7:39 - loss: 2.0200 - regression_loss: 1.4938 - classification_loss: 0.5262\n",
      " 28/500 [>.............................] - ETA: 7:39 - loss: 2.0242 - regression_loss: 1.4941 - classification_loss: 0.5301\n",
      " 29/500 [>.............................] - ETA: 7:38 - loss: 1.9903 - regression_loss: 1.4726 - classification_loss: 0.5178\n",
      " 30/500 [>.............................] - ETA: 7:39 - loss: 1.9910 - regression_loss: 1.4743 - classification_loss: 0.5167\n",
      " 31/500 [>.............................] - ETA: 7:40 - loss: 1.9839 - regression_loss: 1.4708 - classification_loss: 0.5131\n",
      " 32/500 [>.............................] - ETA: 7:40 - loss: 1.9769 - regression_loss: 1.4627 - classification_loss: 0.5142\n",
      " 33/500 [>.............................] - ETA: 7:39 - loss: 1.9971 - regression_loss: 1.4722 - classification_loss: 0.5249\n",
      " 34/500 [=>............................] - ETA: 7:38 - loss: 2.0118 - regression_loss: 1.4845 - classification_loss: 0.5274\n",
      " 35/500 [=>............................] - ETA: 7:37 - loss: 2.0095 - regression_loss: 1.4844 - classification_loss: 0.5251\n",
      " 36/500 [=>............................] - ETA: 7:38 - loss: 1.9984 - regression_loss: 1.4729 - classification_loss: 0.5255\n",
      " 37/500 [=>............................] - ETA: 7:38 - loss: 2.0037 - regression_loss: 1.4787 - classification_loss: 0.5250\n",
      " 38/500 [=>............................] - ETA: 7:34 - loss: 2.0114 - regression_loss: 1.4869 - classification_loss: 0.5245\n",
      " 39/500 [=>............................] - ETA: 7:35 - loss: 2.0100 - regression_loss: 1.4842 - classification_loss: 0.5259\n",
      " 40/500 [=>............................] - ETA: 7:35 - loss: 1.9931 - regression_loss: 1.4726 - classification_loss: 0.5205\n",
      " 41/500 [=>............................] - ETA: 7:34 - loss: 1.9958 - regression_loss: 1.4749 - classification_loss: 0.5209\n",
      " 42/500 [=>............................] - ETA: 7:33 - loss: 1.9895 - regression_loss: 1.4716 - classification_loss: 0.5178\n",
      " 43/500 [=>............................] - ETA: 7:33 - loss: 2.0034 - regression_loss: 1.4790 - classification_loss: 0.5243\n",
      " 44/500 [=>............................] - ETA: 7:32 - loss: 2.0120 - regression_loss: 1.4882 - classification_loss: 0.5238\n",
      " 45/500 [=>............................] - ETA: 7:31 - loss: 2.0080 - regression_loss: 1.4836 - classification_loss: 0.5245\n",
      " 46/500 [=>............................] - ETA: 7:30 - loss: 1.9990 - regression_loss: 1.4785 - classification_loss: 0.5205\n",
      " 47/500 [=>............................] - ETA: 7:29 - loss: 1.9885 - regression_loss: 1.4701 - classification_loss: 0.5184\n",
      " 48/500 [=>............................] - ETA: 7:29 - loss: 1.9985 - regression_loss: 1.4776 - classification_loss: 0.5209\n",
      " 49/500 [=>............................] - ETA: 7:29 - loss: 2.0162 - regression_loss: 1.4893 - classification_loss: 0.5269\n",
      " 50/500 [==>...........................] - ETA: 7:28 - loss: 2.0126 - regression_loss: 1.4843 - classification_loss: 0.5283\n",
      " 51/500 [==>...........................] - ETA: 7:27 - loss: 2.0026 - regression_loss: 1.4772 - classification_loss: 0.5254\n",
      " 52/500 [==>...........................] - ETA: 7:26 - loss: 2.0040 - regression_loss: 1.4798 - classification_loss: 0.5242\n",
      " 53/500 [==>...........................] - ETA: 7:24 - loss: 2.0026 - regression_loss: 1.4759 - classification_loss: 0.5267\n",
      " 54/500 [==>...........................] - ETA: 7:24 - loss: 1.9909 - regression_loss: 1.4677 - classification_loss: 0.5232\n",
      " 55/500 [==>...........................] - ETA: 7:24 - loss: 1.9895 - regression_loss: 1.4691 - classification_loss: 0.5204\n",
      " 56/500 [==>...........................] - ETA: 7:23 - loss: 2.0001 - regression_loss: 1.4771 - classification_loss: 0.5230\n",
      " 57/500 [==>...........................] - ETA: 7:23 - loss: 1.9962 - regression_loss: 1.4757 - classification_loss: 0.5206\n",
      " 58/500 [==>...........................] - ETA: 7:20 - loss: 2.0027 - regression_loss: 1.4841 - classification_loss: 0.5186\n",
      " 59/500 [==>...........................] - ETA: 7:20 - loss: 1.9946 - regression_loss: 1.4787 - classification_loss: 0.5159\n",
      " 60/500 [==>...........................] - ETA: 7:20 - loss: 1.9893 - regression_loss: 1.4732 - classification_loss: 0.5161\n",
      " 61/500 [==>...........................] - ETA: 7:20 - loss: 1.9868 - regression_loss: 1.4717 - classification_loss: 0.5151\n",
      " 62/500 [==>...........................] - ETA: 7:17 - loss: 1.9857 - regression_loss: 1.4717 - classification_loss: 0.5140\n",
      " 63/500 [==>...........................] - ETA: 7:17 - loss: 1.9749 - regression_loss: 1.4635 - classification_loss: 0.5113\n",
      " 64/500 [==>...........................] - ETA: 7:17 - loss: 1.9909 - regression_loss: 1.4733 - classification_loss: 0.5176\n",
      " 65/500 [==>...........................] - ETA: 7:16 - loss: 1.9911 - regression_loss: 1.4718 - classification_loss: 0.5193\n",
      " 66/500 [==>...........................] - ETA: 7:15 - loss: 1.9931 - regression_loss: 1.4760 - classification_loss: 0.5171\n",
      " 67/500 [===>..........................] - ETA: 7:14 - loss: 1.9860 - regression_loss: 1.4705 - classification_loss: 0.5156\n",
      " 68/500 [===>..........................] - ETA: 7:14 - loss: 1.9890 - regression_loss: 1.4723 - classification_loss: 0.5167\n",
      " 69/500 [===>..........................] - ETA: 7:13 - loss: 1.9934 - regression_loss: 1.4762 - classification_loss: 0.5172\n",
      " 70/500 [===>..........................] - ETA: 7:12 - loss: 1.9922 - regression_loss: 1.4732 - classification_loss: 0.5190\n",
      " 71/500 [===>..........................] - ETA: 7:11 - loss: 1.9881 - regression_loss: 1.4699 - classification_loss: 0.5183\n",
      " 72/500 [===>..........................] - ETA: 7:10 - loss: 1.9844 - regression_loss: 1.4666 - classification_loss: 0.5178\n",
      " 73/500 [===>..........................] - ETA: 7:09 - loss: 1.9801 - regression_loss: 1.4622 - classification_loss: 0.5179\n",
      " 74/500 [===>..........................] - ETA: 7:08 - loss: 1.9764 - regression_loss: 1.4584 - classification_loss: 0.5180\n",
      " 75/500 [===>..........................] - ETA: 7:07 - loss: 1.9769 - regression_loss: 1.4610 - classification_loss: 0.5159\n",
      " 76/500 [===>..........................] - ETA: 7:06 - loss: 1.9760 - regression_loss: 1.4612 - classification_loss: 0.5148\n",
      " 77/500 [===>..........................] - ETA: 7:04 - loss: 1.9842 - regression_loss: 1.4679 - classification_loss: 0.5163\n",
      " 78/500 [===>..........................] - ETA: 7:04 - loss: 1.9813 - regression_loss: 1.4652 - classification_loss: 0.5161\n",
      " 79/500 [===>..........................] - ETA: 7:04 - loss: 1.9901 - regression_loss: 1.4725 - classification_loss: 0.5176\n",
      " 80/500 [===>..........................] - ETA: 7:02 - loss: 1.9934 - regression_loss: 1.4768 - classification_loss: 0.5166\n",
      " 81/500 [===>..........................] - ETA: 7:01 - loss: 1.9826 - regression_loss: 1.4686 - classification_loss: 0.5140\n",
      " 82/500 [===>..........................] - ETA: 7:00 - loss: 1.9811 - regression_loss: 1.4677 - classification_loss: 0.5134\n",
      " 83/500 [===>..........................] - ETA: 6:59 - loss: 1.9767 - regression_loss: 1.4647 - classification_loss: 0.5120\n",
      " 84/500 [====>.........................] - ETA: 6:58 - loss: 1.9693 - regression_loss: 1.4591 - classification_loss: 0.5101\n",
      " 85/500 [====>.........................] - ETA: 6:58 - loss: 1.9863 - regression_loss: 1.4702 - classification_loss: 0.5161\n",
      " 86/500 [====>.........................] - ETA: 6:57 - loss: 2.0003 - regression_loss: 1.4801 - classification_loss: 0.5201\n",
      " 87/500 [====>.........................] - ETA: 6:56 - loss: 2.0031 - regression_loss: 1.4827 - classification_loss: 0.5205\n",
      " 88/500 [====>.........................] - ETA: 6:55 - loss: 2.0061 - regression_loss: 1.4848 - classification_loss: 0.5213\n",
      " 89/500 [====>.........................] - ETA: 6:54 - loss: 2.0027 - regression_loss: 1.4818 - classification_loss: 0.5209\n",
      " 90/500 [====>.........................] - ETA: 6:54 - loss: 2.0104 - regression_loss: 1.4866 - classification_loss: 0.5238\n",
      " 91/500 [====>.........................] - ETA: 6:53 - loss: 2.0109 - regression_loss: 1.4878 - classification_loss: 0.5231\n",
      " 92/500 [====>.........................] - ETA: 6:53 - loss: 2.0065 - regression_loss: 1.4841 - classification_loss: 0.5224\n",
      " 93/500 [====>.........................] - ETA: 6:52 - loss: 2.0019 - regression_loss: 1.4819 - classification_loss: 0.5200\n",
      " 94/500 [====>.........................] - ETA: 6:51 - loss: 1.9999 - regression_loss: 1.4808 - classification_loss: 0.5191\n",
      " 95/500 [====>.........................] - ETA: 6:50 - loss: 2.0039 - regression_loss: 1.4816 - classification_loss: 0.5223\n",
      " 96/500 [====>.........................] - ETA: 6:49 - loss: 1.9992 - regression_loss: 1.4778 - classification_loss: 0.5213\n",
      " 97/500 [====>.........................] - ETA: 6:49 - loss: 1.9944 - regression_loss: 1.4745 - classification_loss: 0.5199\n",
      " 98/500 [====>.........................] - ETA: 6:48 - loss: 1.9945 - regression_loss: 1.4753 - classification_loss: 0.5192\n",
      " 99/500 [====>.........................] - ETA: 6:47 - loss: 1.9947 - regression_loss: 1.4739 - classification_loss: 0.5208\n",
      "100/500 [=====>........................] - ETA: 6:46 - loss: 2.0027 - regression_loss: 1.4776 - classification_loss: 0.5251\n",
      "101/500 [=====>........................] - ETA: 6:45 - loss: 2.0020 - regression_loss: 1.4781 - classification_loss: 0.5239\n",
      "102/500 [=====>........................] - ETA: 6:44 - loss: 1.9991 - regression_loss: 1.4759 - classification_loss: 0.5232\n",
      "103/500 [=====>........................] - ETA: 6:42 - loss: 1.9946 - regression_loss: 1.4729 - classification_loss: 0.5217\n",
      "104/500 [=====>........................] - ETA: 6:41 - loss: 1.9913 - regression_loss: 1.4698 - classification_loss: 0.5215\n",
      "105/500 [=====>........................] - ETA: 6:40 - loss: 1.9897 - regression_loss: 1.4684 - classification_loss: 0.5213\n",
      "106/500 [=====>........................] - ETA: 6:39 - loss: 1.9913 - regression_loss: 1.4695 - classification_loss: 0.5218\n",
      "107/500 [=====>........................] - ETA: 6:38 - loss: 1.9973 - regression_loss: 1.4742 - classification_loss: 0.5231\n",
      "108/500 [=====>........................] - ETA: 6:37 - loss: 1.9924 - regression_loss: 1.4716 - classification_loss: 0.5208\n",
      "109/500 [=====>........................] - ETA: 6:36 - loss: 1.9898 - regression_loss: 1.4699 - classification_loss: 0.5199\n",
      "110/500 [=====>........................] - ETA: 6:35 - loss: 1.9850 - regression_loss: 1.4670 - classification_loss: 0.5180\n",
      "111/500 [=====>........................] - ETA: 6:34 - loss: 1.9798 - regression_loss: 1.4627 - classification_loss: 0.5172\n",
      "112/500 [=====>........................] - ETA: 6:33 - loss: 1.9921 - regression_loss: 1.4708 - classification_loss: 0.5214\n",
      "113/500 [=====>........................] - ETA: 6:32 - loss: 1.9826 - regression_loss: 1.4640 - classification_loss: 0.5186\n",
      "114/500 [=====>........................] - ETA: 6:31 - loss: 1.9886 - regression_loss: 1.4674 - classification_loss: 0.5212\n",
      "115/500 [=====>........................] - ETA: 6:29 - loss: 1.9947 - regression_loss: 1.4692 - classification_loss: 0.5255\n",
      "116/500 [=====>........................] - ETA: 6:29 - loss: 1.9876 - regression_loss: 1.4641 - classification_loss: 0.5235\n",
      "117/500 [======>.......................] - ETA: 6:28 - loss: 1.9884 - regression_loss: 1.4645 - classification_loss: 0.5239\n",
      "118/500 [======>.......................] - ETA: 6:27 - loss: 1.9952 - regression_loss: 1.4702 - classification_loss: 0.5249\n",
      "119/500 [======>.......................] - ETA: 6:26 - loss: 1.9958 - regression_loss: 1.4709 - classification_loss: 0.5249\n",
      "120/500 [======>.......................] - ETA: 6:26 - loss: 1.9945 - regression_loss: 1.4687 - classification_loss: 0.5257\n",
      "121/500 [======>.......................] - ETA: 6:25 - loss: 1.9953 - regression_loss: 1.4685 - classification_loss: 0.5268\n",
      "122/500 [======>.......................] - ETA: 6:24 - loss: 2.0029 - regression_loss: 1.4744 - classification_loss: 0.5285\n",
      "123/500 [======>.......................] - ETA: 6:23 - loss: 2.0029 - regression_loss: 1.4754 - classification_loss: 0.5275\n",
      "124/500 [======>.......................] - ETA: 6:21 - loss: 2.0042 - regression_loss: 1.4754 - classification_loss: 0.5288\n",
      "125/500 [======>.......................] - ETA: 6:21 - loss: 2.0038 - regression_loss: 1.4738 - classification_loss: 0.5300\n",
      "126/500 [======>.......................] - ETA: 6:20 - loss: 2.0080 - regression_loss: 1.4762 - classification_loss: 0.5318\n",
      "127/500 [======>.......................] - ETA: 6:19 - loss: 2.0049 - regression_loss: 1.4729 - classification_loss: 0.5319\n",
      "128/500 [======>.......................] - ETA: 6:19 - loss: 2.0055 - regression_loss: 1.4742 - classification_loss: 0.5313\n",
      "129/500 [======>.......................] - ETA: 6:17 - loss: 2.0030 - regression_loss: 1.4728 - classification_loss: 0.5302\n",
      "130/500 [======>.......................] - ETA: 6:17 - loss: 2.0006 - regression_loss: 1.4716 - classification_loss: 0.5290\n",
      "131/500 [======>.......................] - ETA: 6:16 - loss: 2.0070 - regression_loss: 1.4766 - classification_loss: 0.5304\n",
      "132/500 [======>.......................] - ETA: 6:15 - loss: 2.0066 - regression_loss: 1.4755 - classification_loss: 0.5311\n",
      "133/500 [======>.......................] - ETA: 6:14 - loss: 2.0044 - regression_loss: 1.4740 - classification_loss: 0.5304\n",
      "134/500 [=======>......................] - ETA: 6:13 - loss: 2.0108 - regression_loss: 1.4777 - classification_loss: 0.5331\n",
      "135/500 [=======>......................] - ETA: 6:12 - loss: 2.0053 - regression_loss: 1.4730 - classification_loss: 0.5323\n",
      "136/500 [=======>......................] - ETA: 6:11 - loss: 2.0098 - regression_loss: 1.4760 - classification_loss: 0.5338\n",
      "137/500 [=======>......................] - ETA: 6:10 - loss: 2.0095 - regression_loss: 1.4763 - classification_loss: 0.5332\n",
      "138/500 [=======>......................] - ETA: 6:09 - loss: 2.0071 - regression_loss: 1.4753 - classification_loss: 0.5318\n",
      "139/500 [=======>......................] - ETA: 6:08 - loss: 2.0024 - regression_loss: 1.4722 - classification_loss: 0.5302\n",
      "140/500 [=======>......................] - ETA: 6:07 - loss: 2.0010 - regression_loss: 1.4714 - classification_loss: 0.5296\n",
      "141/500 [=======>......................] - ETA: 6:06 - loss: 1.9975 - regression_loss: 1.4693 - classification_loss: 0.5282\n",
      "142/500 [=======>......................] - ETA: 6:04 - loss: 1.9977 - regression_loss: 1.4703 - classification_loss: 0.5273\n",
      "143/500 [=======>......................] - ETA: 6:04 - loss: 2.0010 - regression_loss: 1.4734 - classification_loss: 0.5276\n",
      "144/500 [=======>......................] - ETA: 6:02 - loss: 1.9961 - regression_loss: 1.4698 - classification_loss: 0.5263\n",
      "145/500 [=======>......................] - ETA: 6:01 - loss: 1.9943 - regression_loss: 1.4696 - classification_loss: 0.5247\n",
      "146/500 [=======>......................] - ETA: 6:00 - loss: 1.9877 - regression_loss: 1.4651 - classification_loss: 0.5227\n",
      "147/500 [=======>......................] - ETA: 5:59 - loss: 1.9871 - regression_loss: 1.4645 - classification_loss: 0.5226\n",
      "148/500 [=======>......................] - ETA: 5:58 - loss: 1.9878 - regression_loss: 1.4644 - classification_loss: 0.5235\n",
      "149/500 [=======>......................] - ETA: 5:57 - loss: 1.9815 - regression_loss: 1.4598 - classification_loss: 0.5217\n",
      "150/500 [========>.....................] - ETA: 5:55 - loss: 1.9772 - regression_loss: 1.4565 - classification_loss: 0.5208\n",
      "151/500 [========>.....................] - ETA: 5:55 - loss: 1.9750 - regression_loss: 1.4544 - classification_loss: 0.5206\n",
      "152/500 [========>.....................] - ETA: 5:54 - loss: 1.9833 - regression_loss: 1.4597 - classification_loss: 0.5236\n",
      "153/500 [========>.....................] - ETA: 5:53 - loss: 1.9873 - regression_loss: 1.4630 - classification_loss: 0.5243\n",
      "154/500 [========>.....................] - ETA: 5:52 - loss: 1.9854 - regression_loss: 1.4619 - classification_loss: 0.5235\n",
      "155/500 [========>.....................] - ETA: 5:51 - loss: 1.9927 - regression_loss: 1.4666 - classification_loss: 0.5262\n",
      "156/500 [========>.....................] - ETA: 5:50 - loss: 1.9913 - regression_loss: 1.4659 - classification_loss: 0.5254\n",
      "157/500 [========>.....................] - ETA: 5:49 - loss: 1.9925 - regression_loss: 1.4682 - classification_loss: 0.5243\n",
      "158/500 [========>.....................] - ETA: 5:48 - loss: 1.9906 - regression_loss: 1.4668 - classification_loss: 0.5237\n",
      "159/500 [========>.....................] - ETA: 5:46 - loss: 1.9915 - regression_loss: 1.4676 - classification_loss: 0.5239\n",
      "160/500 [========>.....................] - ETA: 5:45 - loss: 1.9942 - regression_loss: 1.4690 - classification_loss: 0.5251\n",
      "161/500 [========>.....................] - ETA: 5:44 - loss: 1.9928 - regression_loss: 1.4679 - classification_loss: 0.5250\n",
      "162/500 [========>.....................] - ETA: 5:44 - loss: 1.9917 - regression_loss: 1.4671 - classification_loss: 0.5246\n",
      "163/500 [========>.....................] - ETA: 5:42 - loss: 1.9882 - regression_loss: 1.4649 - classification_loss: 0.5233\n",
      "164/500 [========>.....................] - ETA: 5:41 - loss: 1.9892 - regression_loss: 1.4657 - classification_loss: 0.5235\n",
      "165/500 [========>.....................] - ETA: 5:40 - loss: 1.9828 - regression_loss: 1.4608 - classification_loss: 0.5220\n",
      "166/500 [========>.....................] - ETA: 5:39 - loss: 1.9825 - regression_loss: 1.4608 - classification_loss: 0.5216\n",
      "167/500 [=========>....................] - ETA: 5:38 - loss: 1.9830 - regression_loss: 1.4621 - classification_loss: 0.5208\n",
      "168/500 [=========>....................] - ETA: 5:37 - loss: 1.9792 - regression_loss: 1.4599 - classification_loss: 0.5193\n",
      "169/500 [=========>....................] - ETA: 5:36 - loss: 1.9795 - regression_loss: 1.4609 - classification_loss: 0.5186\n",
      "170/500 [=========>....................] - ETA: 5:35 - loss: 1.9744 - regression_loss: 1.4575 - classification_loss: 0.5169\n",
      "171/500 [=========>....................] - ETA: 5:34 - loss: 1.9804 - regression_loss: 1.4625 - classification_loss: 0.5179\n",
      "172/500 [=========>....................] - ETA: 5:33 - loss: 1.9875 - regression_loss: 1.4669 - classification_loss: 0.5206\n",
      "173/500 [=========>....................] - ETA: 5:32 - loss: 1.9894 - regression_loss: 1.4678 - classification_loss: 0.5216\n",
      "174/500 [=========>....................] - ETA: 5:32 - loss: 1.9948 - regression_loss: 1.4720 - classification_loss: 0.5229\n",
      "175/500 [=========>....................] - ETA: 5:31 - loss: 1.9996 - regression_loss: 1.4761 - classification_loss: 0.5235\n",
      "176/500 [=========>....................] - ETA: 5:30 - loss: 1.9989 - regression_loss: 1.4754 - classification_loss: 0.5235\n",
      "177/500 [=========>....................] - ETA: 5:28 - loss: 2.0019 - regression_loss: 1.4765 - classification_loss: 0.5254\n",
      "178/500 [=========>....................] - ETA: 5:27 - loss: 2.0019 - regression_loss: 1.4770 - classification_loss: 0.5249\n",
      "179/500 [=========>....................] - ETA: 5:27 - loss: 2.0019 - regression_loss: 1.4765 - classification_loss: 0.5254\n",
      "180/500 [=========>....................] - ETA: 5:26 - loss: 1.9961 - regression_loss: 1.4718 - classification_loss: 0.5243\n",
      "181/500 [=========>....................] - ETA: 5:24 - loss: 1.9956 - regression_loss: 1.4713 - classification_loss: 0.5242\n",
      "182/500 [=========>....................] - ETA: 5:23 - loss: 1.9937 - regression_loss: 1.4700 - classification_loss: 0.5237\n",
      "183/500 [=========>....................] - ETA: 5:22 - loss: 1.9924 - regression_loss: 1.4691 - classification_loss: 0.5233\n",
      "184/500 [==========>...................] - ETA: 5:21 - loss: 1.9863 - regression_loss: 1.4643 - classification_loss: 0.5220\n",
      "185/500 [==========>...................] - ETA: 5:20 - loss: 1.9812 - regression_loss: 1.4603 - classification_loss: 0.5209\n",
      "186/500 [==========>...................] - ETA: 5:19 - loss: 1.9826 - regression_loss: 1.4617 - classification_loss: 0.5208\n",
      "187/500 [==========>...................] - ETA: 5:18 - loss: 1.9809 - regression_loss: 1.4603 - classification_loss: 0.5206\n",
      "188/500 [==========>...................] - ETA: 5:17 - loss: 1.9860 - regression_loss: 1.4647 - classification_loss: 0.5213\n",
      "189/500 [==========>...................] - ETA: 5:16 - loss: 1.9839 - regression_loss: 1.4632 - classification_loss: 0.5207\n",
      "190/500 [==========>...................] - ETA: 5:15 - loss: 1.9842 - regression_loss: 1.4629 - classification_loss: 0.5213\n",
      "191/500 [==========>...................] - ETA: 5:14 - loss: 1.9844 - regression_loss: 1.4633 - classification_loss: 0.5211\n",
      "192/500 [==========>...................] - ETA: 5:13 - loss: 1.9818 - regression_loss: 1.4612 - classification_loss: 0.5206\n",
      "193/500 [==========>...................] - ETA: 5:12 - loss: 1.9880 - regression_loss: 1.4658 - classification_loss: 0.5222\n",
      "194/500 [==========>...................] - ETA: 5:11 - loss: 1.9864 - regression_loss: 1.4652 - classification_loss: 0.5212\n",
      "195/500 [==========>...................] - ETA: 5:10 - loss: 1.9859 - regression_loss: 1.4652 - classification_loss: 0.5207\n",
      "196/500 [==========>...................] - ETA: 5:09 - loss: 1.9884 - regression_loss: 1.4679 - classification_loss: 0.5206\n",
      "197/500 [==========>...................] - ETA: 5:08 - loss: 1.9875 - regression_loss: 1.4666 - classification_loss: 0.5210\n",
      "198/500 [==========>...................] - ETA: 5:07 - loss: 1.9867 - regression_loss: 1.4656 - classification_loss: 0.5211\n",
      "199/500 [==========>...................] - ETA: 5:06 - loss: 1.9903 - regression_loss: 1.4672 - classification_loss: 0.5231\n",
      "200/500 [===========>..................] - ETA: 5:05 - loss: 1.9895 - regression_loss: 1.4669 - classification_loss: 0.5226\n",
      "201/500 [===========>..................] - ETA: 5:05 - loss: 1.9877 - regression_loss: 1.4651 - classification_loss: 0.5226\n",
      "202/500 [===========>..................] - ETA: 5:04 - loss: 1.9884 - regression_loss: 1.4645 - classification_loss: 0.5239\n",
      "203/500 [===========>..................] - ETA: 5:03 - loss: 1.9860 - regression_loss: 1.4629 - classification_loss: 0.5231\n",
      "204/500 [===========>..................] - ETA: 5:02 - loss: 1.9841 - regression_loss: 1.4614 - classification_loss: 0.5227\n",
      "205/500 [===========>..................] - ETA: 5:01 - loss: 1.9844 - regression_loss: 1.4613 - classification_loss: 0.5232\n",
      "206/500 [===========>..................] - ETA: 4:59 - loss: 1.9820 - regression_loss: 1.4594 - classification_loss: 0.5226\n",
      "207/500 [===========>..................] - ETA: 4:59 - loss: 1.9795 - regression_loss: 1.4583 - classification_loss: 0.5213\n",
      "208/500 [===========>..................] - ETA: 4:58 - loss: 1.9771 - regression_loss: 1.4563 - classification_loss: 0.5208\n",
      "209/500 [===========>..................] - ETA: 4:57 - loss: 1.9769 - regression_loss: 1.4555 - classification_loss: 0.5214\n",
      "210/500 [===========>..................] - ETA: 4:56 - loss: 1.9790 - regression_loss: 1.4571 - classification_loss: 0.5220\n",
      "211/500 [===========>..................] - ETA: 4:55 - loss: 1.9820 - regression_loss: 1.4587 - classification_loss: 0.5233\n",
      "212/500 [===========>..................] - ETA: 4:54 - loss: 1.9851 - regression_loss: 1.4605 - classification_loss: 0.5247\n",
      "213/500 [===========>..................] - ETA: 4:53 - loss: 1.9845 - regression_loss: 1.4602 - classification_loss: 0.5243\n",
      "214/500 [===========>..................] - ETA: 4:52 - loss: 1.9819 - regression_loss: 1.4585 - classification_loss: 0.5235\n",
      "215/500 [===========>..................] - ETA: 4:51 - loss: 1.9838 - regression_loss: 1.4601 - classification_loss: 0.5238\n",
      "216/500 [===========>..................] - ETA: 4:50 - loss: 1.9830 - regression_loss: 1.4597 - classification_loss: 0.5233\n",
      "217/500 [============>.................] - ETA: 4:49 - loss: 1.9861 - regression_loss: 1.4619 - classification_loss: 0.5242\n",
      "218/500 [============>.................] - ETA: 4:48 - loss: 1.9811 - regression_loss: 1.4579 - classification_loss: 0.5232\n",
      "219/500 [============>.................] - ETA: 4:47 - loss: 1.9813 - regression_loss: 1.4582 - classification_loss: 0.5231\n",
      "220/500 [============>.................] - ETA: 4:46 - loss: 1.9807 - regression_loss: 1.4579 - classification_loss: 0.5228\n",
      "221/500 [============>.................] - ETA: 4:45 - loss: 1.9799 - regression_loss: 1.4574 - classification_loss: 0.5224\n",
      "222/500 [============>.................] - ETA: 4:44 - loss: 1.9825 - regression_loss: 1.4603 - classification_loss: 0.5223\n",
      "223/500 [============>.................] - ETA: 4:43 - loss: 1.9801 - regression_loss: 1.4584 - classification_loss: 0.5217\n",
      "224/500 [============>.................] - ETA: 4:42 - loss: 1.9806 - regression_loss: 1.4594 - classification_loss: 0.5212\n",
      "225/500 [============>.................] - ETA: 4:41 - loss: 1.9784 - regression_loss: 1.4577 - classification_loss: 0.5207\n",
      "226/500 [============>.................] - ETA: 4:40 - loss: 1.9790 - regression_loss: 1.4582 - classification_loss: 0.5208\n",
      "227/500 [============>.................] - ETA: 4:39 - loss: 1.9774 - regression_loss: 1.4573 - classification_loss: 0.5202\n",
      "228/500 [============>.................] - ETA: 4:38 - loss: 1.9766 - regression_loss: 1.4568 - classification_loss: 0.5198\n",
      "229/500 [============>.................] - ETA: 4:37 - loss: 1.9795 - regression_loss: 1.4593 - classification_loss: 0.5203\n",
      "230/500 [============>.................] - ETA: 4:35 - loss: 1.9825 - regression_loss: 1.4616 - classification_loss: 0.5209\n",
      "231/500 [============>.................] - ETA: 4:34 - loss: 1.9835 - regression_loss: 1.4624 - classification_loss: 0.5211\n",
      "232/500 [============>.................] - ETA: 4:33 - loss: 1.9835 - regression_loss: 1.4628 - classification_loss: 0.5207\n",
      "233/500 [============>.................] - ETA: 4:32 - loss: 1.9821 - regression_loss: 1.4618 - classification_loss: 0.5203\n",
      "234/500 [=============>................] - ETA: 4:31 - loss: 1.9818 - regression_loss: 1.4610 - classification_loss: 0.5209\n",
      "235/500 [=============>................] - ETA: 4:31 - loss: 1.9806 - regression_loss: 1.4590 - classification_loss: 0.5216\n",
      "236/500 [=============>................] - ETA: 4:30 - loss: 1.9786 - regression_loss: 1.4579 - classification_loss: 0.5207\n",
      "237/500 [=============>................] - ETA: 4:29 - loss: 1.9788 - regression_loss: 1.4581 - classification_loss: 0.5207\n",
      "238/500 [=============>................] - ETA: 4:27 - loss: 1.9770 - regression_loss: 1.4565 - classification_loss: 0.5205\n",
      "239/500 [=============>................] - ETA: 4:26 - loss: 1.9788 - regression_loss: 1.4581 - classification_loss: 0.5206\n",
      "240/500 [=============>................] - ETA: 4:25 - loss: 1.9802 - regression_loss: 1.4589 - classification_loss: 0.5213\n",
      "241/500 [=============>................] - ETA: 4:24 - loss: 1.9817 - regression_loss: 1.4593 - classification_loss: 0.5224\n",
      "242/500 [=============>................] - ETA: 4:23 - loss: 1.9781 - regression_loss: 1.4564 - classification_loss: 0.5217\n",
      "243/500 [=============>................] - ETA: 4:22 - loss: 1.9738 - regression_loss: 1.4531 - classification_loss: 0.5207\n",
      "244/500 [=============>................] - ETA: 4:21 - loss: 1.9745 - regression_loss: 1.4536 - classification_loss: 0.5209\n",
      "245/500 [=============>................] - ETA: 4:20 - loss: 1.9756 - regression_loss: 1.4542 - classification_loss: 0.5214\n",
      "246/500 [=============>................] - ETA: 4:19 - loss: 1.9741 - regression_loss: 1.4536 - classification_loss: 0.5205\n",
      "247/500 [=============>................] - ETA: 4:18 - loss: 1.9773 - regression_loss: 1.4564 - classification_loss: 0.5209\n",
      "248/500 [=============>................] - ETA: 4:17 - loss: 1.9784 - regression_loss: 1.4570 - classification_loss: 0.5214\n",
      "249/500 [=============>................] - ETA: 4:16 - loss: 1.9746 - regression_loss: 1.4542 - classification_loss: 0.5204\n",
      "250/500 [==============>...............] - ETA: 4:15 - loss: 1.9735 - regression_loss: 1.4536 - classification_loss: 0.5199\n",
      "251/500 [==============>...............] - ETA: 4:14 - loss: 1.9738 - regression_loss: 1.4539 - classification_loss: 0.5199\n",
      "252/500 [==============>...............] - ETA: 4:13 - loss: 1.9727 - regression_loss: 1.4531 - classification_loss: 0.5196\n",
      "253/500 [==============>...............] - ETA: 4:12 - loss: 1.9704 - regression_loss: 1.4515 - classification_loss: 0.5188\n",
      "254/500 [==============>...............] - ETA: 4:11 - loss: 1.9720 - regression_loss: 1.4536 - classification_loss: 0.5184\n",
      "255/500 [==============>...............] - ETA: 4:10 - loss: 1.9736 - regression_loss: 1.4551 - classification_loss: 0.5185\n",
      "256/500 [==============>...............] - ETA: 4:09 - loss: 1.9713 - regression_loss: 1.4534 - classification_loss: 0.5179\n",
      "257/500 [==============>...............] - ETA: 4:08 - loss: 1.9691 - regression_loss: 1.4516 - classification_loss: 0.5175\n",
      "258/500 [==============>...............] - ETA: 4:07 - loss: 1.9683 - regression_loss: 1.4510 - classification_loss: 0.5173\n",
      "259/500 [==============>...............] - ETA: 4:06 - loss: 1.9682 - regression_loss: 1.4511 - classification_loss: 0.5171\n",
      "260/500 [==============>...............] - ETA: 4:05 - loss: 1.9691 - regression_loss: 1.4521 - classification_loss: 0.5170\n",
      "261/500 [==============>...............] - ETA: 4:04 - loss: 1.9675 - regression_loss: 1.4512 - classification_loss: 0.5163\n",
      "262/500 [==============>...............] - ETA: 4:03 - loss: 1.9688 - regression_loss: 1.4521 - classification_loss: 0.5166\n",
      "263/500 [==============>...............] - ETA: 4:02 - loss: 1.9693 - regression_loss: 1.4527 - classification_loss: 0.5166\n",
      "264/500 [==============>...............] - ETA: 4:01 - loss: 1.9697 - regression_loss: 1.4530 - classification_loss: 0.5168\n",
      "265/500 [==============>...............] - ETA: 3:59 - loss: 1.9720 - regression_loss: 1.4541 - classification_loss: 0.5179\n",
      "266/500 [==============>...............] - ETA: 3:59 - loss: 1.9710 - regression_loss: 1.4535 - classification_loss: 0.5175\n",
      "267/500 [===============>..............] - ETA: 3:57 - loss: 1.9729 - regression_loss: 1.4554 - classification_loss: 0.5175\n",
      "268/500 [===============>..............] - ETA: 3:56 - loss: 1.9702 - regression_loss: 1.4532 - classification_loss: 0.5170\n",
      "269/500 [===============>..............] - ETA: 3:55 - loss: 1.9703 - regression_loss: 1.4528 - classification_loss: 0.5174\n",
      "270/500 [===============>..............] - ETA: 3:54 - loss: 1.9676 - regression_loss: 1.4511 - classification_loss: 0.5165\n",
      "271/500 [===============>..............] - ETA: 3:53 - loss: 1.9654 - regression_loss: 1.4497 - classification_loss: 0.5158\n",
      "272/500 [===============>..............] - ETA: 3:52 - loss: 1.9686 - regression_loss: 1.4524 - classification_loss: 0.5162\n",
      "273/500 [===============>..............] - ETA: 3:51 - loss: 1.9679 - regression_loss: 1.4523 - classification_loss: 0.5156\n",
      "274/500 [===============>..............] - ETA: 3:50 - loss: 1.9697 - regression_loss: 1.4540 - classification_loss: 0.5157\n",
      "275/500 [===============>..............] - ETA: 3:49 - loss: 1.9717 - regression_loss: 1.4550 - classification_loss: 0.5166\n",
      "276/500 [===============>..............] - ETA: 3:48 - loss: 1.9704 - regression_loss: 1.4543 - classification_loss: 0.5161\n",
      "277/500 [===============>..............] - ETA: 3:47 - loss: 1.9690 - regression_loss: 1.4535 - classification_loss: 0.5155\n",
      "278/500 [===============>..............] - ETA: 3:46 - loss: 1.9722 - regression_loss: 1.4556 - classification_loss: 0.5165\n",
      "279/500 [===============>..............] - ETA: 3:45 - loss: 1.9735 - regression_loss: 1.4567 - classification_loss: 0.5168\n",
      "280/500 [===============>..............] - ETA: 3:44 - loss: 1.9735 - regression_loss: 1.4570 - classification_loss: 0.5166\n",
      "281/500 [===============>..............] - ETA: 3:43 - loss: 1.9745 - regression_loss: 1.4578 - classification_loss: 0.5167\n",
      "282/500 [===============>..............] - ETA: 3:42 - loss: 1.9720 - regression_loss: 1.4556 - classification_loss: 0.5165\n",
      "283/500 [===============>..............] - ETA: 3:41 - loss: 1.9747 - regression_loss: 1.4581 - classification_loss: 0.5166\n",
      "284/500 [================>.............] - ETA: 3:40 - loss: 1.9727 - regression_loss: 1.4571 - classification_loss: 0.5156\n",
      "285/500 [================>.............] - ETA: 3:39 - loss: 1.9716 - regression_loss: 1.4564 - classification_loss: 0.5152\n",
      "286/500 [================>.............] - ETA: 3:38 - loss: 1.9720 - regression_loss: 1.4567 - classification_loss: 0.5153\n",
      "287/500 [================>.............] - ETA: 3:37 - loss: 1.9716 - regression_loss: 1.4569 - classification_loss: 0.5147\n",
      "288/500 [================>.............] - ETA: 3:36 - loss: 1.9738 - regression_loss: 1.4592 - classification_loss: 0.5146\n",
      "289/500 [================>.............] - ETA: 3:35 - loss: 1.9722 - regression_loss: 1.4581 - classification_loss: 0.5141\n",
      "290/500 [================>.............] - ETA: 3:34 - loss: 1.9757 - regression_loss: 1.4613 - classification_loss: 0.5145\n",
      "291/500 [================>.............] - ETA: 3:33 - loss: 1.9752 - regression_loss: 1.4612 - classification_loss: 0.5140\n",
      "292/500 [================>.............] - ETA: 3:32 - loss: 1.9736 - regression_loss: 1.4601 - classification_loss: 0.5136\n",
      "293/500 [================>.............] - ETA: 3:31 - loss: 1.9731 - regression_loss: 1.4600 - classification_loss: 0.5131\n",
      "294/500 [================>.............] - ETA: 3:30 - loss: 1.9714 - regression_loss: 1.4591 - classification_loss: 0.5124\n",
      "295/500 [================>.............] - ETA: 3:29 - loss: 1.9718 - regression_loss: 1.4589 - classification_loss: 0.5129\n",
      "296/500 [================>.............] - ETA: 3:28 - loss: 1.9702 - regression_loss: 1.4583 - classification_loss: 0.5120\n",
      "297/500 [================>.............] - ETA: 3:27 - loss: 1.9742 - regression_loss: 1.4615 - classification_loss: 0.5127\n",
      "298/500 [================>.............] - ETA: 3:26 - loss: 1.9746 - regression_loss: 1.4613 - classification_loss: 0.5133\n",
      "299/500 [================>.............] - ETA: 3:25 - loss: 1.9755 - regression_loss: 1.4622 - classification_loss: 0.5133\n",
      "300/500 [=================>............] - ETA: 3:24 - loss: 1.9754 - regression_loss: 1.4620 - classification_loss: 0.5134\n",
      "301/500 [=================>............] - ETA: 3:23 - loss: 1.9762 - regression_loss: 1.4626 - classification_loss: 0.5136\n",
      "302/500 [=================>............] - ETA: 3:22 - loss: 1.9764 - regression_loss: 1.4626 - classification_loss: 0.5138\n",
      "303/500 [=================>............] - ETA: 3:21 - loss: 1.9773 - regression_loss: 1.4631 - classification_loss: 0.5142\n",
      "304/500 [=================>............] - ETA: 3:20 - loss: 1.9761 - regression_loss: 1.4626 - classification_loss: 0.5135\n",
      "305/500 [=================>............] - ETA: 3:19 - loss: 1.9781 - regression_loss: 1.4638 - classification_loss: 0.5143\n",
      "306/500 [=================>............] - ETA: 3:18 - loss: 1.9798 - regression_loss: 1.4650 - classification_loss: 0.5148\n",
      "307/500 [=================>............] - ETA: 3:17 - loss: 1.9769 - regression_loss: 1.4629 - classification_loss: 0.5139\n",
      "308/500 [=================>............] - ETA: 3:16 - loss: 1.9791 - regression_loss: 1.4641 - classification_loss: 0.5151\n",
      "309/500 [=================>............] - ETA: 3:15 - loss: 1.9806 - regression_loss: 1.4657 - classification_loss: 0.5149\n",
      "310/500 [=================>............] - ETA: 3:14 - loss: 1.9827 - regression_loss: 1.4669 - classification_loss: 0.5158\n",
      "311/500 [=================>............] - ETA: 3:13 - loss: 1.9834 - regression_loss: 1.4674 - classification_loss: 0.5160\n",
      "312/500 [=================>............] - ETA: 3:12 - loss: 1.9846 - regression_loss: 1.4681 - classification_loss: 0.5165\n",
      "313/500 [=================>............] - ETA: 3:11 - loss: 1.9846 - regression_loss: 1.4676 - classification_loss: 0.5170\n",
      "314/500 [=================>............] - ETA: 3:10 - loss: 1.9842 - regression_loss: 1.4674 - classification_loss: 0.5168\n",
      "315/500 [=================>............] - ETA: 3:09 - loss: 1.9851 - regression_loss: 1.4677 - classification_loss: 0.5174\n",
      "316/500 [=================>............] - ETA: 3:08 - loss: 1.9830 - regression_loss: 1.4661 - classification_loss: 0.5169\n",
      "317/500 [==================>...........] - ETA: 3:07 - loss: 1.9842 - regression_loss: 1.4669 - classification_loss: 0.5173\n",
      "318/500 [==================>...........] - ETA: 3:06 - loss: 1.9849 - regression_loss: 1.4666 - classification_loss: 0.5183\n",
      "319/500 [==================>...........] - ETA: 3:05 - loss: 1.9851 - regression_loss: 1.4665 - classification_loss: 0.5187\n",
      "320/500 [==================>...........] - ETA: 3:04 - loss: 1.9836 - regression_loss: 1.4656 - classification_loss: 0.5179\n",
      "321/500 [==================>...........] - ETA: 3:03 - loss: 1.9836 - regression_loss: 1.4658 - classification_loss: 0.5178\n",
      "322/500 [==================>...........] - ETA: 3:02 - loss: 1.9814 - regression_loss: 1.4646 - classification_loss: 0.5168\n",
      "323/500 [==================>...........] - ETA: 3:01 - loss: 1.9837 - regression_loss: 1.4657 - classification_loss: 0.5180\n",
      "324/500 [==================>...........] - ETA: 3:00 - loss: 1.9836 - regression_loss: 1.4659 - classification_loss: 0.5177\n",
      "325/500 [==================>...........] - ETA: 2:58 - loss: 1.9823 - regression_loss: 1.4651 - classification_loss: 0.5173\n",
      "326/500 [==================>...........] - ETA: 2:57 - loss: 1.9831 - regression_loss: 1.4656 - classification_loss: 0.5175\n",
      "327/500 [==================>...........] - ETA: 2:56 - loss: 1.9847 - regression_loss: 1.4666 - classification_loss: 0.5181\n",
      "328/500 [==================>...........] - ETA: 2:55 - loss: 1.9856 - regression_loss: 1.4670 - classification_loss: 0.5186\n",
      "329/500 [==================>...........] - ETA: 2:54 - loss: 1.9844 - regression_loss: 1.4660 - classification_loss: 0.5183\n",
      "330/500 [==================>...........] - ETA: 2:53 - loss: 1.9840 - regression_loss: 1.4663 - classification_loss: 0.5177\n",
      "331/500 [==================>...........] - ETA: 2:52 - loss: 1.9843 - regression_loss: 1.4663 - classification_loss: 0.5179\n",
      "332/500 [==================>...........] - ETA: 2:51 - loss: 1.9828 - regression_loss: 1.4653 - classification_loss: 0.5175\n",
      "333/500 [==================>...........] - ETA: 2:50 - loss: 1.9804 - regression_loss: 1.4634 - classification_loss: 0.5169\n",
      "334/500 [===================>..........] - ETA: 2:49 - loss: 1.9798 - regression_loss: 1.4633 - classification_loss: 0.5165\n",
      "335/500 [===================>..........] - ETA: 2:48 - loss: 1.9784 - regression_loss: 1.4625 - classification_loss: 0.5158\n",
      "336/500 [===================>..........] - ETA: 2:47 - loss: 1.9775 - regression_loss: 1.4620 - classification_loss: 0.5155\n",
      "337/500 [===================>..........] - ETA: 2:46 - loss: 1.9769 - regression_loss: 1.4614 - classification_loss: 0.5155\n",
      "338/500 [===================>..........] - ETA: 2:45 - loss: 1.9747 - regression_loss: 1.4598 - classification_loss: 0.5149\n",
      "339/500 [===================>..........] - ETA: 2:44 - loss: 1.9767 - regression_loss: 1.4612 - classification_loss: 0.5155\n",
      "340/500 [===================>..........] - ETA: 2:43 - loss: 1.9770 - regression_loss: 1.4615 - classification_loss: 0.5155\n",
      "341/500 [===================>..........] - ETA: 2:42 - loss: 1.9766 - regression_loss: 1.4610 - classification_loss: 0.5156\n",
      "342/500 [===================>..........] - ETA: 2:41 - loss: 1.9767 - regression_loss: 1.4607 - classification_loss: 0.5160\n",
      "343/500 [===================>..........] - ETA: 2:40 - loss: 1.9758 - regression_loss: 1.4596 - classification_loss: 0.5162\n",
      "344/500 [===================>..........] - ETA: 2:39 - loss: 1.9787 - regression_loss: 1.4617 - classification_loss: 0.5170\n",
      "345/500 [===================>..........] - ETA: 2:38 - loss: 1.9776 - regression_loss: 1.4612 - classification_loss: 0.5164\n",
      "346/500 [===================>..........] - ETA: 2:37 - loss: 1.9751 - regression_loss: 1.4593 - classification_loss: 0.5158\n",
      "347/500 [===================>..........] - ETA: 2:36 - loss: 1.9756 - regression_loss: 1.4596 - classification_loss: 0.5160\n",
      "348/500 [===================>..........] - ETA: 2:35 - loss: 1.9768 - regression_loss: 1.4607 - classification_loss: 0.5161\n",
      "349/500 [===================>..........] - ETA: 2:34 - loss: 1.9757 - regression_loss: 1.4602 - classification_loss: 0.5155\n",
      "350/500 [====================>.........] - ETA: 2:33 - loss: 1.9741 - regression_loss: 1.4590 - classification_loss: 0.5150\n",
      "351/500 [====================>.........] - ETA: 2:32 - loss: 1.9725 - regression_loss: 1.4578 - classification_loss: 0.5148\n",
      "352/500 [====================>.........] - ETA: 2:31 - loss: 1.9727 - regression_loss: 1.4581 - classification_loss: 0.5146\n",
      "353/500 [====================>.........] - ETA: 2:30 - loss: 1.9726 - regression_loss: 1.4581 - classification_loss: 0.5144\n",
      "354/500 [====================>.........] - ETA: 2:29 - loss: 1.9727 - regression_loss: 1.4582 - classification_loss: 0.5145\n",
      "355/500 [====================>.........] - ETA: 2:28 - loss: 1.9732 - regression_loss: 1.4586 - classification_loss: 0.5145\n",
      "356/500 [====================>.........] - ETA: 2:27 - loss: 1.9723 - regression_loss: 1.4578 - classification_loss: 0.5145\n",
      "357/500 [====================>.........] - ETA: 2:26 - loss: 1.9724 - regression_loss: 1.4578 - classification_loss: 0.5147\n",
      "358/500 [====================>.........] - ETA: 2:25 - loss: 1.9715 - regression_loss: 1.4569 - classification_loss: 0.5146\n",
      "359/500 [====================>.........] - ETA: 2:24 - loss: 1.9715 - regression_loss: 1.4564 - classification_loss: 0.5151\n",
      "360/500 [====================>.........] - ETA: 2:23 - loss: 1.9742 - regression_loss: 1.4587 - classification_loss: 0.5155\n",
      "361/500 [====================>.........] - ETA: 2:22 - loss: 1.9723 - regression_loss: 1.4572 - classification_loss: 0.5150\n",
      "362/500 [====================>.........] - ETA: 2:21 - loss: 1.9724 - regression_loss: 1.4574 - classification_loss: 0.5149\n",
      "363/500 [====================>.........] - ETA: 2:20 - loss: 1.9711 - regression_loss: 1.4566 - classification_loss: 0.5144\n",
      "364/500 [====================>.........] - ETA: 2:19 - loss: 1.9710 - regression_loss: 1.4571 - classification_loss: 0.5139\n",
      "365/500 [====================>.........] - ETA: 2:18 - loss: 1.9707 - regression_loss: 1.4572 - classification_loss: 0.5135\n",
      "366/500 [====================>.........] - ETA: 2:17 - loss: 1.9728 - regression_loss: 1.4588 - classification_loss: 0.5140\n",
      "367/500 [=====================>........] - ETA: 2:16 - loss: 1.9723 - regression_loss: 1.4588 - classification_loss: 0.5135\n",
      "368/500 [=====================>........] - ETA: 2:14 - loss: 1.9708 - regression_loss: 1.4579 - classification_loss: 0.5129\n",
      "369/500 [=====================>........] - ETA: 2:13 - loss: 1.9706 - regression_loss: 1.4579 - classification_loss: 0.5127\n",
      "370/500 [=====================>........] - ETA: 2:12 - loss: 1.9738 - regression_loss: 1.4599 - classification_loss: 0.5140\n",
      "371/500 [=====================>........] - ETA: 2:11 - loss: 1.9743 - regression_loss: 1.4604 - classification_loss: 0.5139\n",
      "372/500 [=====================>........] - ETA: 2:10 - loss: 1.9730 - regression_loss: 1.4590 - classification_loss: 0.5140\n",
      "373/500 [=====================>........] - ETA: 2:09 - loss: 1.9757 - regression_loss: 1.4612 - classification_loss: 0.5145\n",
      "374/500 [=====================>........] - ETA: 2:08 - loss: 1.9767 - regression_loss: 1.4614 - classification_loss: 0.5154\n",
      "375/500 [=====================>........] - ETA: 2:07 - loss: 1.9778 - regression_loss: 1.4626 - classification_loss: 0.5152\n",
      "376/500 [=====================>........] - ETA: 2:06 - loss: 1.9771 - regression_loss: 1.4619 - classification_loss: 0.5152\n",
      "377/500 [=====================>........] - ETA: 2:05 - loss: 1.9774 - regression_loss: 1.4623 - classification_loss: 0.5152\n",
      "378/500 [=====================>........] - ETA: 2:04 - loss: 1.9772 - regression_loss: 1.4623 - classification_loss: 0.5149\n",
      "379/500 [=====================>........] - ETA: 2:03 - loss: 1.9751 - regression_loss: 1.4608 - classification_loss: 0.5144\n",
      "380/500 [=====================>........] - ETA: 2:02 - loss: 1.9749 - regression_loss: 1.4606 - classification_loss: 0.5143\n",
      "381/500 [=====================>........] - ETA: 2:01 - loss: 1.9761 - regression_loss: 1.4617 - classification_loss: 0.5144\n",
      "382/500 [=====================>........] - ETA: 2:00 - loss: 1.9762 - regression_loss: 1.4617 - classification_loss: 0.5145\n",
      "383/500 [=====================>........] - ETA: 1:59 - loss: 1.9758 - regression_loss: 1.4615 - classification_loss: 0.5143\n",
      "384/500 [======================>.......] - ETA: 1:58 - loss: 1.9756 - regression_loss: 1.4614 - classification_loss: 0.5142\n",
      "385/500 [======================>.......] - ETA: 1:57 - loss: 1.9744 - regression_loss: 1.4608 - classification_loss: 0.5137\n",
      "386/500 [======================>.......] - ETA: 1:56 - loss: 1.9747 - regression_loss: 1.4613 - classification_loss: 0.5134\n",
      "387/500 [======================>.......] - ETA: 1:55 - loss: 1.9731 - regression_loss: 1.4602 - classification_loss: 0.5129\n",
      "388/500 [======================>.......] - ETA: 1:54 - loss: 1.9728 - regression_loss: 1.4600 - classification_loss: 0.5128\n",
      "389/500 [======================>.......] - ETA: 1:53 - loss: 1.9733 - regression_loss: 1.4606 - classification_loss: 0.5127\n",
      "390/500 [======================>.......] - ETA: 1:52 - loss: 1.9730 - regression_loss: 1.4601 - classification_loss: 0.5129\n",
      "391/500 [======================>.......] - ETA: 1:51 - loss: 1.9712 - regression_loss: 1.4591 - classification_loss: 0.5121\n",
      "392/500 [======================>.......] - ETA: 1:50 - loss: 1.9713 - regression_loss: 1.4591 - classification_loss: 0.5122\n",
      "393/500 [======================>.......] - ETA: 1:49 - loss: 1.9707 - regression_loss: 1.4587 - classification_loss: 0.5120\n",
      "394/500 [======================>.......] - ETA: 1:48 - loss: 1.9684 - regression_loss: 1.4573 - classification_loss: 0.5111\n",
      "395/500 [======================>.......] - ETA: 1:47 - loss: 1.9688 - regression_loss: 1.4570 - classification_loss: 0.5118\n",
      "396/500 [======================>.......] - ETA: 1:46 - loss: 1.9693 - regression_loss: 1.4570 - classification_loss: 0.5123\n",
      "397/500 [======================>.......] - ETA: 1:45 - loss: 1.9671 - regression_loss: 1.4555 - classification_loss: 0.5116\n",
      "398/500 [======================>.......] - ETA: 1:44 - loss: 1.9644 - regression_loss: 1.4536 - classification_loss: 0.5108\n",
      "399/500 [======================>.......] - ETA: 1:43 - loss: 1.9664 - regression_loss: 1.4551 - classification_loss: 0.5113\n",
      "400/500 [=======================>......] - ETA: 1:42 - loss: 1.9687 - regression_loss: 1.4566 - classification_loss: 0.5121\n",
      "401/500 [=======================>......] - ETA: 1:41 - loss: 1.9680 - regression_loss: 1.4563 - classification_loss: 0.5117\n",
      "402/500 [=======================>......] - ETA: 1:40 - loss: 1.9677 - regression_loss: 1.4562 - classification_loss: 0.5115\n",
      "403/500 [=======================>......] - ETA: 1:39 - loss: 1.9670 - regression_loss: 1.4554 - classification_loss: 0.5116\n",
      "404/500 [=======================>......] - ETA: 1:38 - loss: 1.9672 - regression_loss: 1.4556 - classification_loss: 0.5116\n",
      "405/500 [=======================>......] - ETA: 1:37 - loss: 1.9683 - regression_loss: 1.4564 - classification_loss: 0.5119\n",
      "406/500 [=======================>......] - ETA: 1:36 - loss: 1.9679 - regression_loss: 1.4562 - classification_loss: 0.5117\n",
      "407/500 [=======================>......] - ETA: 1:35 - loss: 1.9686 - regression_loss: 1.4568 - classification_loss: 0.5118\n",
      "408/500 [=======================>......] - ETA: 1:34 - loss: 1.9677 - regression_loss: 1.4561 - classification_loss: 0.5116\n",
      "409/500 [=======================>......] - ETA: 1:33 - loss: 1.9668 - regression_loss: 1.4555 - classification_loss: 0.5113\n",
      "410/500 [=======================>......] - ETA: 1:32 - loss: 1.9652 - regression_loss: 1.4546 - classification_loss: 0.5107\n",
      "411/500 [=======================>......] - ETA: 1:31 - loss: 1.9648 - regression_loss: 1.4542 - classification_loss: 0.5106\n",
      "412/500 [=======================>......] - ETA: 1:29 - loss: 1.9643 - regression_loss: 1.4540 - classification_loss: 0.5103\n",
      "413/500 [=======================>......] - ETA: 1:28 - loss: 1.9632 - regression_loss: 1.4533 - classification_loss: 0.5099\n",
      "414/500 [=======================>......] - ETA: 1:27 - loss: 1.9644 - regression_loss: 1.4539 - classification_loss: 0.5104\n",
      "415/500 [=======================>......] - ETA: 1:26 - loss: 1.9639 - regression_loss: 1.4533 - classification_loss: 0.5105\n",
      "416/500 [=======================>......] - ETA: 1:25 - loss: 1.9625 - regression_loss: 1.4523 - classification_loss: 0.5102\n",
      "417/500 [========================>.....] - ETA: 1:24 - loss: 1.9629 - regression_loss: 1.4527 - classification_loss: 0.5102\n",
      "418/500 [========================>.....] - ETA: 1:23 - loss: 1.9651 - regression_loss: 1.4543 - classification_loss: 0.5108\n",
      "419/500 [========================>.....] - ETA: 1:22 - loss: 1.9644 - regression_loss: 1.4540 - classification_loss: 0.5103\n",
      "420/500 [========================>.....] - ETA: 1:21 - loss: 1.9647 - regression_loss: 1.4544 - classification_loss: 0.5103\n",
      "421/500 [========================>.....] - ETA: 1:20 - loss: 1.9670 - regression_loss: 1.4559 - classification_loss: 0.5112\n",
      "422/500 [========================>.....] - ETA: 1:19 - loss: 1.9675 - regression_loss: 1.4558 - classification_loss: 0.5117\n",
      "423/500 [========================>.....] - ETA: 1:18 - loss: 1.9698 - regression_loss: 1.4572 - classification_loss: 0.5126\n",
      "424/500 [========================>.....] - ETA: 1:17 - loss: 1.9691 - regression_loss: 1.4567 - classification_loss: 0.5125\n",
      "425/500 [========================>.....] - ETA: 1:16 - loss: 1.9695 - regression_loss: 1.4571 - classification_loss: 0.5124\n",
      "426/500 [========================>.....] - ETA: 1:15 - loss: 1.9688 - regression_loss: 1.4566 - classification_loss: 0.5123\n",
      "427/500 [========================>.....] - ETA: 1:14 - loss: 1.9710 - regression_loss: 1.4586 - classification_loss: 0.5124\n",
      "428/500 [========================>.....] - ETA: 1:13 - loss: 1.9710 - regression_loss: 1.4586 - classification_loss: 0.5123\n",
      "429/500 [========================>.....] - ETA: 1:12 - loss: 1.9719 - regression_loss: 1.4598 - classification_loss: 0.5121\n",
      "430/500 [========================>.....] - ETA: 1:11 - loss: 1.9716 - regression_loss: 1.4593 - classification_loss: 0.5123\n",
      "431/500 [========================>.....] - ETA: 1:10 - loss: 1.9730 - regression_loss: 1.4604 - classification_loss: 0.5126\n",
      "432/500 [========================>.....] - ETA: 1:09 - loss: 1.9752 - regression_loss: 1.4616 - classification_loss: 0.5136\n",
      "433/500 [========================>.....] - ETA: 1:08 - loss: 1.9746 - regression_loss: 1.4612 - classification_loss: 0.5134\n",
      "434/500 [=========================>....] - ETA: 1:07 - loss: 1.9738 - regression_loss: 1.4608 - classification_loss: 0.5130\n",
      "435/500 [=========================>....] - ETA: 1:06 - loss: 1.9755 - regression_loss: 1.4618 - classification_loss: 0.5137\n",
      "436/500 [=========================>....] - ETA: 1:05 - loss: 1.9755 - regression_loss: 1.4617 - classification_loss: 0.5138\n",
      "437/500 [=========================>....] - ETA: 1:04 - loss: 1.9766 - regression_loss: 1.4630 - classification_loss: 0.5136\n",
      "438/500 [=========================>....] - ETA: 1:03 - loss: 1.9776 - regression_loss: 1.4636 - classification_loss: 0.5139\n",
      "439/500 [=========================>....] - ETA: 1:02 - loss: 1.9771 - regression_loss: 1.4633 - classification_loss: 0.5138\n",
      "440/500 [=========================>....] - ETA: 1:01 - loss: 1.9746 - regression_loss: 1.4614 - classification_loss: 0.5132\n",
      "441/500 [=========================>....] - ETA: 1:00 - loss: 1.9757 - regression_loss: 1.4624 - classification_loss: 0.5133\n",
      "442/500 [=========================>....] - ETA: 59s - loss: 1.9752 - regression_loss: 1.4623 - classification_loss: 0.5130 \n",
      "443/500 [=========================>....] - ETA: 58s - loss: 1.9750 - regression_loss: 1.4624 - classification_loss: 0.5126\n",
      "444/500 [=========================>....] - ETA: 57s - loss: 1.9749 - regression_loss: 1.4624 - classification_loss: 0.5125\n",
      "445/500 [=========================>....] - ETA: 56s - loss: 1.9765 - regression_loss: 1.4635 - classification_loss: 0.5130\n",
      "446/500 [=========================>....] - ETA: 55s - loss: 1.9768 - regression_loss: 1.4635 - classification_loss: 0.5133\n",
      "447/500 [=========================>....] - ETA: 54s - loss: 1.9783 - regression_loss: 1.4644 - classification_loss: 0.5139\n",
      "448/500 [=========================>....] - ETA: 53s - loss: 1.9783 - regression_loss: 1.4646 - classification_loss: 0.5137\n",
      "449/500 [=========================>....] - ETA: 52s - loss: 1.9771 - regression_loss: 1.4637 - classification_loss: 0.5134\n",
      "450/500 [==========================>...] - ETA: 51s - loss: 1.9785 - regression_loss: 1.4648 - classification_loss: 0.5137\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 1.9774 - regression_loss: 1.4640 - classification_loss: 0.5134\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 1.9791 - regression_loss: 1.4655 - classification_loss: 0.5136\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.9809 - regression_loss: 1.4671 - classification_loss: 0.5138\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.9845 - regression_loss: 1.4700 - classification_loss: 0.5146\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.9843 - regression_loss: 1.4698 - classification_loss: 0.5145\n",
      "456/500 [==========================>...] - ETA: 44s - loss: 1.9852 - regression_loss: 1.4703 - classification_loss: 0.5149\n",
      "457/500 [==========================>...] - ETA: 43s - loss: 1.9833 - regression_loss: 1.4687 - classification_loss: 0.5145\n",
      "458/500 [==========================>...] - ETA: 42s - loss: 1.9836 - regression_loss: 1.4687 - classification_loss: 0.5148\n",
      "459/500 [==========================>...] - ETA: 41s - loss: 1.9814 - regression_loss: 1.4671 - classification_loss: 0.5143\n",
      "460/500 [==========================>...] - ETA: 40s - loss: 1.9821 - regression_loss: 1.4677 - classification_loss: 0.5144\n",
      "461/500 [==========================>...] - ETA: 39s - loss: 1.9817 - regression_loss: 1.4676 - classification_loss: 0.5141\n",
      "462/500 [==========================>...] - ETA: 38s - loss: 1.9811 - regression_loss: 1.4672 - classification_loss: 0.5140\n",
      "463/500 [==========================>...] - ETA: 37s - loss: 1.9802 - regression_loss: 1.4662 - classification_loss: 0.5139\n",
      "464/500 [==========================>...] - ETA: 36s - loss: 1.9831 - regression_loss: 1.4682 - classification_loss: 0.5149\n",
      "465/500 [==========================>...] - ETA: 35s - loss: 1.9840 - regression_loss: 1.4687 - classification_loss: 0.5153\n",
      "466/500 [==========================>...] - ETA: 34s - loss: 1.9830 - regression_loss: 1.4678 - classification_loss: 0.5152\n",
      "467/500 [===========================>..] - ETA: 33s - loss: 1.9842 - regression_loss: 1.4689 - classification_loss: 0.5153\n",
      "468/500 [===========================>..] - ETA: 32s - loss: 1.9843 - regression_loss: 1.4693 - classification_loss: 0.5151\n",
      "469/500 [===========================>..] - ETA: 31s - loss: 1.9839 - regression_loss: 1.4693 - classification_loss: 0.5146\n",
      "470/500 [===========================>..] - ETA: 30s - loss: 1.9838 - regression_loss: 1.4690 - classification_loss: 0.5148\n",
      "471/500 [===========================>..] - ETA: 29s - loss: 1.9833 - regression_loss: 1.4687 - classification_loss: 0.5147\n",
      "472/500 [===========================>..] - ETA: 28s - loss: 1.9837 - regression_loss: 1.4686 - classification_loss: 0.5151\n",
      "473/500 [===========================>..] - ETA: 27s - loss: 1.9831 - regression_loss: 1.4683 - classification_loss: 0.5148\n",
      "474/500 [===========================>..] - ETA: 26s - loss: 1.9812 - regression_loss: 1.4668 - classification_loss: 0.5144\n",
      "475/500 [===========================>..] - ETA: 25s - loss: 1.9834 - regression_loss: 1.4684 - classification_loss: 0.5150\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 1.9839 - regression_loss: 1.4687 - classification_loss: 0.5151\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.9853 - regression_loss: 1.4699 - classification_loss: 0.5154\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.9871 - regression_loss: 1.4713 - classification_loss: 0.5158\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.9862 - regression_loss: 1.4706 - classification_loss: 0.5156\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.9861 - regression_loss: 1.4705 - classification_loss: 0.5156\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.9849 - regression_loss: 1.4696 - classification_loss: 0.5153\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.9851 - regression_loss: 1.4699 - classification_loss: 0.5151\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.9861 - regression_loss: 1.4706 - classification_loss: 0.5155\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.9853 - regression_loss: 1.4700 - classification_loss: 0.5153\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.9868 - regression_loss: 1.4711 - classification_loss: 0.5157\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.9885 - regression_loss: 1.4724 - classification_loss: 0.5161\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.9880 - regression_loss: 1.4720 - classification_loss: 0.5160\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.9863 - regression_loss: 1.4707 - classification_loss: 0.5156\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.9857 - regression_loss: 1.4703 - classification_loss: 0.5153\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.9860 - regression_loss: 1.4703 - classification_loss: 0.5157\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.9842 - regression_loss: 1.4691 - classification_loss: 0.5151 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.9851 - regression_loss: 1.4700 - classification_loss: 0.5151\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.9876 - regression_loss: 1.4715 - classification_loss: 0.5160\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.9860 - regression_loss: 1.4702 - classification_loss: 0.5158\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.9853 - regression_loss: 1.4697 - classification_loss: 0.5155\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.9843 - regression_loss: 1.4690 - classification_loss: 0.5153\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.9838 - regression_loss: 1.4685 - classification_loss: 0.5153\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.9832 - regression_loss: 1.4679 - classification_loss: 0.5153\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.9836 - regression_loss: 1.4682 - classification_loss: 0.5154\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9822 - regression_loss: 1.4669 - classification_loss: 0.5152\n",
      "Epoch 00013: saving model to ./snapshots\\resnet50_csv_13.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "\n",
      "500/500 [==============================] - 511s 1s/step - loss: 1.9822 - regression_loss: 1.4669 - classification_loss: 0.5152\n",
      "Epoch 14/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 1.7599 - regression_loss: 1.2544 - classification_loss: 0.5055\n",
      "  2/500 [..............................] - ETA: 4:03 - loss: 1.8616 - regression_loss: 1.4014 - classification_loss: 0.4601\n",
      "  3/500 [..............................] - ETA: 4:50 - loss: 1.8070 - regression_loss: 1.3553 - classification_loss: 0.4516\n",
      "  4/500 [..............................] - ETA: 6:03 - loss: 1.9784 - regression_loss: 1.5131 - classification_loss: 0.4653\n",
      "  5/500 [..............................] - ETA: 6:26 - loss: 1.9386 - regression_loss: 1.4611 - classification_loss: 0.4775\n",
      "  6/500 [..............................] - ETA: 6:45 - loss: 1.9417 - regression_loss: 1.4249 - classification_loss: 0.5169\n",
      "  7/500 [..............................] - ETA: 7:02 - loss: 2.0625 - regression_loss: 1.5120 - classification_loss: 0.5506\n",
      "  8/500 [..............................] - ETA: 7:14 - loss: 2.1424 - regression_loss: 1.5776 - classification_loss: 0.5648\n",
      "  9/500 [..............................] - ETA: 7:24 - loss: 2.1047 - regression_loss: 1.5511 - classification_loss: 0.5536\n",
      " 10/500 [..............................] - ETA: 7:33 - loss: 2.1128 - regression_loss: 1.5646 - classification_loss: 0.5482\n",
      " 11/500 [..............................] - ETA: 7:38 - loss: 2.0391 - regression_loss: 1.5124 - classification_loss: 0.5267\n",
      " 12/500 [..............................] - ETA: 7:39 - loss: 2.0498 - regression_loss: 1.5318 - classification_loss: 0.5180\n",
      " 13/500 [..............................] - ETA: 7:43 - loss: 2.0473 - regression_loss: 1.5337 - classification_loss: 0.5137\n",
      " 14/500 [..............................] - ETA: 7:43 - loss: 1.9909 - regression_loss: 1.4846 - classification_loss: 0.5063\n",
      " 15/500 [..............................] - ETA: 7:46 - loss: 1.9697 - regression_loss: 1.4666 - classification_loss: 0.5031\n",
      " 16/500 [..............................] - ETA: 7:46 - loss: 1.9697 - regression_loss: 1.4748 - classification_loss: 0.4948\n",
      " 17/500 [>.............................] - ETA: 7:45 - loss: 1.9614 - regression_loss: 1.4668 - classification_loss: 0.4946\n",
      " 18/500 [>.............................] - ETA: 7:42 - loss: 1.9648 - regression_loss: 1.4752 - classification_loss: 0.4895\n",
      " 19/500 [>.............................] - ETA: 7:45 - loss: 1.9681 - regression_loss: 1.4686 - classification_loss: 0.4995\n",
      " 20/500 [>.............................] - ETA: 7:47 - loss: 1.9731 - regression_loss: 1.4775 - classification_loss: 0.4956\n",
      " 21/500 [>.............................] - ETA: 7:48 - loss: 1.9197 - regression_loss: 1.4305 - classification_loss: 0.4892\n",
      " 22/500 [>.............................] - ETA: 7:47 - loss: 1.9480 - regression_loss: 1.4482 - classification_loss: 0.4998\n",
      " 23/500 [>.............................] - ETA: 7:48 - loss: 1.9072 - regression_loss: 1.4169 - classification_loss: 0.4903\n",
      " 24/500 [>.............................] - ETA: 7:42 - loss: 1.8672 - regression_loss: 1.3862 - classification_loss: 0.4811\n",
      " 25/500 [>.............................] - ETA: 7:46 - loss: 1.9075 - regression_loss: 1.4162 - classification_loss: 0.4913\n",
      " 26/500 [>.............................] - ETA: 7:45 - loss: 1.8869 - regression_loss: 1.4022 - classification_loss: 0.4847\n",
      " 27/500 [>.............................] - ETA: 7:42 - loss: 1.8777 - regression_loss: 1.3940 - classification_loss: 0.4837\n",
      " 28/500 [>.............................] - ETA: 7:44 - loss: 1.8515 - regression_loss: 1.3729 - classification_loss: 0.4785\n",
      " 29/500 [>.............................] - ETA: 7:45 - loss: 1.8538 - regression_loss: 1.3759 - classification_loss: 0.4778\n",
      " 30/500 [>.............................] - ETA: 7:43 - loss: 1.8483 - regression_loss: 1.3661 - classification_loss: 0.4823\n",
      " 31/500 [>.............................] - ETA: 7:44 - loss: 1.8586 - regression_loss: 1.3739 - classification_loss: 0.4847\n",
      " 32/500 [>.............................] - ETA: 7:44 - loss: 1.8739 - regression_loss: 1.3845 - classification_loss: 0.4894\n",
      " 33/500 [>.............................] - ETA: 7:43 - loss: 1.8781 - regression_loss: 1.3857 - classification_loss: 0.4924\n",
      " 34/500 [=>............................] - ETA: 7:42 - loss: 1.8795 - regression_loss: 1.3868 - classification_loss: 0.4927\n",
      " 35/500 [=>............................] - ETA: 7:41 - loss: 1.8816 - regression_loss: 1.3879 - classification_loss: 0.4937\n",
      " 36/500 [=>............................] - ETA: 7:40 - loss: 1.8797 - regression_loss: 1.3872 - classification_loss: 0.4926\n",
      " 37/500 [=>............................] - ETA: 7:39 - loss: 1.8792 - regression_loss: 1.3863 - classification_loss: 0.4929\n",
      " 38/500 [=>............................] - ETA: 7:39 - loss: 1.8883 - regression_loss: 1.3959 - classification_loss: 0.4924\n",
      " 39/500 [=>............................] - ETA: 7:39 - loss: 1.8884 - regression_loss: 1.3974 - classification_loss: 0.4910\n",
      " 40/500 [=>............................] - ETA: 7:35 - loss: 1.8952 - regression_loss: 1.4046 - classification_loss: 0.4906\n",
      " 41/500 [=>............................] - ETA: 7:36 - loss: 1.8931 - regression_loss: 1.4057 - classification_loss: 0.4874\n",
      " 42/500 [=>............................] - ETA: 7:35 - loss: 1.9072 - regression_loss: 1.4145 - classification_loss: 0.4927\n",
      " 43/500 [=>............................] - ETA: 7:34 - loss: 1.9104 - regression_loss: 1.4119 - classification_loss: 0.4985\n",
      " 44/500 [=>............................] - ETA: 7:33 - loss: 1.8952 - regression_loss: 1.4020 - classification_loss: 0.4933\n",
      " 45/500 [=>............................] - ETA: 7:32 - loss: 1.8983 - regression_loss: 1.4045 - classification_loss: 0.4938\n",
      " 46/500 [=>............................] - ETA: 7:31 - loss: 1.9159 - regression_loss: 1.4150 - classification_loss: 0.5009\n",
      " 47/500 [=>............................] - ETA: 7:30 - loss: 1.9109 - regression_loss: 1.4133 - classification_loss: 0.4976\n",
      " 48/500 [=>............................] - ETA: 7:29 - loss: 1.9043 - regression_loss: 1.4093 - classification_loss: 0.4951\n",
      " 49/500 [=>............................] - ETA: 7:26 - loss: 1.9031 - regression_loss: 1.4037 - classification_loss: 0.4993\n",
      " 50/500 [==>...........................] - ETA: 7:27 - loss: 1.9155 - regression_loss: 1.4122 - classification_loss: 0.5033\n",
      " 51/500 [==>...........................] - ETA: 7:26 - loss: 1.9323 - regression_loss: 1.4246 - classification_loss: 0.5077\n",
      " 52/500 [==>...........................] - ETA: 7:25 - loss: 1.9314 - regression_loss: 1.4256 - classification_loss: 0.5058\n",
      " 53/500 [==>...........................] - ETA: 7:25 - loss: 1.9371 - regression_loss: 1.4288 - classification_loss: 0.5083\n",
      " 54/500 [==>...........................] - ETA: 7:22 - loss: 1.9335 - regression_loss: 1.4286 - classification_loss: 0.5050\n",
      " 55/500 [==>...........................] - ETA: 7:23 - loss: 1.9189 - regression_loss: 1.4171 - classification_loss: 0.5019\n",
      " 56/500 [==>...........................] - ETA: 7:22 - loss: 1.9076 - regression_loss: 1.4091 - classification_loss: 0.4985\n",
      " 57/500 [==>...........................] - ETA: 7:21 - loss: 1.9036 - regression_loss: 1.4059 - classification_loss: 0.4976\n",
      " 58/500 [==>...........................] - ETA: 7:20 - loss: 1.9064 - regression_loss: 1.4080 - classification_loss: 0.4983\n",
      " 59/500 [==>...........................] - ETA: 7:20 - loss: 1.9088 - regression_loss: 1.4106 - classification_loss: 0.4982\n",
      " 60/500 [==>...........................] - ETA: 7:20 - loss: 1.9110 - regression_loss: 1.4112 - classification_loss: 0.4998\n",
      " 61/500 [==>...........................] - ETA: 7:20 - loss: 1.9283 - regression_loss: 1.4208 - classification_loss: 0.5075\n",
      " 62/500 [==>...........................] - ETA: 7:21 - loss: 1.9250 - regression_loss: 1.4201 - classification_loss: 0.5049\n",
      " 63/500 [==>...........................] - ETA: 7:20 - loss: 1.9258 - regression_loss: 1.4197 - classification_loss: 0.5061\n",
      " 64/500 [==>...........................] - ETA: 7:19 - loss: 1.9436 - regression_loss: 1.4328 - classification_loss: 0.5108\n",
      " 65/500 [==>...........................] - ETA: 7:18 - loss: 1.9447 - regression_loss: 1.4311 - classification_loss: 0.5137\n",
      " 66/500 [==>...........................] - ETA: 7:18 - loss: 1.9630 - regression_loss: 1.4404 - classification_loss: 0.5226\n",
      " 67/500 [===>..........................] - ETA: 7:17 - loss: 1.9567 - regression_loss: 1.4366 - classification_loss: 0.5201\n",
      " 68/500 [===>..........................] - ETA: 7:15 - loss: 1.9516 - regression_loss: 1.4340 - classification_loss: 0.5175\n",
      " 69/500 [===>..........................] - ETA: 7:15 - loss: 1.9497 - regression_loss: 1.4349 - classification_loss: 0.5148\n",
      " 70/500 [===>..........................] - ETA: 7:14 - loss: 1.9520 - regression_loss: 1.4365 - classification_loss: 0.5155\n",
      " 71/500 [===>..........................] - ETA: 7:14 - loss: 1.9438 - regression_loss: 1.4298 - classification_loss: 0.5139\n",
      " 72/500 [===>..........................] - ETA: 7:13 - loss: 1.9495 - regression_loss: 1.4344 - classification_loss: 0.5151\n",
      " 73/500 [===>..........................] - ETA: 7:13 - loss: 1.9579 - regression_loss: 1.4430 - classification_loss: 0.5149\n",
      " 74/500 [===>..........................] - ETA: 7:12 - loss: 1.9637 - regression_loss: 1.4459 - classification_loss: 0.5178\n",
      " 75/500 [===>..........................] - ETA: 7:11 - loss: 1.9541 - regression_loss: 1.4380 - classification_loss: 0.5161\n",
      " 76/500 [===>..........................] - ETA: 7:10 - loss: 1.9514 - regression_loss: 1.4371 - classification_loss: 0.5143\n",
      " 77/500 [===>..........................] - ETA: 7:08 - loss: 1.9423 - regression_loss: 1.4308 - classification_loss: 0.5116\n",
      " 78/500 [===>..........................] - ETA: 7:07 - loss: 1.9502 - regression_loss: 1.4362 - classification_loss: 0.5140\n",
      " 79/500 [===>..........................] - ETA: 7:07 - loss: 1.9491 - regression_loss: 1.4347 - classification_loss: 0.5144\n",
      " 80/500 [===>..........................] - ETA: 7:05 - loss: 1.9482 - regression_loss: 1.4355 - classification_loss: 0.5127\n",
      " 81/500 [===>..........................] - ETA: 7:04 - loss: 1.9428 - regression_loss: 1.4332 - classification_loss: 0.5096\n",
      " 82/500 [===>..........................] - ETA: 7:03 - loss: 1.9394 - regression_loss: 1.4308 - classification_loss: 0.5086\n",
      " 83/500 [===>..........................] - ETA: 7:02 - loss: 1.9351 - regression_loss: 1.4289 - classification_loss: 0.5061\n",
      " 84/500 [====>.........................] - ETA: 7:01 - loss: 1.9356 - regression_loss: 1.4291 - classification_loss: 0.5065\n",
      " 85/500 [====>.........................] - ETA: 7:00 - loss: 1.9352 - regression_loss: 1.4285 - classification_loss: 0.5067\n",
      " 86/500 [====>.........................] - ETA: 6:59 - loss: 1.9452 - regression_loss: 1.4338 - classification_loss: 0.5113\n",
      " 87/500 [====>.........................] - ETA: 6:59 - loss: 1.9370 - regression_loss: 1.4277 - classification_loss: 0.5093\n",
      " 88/500 [====>.........................] - ETA: 6:58 - loss: 1.9322 - regression_loss: 1.4253 - classification_loss: 0.5069\n",
      " 89/500 [====>.........................] - ETA: 6:57 - loss: 1.9247 - regression_loss: 1.4203 - classification_loss: 0.5045\n",
      " 90/500 [====>.........................] - ETA: 6:56 - loss: 1.9248 - regression_loss: 1.4214 - classification_loss: 0.5034\n",
      " 91/500 [====>.........................] - ETA: 6:55 - loss: 1.9237 - regression_loss: 1.4213 - classification_loss: 0.5024\n",
      " 92/500 [====>.........................] - ETA: 6:54 - loss: 1.9241 - regression_loss: 1.4222 - classification_loss: 0.5019\n",
      " 93/500 [====>.........................] - ETA: 6:53 - loss: 1.9372 - regression_loss: 1.4309 - classification_loss: 0.5063\n",
      " 94/500 [====>.........................] - ETA: 6:52 - loss: 1.9409 - regression_loss: 1.4324 - classification_loss: 0.5085\n",
      " 95/500 [====>.........................] - ETA: 6:51 - loss: 1.9421 - regression_loss: 1.4333 - classification_loss: 0.5089\n",
      " 96/500 [====>.........................] - ETA: 6:50 - loss: 1.9363 - regression_loss: 1.4295 - classification_loss: 0.5068\n",
      " 97/500 [====>.........................] - ETA: 6:49 - loss: 1.9253 - regression_loss: 1.4211 - classification_loss: 0.5041\n",
      " 98/500 [====>.........................] - ETA: 6:48 - loss: 1.9286 - regression_loss: 1.4227 - classification_loss: 0.5059\n",
      " 99/500 [====>.........................] - ETA: 6:47 - loss: 1.9337 - regression_loss: 1.4280 - classification_loss: 0.5057\n",
      "100/500 [=====>........................] - ETA: 6:46 - loss: 1.9249 - regression_loss: 1.4217 - classification_loss: 0.5032\n",
      "101/500 [=====>........................] - ETA: 6:45 - loss: 1.9277 - regression_loss: 1.4242 - classification_loss: 0.5035\n",
      "102/500 [=====>........................] - ETA: 6:43 - loss: 1.9219 - regression_loss: 1.4207 - classification_loss: 0.5012\n",
      "103/500 [=====>........................] - ETA: 6:43 - loss: 1.9263 - regression_loss: 1.4245 - classification_loss: 0.5018\n",
      "104/500 [=====>........................] - ETA: 6:42 - loss: 1.9329 - regression_loss: 1.4302 - classification_loss: 0.5027\n",
      "105/500 [=====>........................] - ETA: 6:41 - loss: 1.9281 - regression_loss: 1.4268 - classification_loss: 0.5013\n",
      "106/500 [=====>........................] - ETA: 6:40 - loss: 1.9269 - regression_loss: 1.4266 - classification_loss: 0.5003\n",
      "107/500 [=====>........................] - ETA: 6:39 - loss: 1.9229 - regression_loss: 1.4231 - classification_loss: 0.4998\n",
      "108/500 [=====>........................] - ETA: 6:38 - loss: 1.9224 - regression_loss: 1.4233 - classification_loss: 0.4991\n",
      "109/500 [=====>........................] - ETA: 6:37 - loss: 1.9218 - regression_loss: 1.4222 - classification_loss: 0.4996\n",
      "110/500 [=====>........................] - ETA: 6:36 - loss: 1.9236 - regression_loss: 1.4227 - classification_loss: 0.5009\n",
      "111/500 [=====>........................] - ETA: 6:35 - loss: 1.9302 - regression_loss: 1.4265 - classification_loss: 0.5037\n",
      "112/500 [=====>........................] - ETA: 6:34 - loss: 1.9273 - regression_loss: 1.4252 - classification_loss: 0.5020\n",
      "113/500 [=====>........................] - ETA: 6:33 - loss: 1.9265 - regression_loss: 1.4255 - classification_loss: 0.5010\n",
      "114/500 [=====>........................] - ETA: 6:32 - loss: 1.9284 - regression_loss: 1.4268 - classification_loss: 0.5016\n",
      "115/500 [=====>........................] - ETA: 6:31 - loss: 1.9260 - regression_loss: 1.4251 - classification_loss: 0.5008\n",
      "116/500 [=====>........................] - ETA: 6:30 - loss: 1.9267 - regression_loss: 1.4274 - classification_loss: 0.4994\n",
      "117/500 [======>.......................] - ETA: 6:29 - loss: 1.9289 - regression_loss: 1.4292 - classification_loss: 0.4997\n",
      "118/500 [======>.......................] - ETA: 6:28 - loss: 1.9244 - regression_loss: 1.4258 - classification_loss: 0.4985\n",
      "119/500 [======>.......................] - ETA: 6:26 - loss: 1.9218 - regression_loss: 1.4227 - classification_loss: 0.4991\n",
      "120/500 [======>.......................] - ETA: 6:26 - loss: 1.9240 - regression_loss: 1.4252 - classification_loss: 0.4989\n",
      "121/500 [======>.......................] - ETA: 6:25 - loss: 1.9224 - regression_loss: 1.4241 - classification_loss: 0.4984\n",
      "122/500 [======>.......................] - ETA: 6:23 - loss: 1.9167 - regression_loss: 1.4194 - classification_loss: 0.4973\n",
      "123/500 [======>.......................] - ETA: 6:22 - loss: 1.9167 - regression_loss: 1.4189 - classification_loss: 0.4978\n",
      "124/500 [======>.......................] - ETA: 6:21 - loss: 1.9225 - regression_loss: 1.4223 - classification_loss: 0.5003\n",
      "125/500 [======>.......................] - ETA: 6:20 - loss: 1.9225 - regression_loss: 1.4233 - classification_loss: 0.4992\n",
      "126/500 [======>.......................] - ETA: 6:19 - loss: 1.9163 - regression_loss: 1.4189 - classification_loss: 0.4974\n",
      "127/500 [======>.......................] - ETA: 6:18 - loss: 1.9171 - regression_loss: 1.4199 - classification_loss: 0.4972\n",
      "128/500 [======>.......................] - ETA: 6:17 - loss: 1.9118 - regression_loss: 1.4160 - classification_loss: 0.4958\n",
      "129/500 [======>.......................] - ETA: 6:16 - loss: 1.9050 - regression_loss: 1.4110 - classification_loss: 0.4941\n",
      "130/500 [======>.......................] - ETA: 6:15 - loss: 1.9040 - regression_loss: 1.4095 - classification_loss: 0.4945\n",
      "131/500 [======>.......................] - ETA: 6:14 - loss: 1.9014 - regression_loss: 1.4073 - classification_loss: 0.4941\n",
      "132/500 [======>.......................] - ETA: 6:13 - loss: 1.8993 - regression_loss: 1.4064 - classification_loss: 0.4929\n",
      "133/500 [======>.......................] - ETA: 6:12 - loss: 1.9003 - regression_loss: 1.4074 - classification_loss: 0.4928\n",
      "134/500 [=======>......................] - ETA: 6:11 - loss: 1.9000 - regression_loss: 1.4081 - classification_loss: 0.4920\n",
      "135/500 [=======>......................] - ETA: 6:11 - loss: 1.8966 - regression_loss: 1.4057 - classification_loss: 0.4909\n",
      "136/500 [=======>......................] - ETA: 6:10 - loss: 1.8951 - regression_loss: 1.4046 - classification_loss: 0.4905\n",
      "137/500 [=======>......................] - ETA: 6:09 - loss: 1.8912 - regression_loss: 1.4024 - classification_loss: 0.4888\n",
      "138/500 [=======>......................] - ETA: 6:08 - loss: 1.8953 - regression_loss: 1.4061 - classification_loss: 0.4892\n",
      "139/500 [=======>......................] - ETA: 6:07 - loss: 1.8930 - regression_loss: 1.4045 - classification_loss: 0.4886\n",
      "140/500 [=======>......................] - ETA: 6:06 - loss: 1.8949 - regression_loss: 1.4066 - classification_loss: 0.4883\n",
      "141/500 [=======>......................] - ETA: 6:05 - loss: 1.8924 - regression_loss: 1.4049 - classification_loss: 0.4875\n",
      "142/500 [=======>......................] - ETA: 6:06 - loss: 1.9011 - regression_loss: 1.4106 - classification_loss: 0.4905\n",
      "143/500 [=======>......................] - ETA: 6:05 - loss: 1.9001 - regression_loss: 1.4099 - classification_loss: 0.4902\n",
      "144/500 [=======>......................] - ETA: 6:04 - loss: 1.8916 - regression_loss: 1.4033 - classification_loss: 0.4883\n",
      "145/500 [=======>......................] - ETA: 6:03 - loss: 1.8928 - regression_loss: 1.4044 - classification_loss: 0.4885\n",
      "146/500 [=======>......................] - ETA: 6:02 - loss: 1.8943 - regression_loss: 1.4060 - classification_loss: 0.4882\n",
      "147/500 [=======>......................] - ETA: 6:01 - loss: 1.8869 - regression_loss: 1.4000 - classification_loss: 0.4869\n",
      "148/500 [=======>......................] - ETA: 6:00 - loss: 1.8849 - regression_loss: 1.3972 - classification_loss: 0.4876\n",
      "149/500 [=======>......................] - ETA: 5:59 - loss: 1.8903 - regression_loss: 1.4007 - classification_loss: 0.4897\n",
      "150/500 [========>.....................] - ETA: 5:58 - loss: 1.8951 - regression_loss: 1.4042 - classification_loss: 0.4909\n",
      "151/500 [========>.....................] - ETA: 5:56 - loss: 1.8948 - regression_loss: 1.4035 - classification_loss: 0.4913\n",
      "152/500 [========>.....................] - ETA: 5:56 - loss: 1.8993 - regression_loss: 1.4074 - classification_loss: 0.4919\n",
      "153/500 [========>.....................] - ETA: 5:55 - loss: 1.8992 - regression_loss: 1.4067 - classification_loss: 0.4925\n",
      "154/500 [========>.....................] - ETA: 5:54 - loss: 1.8955 - regression_loss: 1.4046 - classification_loss: 0.4910\n",
      "155/500 [========>.....................] - ETA: 5:52 - loss: 1.8899 - regression_loss: 1.4007 - classification_loss: 0.4892\n",
      "156/500 [========>.....................] - ETA: 5:51 - loss: 1.8854 - regression_loss: 1.3970 - classification_loss: 0.4884\n",
      "157/500 [========>.....................] - ETA: 5:50 - loss: 1.8890 - regression_loss: 1.3994 - classification_loss: 0.4897\n",
      "158/500 [========>.....................] - ETA: 5:49 - loss: 1.8885 - regression_loss: 1.3993 - classification_loss: 0.4893\n",
      "159/500 [========>.....................] - ETA: 5:49 - loss: 1.8913 - regression_loss: 1.4023 - classification_loss: 0.4890\n",
      "160/500 [========>.....................] - ETA: 5:48 - loss: 1.8911 - regression_loss: 1.4025 - classification_loss: 0.4886\n",
      "161/500 [========>.....................] - ETA: 5:46 - loss: 1.8866 - regression_loss: 1.3991 - classification_loss: 0.4875\n",
      "162/500 [========>.....................] - ETA: 5:45 - loss: 1.8912 - regression_loss: 1.4021 - classification_loss: 0.4891\n",
      "163/500 [========>.....................] - ETA: 5:44 - loss: 1.8918 - regression_loss: 1.4029 - classification_loss: 0.4889\n",
      "164/500 [========>.....................] - ETA: 5:43 - loss: 1.8985 - regression_loss: 1.4079 - classification_loss: 0.4906\n",
      "165/500 [========>.....................] - ETA: 5:43 - loss: 1.9061 - regression_loss: 1.4131 - classification_loss: 0.4929\n",
      "166/500 [========>.....................] - ETA: 5:41 - loss: 1.9125 - regression_loss: 1.4172 - classification_loss: 0.4953\n",
      "167/500 [=========>....................] - ETA: 5:40 - loss: 1.9162 - regression_loss: 1.4202 - classification_loss: 0.4960\n",
      "168/500 [=========>....................] - ETA: 5:39 - loss: 1.9139 - regression_loss: 1.4188 - classification_loss: 0.4951\n",
      "169/500 [=========>....................] - ETA: 5:38 - loss: 1.9170 - regression_loss: 1.4207 - classification_loss: 0.4963\n",
      "170/500 [=========>....................] - ETA: 5:38 - loss: 1.9178 - regression_loss: 1.4220 - classification_loss: 0.4957\n",
      "171/500 [=========>....................] - ETA: 5:36 - loss: 1.9166 - regression_loss: 1.4206 - classification_loss: 0.4960\n",
      "172/500 [=========>....................] - ETA: 5:35 - loss: 1.9200 - regression_loss: 1.4239 - classification_loss: 0.4962\n",
      "173/500 [=========>....................] - ETA: 5:34 - loss: 1.9192 - regression_loss: 1.4236 - classification_loss: 0.4956\n",
      "174/500 [=========>....................] - ETA: 5:33 - loss: 1.9183 - regression_loss: 1.4225 - classification_loss: 0.4958\n",
      "175/500 [=========>....................] - ETA: 5:32 - loss: 1.9219 - regression_loss: 1.4248 - classification_loss: 0.4971\n",
      "176/500 [=========>....................] - ETA: 5:31 - loss: 1.9239 - regression_loss: 1.4265 - classification_loss: 0.4974\n",
      "177/500 [=========>....................] - ETA: 5:30 - loss: 1.9278 - regression_loss: 1.4292 - classification_loss: 0.4986\n",
      "178/500 [=========>....................] - ETA: 5:29 - loss: 1.9253 - regression_loss: 1.4277 - classification_loss: 0.4976\n",
      "179/500 [=========>....................] - ETA: 5:28 - loss: 1.9212 - regression_loss: 1.4248 - classification_loss: 0.4964\n",
      "180/500 [=========>....................] - ETA: 5:27 - loss: 1.9222 - regression_loss: 1.4258 - classification_loss: 0.4965\n",
      "181/500 [=========>....................] - ETA: 5:26 - loss: 1.9216 - regression_loss: 1.4258 - classification_loss: 0.4958\n",
      "182/500 [=========>....................] - ETA: 5:25 - loss: 1.9204 - regression_loss: 1.4244 - classification_loss: 0.4960\n",
      "183/500 [=========>....................] - ETA: 5:24 - loss: 1.9194 - regression_loss: 1.4243 - classification_loss: 0.4951\n",
      "184/500 [==========>...................] - ETA: 5:23 - loss: 1.9180 - regression_loss: 1.4232 - classification_loss: 0.4948\n",
      "185/500 [==========>...................] - ETA: 5:22 - loss: 1.9189 - regression_loss: 1.4244 - classification_loss: 0.4945\n",
      "186/500 [==========>...................] - ETA: 5:21 - loss: 1.9161 - regression_loss: 1.4221 - classification_loss: 0.4940\n",
      "187/500 [==========>...................] - ETA: 5:20 - loss: 1.9126 - regression_loss: 1.4194 - classification_loss: 0.4932\n",
      "188/500 [==========>...................] - ETA: 5:19 - loss: 1.9069 - regression_loss: 1.4147 - classification_loss: 0.4923\n",
      "189/500 [==========>...................] - ETA: 5:18 - loss: 1.9032 - regression_loss: 1.4117 - classification_loss: 0.4916\n",
      "190/500 [==========>...................] - ETA: 5:17 - loss: 1.9048 - regression_loss: 1.4135 - classification_loss: 0.4912\n",
      "191/500 [==========>...................] - ETA: 5:16 - loss: 1.9043 - regression_loss: 1.4139 - classification_loss: 0.4903\n",
      "192/500 [==========>...................] - ETA: 5:15 - loss: 1.9026 - regression_loss: 1.4126 - classification_loss: 0.4900\n",
      "193/500 [==========>...................] - ETA: 5:14 - loss: 1.9053 - regression_loss: 1.4135 - classification_loss: 0.4918\n",
      "194/500 [==========>...................] - ETA: 5:13 - loss: 1.9064 - regression_loss: 1.4147 - classification_loss: 0.4917\n",
      "195/500 [==========>...................] - ETA: 5:12 - loss: 1.9069 - regression_loss: 1.4159 - classification_loss: 0.4910\n",
      "196/500 [==========>...................] - ETA: 5:11 - loss: 1.9119 - regression_loss: 1.4201 - classification_loss: 0.4918\n",
      "197/500 [==========>...................] - ETA: 5:10 - loss: 1.9088 - regression_loss: 1.4174 - classification_loss: 0.4915\n",
      "198/500 [==========>...................] - ETA: 5:09 - loss: 1.9070 - regression_loss: 1.4159 - classification_loss: 0.4911\n",
      "199/500 [==========>...................] - ETA: 5:08 - loss: 1.9103 - regression_loss: 1.4189 - classification_loss: 0.4913\n",
      "200/500 [===========>..................] - ETA: 5:07 - loss: 1.9088 - regression_loss: 1.4181 - classification_loss: 0.4907\n",
      "201/500 [===========>..................] - ETA: 5:06 - loss: 1.9122 - regression_loss: 1.4203 - classification_loss: 0.4919\n",
      "202/500 [===========>..................] - ETA: 5:05 - loss: 1.9195 - regression_loss: 1.4254 - classification_loss: 0.4941\n",
      "203/500 [===========>..................] - ETA: 5:04 - loss: 1.9193 - regression_loss: 1.4254 - classification_loss: 0.4939\n",
      "204/500 [===========>..................] - ETA: 5:03 - loss: 1.9196 - regression_loss: 1.4260 - classification_loss: 0.4937\n",
      "205/500 [===========>..................] - ETA: 5:02 - loss: 1.9189 - regression_loss: 1.4249 - classification_loss: 0.4940\n",
      "206/500 [===========>..................] - ETA: 5:01 - loss: 1.9175 - regression_loss: 1.4244 - classification_loss: 0.4931\n",
      "207/500 [===========>..................] - ETA: 5:00 - loss: 1.9204 - regression_loss: 1.4258 - classification_loss: 0.4946\n",
      "208/500 [===========>..................] - ETA: 4:59 - loss: 1.9186 - regression_loss: 1.4245 - classification_loss: 0.4941\n",
      "209/500 [===========>..................] - ETA: 4:58 - loss: 1.9173 - regression_loss: 1.4239 - classification_loss: 0.4934\n",
      "210/500 [===========>..................] - ETA: 4:57 - loss: 1.9175 - regression_loss: 1.4240 - classification_loss: 0.4935\n",
      "211/500 [===========>..................] - ETA: 4:56 - loss: 1.9176 - regression_loss: 1.4244 - classification_loss: 0.4932\n",
      "212/500 [===========>..................] - ETA: 4:55 - loss: 1.9163 - regression_loss: 1.4233 - classification_loss: 0.4930\n",
      "213/500 [===========>..................] - ETA: 4:57 - loss: 1.9125 - regression_loss: 1.4206 - classification_loss: 0.4920\n",
      "214/500 [===========>..................] - ETA: 4:56 - loss: 1.9138 - regression_loss: 1.4222 - classification_loss: 0.4916\n",
      "215/500 [===========>..................] - ETA: 4:55 - loss: 1.9126 - regression_loss: 1.4212 - classification_loss: 0.4914\n",
      "216/500 [===========>..................] - ETA: 4:54 - loss: 1.9154 - regression_loss: 1.4239 - classification_loss: 0.4916\n",
      "217/500 [============>.................] - ETA: 4:53 - loss: 1.9173 - regression_loss: 1.4259 - classification_loss: 0.4914\n",
      "218/500 [============>.................] - ETA: 4:52 - loss: 1.9219 - regression_loss: 1.4281 - classification_loss: 0.4938\n",
      "219/500 [============>.................] - ETA: 4:51 - loss: 1.9223 - regression_loss: 1.4284 - classification_loss: 0.4939\n",
      "220/500 [============>.................] - ETA: 4:50 - loss: 1.9269 - regression_loss: 1.4311 - classification_loss: 0.4958\n",
      "221/500 [============>.................] - ETA: 4:49 - loss: 1.9276 - regression_loss: 1.4313 - classification_loss: 0.4963\n",
      "222/500 [============>.................] - ETA: 4:48 - loss: 1.9312 - regression_loss: 1.4346 - classification_loss: 0.4966\n",
      "223/500 [============>.................] - ETA: 4:47 - loss: 1.9313 - regression_loss: 1.4350 - classification_loss: 0.4964\n",
      "224/500 [============>.................] - ETA: 4:45 - loss: 1.9311 - regression_loss: 1.4350 - classification_loss: 0.4960\n",
      "225/500 [============>.................] - ETA: 4:44 - loss: 1.9270 - regression_loss: 1.4320 - classification_loss: 0.4950\n",
      "226/500 [============>.................] - ETA: 4:43 - loss: 1.9276 - regression_loss: 1.4328 - classification_loss: 0.4948\n",
      "227/500 [============>.................] - ETA: 4:42 - loss: 1.9275 - regression_loss: 1.4329 - classification_loss: 0.4946\n",
      "228/500 [============>.................] - ETA: 4:41 - loss: 1.9264 - regression_loss: 1.4320 - classification_loss: 0.4944\n",
      "229/500 [============>.................] - ETA: 4:40 - loss: 1.9247 - regression_loss: 1.4308 - classification_loss: 0.4939\n",
      "230/500 [============>.................] - ETA: 4:39 - loss: 1.9254 - regression_loss: 1.4314 - classification_loss: 0.4939\n",
      "231/500 [============>.................] - ETA: 4:38 - loss: 1.9231 - regression_loss: 1.4301 - classification_loss: 0.4931\n",
      "232/500 [============>.................] - ETA: 4:37 - loss: 1.9200 - regression_loss: 1.4276 - classification_loss: 0.4924\n",
      "233/500 [============>.................] - ETA: 4:36 - loss: 1.9194 - regression_loss: 1.4272 - classification_loss: 0.4922\n",
      "234/500 [=============>................] - ETA: 4:35 - loss: 1.9186 - regression_loss: 1.4263 - classification_loss: 0.4923\n",
      "235/500 [=============>................] - ETA: 4:34 - loss: 1.9204 - regression_loss: 1.4280 - classification_loss: 0.4924\n",
      "236/500 [=============>................] - ETA: 4:33 - loss: 1.9197 - regression_loss: 1.4279 - classification_loss: 0.4918\n",
      "237/500 [=============>................] - ETA: 4:32 - loss: 1.9172 - regression_loss: 1.4264 - classification_loss: 0.4908\n",
      "238/500 [=============>................] - ETA: 4:30 - loss: 1.9159 - regression_loss: 1.4255 - classification_loss: 0.4903\n",
      "239/500 [=============>................] - ETA: 4:29 - loss: 1.9147 - regression_loss: 1.4244 - classification_loss: 0.4903\n",
      "240/500 [=============>................] - ETA: 4:28 - loss: 1.9117 - regression_loss: 1.4224 - classification_loss: 0.4893\n",
      "241/500 [=============>................] - ETA: 4:27 - loss: 1.9119 - regression_loss: 1.4229 - classification_loss: 0.4890\n",
      "242/500 [=============>................] - ETA: 4:26 - loss: 1.9106 - regression_loss: 1.4222 - classification_loss: 0.4884\n",
      "243/500 [=============>................] - ETA: 4:25 - loss: 1.9149 - regression_loss: 1.4256 - classification_loss: 0.4893\n",
      "244/500 [=============>................] - ETA: 4:24 - loss: 1.9122 - regression_loss: 1.4239 - classification_loss: 0.4883\n",
      "245/500 [=============>................] - ETA: 4:23 - loss: 1.9101 - regression_loss: 1.4226 - classification_loss: 0.4875\n",
      "246/500 [=============>................] - ETA: 4:22 - loss: 1.9131 - regression_loss: 1.4247 - classification_loss: 0.4885\n",
      "247/500 [=============>................] - ETA: 4:21 - loss: 1.9136 - regression_loss: 1.4247 - classification_loss: 0.4888\n",
      "248/500 [=============>................] - ETA: 4:20 - loss: 1.9156 - regression_loss: 1.4256 - classification_loss: 0.4900\n",
      "249/500 [=============>................] - ETA: 4:19 - loss: 1.9170 - regression_loss: 1.4262 - classification_loss: 0.4908\n",
      "250/500 [==============>...............] - ETA: 4:18 - loss: 1.9167 - regression_loss: 1.4258 - classification_loss: 0.4908\n",
      "251/500 [==============>...............] - ETA: 4:17 - loss: 1.9147 - regression_loss: 1.4248 - classification_loss: 0.4899\n",
      "252/500 [==============>...............] - ETA: 4:16 - loss: 1.9148 - regression_loss: 1.4244 - classification_loss: 0.4904\n",
      "253/500 [==============>...............] - ETA: 4:15 - loss: 1.9123 - regression_loss: 1.4228 - classification_loss: 0.4895\n",
      "254/500 [==============>...............] - ETA: 4:14 - loss: 1.9160 - regression_loss: 1.4258 - classification_loss: 0.4901\n",
      "255/500 [==============>...............] - ETA: 4:13 - loss: 1.9142 - regression_loss: 1.4247 - classification_loss: 0.4894\n",
      "256/500 [==============>...............] - ETA: 4:12 - loss: 1.9124 - regression_loss: 1.4235 - classification_loss: 0.4889\n",
      "257/500 [==============>...............] - ETA: 4:11 - loss: 1.9178 - regression_loss: 1.4278 - classification_loss: 0.4901\n",
      "258/500 [==============>...............] - ETA: 4:10 - loss: 1.9179 - regression_loss: 1.4277 - classification_loss: 0.4903\n",
      "259/500 [==============>...............] - ETA: 4:09 - loss: 1.9174 - regression_loss: 1.4267 - classification_loss: 0.4907\n",
      "260/500 [==============>...............] - ETA: 4:07 - loss: 1.9165 - regression_loss: 1.4260 - classification_loss: 0.4905\n",
      "261/500 [==============>...............] - ETA: 4:06 - loss: 1.9165 - regression_loss: 1.4264 - classification_loss: 0.4901\n",
      "262/500 [==============>...............] - ETA: 4:06 - loss: 1.9172 - regression_loss: 1.4265 - classification_loss: 0.4907\n",
      "263/500 [==============>...............] - ETA: 4:04 - loss: 1.9169 - regression_loss: 1.4267 - classification_loss: 0.4903\n",
      "264/500 [==============>...............] - ETA: 4:03 - loss: 1.9220 - regression_loss: 1.4298 - classification_loss: 0.4921\n",
      "265/500 [==============>...............] - ETA: 4:06 - loss: 1.9245 - regression_loss: 1.4311 - classification_loss: 0.4935\n",
      "266/500 [==============>...............] - ETA: 4:05 - loss: 1.9255 - regression_loss: 1.4310 - classification_loss: 0.4945\n",
      "267/500 [===============>..............] - ETA: 4:04 - loss: 1.9239 - regression_loss: 1.4296 - classification_loss: 0.4942\n",
      "268/500 [===============>..............] - ETA: 4:03 - loss: 1.9212 - regression_loss: 1.4276 - classification_loss: 0.4936\n",
      "269/500 [===============>..............] - ETA: 4:01 - loss: 1.9208 - regression_loss: 1.4269 - classification_loss: 0.4939\n",
      "270/500 [===============>..............] - ETA: 4:00 - loss: 1.9197 - regression_loss: 1.4262 - classification_loss: 0.4934\n",
      "271/500 [===============>..............] - ETA: 3:59 - loss: 1.9178 - regression_loss: 1.4249 - classification_loss: 0.4930\n",
      "272/500 [===============>..............] - ETA: 3:58 - loss: 1.9194 - regression_loss: 1.4263 - classification_loss: 0.4930\n",
      "273/500 [===============>..............] - ETA: 3:57 - loss: 1.9191 - regression_loss: 1.4261 - classification_loss: 0.4929\n",
      "274/500 [===============>..............] - ETA: 3:56 - loss: 1.9184 - regression_loss: 1.4264 - classification_loss: 0.4920\n",
      "275/500 [===============>..............] - ETA: 3:55 - loss: 1.9174 - regression_loss: 1.4250 - classification_loss: 0.4924\n",
      "276/500 [===============>..............] - ETA: 3:54 - loss: 1.9198 - regression_loss: 1.4272 - classification_loss: 0.4926\n",
      "277/500 [===============>..............] - ETA: 3:53 - loss: 1.9177 - regression_loss: 1.4257 - classification_loss: 0.4920\n",
      "278/500 [===============>..............] - ETA: 3:52 - loss: 1.9171 - regression_loss: 1.4255 - classification_loss: 0.4916\n",
      "279/500 [===============>..............] - ETA: 3:51 - loss: 1.9186 - regression_loss: 1.4259 - classification_loss: 0.4927\n",
      "280/500 [===============>..............] - ETA: 3:50 - loss: 1.9170 - regression_loss: 1.4250 - classification_loss: 0.4920\n",
      "281/500 [===============>..............] - ETA: 3:49 - loss: 1.9169 - regression_loss: 1.4247 - classification_loss: 0.4922\n",
      "282/500 [===============>..............] - ETA: 3:48 - loss: 1.9160 - regression_loss: 1.4238 - classification_loss: 0.4922\n",
      "283/500 [===============>..............] - ETA: 3:47 - loss: 1.9160 - regression_loss: 1.4243 - classification_loss: 0.4917\n",
      "284/500 [================>.............] - ETA: 3:46 - loss: 1.9147 - regression_loss: 1.4236 - classification_loss: 0.4912\n",
      "285/500 [================>.............] - ETA: 3:44 - loss: 1.9142 - regression_loss: 1.4234 - classification_loss: 0.4909\n",
      "286/500 [================>.............] - ETA: 3:43 - loss: 1.9133 - regression_loss: 1.4228 - classification_loss: 0.4905\n",
      "287/500 [================>.............] - ETA: 3:42 - loss: 1.9157 - regression_loss: 1.4250 - classification_loss: 0.4907\n",
      "288/500 [================>.............] - ETA: 3:41 - loss: 1.9171 - regression_loss: 1.4263 - classification_loss: 0.4908\n",
      "289/500 [================>.............] - ETA: 3:40 - loss: 1.9148 - regression_loss: 1.4246 - classification_loss: 0.4903\n",
      "290/500 [================>.............] - ETA: 3:39 - loss: 1.9154 - regression_loss: 1.4255 - classification_loss: 0.4898\n",
      "291/500 [================>.............] - ETA: 3:38 - loss: 1.9135 - regression_loss: 1.4246 - classification_loss: 0.4889\n",
      "292/500 [================>.............] - ETA: 3:37 - loss: 1.9156 - regression_loss: 1.4265 - classification_loss: 0.4890\n",
      "293/500 [================>.............] - ETA: 3:36 - loss: 1.9143 - regression_loss: 1.4251 - classification_loss: 0.4892\n",
      "294/500 [================>.............] - ETA: 3:35 - loss: 1.9154 - regression_loss: 1.4263 - classification_loss: 0.4891\n",
      "295/500 [================>.............] - ETA: 3:34 - loss: 1.9121 - regression_loss: 1.4240 - classification_loss: 0.4881\n",
      "296/500 [================>.............] - ETA: 3:33 - loss: 1.9130 - regression_loss: 1.4247 - classification_loss: 0.4884\n",
      "297/500 [================>.............] - ETA: 3:32 - loss: 1.9139 - regression_loss: 1.4255 - classification_loss: 0.4884\n",
      "298/500 [================>.............] - ETA: 3:31 - loss: 1.9151 - regression_loss: 1.4264 - classification_loss: 0.4887\n",
      "299/500 [================>.............] - ETA: 3:29 - loss: 1.9137 - regression_loss: 1.4250 - classification_loss: 0.4886\n",
      "300/500 [=================>............] - ETA: 3:29 - loss: 1.9153 - regression_loss: 1.4261 - classification_loss: 0.4891\n",
      "301/500 [=================>............] - ETA: 3:27 - loss: 1.9129 - regression_loss: 1.4244 - classification_loss: 0.4885\n",
      "302/500 [=================>............] - ETA: 3:26 - loss: 1.9140 - regression_loss: 1.4254 - classification_loss: 0.4886\n",
      "303/500 [=================>............] - ETA: 3:25 - loss: 1.9120 - regression_loss: 1.4241 - classification_loss: 0.4879\n",
      "304/500 [=================>............] - ETA: 3:24 - loss: 1.9146 - regression_loss: 1.4257 - classification_loss: 0.4888\n",
      "305/500 [=================>............] - ETA: 3:23 - loss: 1.9141 - regression_loss: 1.4253 - classification_loss: 0.4888\n",
      "306/500 [=================>............] - ETA: 3:22 - loss: 1.9133 - regression_loss: 1.4247 - classification_loss: 0.4886\n",
      "307/500 [=================>............] - ETA: 3:21 - loss: 1.9121 - regression_loss: 1.4239 - classification_loss: 0.4882\n",
      "308/500 [=================>............] - ETA: 3:20 - loss: 1.9155 - regression_loss: 1.4260 - classification_loss: 0.4895\n",
      "309/500 [=================>............] - ETA: 3:19 - loss: 1.9160 - regression_loss: 1.4267 - classification_loss: 0.4893\n",
      "310/500 [=================>............] - ETA: 3:18 - loss: 1.9171 - regression_loss: 1.4276 - classification_loss: 0.4895\n",
      "311/500 [=================>............] - ETA: 3:17 - loss: 1.9159 - regression_loss: 1.4268 - classification_loss: 0.4891\n",
      "312/500 [=================>............] - ETA: 3:16 - loss: 1.9135 - regression_loss: 1.4249 - classification_loss: 0.4886\n",
      "313/500 [=================>............] - ETA: 3:15 - loss: 1.9117 - regression_loss: 1.4238 - classification_loss: 0.4879\n",
      "314/500 [=================>............] - ETA: 3:14 - loss: 1.9107 - regression_loss: 1.4229 - classification_loss: 0.4878\n",
      "315/500 [=================>............] - ETA: 3:13 - loss: 1.9129 - regression_loss: 1.4243 - classification_loss: 0.4886\n",
      "316/500 [=================>............] - ETA: 3:12 - loss: 1.9119 - regression_loss: 1.4234 - classification_loss: 0.4885\n",
      "317/500 [==================>...........] - ETA: 3:11 - loss: 1.9124 - regression_loss: 1.4239 - classification_loss: 0.4885\n",
      "318/500 [==================>...........] - ETA: 3:10 - loss: 1.9145 - regression_loss: 1.4260 - classification_loss: 0.4885\n",
      "319/500 [==================>...........] - ETA: 3:08 - loss: 1.9166 - regression_loss: 1.4277 - classification_loss: 0.4889\n",
      "320/500 [==================>...........] - ETA: 3:07 - loss: 1.9166 - regression_loss: 1.4274 - classification_loss: 0.4892\n",
      "321/500 [==================>...........] - ETA: 3:06 - loss: 1.9179 - regression_loss: 1.4288 - classification_loss: 0.4891\n",
      "322/500 [==================>...........] - ETA: 3:05 - loss: 1.9168 - regression_loss: 1.4282 - classification_loss: 0.4886\n",
      "323/500 [==================>...........] - ETA: 3:04 - loss: 1.9147 - regression_loss: 1.4266 - classification_loss: 0.4881\n",
      "324/500 [==================>...........] - ETA: 3:03 - loss: 1.9162 - regression_loss: 1.4278 - classification_loss: 0.4885\n",
      "325/500 [==================>...........] - ETA: 3:02 - loss: 1.9140 - regression_loss: 1.4259 - classification_loss: 0.4881\n",
      "326/500 [==================>...........] - ETA: 3:01 - loss: 1.9118 - regression_loss: 1.4243 - classification_loss: 0.4875\n",
      "327/500 [==================>...........] - ETA: 3:00 - loss: 1.9116 - regression_loss: 1.4243 - classification_loss: 0.4873\n",
      "328/500 [==================>...........] - ETA: 2:59 - loss: 1.9141 - regression_loss: 1.4267 - classification_loss: 0.4874\n",
      "329/500 [==================>...........] - ETA: 2:58 - loss: 1.9144 - regression_loss: 1.4275 - classification_loss: 0.4870\n",
      "330/500 [==================>...........] - ETA: 2:57 - loss: 1.9179 - regression_loss: 1.4297 - classification_loss: 0.4882\n",
      "331/500 [==================>...........] - ETA: 2:56 - loss: 1.9169 - regression_loss: 1.4291 - classification_loss: 0.4878\n",
      "332/500 [==================>...........] - ETA: 2:55 - loss: 1.9146 - regression_loss: 1.4272 - classification_loss: 0.4875\n",
      "333/500 [==================>...........] - ETA: 2:54 - loss: 1.9120 - regression_loss: 1.4251 - classification_loss: 0.4869\n",
      "334/500 [===================>..........] - ETA: 2:53 - loss: 1.9127 - regression_loss: 1.4258 - classification_loss: 0.4869\n",
      "335/500 [===================>..........] - ETA: 2:52 - loss: 1.9125 - regression_loss: 1.4260 - classification_loss: 0.4865\n",
      "336/500 [===================>..........] - ETA: 2:50 - loss: 1.9116 - regression_loss: 1.4250 - classification_loss: 0.4866\n",
      "337/500 [===================>..........] - ETA: 2:49 - loss: 1.9121 - regression_loss: 1.4252 - classification_loss: 0.4869\n",
      "338/500 [===================>..........] - ETA: 2:48 - loss: 1.9110 - regression_loss: 1.4245 - classification_loss: 0.4865\n",
      "339/500 [===================>..........] - ETA: 2:47 - loss: 1.9101 - regression_loss: 1.4239 - classification_loss: 0.4862\n",
      "340/500 [===================>..........] - ETA: 2:46 - loss: 1.9095 - regression_loss: 1.4233 - classification_loss: 0.4862\n",
      "341/500 [===================>..........] - ETA: 2:45 - loss: 1.9100 - regression_loss: 1.4237 - classification_loss: 0.4863\n",
      "342/500 [===================>..........] - ETA: 2:44 - loss: 1.9084 - regression_loss: 1.4227 - classification_loss: 0.4857\n",
      "343/500 [===================>..........] - ETA: 2:43 - loss: 1.9088 - regression_loss: 1.4231 - classification_loss: 0.4857\n",
      "344/500 [===================>..........] - ETA: 2:42 - loss: 1.9092 - regression_loss: 1.4236 - classification_loss: 0.4856\n",
      "345/500 [===================>..........] - ETA: 2:41 - loss: 1.9086 - regression_loss: 1.4232 - classification_loss: 0.4854\n",
      "346/500 [===================>..........] - ETA: 2:40 - loss: 1.9072 - regression_loss: 1.4223 - classification_loss: 0.4849\n",
      "347/500 [===================>..........] - ETA: 2:39 - loss: 1.9043 - regression_loss: 1.4203 - classification_loss: 0.4840\n",
      "348/500 [===================>..........] - ETA: 2:38 - loss: 1.9054 - regression_loss: 1.4208 - classification_loss: 0.4846\n",
      "349/500 [===================>..........] - ETA: 2:37 - loss: 1.9059 - regression_loss: 1.4213 - classification_loss: 0.4846\n",
      "350/500 [====================>.........] - ETA: 2:36 - loss: 1.9072 - regression_loss: 1.4214 - classification_loss: 0.4857\n",
      "351/500 [====================>.........] - ETA: 2:35 - loss: 1.9077 - regression_loss: 1.4220 - classification_loss: 0.4857\n",
      "352/500 [====================>.........] - ETA: 2:34 - loss: 1.9100 - regression_loss: 1.4242 - classification_loss: 0.4858\n",
      "353/500 [====================>.........] - ETA: 2:33 - loss: 1.9134 - regression_loss: 1.4267 - classification_loss: 0.4867\n",
      "354/500 [====================>.........] - ETA: 2:32 - loss: 1.9124 - regression_loss: 1.4255 - classification_loss: 0.4869\n",
      "355/500 [====================>.........] - ETA: 2:30 - loss: 1.9108 - regression_loss: 1.4243 - classification_loss: 0.4865\n",
      "356/500 [====================>.........] - ETA: 2:30 - loss: 1.9101 - regression_loss: 1.4231 - classification_loss: 0.4870\n",
      "357/500 [====================>.........] - ETA: 2:28 - loss: 1.9098 - regression_loss: 1.4224 - classification_loss: 0.4874\n",
      "358/500 [====================>.........] - ETA: 2:27 - loss: 1.9089 - regression_loss: 1.4220 - classification_loss: 0.4869\n",
      "359/500 [====================>.........] - ETA: 2:26 - loss: 1.9081 - regression_loss: 1.4216 - classification_loss: 0.4865\n",
      "360/500 [====================>.........] - ETA: 2:25 - loss: 1.9093 - regression_loss: 1.4226 - classification_loss: 0.4867\n",
      "361/500 [====================>.........] - ETA: 2:24 - loss: 1.9109 - regression_loss: 1.4235 - classification_loss: 0.4874\n",
      "362/500 [====================>.........] - ETA: 2:23 - loss: 1.9111 - regression_loss: 1.4233 - classification_loss: 0.4878\n",
      "363/500 [====================>.........] - ETA: 2:22 - loss: 1.9101 - regression_loss: 1.4227 - classification_loss: 0.4874\n",
      "364/500 [====================>.........] - ETA: 2:21 - loss: 1.9121 - regression_loss: 1.4237 - classification_loss: 0.4884\n",
      "365/500 [====================>.........] - ETA: 2:20 - loss: 1.9113 - regression_loss: 1.4232 - classification_loss: 0.4882\n",
      "366/500 [====================>.........] - ETA: 2:19 - loss: 1.9101 - regression_loss: 1.4222 - classification_loss: 0.4878\n",
      "367/500 [=====================>........] - ETA: 2:18 - loss: 1.9075 - regression_loss: 1.4203 - classification_loss: 0.4872\n",
      "368/500 [=====================>........] - ETA: 2:17 - loss: 1.9088 - regression_loss: 1.4208 - classification_loss: 0.4880\n",
      "369/500 [=====================>........] - ETA: 2:16 - loss: 1.9116 - regression_loss: 1.4231 - classification_loss: 0.4885\n",
      "370/500 [=====================>........] - ETA: 2:15 - loss: 1.9116 - regression_loss: 1.4232 - classification_loss: 0.4884\n",
      "371/500 [=====================>........] - ETA: 2:14 - loss: 1.9124 - regression_loss: 1.4238 - classification_loss: 0.4886\n",
      "372/500 [=====================>........] - ETA: 2:13 - loss: 1.9127 - regression_loss: 1.4242 - classification_loss: 0.4885\n",
      "373/500 [=====================>........] - ETA: 2:12 - loss: 1.9136 - regression_loss: 1.4250 - classification_loss: 0.4886\n",
      "374/500 [=====================>........] - ETA: 2:10 - loss: 1.9145 - regression_loss: 1.4260 - classification_loss: 0.4886\n",
      "375/500 [=====================>........] - ETA: 2:09 - loss: 1.9166 - regression_loss: 1.4268 - classification_loss: 0.4898\n",
      "376/500 [=====================>........] - ETA: 2:08 - loss: 1.9147 - regression_loss: 1.4254 - classification_loss: 0.4894\n",
      "377/500 [=====================>........] - ETA: 2:07 - loss: 1.9179 - regression_loss: 1.4274 - classification_loss: 0.4905\n",
      "378/500 [=====================>........] - ETA: 2:06 - loss: 1.9176 - regression_loss: 1.4271 - classification_loss: 0.4905\n",
      "379/500 [=====================>........] - ETA: 2:05 - loss: 1.9162 - regression_loss: 1.4264 - classification_loss: 0.4898\n",
      "380/500 [=====================>........] - ETA: 2:04 - loss: 1.9150 - regression_loss: 1.4256 - classification_loss: 0.4894\n",
      "381/500 [=====================>........] - ETA: 2:03 - loss: 1.9145 - regression_loss: 1.4256 - classification_loss: 0.4889\n",
      "382/500 [=====================>........] - ETA: 2:02 - loss: 1.9158 - regression_loss: 1.4264 - classification_loss: 0.4894\n",
      "383/500 [=====================>........] - ETA: 2:01 - loss: 1.9168 - regression_loss: 1.4275 - classification_loss: 0.4893\n",
      "384/500 [======================>.......] - ETA: 2:00 - loss: 1.9152 - regression_loss: 1.4265 - classification_loss: 0.4887\n",
      "385/500 [======================>.......] - ETA: 1:59 - loss: 1.9164 - regression_loss: 1.4272 - classification_loss: 0.4892\n",
      "386/500 [======================>.......] - ETA: 1:58 - loss: 1.9183 - regression_loss: 1.4283 - classification_loss: 0.4899\n",
      "387/500 [======================>.......] - ETA: 1:57 - loss: 1.9191 - regression_loss: 1.4288 - classification_loss: 0.4903\n",
      "388/500 [======================>.......] - ETA: 1:56 - loss: 1.9203 - regression_loss: 1.4298 - classification_loss: 0.4905\n",
      "389/500 [======================>.......] - ETA: 1:55 - loss: 1.9191 - regression_loss: 1.4291 - classification_loss: 0.4901\n",
      "390/500 [======================>.......] - ETA: 1:54 - loss: 1.9191 - regression_loss: 1.4291 - classification_loss: 0.4900\n",
      "391/500 [======================>.......] - ETA: 1:53 - loss: 1.9194 - regression_loss: 1.4296 - classification_loss: 0.4899\n",
      "392/500 [======================>.......] - ETA: 1:52 - loss: 1.9197 - regression_loss: 1.4298 - classification_loss: 0.4898\n",
      "393/500 [======================>.......] - ETA: 1:51 - loss: 1.9183 - regression_loss: 1.4285 - classification_loss: 0.4898\n",
      "394/500 [======================>.......] - ETA: 1:50 - loss: 1.9186 - regression_loss: 1.4288 - classification_loss: 0.4898\n",
      "395/500 [======================>.......] - ETA: 1:49 - loss: 1.9202 - regression_loss: 1.4306 - classification_loss: 0.4896\n",
      "396/500 [======================>.......] - ETA: 1:48 - loss: 1.9178 - regression_loss: 1.4290 - classification_loss: 0.4888\n",
      "397/500 [======================>.......] - ETA: 1:47 - loss: 1.9180 - regression_loss: 1.4289 - classification_loss: 0.4892\n",
      "398/500 [======================>.......] - ETA: 1:46 - loss: 1.9205 - regression_loss: 1.4305 - classification_loss: 0.4901\n",
      "399/500 [======================>.......] - ETA: 1:44 - loss: 1.9210 - regression_loss: 1.4311 - classification_loss: 0.4899\n",
      "400/500 [=======================>......] - ETA: 1:43 - loss: 1.9230 - regression_loss: 1.4328 - classification_loss: 0.4901\n",
      "401/500 [=======================>......] - ETA: 1:42 - loss: 1.9207 - regression_loss: 1.4312 - classification_loss: 0.4895\n",
      "402/500 [=======================>......] - ETA: 1:41 - loss: 1.9212 - regression_loss: 1.4318 - classification_loss: 0.4894\n",
      "403/500 [=======================>......] - ETA: 1:40 - loss: 1.9216 - regression_loss: 1.4319 - classification_loss: 0.4897\n",
      "404/500 [=======================>......] - ETA: 1:39 - loss: 1.9212 - regression_loss: 1.4317 - classification_loss: 0.4895\n",
      "405/500 [=======================>......] - ETA: 1:38 - loss: 1.9198 - regression_loss: 1.4307 - classification_loss: 0.4891\n",
      "406/500 [=======================>......] - ETA: 1:37 - loss: 1.9202 - regression_loss: 1.4312 - classification_loss: 0.4890\n",
      "407/500 [=======================>......] - ETA: 1:36 - loss: 1.9217 - regression_loss: 1.4324 - classification_loss: 0.4893\n",
      "408/500 [=======================>......] - ETA: 1:35 - loss: 1.9204 - regression_loss: 1.4314 - classification_loss: 0.4890\n",
      "409/500 [=======================>......] - ETA: 1:34 - loss: 1.9221 - regression_loss: 1.4325 - classification_loss: 0.4897\n",
      "410/500 [=======================>......] - ETA: 1:33 - loss: 1.9233 - regression_loss: 1.4337 - classification_loss: 0.4896\n",
      "411/500 [=======================>......] - ETA: 1:32 - loss: 1.9213 - regression_loss: 1.4320 - classification_loss: 0.4893\n",
      "412/500 [=======================>......] - ETA: 1:31 - loss: 1.9211 - regression_loss: 1.4319 - classification_loss: 0.4891\n",
      "413/500 [=======================>......] - ETA: 1:30 - loss: 1.9219 - regression_loss: 1.4329 - classification_loss: 0.4891\n",
      "414/500 [=======================>......] - ETA: 1:29 - loss: 1.9243 - regression_loss: 1.4344 - classification_loss: 0.4900\n",
      "415/500 [=======================>......] - ETA: 1:28 - loss: 1.9242 - regression_loss: 1.4343 - classification_loss: 0.4899\n",
      "416/500 [=======================>......] - ETA: 1:27 - loss: 1.9230 - regression_loss: 1.4334 - classification_loss: 0.4896\n",
      "417/500 [========================>.....] - ETA: 1:26 - loss: 1.9220 - regression_loss: 1.4328 - classification_loss: 0.4891\n",
      "418/500 [========================>.....] - ETA: 1:25 - loss: 1.9234 - regression_loss: 1.4340 - classification_loss: 0.4893\n",
      "419/500 [========================>.....] - ETA: 1:24 - loss: 1.9221 - regression_loss: 1.4329 - classification_loss: 0.4892\n",
      "420/500 [========================>.....] - ETA: 1:23 - loss: 1.9209 - regression_loss: 1.4322 - classification_loss: 0.4887\n",
      "421/500 [========================>.....] - ETA: 1:22 - loss: 1.9233 - regression_loss: 1.4337 - classification_loss: 0.4896\n",
      "422/500 [========================>.....] - ETA: 1:20 - loss: 1.9243 - regression_loss: 1.4340 - classification_loss: 0.4903\n",
      "423/500 [========================>.....] - ETA: 1:19 - loss: 1.9238 - regression_loss: 1.4335 - classification_loss: 0.4902\n",
      "424/500 [========================>.....] - ETA: 1:18 - loss: 1.9253 - regression_loss: 1.4350 - classification_loss: 0.4903\n",
      "425/500 [========================>.....] - ETA: 1:17 - loss: 1.9255 - regression_loss: 1.4347 - classification_loss: 0.4908\n",
      "426/500 [========================>.....] - ETA: 1:16 - loss: 1.9249 - regression_loss: 1.4345 - classification_loss: 0.4904\n",
      "427/500 [========================>.....] - ETA: 1:15 - loss: 1.9247 - regression_loss: 1.4345 - classification_loss: 0.4902\n",
      "428/500 [========================>.....] - ETA: 1:14 - loss: 1.9240 - regression_loss: 1.4340 - classification_loss: 0.4899\n",
      "429/500 [========================>.....] - ETA: 1:13 - loss: 1.9259 - regression_loss: 1.4359 - classification_loss: 0.4900\n",
      "430/500 [========================>.....] - ETA: 1:12 - loss: 1.9238 - regression_loss: 1.4343 - classification_loss: 0.4895\n",
      "431/500 [========================>.....] - ETA: 1:11 - loss: 1.9253 - regression_loss: 1.4354 - classification_loss: 0.4900\n",
      "432/500 [========================>.....] - ETA: 1:10 - loss: 1.9259 - regression_loss: 1.4360 - classification_loss: 0.4899\n",
      "433/500 [========================>.....] - ETA: 1:09 - loss: 1.9270 - regression_loss: 1.4364 - classification_loss: 0.4906\n",
      "434/500 [=========================>....] - ETA: 1:08 - loss: 1.9292 - regression_loss: 1.4385 - classification_loss: 0.4907\n",
      "435/500 [=========================>....] - ETA: 1:07 - loss: 1.9276 - regression_loss: 1.4375 - classification_loss: 0.4901\n",
      "436/500 [=========================>....] - ETA: 1:06 - loss: 1.9261 - regression_loss: 1.4365 - classification_loss: 0.4896\n",
      "437/500 [=========================>....] - ETA: 1:05 - loss: 1.9255 - regression_loss: 1.4362 - classification_loss: 0.4894\n",
      "438/500 [=========================>....] - ETA: 1:04 - loss: 1.9249 - regression_loss: 1.4355 - classification_loss: 0.4894\n",
      "439/500 [=========================>....] - ETA: 1:03 - loss: 1.9253 - regression_loss: 1.4359 - classification_loss: 0.4894\n",
      "440/500 [=========================>....] - ETA: 1:02 - loss: 1.9242 - regression_loss: 1.4351 - classification_loss: 0.4891\n",
      "441/500 [=========================>....] - ETA: 1:01 - loss: 1.9241 - regression_loss: 1.4351 - classification_loss: 0.4889\n",
      "442/500 [=========================>....] - ETA: 1:00 - loss: 1.9235 - regression_loss: 1.4348 - classification_loss: 0.4887\n",
      "443/500 [=========================>....] - ETA: 59s - loss: 1.9224 - regression_loss: 1.4341 - classification_loss: 0.4883 \n",
      "444/500 [=========================>....] - ETA: 58s - loss: 1.9211 - regression_loss: 1.4331 - classification_loss: 0.4880\n",
      "445/500 [=========================>....] - ETA: 57s - loss: 1.9228 - regression_loss: 1.4344 - classification_loss: 0.4884\n",
      "446/500 [=========================>....] - ETA: 56s - loss: 1.9222 - regression_loss: 1.4339 - classification_loss: 0.4882\n",
      "447/500 [=========================>....] - ETA: 54s - loss: 1.9233 - regression_loss: 1.4347 - classification_loss: 0.4886\n",
      "448/500 [=========================>....] - ETA: 53s - loss: 1.9233 - regression_loss: 1.4348 - classification_loss: 0.4885\n",
      "449/500 [=========================>....] - ETA: 52s - loss: 1.9237 - regression_loss: 1.4352 - classification_loss: 0.4886\n",
      "450/500 [==========================>...] - ETA: 51s - loss: 1.9228 - regression_loss: 1.4344 - classification_loss: 0.4884\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 1.9215 - regression_loss: 1.4333 - classification_loss: 0.4882\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 1.9224 - regression_loss: 1.4335 - classification_loss: 0.4889\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.9219 - regression_loss: 1.4335 - classification_loss: 0.4885\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.9212 - regression_loss: 1.4330 - classification_loss: 0.4882\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.9211 - regression_loss: 1.4332 - classification_loss: 0.4879\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.9209 - regression_loss: 1.4331 - classification_loss: 0.4878\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.9224 - regression_loss: 1.4342 - classification_loss: 0.4882\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.9204 - regression_loss: 1.4328 - classification_loss: 0.4877\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.9216 - regression_loss: 1.4338 - classification_loss: 0.4878\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.9210 - regression_loss: 1.4335 - classification_loss: 0.4875\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 1.9235 - regression_loss: 1.4353 - classification_loss: 0.4882\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 1.9234 - regression_loss: 1.4352 - classification_loss: 0.4882\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 1.9230 - regression_loss: 1.4351 - classification_loss: 0.4880\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 1.9227 - regression_loss: 1.4351 - classification_loss: 0.4876\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 1.9221 - regression_loss: 1.4347 - classification_loss: 0.4874\n",
      "466/500 [==========================>...] - ETA: 35s - loss: 1.9210 - regression_loss: 1.4335 - classification_loss: 0.4875\n",
      "467/500 [===========================>..] - ETA: 34s - loss: 1.9220 - regression_loss: 1.4341 - classification_loss: 0.4879\n",
      "468/500 [===========================>..] - ETA: 33s - loss: 1.9210 - regression_loss: 1.4333 - classification_loss: 0.4877\n",
      "469/500 [===========================>..] - ETA: 32s - loss: 1.9200 - regression_loss: 1.4326 - classification_loss: 0.4874\n",
      "470/500 [===========================>..] - ETA: 31s - loss: 1.9188 - regression_loss: 1.4317 - classification_loss: 0.4871\n",
      "471/500 [===========================>..] - ETA: 30s - loss: 1.9197 - regression_loss: 1.4324 - classification_loss: 0.4872\n",
      "472/500 [===========================>..] - ETA: 29s - loss: 1.9204 - regression_loss: 1.4330 - classification_loss: 0.4874\n",
      "473/500 [===========================>..] - ETA: 27s - loss: 1.9196 - regression_loss: 1.4324 - classification_loss: 0.4872\n",
      "474/500 [===========================>..] - ETA: 26s - loss: 1.9197 - regression_loss: 1.4326 - classification_loss: 0.4871\n",
      "475/500 [===========================>..] - ETA: 25s - loss: 1.9188 - regression_loss: 1.4320 - classification_loss: 0.4868\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 1.9182 - regression_loss: 1.4314 - classification_loss: 0.4869\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.9191 - regression_loss: 1.4319 - classification_loss: 0.4872\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.9192 - regression_loss: 1.4324 - classification_loss: 0.4869\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.9207 - regression_loss: 1.4336 - classification_loss: 0.4871\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.9204 - regression_loss: 1.4331 - classification_loss: 0.4873\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.9221 - regression_loss: 1.4341 - classification_loss: 0.4879\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.9205 - regression_loss: 1.4330 - classification_loss: 0.4875\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.9219 - regression_loss: 1.4336 - classification_loss: 0.4883\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.9213 - regression_loss: 1.4330 - classification_loss: 0.4883\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.9228 - regression_loss: 1.4337 - classification_loss: 0.4891\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.9227 - regression_loss: 1.4338 - classification_loss: 0.4888\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.9244 - regression_loss: 1.4351 - classification_loss: 0.4893\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.9230 - regression_loss: 1.4343 - classification_loss: 0.4888\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.9233 - regression_loss: 1.4345 - classification_loss: 0.4888\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.9230 - regression_loss: 1.4342 - classification_loss: 0.4888\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.9246 - regression_loss: 1.4357 - classification_loss: 0.4889 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.9263 - regression_loss: 1.4369 - classification_loss: 0.4894\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.9272 - regression_loss: 1.4376 - classification_loss: 0.4896\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.9265 - regression_loss: 1.4369 - classification_loss: 0.4895\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.9266 - regression_loss: 1.4369 - classification_loss: 0.4897\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.9270 - regression_loss: 1.4371 - classification_loss: 0.4899\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.9284 - regression_loss: 1.4383 - classification_loss: 0.4901\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.9302 - regression_loss: 1.4391 - classification_loss: 0.4911\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.9309 - regression_loss: 1.4398 - classification_loss: 0.4911\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9297 - regression_loss: 1.4391 - classification_loss: 0.4906\n",
      "Epoch 00014: saving model to ./snapshots\\resnet50_csv_14.h5\n",
      "\n",
      "500/500 [==============================] - 519s 1s/step - loss: 1.9297 - regression_loss: 1.4391 - classification_loss: 0.4906\n",
      "Epoch 15/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 0.9534 - regression_loss: 0.6732 - classification_loss: 0.2802\n",
      "  2/500 [..............................] - ETA: 4:12 - loss: 1.3178 - regression_loss: 0.9747 - classification_loss: 0.3431\n",
      "  3/500 [..............................] - ETA: 4:52 - loss: 1.4403 - regression_loss: 1.1010 - classification_loss: 0.3393\n",
      "  4/500 [..............................] - ETA: 6:14 - loss: 1.3840 - regression_loss: 1.0200 - classification_loss: 0.3640\n",
      "  5/500 [..............................] - ETA: 6:55 - loss: 1.4957 - regression_loss: 1.1151 - classification_loss: 0.3806\n",
      "  6/500 [..............................] - ETA: 7:06 - loss: 1.7155 - regression_loss: 1.2648 - classification_loss: 0.4507\n",
      "  7/500 [..............................] - ETA: 7:29 - loss: 1.8128 - regression_loss: 1.3021 - classification_loss: 0.5107\n",
      "  8/500 [..............................] - ETA: 8:27 - loss: 1.8268 - regression_loss: 1.3155 - classification_loss: 0.5113\n",
      "  9/500 [..............................] - ETA: 8:24 - loss: 1.8278 - regression_loss: 1.3073 - classification_loss: 0.5205\n",
      " 10/500 [..............................] - ETA: 8:20 - loss: 1.8328 - regression_loss: 1.3221 - classification_loss: 0.5108\n",
      " 11/500 [..............................] - ETA: 8:19 - loss: 1.8169 - regression_loss: 1.3214 - classification_loss: 0.4956\n",
      " 12/500 [..............................] - ETA: 8:16 - loss: 1.8761 - regression_loss: 1.3644 - classification_loss: 0.5117\n",
      " 13/500 [..............................] - ETA: 8:18 - loss: 1.9217 - regression_loss: 1.3995 - classification_loss: 0.5222\n",
      " 14/500 [..............................] - ETA: 8:19 - loss: 1.9476 - regression_loss: 1.4177 - classification_loss: 0.5298\n",
      " 15/500 [..............................] - ETA: 8:17 - loss: 2.0075 - regression_loss: 1.4708 - classification_loss: 0.5367\n",
      " 16/500 [..............................] - ETA: 8:17 - loss: 2.0157 - regression_loss: 1.4801 - classification_loss: 0.5356\n",
      " 17/500 [>.............................] - ETA: 8:13 - loss: 1.9854 - regression_loss: 1.4627 - classification_loss: 0.5226\n",
      " 18/500 [>.............................] - ETA: 8:14 - loss: 1.9677 - regression_loss: 1.4465 - classification_loss: 0.5212\n",
      " 19/500 [>.............................] - ETA: 8:14 - loss: 2.0187 - regression_loss: 1.4815 - classification_loss: 0.5372\n",
      " 20/500 [>.............................] - ETA: 8:14 - loss: 1.9851 - regression_loss: 1.4617 - classification_loss: 0.5235\n",
      " 21/500 [>.............................] - ETA: 8:10 - loss: 2.0007 - regression_loss: 1.4730 - classification_loss: 0.5277\n",
      " 22/500 [>.............................] - ETA: 8:09 - loss: 1.9623 - regression_loss: 1.4408 - classification_loss: 0.5215\n",
      " 23/500 [>.............................] - ETA: 8:10 - loss: 1.9659 - regression_loss: 1.4450 - classification_loss: 0.5208\n",
      " 24/500 [>.............................] - ETA: 8:08 - loss: 1.9248 - regression_loss: 1.4159 - classification_loss: 0.5089\n",
      " 25/500 [>.............................] - ETA: 8:10 - loss: 1.8857 - regression_loss: 1.3865 - classification_loss: 0.4992\n",
      " 26/500 [>.............................] - ETA: 8:10 - loss: 1.8895 - regression_loss: 1.3897 - classification_loss: 0.4998\n",
      " 27/500 [>.............................] - ETA: 8:08 - loss: 1.9040 - regression_loss: 1.3948 - classification_loss: 0.5092\n",
      " 28/500 [>.............................] - ETA: 8:08 - loss: 1.9409 - regression_loss: 1.4134 - classification_loss: 0.5274\n",
      " 29/500 [>.............................] - ETA: 8:06 - loss: 1.9478 - regression_loss: 1.4175 - classification_loss: 0.5303\n",
      " 30/500 [>.............................] - ETA: 8:04 - loss: 1.9443 - regression_loss: 1.4206 - classification_loss: 0.5238\n",
      " 31/500 [>.............................] - ETA: 8:02 - loss: 1.9334 - regression_loss: 1.4167 - classification_loss: 0.5167\n",
      " 32/500 [>.............................] - ETA: 8:02 - loss: 1.9382 - regression_loss: 1.4207 - classification_loss: 0.5175\n",
      " 33/500 [>.............................] - ETA: 8:02 - loss: 1.9754 - regression_loss: 1.4464 - classification_loss: 0.5290\n",
      " 34/500 [=>............................] - ETA: 8:00 - loss: 1.9955 - regression_loss: 1.4634 - classification_loss: 0.5320\n",
      " 35/500 [=>............................] - ETA: 8:00 - loss: 1.9980 - regression_loss: 1.4667 - classification_loss: 0.5313\n",
      " 36/500 [=>............................] - ETA: 8:00 - loss: 2.0094 - regression_loss: 1.4729 - classification_loss: 0.5365\n",
      " 37/500 [=>............................] - ETA: 7:58 - loss: 1.9909 - regression_loss: 1.4626 - classification_loss: 0.5282\n",
      " 38/500 [=>............................] - ETA: 7:58 - loss: 1.9697 - regression_loss: 1.4479 - classification_loss: 0.5217\n",
      " 39/500 [=>............................] - ETA: 7:56 - loss: 1.9536 - regression_loss: 1.4383 - classification_loss: 0.5153\n",
      " 40/500 [=>............................] - ETA: 7:54 - loss: 1.9729 - regression_loss: 1.4520 - classification_loss: 0.5209\n",
      " 41/500 [=>............................] - ETA: 7:53 - loss: 1.9664 - regression_loss: 1.4450 - classification_loss: 0.5213\n",
      " 42/500 [=>............................] - ETA: 7:52 - loss: 1.9749 - regression_loss: 1.4535 - classification_loss: 0.5214\n",
      " 43/500 [=>............................] - ETA: 7:51 - loss: 1.9931 - regression_loss: 1.4663 - classification_loss: 0.5269\n",
      " 44/500 [=>............................] - ETA: 7:49 - loss: 2.0108 - regression_loss: 1.4780 - classification_loss: 0.5329\n",
      " 45/500 [=>............................] - ETA: 7:48 - loss: 2.0008 - regression_loss: 1.4703 - classification_loss: 0.5305\n",
      " 46/500 [=>............................] - ETA: 7:47 - loss: 1.9887 - regression_loss: 1.4632 - classification_loss: 0.5255\n",
      " 47/500 [=>............................] - ETA: 7:46 - loss: 1.9855 - regression_loss: 1.4597 - classification_loss: 0.5258\n",
      " 48/500 [=>............................] - ETA: 7:45 - loss: 1.9811 - regression_loss: 1.4599 - classification_loss: 0.5213\n",
      " 49/500 [=>............................] - ETA: 7:44 - loss: 1.9656 - regression_loss: 1.4489 - classification_loss: 0.5167\n",
      " 50/500 [==>...........................] - ETA: 7:44 - loss: 1.9725 - regression_loss: 1.4533 - classification_loss: 0.5192\n",
      " 51/500 [==>...........................] - ETA: 7:43 - loss: 1.9702 - regression_loss: 1.4526 - classification_loss: 0.5177\n",
      " 52/500 [==>...........................] - ETA: 7:42 - loss: 1.9668 - regression_loss: 1.4510 - classification_loss: 0.5159\n",
      " 53/500 [==>...........................] - ETA: 7:41 - loss: 1.9741 - regression_loss: 1.4604 - classification_loss: 0.5137\n",
      " 54/500 [==>...........................] - ETA: 7:40 - loss: 1.9833 - regression_loss: 1.4641 - classification_loss: 0.5192\n",
      " 55/500 [==>...........................] - ETA: 7:38 - loss: 1.9684 - regression_loss: 1.4533 - classification_loss: 0.5150\n",
      " 56/500 [==>...........................] - ETA: 7:37 - loss: 1.9623 - regression_loss: 1.4517 - classification_loss: 0.5106\n",
      " 57/500 [==>...........................] - ETA: 7:35 - loss: 1.9649 - regression_loss: 1.4537 - classification_loss: 0.5112\n",
      " 58/500 [==>...........................] - ETA: 7:34 - loss: 1.9573 - regression_loss: 1.4471 - classification_loss: 0.5103\n",
      " 59/500 [==>...........................] - ETA: 7:31 - loss: 1.9602 - regression_loss: 1.4505 - classification_loss: 0.5097\n",
      " 60/500 [==>...........................] - ETA: 7:31 - loss: 1.9602 - regression_loss: 1.4521 - classification_loss: 0.5081\n",
      " 61/500 [==>...........................] - ETA: 7:29 - loss: 1.9549 - regression_loss: 1.4501 - classification_loss: 0.5049\n",
      " 62/500 [==>...........................] - ETA: 7:28 - loss: 1.9580 - regression_loss: 1.4532 - classification_loss: 0.5048\n",
      " 63/500 [==>...........................] - ETA: 7:27 - loss: 1.9617 - regression_loss: 1.4549 - classification_loss: 0.5068\n",
      " 64/500 [==>...........................] - ETA: 7:26 - loss: 1.9664 - regression_loss: 1.4576 - classification_loss: 0.5089\n",
      " 65/500 [==>...........................] - ETA: 7:25 - loss: 1.9648 - regression_loss: 1.4570 - classification_loss: 0.5078\n",
      " 66/500 [==>...........................] - ETA: 7:25 - loss: 1.9680 - regression_loss: 1.4560 - classification_loss: 0.5120\n",
      " 67/500 [===>..........................] - ETA: 7:23 - loss: 1.9676 - regression_loss: 1.4568 - classification_loss: 0.5107\n",
      " 68/500 [===>..........................] - ETA: 7:23 - loss: 1.9683 - regression_loss: 1.4593 - classification_loss: 0.5090\n",
      " 69/500 [===>..........................] - ETA: 7:22 - loss: 1.9691 - regression_loss: 1.4597 - classification_loss: 0.5094\n",
      " 70/500 [===>..........................] - ETA: 7:21 - loss: 1.9729 - regression_loss: 1.4615 - classification_loss: 0.5114\n",
      " 71/500 [===>..........................] - ETA: 7:20 - loss: 1.9688 - regression_loss: 1.4587 - classification_loss: 0.5101\n",
      " 72/500 [===>..........................] - ETA: 7:19 - loss: 1.9689 - regression_loss: 1.4598 - classification_loss: 0.5092\n",
      " 73/500 [===>..........................] - ETA: 7:18 - loss: 1.9705 - regression_loss: 1.4609 - classification_loss: 0.5096\n",
      " 74/500 [===>..........................] - ETA: 7:17 - loss: 1.9614 - regression_loss: 1.4543 - classification_loss: 0.5071\n",
      " 75/500 [===>..........................] - ETA: 7:15 - loss: 1.9585 - regression_loss: 1.4504 - classification_loss: 0.5082\n",
      " 76/500 [===>..........................] - ETA: 7:14 - loss: 1.9557 - regression_loss: 1.4479 - classification_loss: 0.5079\n",
      " 77/500 [===>..........................] - ETA: 7:13 - loss: 1.9467 - regression_loss: 1.4414 - classification_loss: 0.5053\n",
      " 78/500 [===>..........................] - ETA: 7:12 - loss: 1.9457 - regression_loss: 1.4431 - classification_loss: 0.5026\n",
      " 79/500 [===>..........................] - ETA: 7:10 - loss: 1.9454 - regression_loss: 1.4447 - classification_loss: 0.5008\n",
      " 80/500 [===>..........................] - ETA: 7:09 - loss: 1.9404 - regression_loss: 1.4413 - classification_loss: 0.4991\n",
      " 81/500 [===>..........................] - ETA: 7:08 - loss: 1.9475 - regression_loss: 1.4483 - classification_loss: 0.4992\n",
      " 82/500 [===>..........................] - ETA: 7:07 - loss: 1.9522 - regression_loss: 1.4513 - classification_loss: 0.5009\n",
      " 83/500 [===>..........................] - ETA: 7:05 - loss: 1.9473 - regression_loss: 1.4484 - classification_loss: 0.4988\n",
      " 84/500 [====>.........................] - ETA: 7:05 - loss: 1.9396 - regression_loss: 1.4423 - classification_loss: 0.4973\n",
      " 85/500 [====>.........................] - ETA: 7:04 - loss: 1.9343 - regression_loss: 1.4385 - classification_loss: 0.4958\n",
      " 86/500 [====>.........................] - ETA: 7:03 - loss: 1.9326 - regression_loss: 1.4385 - classification_loss: 0.4941\n",
      " 87/500 [====>.........................] - ETA: 7:01 - loss: 1.9345 - regression_loss: 1.4404 - classification_loss: 0.4941\n",
      " 88/500 [====>.........................] - ETA: 7:01 - loss: 1.9263 - regression_loss: 1.4346 - classification_loss: 0.4917\n",
      " 89/500 [====>.........................] - ETA: 7:00 - loss: 1.9238 - regression_loss: 1.4333 - classification_loss: 0.4905\n",
      " 90/500 [====>.........................] - ETA: 6:59 - loss: 1.9298 - regression_loss: 1.4364 - classification_loss: 0.4934\n",
      " 91/500 [====>.........................] - ETA: 6:58 - loss: 1.9362 - regression_loss: 1.4419 - classification_loss: 0.4942\n",
      " 92/500 [====>.........................] - ETA: 6:58 - loss: 1.9481 - regression_loss: 1.4487 - classification_loss: 0.4994\n",
      " 93/500 [====>.........................] - ETA: 6:57 - loss: 1.9543 - regression_loss: 1.4539 - classification_loss: 0.5004\n",
      " 94/500 [====>.........................] - ETA: 6:56 - loss: 1.9481 - regression_loss: 1.4505 - classification_loss: 0.4977\n",
      " 95/500 [====>.........................] - ETA: 6:54 - loss: 1.9448 - regression_loss: 1.4484 - classification_loss: 0.4964\n",
      " 96/500 [====>.........................] - ETA: 6:53 - loss: 1.9444 - regression_loss: 1.4495 - classification_loss: 0.4949\n",
      " 97/500 [====>.........................] - ETA: 6:53 - loss: 1.9583 - regression_loss: 1.4588 - classification_loss: 0.4995\n",
      " 98/500 [====>.........................] - ETA: 6:52 - loss: 1.9657 - regression_loss: 1.4639 - classification_loss: 0.5018\n",
      " 99/500 [====>.........................] - ETA: 6:51 - loss: 1.9680 - regression_loss: 1.4669 - classification_loss: 0.5011\n",
      "100/500 [=====>........................] - ETA: 6:51 - loss: 1.9644 - regression_loss: 1.4644 - classification_loss: 0.5000\n",
      "101/500 [=====>........................] - ETA: 6:49 - loss: 1.9702 - regression_loss: 1.4682 - classification_loss: 0.5021\n",
      "102/500 [=====>........................] - ETA: 6:49 - loss: 1.9757 - regression_loss: 1.4722 - classification_loss: 0.5035\n",
      "103/500 [=====>........................] - ETA: 6:47 - loss: 1.9761 - regression_loss: 1.4728 - classification_loss: 0.5033\n",
      "104/500 [=====>........................] - ETA: 6:46 - loss: 1.9658 - regression_loss: 1.4653 - classification_loss: 0.5004\n",
      "105/500 [=====>........................] - ETA: 6:44 - loss: 1.9672 - regression_loss: 1.4678 - classification_loss: 0.4994\n",
      "106/500 [=====>........................] - ETA: 6:44 - loss: 1.9693 - regression_loss: 1.4695 - classification_loss: 0.4998\n",
      "107/500 [=====>........................] - ETA: 6:43 - loss: 1.9639 - regression_loss: 1.4653 - classification_loss: 0.4986\n",
      "108/500 [=====>........................] - ETA: 6:42 - loss: 1.9614 - regression_loss: 1.4641 - classification_loss: 0.4973\n",
      "109/500 [=====>........................] - ETA: 6:41 - loss: 1.9559 - regression_loss: 1.4596 - classification_loss: 0.4963\n",
      "110/500 [=====>........................] - ETA: 6:40 - loss: 1.9554 - regression_loss: 1.4592 - classification_loss: 0.4963\n",
      "111/500 [=====>........................] - ETA: 6:39 - loss: 1.9581 - regression_loss: 1.4584 - classification_loss: 0.4997\n",
      "112/500 [=====>........................] - ETA: 6:38 - loss: 1.9524 - regression_loss: 1.4547 - classification_loss: 0.4977\n",
      "113/500 [=====>........................] - ETA: 6:37 - loss: 1.9483 - regression_loss: 1.4520 - classification_loss: 0.4964\n",
      "114/500 [=====>........................] - ETA: 6:35 - loss: 1.9505 - regression_loss: 1.4524 - classification_loss: 0.4981\n",
      "115/500 [=====>........................] - ETA: 6:35 - loss: 1.9599 - regression_loss: 1.4582 - classification_loss: 0.5017\n",
      "116/500 [=====>........................] - ETA: 6:33 - loss: 1.9604 - regression_loss: 1.4586 - classification_loss: 0.5017\n",
      "117/500 [======>.......................] - ETA: 6:33 - loss: 1.9530 - regression_loss: 1.4523 - classification_loss: 0.5006\n",
      "118/500 [======>.......................] - ETA: 6:32 - loss: 1.9502 - regression_loss: 1.4508 - classification_loss: 0.4993\n",
      "119/500 [======>.......................] - ETA: 6:31 - loss: 1.9519 - regression_loss: 1.4520 - classification_loss: 0.4999\n",
      "120/500 [======>.......................] - ETA: 6:30 - loss: 1.9535 - regression_loss: 1.4529 - classification_loss: 0.5005\n",
      "121/500 [======>.......................] - ETA: 6:29 - loss: 1.9559 - regression_loss: 1.4560 - classification_loss: 0.4998\n",
      "122/500 [======>.......................] - ETA: 6:27 - loss: 1.9539 - regression_loss: 1.4540 - classification_loss: 0.4999\n",
      "123/500 [======>.......................] - ETA: 6:27 - loss: 1.9536 - regression_loss: 1.4543 - classification_loss: 0.4994\n",
      "124/500 [======>.......................] - ETA: 6:26 - loss: 1.9612 - regression_loss: 1.4576 - classification_loss: 0.5036\n",
      "125/500 [======>.......................] - ETA: 6:25 - loss: 1.9660 - regression_loss: 1.4608 - classification_loss: 0.5051\n",
      "126/500 [======>.......................] - ETA: 6:23 - loss: 1.9636 - regression_loss: 1.4602 - classification_loss: 0.5034\n",
      "127/500 [======>.......................] - ETA: 6:22 - loss: 1.9622 - regression_loss: 1.4592 - classification_loss: 0.5030\n",
      "128/500 [======>.......................] - ETA: 6:21 - loss: 1.9601 - regression_loss: 1.4588 - classification_loss: 0.5012\n",
      "129/500 [======>.......................] - ETA: 6:20 - loss: 1.9539 - regression_loss: 1.4545 - classification_loss: 0.4994\n",
      "130/500 [======>.......................] - ETA: 6:19 - loss: 1.9495 - regression_loss: 1.4516 - classification_loss: 0.4978\n",
      "131/500 [======>.......................] - ETA: 6:18 - loss: 1.9564 - regression_loss: 1.4579 - classification_loss: 0.4985\n",
      "132/500 [======>.......................] - ETA: 6:17 - loss: 1.9526 - regression_loss: 1.4555 - classification_loss: 0.4971\n",
      "133/500 [======>.......................] - ETA: 6:15 - loss: 1.9507 - regression_loss: 1.4531 - classification_loss: 0.4976\n",
      "134/500 [=======>......................] - ETA: 6:15 - loss: 1.9480 - regression_loss: 1.4514 - classification_loss: 0.4966\n",
      "135/500 [=======>......................] - ETA: 6:14 - loss: 1.9495 - regression_loss: 1.4523 - classification_loss: 0.4973\n",
      "136/500 [=======>......................] - ETA: 6:13 - loss: 1.9466 - regression_loss: 1.4496 - classification_loss: 0.4970\n",
      "137/500 [=======>......................] - ETA: 6:12 - loss: 1.9503 - regression_loss: 1.4519 - classification_loss: 0.4984\n",
      "138/500 [=======>......................] - ETA: 6:11 - loss: 1.9477 - regression_loss: 1.4494 - classification_loss: 0.4984\n",
      "139/500 [=======>......................] - ETA: 6:10 - loss: 1.9497 - regression_loss: 1.4503 - classification_loss: 0.4995\n",
      "140/500 [=======>......................] - ETA: 6:08 - loss: 1.9484 - regression_loss: 1.4494 - classification_loss: 0.4990\n",
      "141/500 [=======>......................] - ETA: 6:08 - loss: 1.9567 - regression_loss: 1.4554 - classification_loss: 0.5013\n",
      "142/500 [=======>......................] - ETA: 6:07 - loss: 1.9502 - regression_loss: 1.4505 - classification_loss: 0.4996\n",
      "143/500 [=======>......................] - ETA: 6:06 - loss: 1.9453 - regression_loss: 1.4475 - classification_loss: 0.4978\n",
      "144/500 [=======>......................] - ETA: 6:05 - loss: 1.9538 - regression_loss: 1.4535 - classification_loss: 0.5003\n",
      "145/500 [=======>......................] - ETA: 6:04 - loss: 1.9581 - regression_loss: 1.4567 - classification_loss: 0.5014\n",
      "146/500 [=======>......................] - ETA: 6:03 - loss: 1.9554 - regression_loss: 1.4548 - classification_loss: 0.5006\n",
      "147/500 [=======>......................] - ETA: 6:02 - loss: 1.9500 - regression_loss: 1.4507 - classification_loss: 0.4994\n",
      "148/500 [=======>......................] - ETA: 6:01 - loss: 1.9514 - regression_loss: 1.4520 - classification_loss: 0.4994\n",
      "149/500 [=======>......................] - ETA: 5:59 - loss: 1.9489 - regression_loss: 1.4499 - classification_loss: 0.4990\n",
      "150/500 [========>.....................] - ETA: 5:59 - loss: 1.9493 - regression_loss: 1.4499 - classification_loss: 0.4994\n",
      "151/500 [========>.....................] - ETA: 5:58 - loss: 1.9485 - regression_loss: 1.4496 - classification_loss: 0.4989\n",
      "152/500 [========>.....................] - ETA: 5:57 - loss: 1.9464 - regression_loss: 1.4478 - classification_loss: 0.4986\n",
      "153/500 [========>.....................] - ETA: 5:56 - loss: 1.9413 - regression_loss: 1.4438 - classification_loss: 0.4975\n",
      "154/500 [========>.....................] - ETA: 5:55 - loss: 1.9360 - regression_loss: 1.4399 - classification_loss: 0.4961\n",
      "155/500 [========>.....................] - ETA: 5:54 - loss: 1.9373 - regression_loss: 1.4414 - classification_loss: 0.4959\n",
      "156/500 [========>.....................] - ETA: 5:53 - loss: 1.9463 - regression_loss: 1.4471 - classification_loss: 0.4991\n",
      "157/500 [========>.....................] - ETA: 5:52 - loss: 1.9448 - regression_loss: 1.4460 - classification_loss: 0.4988\n",
      "158/500 [========>.....................] - ETA: 5:51 - loss: 1.9505 - regression_loss: 1.4498 - classification_loss: 0.5008\n",
      "159/500 [========>.....................] - ETA: 5:50 - loss: 1.9485 - regression_loss: 1.4481 - classification_loss: 0.5004\n",
      "160/500 [========>.....................] - ETA: 5:48 - loss: 1.9430 - regression_loss: 1.4443 - classification_loss: 0.4987\n",
      "161/500 [========>.....................] - ETA: 5:48 - loss: 1.9455 - regression_loss: 1.4460 - classification_loss: 0.4995\n",
      "162/500 [========>.....................] - ETA: 5:47 - loss: 1.9438 - regression_loss: 1.4452 - classification_loss: 0.4986\n",
      "163/500 [========>.....................] - ETA: 5:46 - loss: 1.9424 - regression_loss: 1.4442 - classification_loss: 0.4982\n",
      "164/500 [========>.....................] - ETA: 5:45 - loss: 1.9469 - regression_loss: 1.4461 - classification_loss: 0.5009\n",
      "165/500 [========>.....................] - ETA: 5:44 - loss: 1.9448 - regression_loss: 1.4450 - classification_loss: 0.4997\n",
      "166/500 [========>.....................] - ETA: 5:43 - loss: 1.9465 - regression_loss: 1.4469 - classification_loss: 0.4996\n",
      "167/500 [=========>....................] - ETA: 5:41 - loss: 1.9418 - regression_loss: 1.4433 - classification_loss: 0.4985\n",
      "168/500 [=========>....................] - ETA: 5:40 - loss: 1.9418 - regression_loss: 1.4440 - classification_loss: 0.4978\n",
      "169/500 [=========>....................] - ETA: 5:39 - loss: 1.9403 - regression_loss: 1.4433 - classification_loss: 0.4970\n",
      "170/500 [=========>....................] - ETA: 5:38 - loss: 1.9358 - regression_loss: 1.4394 - classification_loss: 0.4964\n",
      "171/500 [=========>....................] - ETA: 5:37 - loss: 1.9333 - regression_loss: 1.4368 - classification_loss: 0.4965\n",
      "172/500 [=========>....................] - ETA: 5:36 - loss: 1.9304 - regression_loss: 1.4345 - classification_loss: 0.4959\n",
      "173/500 [=========>....................] - ETA: 5:35 - loss: 1.9308 - regression_loss: 1.4352 - classification_loss: 0.4956\n",
      "174/500 [=========>....................] - ETA: 5:34 - loss: 1.9353 - regression_loss: 1.4391 - classification_loss: 0.4963\n",
      "175/500 [=========>....................] - ETA: 5:33 - loss: 1.9332 - regression_loss: 1.4379 - classification_loss: 0.4953\n",
      "176/500 [=========>....................] - ETA: 5:32 - loss: 1.9302 - regression_loss: 1.4363 - classification_loss: 0.4939\n",
      "177/500 [=========>....................] - ETA: 5:31 - loss: 1.9316 - regression_loss: 1.4378 - classification_loss: 0.4938\n",
      "178/500 [=========>....................] - ETA: 5:30 - loss: 1.9300 - regression_loss: 1.4370 - classification_loss: 0.4929\n",
      "179/500 [=========>....................] - ETA: 5:29 - loss: 1.9318 - regression_loss: 1.4387 - classification_loss: 0.4931\n",
      "180/500 [=========>....................] - ETA: 5:29 - loss: 1.9349 - regression_loss: 1.4403 - classification_loss: 0.4946\n",
      "181/500 [=========>....................] - ETA: 5:27 - loss: 1.9393 - regression_loss: 1.4442 - classification_loss: 0.4950\n",
      "182/500 [=========>....................] - ETA: 5:26 - loss: 1.9354 - regression_loss: 1.4414 - classification_loss: 0.4940\n",
      "183/500 [=========>....................] - ETA: 5:25 - loss: 1.9335 - regression_loss: 1.4407 - classification_loss: 0.4928\n",
      "184/500 [==========>...................] - ETA: 5:24 - loss: 1.9323 - regression_loss: 1.4409 - classification_loss: 0.4914\n",
      "185/500 [==========>...................] - ETA: 5:23 - loss: 1.9286 - regression_loss: 1.4376 - classification_loss: 0.4910\n",
      "186/500 [==========>...................] - ETA: 5:22 - loss: 1.9298 - regression_loss: 1.4380 - classification_loss: 0.4918\n",
      "187/500 [==========>...................] - ETA: 5:21 - loss: 1.9259 - regression_loss: 1.4352 - classification_loss: 0.4906\n",
      "188/500 [==========>...................] - ETA: 5:20 - loss: 1.9255 - regression_loss: 1.4350 - classification_loss: 0.4905\n",
      "189/500 [==========>...................] - ETA: 5:19 - loss: 1.9229 - regression_loss: 1.4331 - classification_loss: 0.4898\n",
      "190/500 [==========>...................] - ETA: 5:18 - loss: 1.9241 - regression_loss: 1.4337 - classification_loss: 0.4904\n",
      "191/500 [==========>...................] - ETA: 5:17 - loss: 1.9241 - regression_loss: 1.4337 - classification_loss: 0.4904\n",
      "192/500 [==========>...................] - ETA: 5:16 - loss: 1.9287 - regression_loss: 1.4375 - classification_loss: 0.4911\n",
      "193/500 [==========>...................] - ETA: 5:15 - loss: 1.9309 - regression_loss: 1.4401 - classification_loss: 0.4908\n",
      "194/500 [==========>...................] - ETA: 5:14 - loss: 1.9277 - regression_loss: 1.4377 - classification_loss: 0.4899\n",
      "195/500 [==========>...................] - ETA: 5:13 - loss: 1.9271 - regression_loss: 1.4368 - classification_loss: 0.4903\n",
      "196/500 [==========>...................] - ETA: 5:12 - loss: 1.9341 - regression_loss: 1.4416 - classification_loss: 0.4925\n",
      "197/500 [==========>...................] - ETA: 5:11 - loss: 1.9353 - regression_loss: 1.4424 - classification_loss: 0.4928\n",
      "198/500 [==========>...................] - ETA: 5:10 - loss: 1.9381 - regression_loss: 1.4439 - classification_loss: 0.4942\n",
      "199/500 [==========>...................] - ETA: 5:09 - loss: 1.9370 - regression_loss: 1.4430 - classification_loss: 0.4939\n",
      "200/500 [===========>..................] - ETA: 5:08 - loss: 1.9355 - regression_loss: 1.4421 - classification_loss: 0.4934\n",
      "201/500 [===========>..................] - ETA: 5:06 - loss: 1.9347 - regression_loss: 1.4416 - classification_loss: 0.4930\n",
      "202/500 [===========>..................] - ETA: 5:05 - loss: 1.9311 - regression_loss: 1.4386 - classification_loss: 0.4925\n",
      "203/500 [===========>..................] - ETA: 5:04 - loss: 1.9281 - regression_loss: 1.4368 - classification_loss: 0.4913\n",
      "204/500 [===========>..................] - ETA: 5:03 - loss: 1.9278 - regression_loss: 1.4364 - classification_loss: 0.4913\n",
      "205/500 [===========>..................] - ETA: 5:02 - loss: 1.9311 - regression_loss: 1.4388 - classification_loss: 0.4923\n",
      "206/500 [===========>..................] - ETA: 5:01 - loss: 1.9341 - regression_loss: 1.4410 - classification_loss: 0.4931\n",
      "207/500 [===========>..................] - ETA: 5:00 - loss: 1.9361 - regression_loss: 1.4424 - classification_loss: 0.4937\n",
      "208/500 [===========>..................] - ETA: 4:59 - loss: 1.9332 - regression_loss: 1.4403 - classification_loss: 0.4929\n",
      "209/500 [===========>..................] - ETA: 4:58 - loss: 1.9342 - regression_loss: 1.4415 - classification_loss: 0.4928\n",
      "210/500 [===========>..................] - ETA: 4:57 - loss: 1.9389 - regression_loss: 1.4447 - classification_loss: 0.4943\n",
      "211/500 [===========>..................] - ETA: 4:56 - loss: 1.9417 - regression_loss: 1.4472 - classification_loss: 0.4945\n",
      "212/500 [===========>..................] - ETA: 4:55 - loss: 1.9410 - regression_loss: 1.4470 - classification_loss: 0.4940\n",
      "213/500 [===========>..................] - ETA: 4:54 - loss: 1.9361 - regression_loss: 1.4429 - classification_loss: 0.4932\n",
      "214/500 [===========>..................] - ETA: 4:53 - loss: 1.9380 - regression_loss: 1.4449 - classification_loss: 0.4931\n",
      "215/500 [===========>..................] - ETA: 4:52 - loss: 1.9387 - regression_loss: 1.4458 - classification_loss: 0.4929\n",
      "216/500 [===========>..................] - ETA: 4:51 - loss: 1.9416 - regression_loss: 1.4478 - classification_loss: 0.4937\n",
      "217/500 [============>.................] - ETA: 4:50 - loss: 1.9411 - regression_loss: 1.4477 - classification_loss: 0.4934\n",
      "218/500 [============>.................] - ETA: 4:49 - loss: 1.9418 - regression_loss: 1.4482 - classification_loss: 0.4936\n",
      "219/500 [============>.................] - ETA: 4:48 - loss: 1.9425 - regression_loss: 1.4484 - classification_loss: 0.4941\n",
      "220/500 [============>.................] - ETA: 4:47 - loss: 1.9416 - regression_loss: 1.4484 - classification_loss: 0.4931\n",
      "221/500 [============>.................] - ETA: 4:49 - loss: 1.9410 - regression_loss: 1.4485 - classification_loss: 0.4925\n",
      "222/500 [============>.................] - ETA: 4:48 - loss: 1.9408 - regression_loss: 1.4487 - classification_loss: 0.4921\n",
      "223/500 [============>.................] - ETA: 4:47 - loss: 1.9438 - regression_loss: 1.4512 - classification_loss: 0.4926\n",
      "224/500 [============>.................] - ETA: 4:46 - loss: 1.9420 - regression_loss: 1.4501 - classification_loss: 0.4919\n",
      "225/500 [============>.................] - ETA: 4:45 - loss: 1.9422 - regression_loss: 1.4500 - classification_loss: 0.4922\n",
      "226/500 [============>.................] - ETA: 4:44 - loss: 1.9414 - regression_loss: 1.4497 - classification_loss: 0.4917\n",
      "227/500 [============>.................] - ETA: 4:43 - loss: 1.9418 - regression_loss: 1.4502 - classification_loss: 0.4916\n",
      "228/500 [============>.................] - ETA: 4:42 - loss: 1.9379 - regression_loss: 1.4474 - classification_loss: 0.4905\n",
      "229/500 [============>.................] - ETA: 4:41 - loss: 1.9372 - regression_loss: 1.4467 - classification_loss: 0.4905\n",
      "230/500 [============>.................] - ETA: 4:40 - loss: 1.9374 - regression_loss: 1.4471 - classification_loss: 0.4903\n",
      "231/500 [============>.................] - ETA: 4:39 - loss: 1.9371 - regression_loss: 1.4468 - classification_loss: 0.4904\n",
      "232/500 [============>.................] - ETA: 4:40 - loss: 1.9366 - regression_loss: 1.4464 - classification_loss: 0.4901\n",
      "233/500 [============>.................] - ETA: 4:39 - loss: 1.9379 - regression_loss: 1.4473 - classification_loss: 0.4906\n",
      "234/500 [=============>................] - ETA: 4:38 - loss: 1.9365 - regression_loss: 1.4464 - classification_loss: 0.4901\n",
      "235/500 [=============>................] - ETA: 4:37 - loss: 1.9367 - regression_loss: 1.4467 - classification_loss: 0.4900\n",
      "236/500 [=============>................] - ETA: 4:36 - loss: 1.9381 - regression_loss: 1.4478 - classification_loss: 0.4903\n",
      "237/500 [=============>................] - ETA: 4:35 - loss: 1.9403 - regression_loss: 1.4496 - classification_loss: 0.4908\n",
      "238/500 [=============>................] - ETA: 4:33 - loss: 1.9395 - regression_loss: 1.4490 - classification_loss: 0.4905\n",
      "239/500 [=============>................] - ETA: 4:32 - loss: 1.9403 - regression_loss: 1.4499 - classification_loss: 0.4904\n",
      "240/500 [=============>................] - ETA: 4:31 - loss: 1.9367 - regression_loss: 1.4466 - classification_loss: 0.4900\n",
      "241/500 [=============>................] - ETA: 4:30 - loss: 1.9344 - regression_loss: 1.4453 - classification_loss: 0.4891\n",
      "242/500 [=============>................] - ETA: 4:29 - loss: 1.9344 - regression_loss: 1.4455 - classification_loss: 0.4889\n",
      "243/500 [=============>................] - ETA: 4:28 - loss: 1.9333 - regression_loss: 1.4448 - classification_loss: 0.4885\n",
      "244/500 [=============>................] - ETA: 4:27 - loss: 1.9319 - regression_loss: 1.4441 - classification_loss: 0.4878\n",
      "245/500 [=============>................] - ETA: 4:26 - loss: 1.9331 - regression_loss: 1.4453 - classification_loss: 0.4878\n",
      "246/500 [=============>................] - ETA: 4:25 - loss: 1.9319 - regression_loss: 1.4450 - classification_loss: 0.4869\n",
      "247/500 [=============>................] - ETA: 4:24 - loss: 1.9306 - regression_loss: 1.4436 - classification_loss: 0.4870\n",
      "248/500 [=============>................] - ETA: 4:23 - loss: 1.9309 - regression_loss: 1.4439 - classification_loss: 0.4870\n",
      "249/500 [=============>................] - ETA: 4:22 - loss: 1.9333 - regression_loss: 1.4452 - classification_loss: 0.4881\n",
      "250/500 [==============>...............] - ETA: 4:21 - loss: 1.9381 - regression_loss: 1.4480 - classification_loss: 0.4901\n",
      "251/500 [==============>...............] - ETA: 4:20 - loss: 1.9400 - regression_loss: 1.4493 - classification_loss: 0.4907\n",
      "252/500 [==============>...............] - ETA: 4:19 - loss: 1.9367 - regression_loss: 1.4468 - classification_loss: 0.4899\n",
      "253/500 [==============>...............] - ETA: 4:18 - loss: 1.9387 - regression_loss: 1.4480 - classification_loss: 0.4907\n",
      "254/500 [==============>...............] - ETA: 4:17 - loss: 1.9386 - regression_loss: 1.4476 - classification_loss: 0.4910\n",
      "255/500 [==============>...............] - ETA: 4:16 - loss: 1.9356 - regression_loss: 1.4457 - classification_loss: 0.4900\n",
      "256/500 [==============>...............] - ETA: 4:14 - loss: 1.9364 - regression_loss: 1.4464 - classification_loss: 0.4900\n",
      "257/500 [==============>...............] - ETA: 4:13 - loss: 1.9354 - regression_loss: 1.4458 - classification_loss: 0.4896\n",
      "258/500 [==============>...............] - ETA: 4:12 - loss: 1.9332 - regression_loss: 1.4439 - classification_loss: 0.4893\n",
      "259/500 [==============>...............] - ETA: 4:11 - loss: 1.9337 - regression_loss: 1.4445 - classification_loss: 0.4893\n",
      "260/500 [==============>...............] - ETA: 4:10 - loss: 1.9344 - regression_loss: 1.4446 - classification_loss: 0.4897\n",
      "261/500 [==============>...............] - ETA: 4:09 - loss: 1.9354 - regression_loss: 1.4458 - classification_loss: 0.4896\n",
      "262/500 [==============>...............] - ETA: 4:08 - loss: 1.9352 - regression_loss: 1.4458 - classification_loss: 0.4894\n",
      "263/500 [==============>...............] - ETA: 4:07 - loss: 1.9355 - regression_loss: 1.4457 - classification_loss: 0.4899\n",
      "264/500 [==============>...............] - ETA: 4:06 - loss: 1.9343 - regression_loss: 1.4445 - classification_loss: 0.4898\n",
      "265/500 [==============>...............] - ETA: 4:05 - loss: 1.9320 - regression_loss: 1.4424 - classification_loss: 0.4895\n",
      "266/500 [==============>...............] - ETA: 4:04 - loss: 1.9312 - regression_loss: 1.4419 - classification_loss: 0.4892\n",
      "267/500 [===============>..............] - ETA: 4:03 - loss: 1.9319 - regression_loss: 1.4429 - classification_loss: 0.4890\n",
      "268/500 [===============>..............] - ETA: 4:02 - loss: 1.9299 - regression_loss: 1.4412 - classification_loss: 0.4887\n",
      "269/500 [===============>..............] - ETA: 4:01 - loss: 1.9295 - regression_loss: 1.4414 - classification_loss: 0.4881\n",
      "270/500 [===============>..............] - ETA: 3:59 - loss: 1.9313 - regression_loss: 1.4426 - classification_loss: 0.4887\n",
      "271/500 [===============>..............] - ETA: 3:58 - loss: 1.9311 - regression_loss: 1.4422 - classification_loss: 0.4889\n",
      "272/500 [===============>..............] - ETA: 3:57 - loss: 1.9279 - regression_loss: 1.4400 - classification_loss: 0.4879\n",
      "273/500 [===============>..............] - ETA: 3:56 - loss: 1.9261 - regression_loss: 1.4388 - classification_loss: 0.4873\n",
      "274/500 [===============>..............] - ETA: 3:55 - loss: 1.9237 - regression_loss: 1.4369 - classification_loss: 0.4868\n",
      "275/500 [===============>..............] - ETA: 3:54 - loss: 1.9230 - regression_loss: 1.4362 - classification_loss: 0.4869\n",
      "276/500 [===============>..............] - ETA: 3:53 - loss: 1.9213 - regression_loss: 1.4343 - classification_loss: 0.4870\n",
      "277/500 [===============>..............] - ETA: 3:52 - loss: 1.9205 - regression_loss: 1.4335 - classification_loss: 0.4870\n",
      "278/500 [===============>..............] - ETA: 3:51 - loss: 1.9187 - regression_loss: 1.4321 - classification_loss: 0.4866\n",
      "279/500 [===============>..............] - ETA: 3:50 - loss: 1.9199 - regression_loss: 1.4328 - classification_loss: 0.4871\n",
      "280/500 [===============>..............] - ETA: 3:49 - loss: 1.9190 - regression_loss: 1.4317 - classification_loss: 0.4872\n",
      "281/500 [===============>..............] - ETA: 3:48 - loss: 1.9192 - regression_loss: 1.4323 - classification_loss: 0.4869\n",
      "282/500 [===============>..............] - ETA: 3:47 - loss: 1.9210 - regression_loss: 1.4335 - classification_loss: 0.4875\n",
      "283/500 [===============>..............] - ETA: 3:46 - loss: 1.9245 - regression_loss: 1.4367 - classification_loss: 0.4879\n",
      "284/500 [================>.............] - ETA: 3:45 - loss: 1.9222 - regression_loss: 1.4347 - classification_loss: 0.4875\n",
      "285/500 [================>.............] - ETA: 3:44 - loss: 1.9219 - regression_loss: 1.4345 - classification_loss: 0.4874\n",
      "286/500 [================>.............] - ETA: 3:43 - loss: 1.9214 - regression_loss: 1.4339 - classification_loss: 0.4875\n",
      "287/500 [================>.............] - ETA: 3:41 - loss: 1.9204 - regression_loss: 1.4326 - classification_loss: 0.4878\n",
      "288/500 [================>.............] - ETA: 3:40 - loss: 1.9209 - regression_loss: 1.4335 - classification_loss: 0.4874\n",
      "289/500 [================>.............] - ETA: 3:39 - loss: 1.9188 - regression_loss: 1.4315 - classification_loss: 0.4873\n",
      "290/500 [================>.............] - ETA: 3:38 - loss: 1.9234 - regression_loss: 1.4346 - classification_loss: 0.4889\n",
      "291/500 [================>.............] - ETA: 3:37 - loss: 1.9248 - regression_loss: 1.4353 - classification_loss: 0.4896\n",
      "292/500 [================>.............] - ETA: 3:36 - loss: 1.9251 - regression_loss: 1.4352 - classification_loss: 0.4899\n",
      "293/500 [================>.............] - ETA: 3:35 - loss: 1.9270 - regression_loss: 1.4372 - classification_loss: 0.4898\n",
      "294/500 [================>.............] - ETA: 3:34 - loss: 1.9264 - regression_loss: 1.4370 - classification_loss: 0.4894\n",
      "295/500 [================>.............] - ETA: 3:33 - loss: 1.9254 - regression_loss: 1.4365 - classification_loss: 0.4889\n",
      "296/500 [================>.............] - ETA: 3:32 - loss: 1.9227 - regression_loss: 1.4346 - classification_loss: 0.4881\n",
      "297/500 [================>.............] - ETA: 3:31 - loss: 1.9223 - regression_loss: 1.4345 - classification_loss: 0.4878\n",
      "298/500 [================>.............] - ETA: 3:30 - loss: 1.9225 - regression_loss: 1.4348 - classification_loss: 0.4877\n",
      "299/500 [================>.............] - ETA: 3:29 - loss: 1.9223 - regression_loss: 1.4344 - classification_loss: 0.4879\n",
      "300/500 [=================>............] - ETA: 3:28 - loss: 1.9193 - regression_loss: 1.4320 - classification_loss: 0.4873\n",
      "301/500 [=================>............] - ETA: 3:27 - loss: 1.9177 - regression_loss: 1.4311 - classification_loss: 0.4866\n",
      "302/500 [=================>............] - ETA: 3:26 - loss: 1.9191 - regression_loss: 1.4326 - classification_loss: 0.4866\n",
      "303/500 [=================>............] - ETA: 3:25 - loss: 1.9177 - regression_loss: 1.4315 - classification_loss: 0.4862\n",
      "304/500 [=================>............] - ETA: 3:24 - loss: 1.9150 - regression_loss: 1.4295 - classification_loss: 0.4855\n",
      "305/500 [=================>............] - ETA: 3:23 - loss: 1.9163 - regression_loss: 1.4306 - classification_loss: 0.4857\n",
      "306/500 [=================>............] - ETA: 3:23 - loss: 1.9131 - regression_loss: 1.4282 - classification_loss: 0.4849\n",
      "307/500 [=================>............] - ETA: 3:22 - loss: 1.9126 - regression_loss: 1.4280 - classification_loss: 0.4846\n",
      "308/500 [=================>............] - ETA: 3:21 - loss: 1.9145 - regression_loss: 1.4296 - classification_loss: 0.4849\n",
      "309/500 [=================>............] - ETA: 3:20 - loss: 1.9142 - regression_loss: 1.4298 - classification_loss: 0.4844\n",
      "310/500 [=================>............] - ETA: 3:19 - loss: 1.9166 - regression_loss: 1.4319 - classification_loss: 0.4847\n",
      "311/500 [=================>............] - ETA: 3:18 - loss: 1.9158 - regression_loss: 1.4315 - classification_loss: 0.4843\n",
      "312/500 [=================>............] - ETA: 3:17 - loss: 1.9172 - regression_loss: 1.4325 - classification_loss: 0.4848\n",
      "313/500 [=================>............] - ETA: 3:16 - loss: 1.9148 - regression_loss: 1.4307 - classification_loss: 0.4840\n",
      "314/500 [=================>............] - ETA: 3:15 - loss: 1.9141 - regression_loss: 1.4305 - classification_loss: 0.4836\n",
      "315/500 [=================>............] - ETA: 3:13 - loss: 1.9144 - regression_loss: 1.4305 - classification_loss: 0.4839\n",
      "316/500 [=================>............] - ETA: 3:12 - loss: 1.9150 - regression_loss: 1.4312 - classification_loss: 0.4838\n",
      "317/500 [==================>...........] - ETA: 3:11 - loss: 1.9131 - regression_loss: 1.4296 - classification_loss: 0.4835\n",
      "318/500 [==================>...........] - ETA: 3:10 - loss: 1.9147 - regression_loss: 1.4305 - classification_loss: 0.4842\n",
      "319/500 [==================>...........] - ETA: 3:09 - loss: 1.9148 - regression_loss: 1.4308 - classification_loss: 0.4839\n",
      "320/500 [==================>...........] - ETA: 3:08 - loss: 1.9127 - regression_loss: 1.4291 - classification_loss: 0.4836\n",
      "321/500 [==================>...........] - ETA: 3:07 - loss: 1.9135 - regression_loss: 1.4299 - classification_loss: 0.4835\n",
      "322/500 [==================>...........] - ETA: 3:06 - loss: 1.9098 - regression_loss: 1.4271 - classification_loss: 0.4827\n",
      "323/500 [==================>...........] - ETA: 3:05 - loss: 1.9079 - regression_loss: 1.4260 - classification_loss: 0.4820\n",
      "324/500 [==================>...........] - ETA: 3:04 - loss: 1.9072 - regression_loss: 1.4254 - classification_loss: 0.4818\n",
      "325/500 [==================>...........] - ETA: 3:03 - loss: 1.9089 - regression_loss: 1.4261 - classification_loss: 0.4828\n",
      "326/500 [==================>...........] - ETA: 3:02 - loss: 1.9076 - regression_loss: 1.4251 - classification_loss: 0.4825\n",
      "327/500 [==================>...........] - ETA: 3:01 - loss: 1.9098 - regression_loss: 1.4268 - classification_loss: 0.4830\n",
      "328/500 [==================>...........] - ETA: 3:00 - loss: 1.9089 - regression_loss: 1.4263 - classification_loss: 0.4825\n",
      "329/500 [==================>...........] - ETA: 2:59 - loss: 1.9103 - regression_loss: 1.4273 - classification_loss: 0.4830\n",
      "330/500 [==================>...........] - ETA: 2:58 - loss: 1.9099 - regression_loss: 1.4270 - classification_loss: 0.4829\n",
      "331/500 [==================>...........] - ETA: 2:57 - loss: 1.9136 - regression_loss: 1.4295 - classification_loss: 0.4841\n",
      "332/500 [==================>...........] - ETA: 2:55 - loss: 1.9133 - regression_loss: 1.4290 - classification_loss: 0.4842\n",
      "333/500 [==================>...........] - ETA: 2:54 - loss: 1.9143 - regression_loss: 1.4299 - classification_loss: 0.4845\n",
      "334/500 [===================>..........] - ETA: 2:53 - loss: 1.9142 - regression_loss: 1.4300 - classification_loss: 0.4842\n",
      "335/500 [===================>..........] - ETA: 2:52 - loss: 1.9119 - regression_loss: 1.4282 - classification_loss: 0.4838\n",
      "336/500 [===================>..........] - ETA: 2:51 - loss: 1.9120 - regression_loss: 1.4276 - classification_loss: 0.4844\n",
      "337/500 [===================>..........] - ETA: 2:50 - loss: 1.9117 - regression_loss: 1.4276 - classification_loss: 0.4840\n",
      "338/500 [===================>..........] - ETA: 2:49 - loss: 1.9128 - regression_loss: 1.4282 - classification_loss: 0.4846\n",
      "339/500 [===================>..........] - ETA: 2:48 - loss: 1.9117 - regression_loss: 1.4276 - classification_loss: 0.4841\n",
      "340/500 [===================>..........] - ETA: 2:47 - loss: 1.9110 - regression_loss: 1.4270 - classification_loss: 0.4840\n",
      "341/500 [===================>..........] - ETA: 2:46 - loss: 1.9089 - regression_loss: 1.4252 - classification_loss: 0.4838\n",
      "342/500 [===================>..........] - ETA: 2:45 - loss: 1.9079 - regression_loss: 1.4246 - classification_loss: 0.4833\n",
      "343/500 [===================>..........] - ETA: 2:44 - loss: 1.9051 - regression_loss: 1.4226 - classification_loss: 0.4825\n",
      "344/500 [===================>..........] - ETA: 2:43 - loss: 1.9073 - regression_loss: 1.4243 - classification_loss: 0.4829\n",
      "345/500 [===================>..........] - ETA: 2:42 - loss: 1.9086 - regression_loss: 1.4255 - classification_loss: 0.4830\n",
      "346/500 [===================>..........] - ETA: 2:41 - loss: 1.9098 - regression_loss: 1.4258 - classification_loss: 0.4840\n",
      "347/500 [===================>..........] - ETA: 2:40 - loss: 1.9122 - regression_loss: 1.4271 - classification_loss: 0.4851\n",
      "348/500 [===================>..........] - ETA: 2:38 - loss: 1.9124 - regression_loss: 1.4271 - classification_loss: 0.4852\n",
      "349/500 [===================>..........] - ETA: 2:37 - loss: 1.9147 - regression_loss: 1.4287 - classification_loss: 0.4860\n",
      "350/500 [====================>.........] - ETA: 2:36 - loss: 1.9138 - regression_loss: 1.4282 - classification_loss: 0.4856\n",
      "351/500 [====================>.........] - ETA: 2:35 - loss: 1.9130 - regression_loss: 1.4280 - classification_loss: 0.4850\n",
      "352/500 [====================>.........] - ETA: 2:34 - loss: 1.9127 - regression_loss: 1.4279 - classification_loss: 0.4848\n",
      "353/500 [====================>.........] - ETA: 2:33 - loss: 1.9132 - regression_loss: 1.4286 - classification_loss: 0.4846\n",
      "354/500 [====================>.........] - ETA: 2:32 - loss: 1.9145 - regression_loss: 1.4296 - classification_loss: 0.4849\n",
      "355/500 [====================>.........] - ETA: 2:31 - loss: 1.9154 - regression_loss: 1.4305 - classification_loss: 0.4849\n",
      "356/500 [====================>.........] - ETA: 2:30 - loss: 1.9166 - regression_loss: 1.4315 - classification_loss: 0.4851\n",
      "357/500 [====================>.........] - ETA: 2:29 - loss: 1.9168 - regression_loss: 1.4316 - classification_loss: 0.4852\n",
      "358/500 [====================>.........] - ETA: 2:28 - loss: 1.9198 - regression_loss: 1.4334 - classification_loss: 0.4864\n",
      "359/500 [====================>.........] - ETA: 2:27 - loss: 1.9221 - regression_loss: 1.4350 - classification_loss: 0.4870\n",
      "360/500 [====================>.........] - ETA: 2:26 - loss: 1.9220 - regression_loss: 1.4350 - classification_loss: 0.4870\n",
      "361/500 [====================>.........] - ETA: 2:25 - loss: 1.9231 - regression_loss: 1.4359 - classification_loss: 0.4873\n",
      "362/500 [====================>.........] - ETA: 2:24 - loss: 1.9220 - regression_loss: 1.4349 - classification_loss: 0.4871\n",
      "363/500 [====================>.........] - ETA: 2:23 - loss: 1.9227 - regression_loss: 1.4353 - classification_loss: 0.4874\n",
      "364/500 [====================>.........] - ETA: 2:22 - loss: 1.9213 - regression_loss: 1.4344 - classification_loss: 0.4869\n",
      "365/500 [====================>.........] - ETA: 2:20 - loss: 1.9211 - regression_loss: 1.4347 - classification_loss: 0.4864\n",
      "366/500 [====================>.........] - ETA: 2:19 - loss: 1.9213 - regression_loss: 1.4347 - classification_loss: 0.4866\n",
      "367/500 [=====================>........] - ETA: 2:18 - loss: 1.9187 - regression_loss: 1.4325 - classification_loss: 0.4861\n",
      "368/500 [=====================>........] - ETA: 2:17 - loss: 1.9172 - regression_loss: 1.4315 - classification_loss: 0.4857\n",
      "369/500 [=====================>........] - ETA: 2:16 - loss: 1.9170 - regression_loss: 1.4313 - classification_loss: 0.4858\n",
      "370/500 [=====================>........] - ETA: 2:15 - loss: 1.9168 - regression_loss: 1.4310 - classification_loss: 0.4858\n",
      "371/500 [=====================>........] - ETA: 2:14 - loss: 1.9162 - regression_loss: 1.4306 - classification_loss: 0.4855\n",
      "372/500 [=====================>........] - ETA: 2:13 - loss: 1.9158 - regression_loss: 1.4307 - classification_loss: 0.4851\n",
      "373/500 [=====================>........] - ETA: 2:12 - loss: 1.9167 - regression_loss: 1.4313 - classification_loss: 0.4854\n",
      "374/500 [=====================>........] - ETA: 2:11 - loss: 1.9164 - regression_loss: 1.4308 - classification_loss: 0.4856\n",
      "375/500 [=====================>........] - ETA: 2:10 - loss: 1.9180 - regression_loss: 1.4320 - classification_loss: 0.4859\n",
      "376/500 [=====================>........] - ETA: 2:09 - loss: 1.9197 - regression_loss: 1.4334 - classification_loss: 0.4863\n",
      "377/500 [=====================>........] - ETA: 2:08 - loss: 1.9208 - regression_loss: 1.4341 - classification_loss: 0.4867\n",
      "378/500 [=====================>........] - ETA: 2:07 - loss: 1.9199 - regression_loss: 1.4333 - classification_loss: 0.4867\n",
      "379/500 [=====================>........] - ETA: 2:06 - loss: 1.9214 - regression_loss: 1.4339 - classification_loss: 0.4875\n",
      "380/500 [=====================>........] - ETA: 2:05 - loss: 1.9220 - regression_loss: 1.4345 - classification_loss: 0.4875\n",
      "381/500 [=====================>........] - ETA: 2:04 - loss: 1.9216 - regression_loss: 1.4342 - classification_loss: 0.4874\n",
      "382/500 [=====================>........] - ETA: 2:02 - loss: 1.9213 - regression_loss: 1.4339 - classification_loss: 0.4874\n",
      "383/500 [=====================>........] - ETA: 2:01 - loss: 1.9199 - regression_loss: 1.4326 - classification_loss: 0.4873\n",
      "384/500 [======================>.......] - ETA: 2:00 - loss: 1.9222 - regression_loss: 1.4347 - classification_loss: 0.4875\n",
      "385/500 [======================>.......] - ETA: 1:59 - loss: 1.9213 - regression_loss: 1.4341 - classification_loss: 0.4872\n",
      "386/500 [======================>.......] - ETA: 1:58 - loss: 1.9224 - regression_loss: 1.4346 - classification_loss: 0.4879\n",
      "387/500 [======================>.......] - ETA: 1:57 - loss: 1.9218 - regression_loss: 1.4344 - classification_loss: 0.4874\n",
      "388/500 [======================>.......] - ETA: 1:56 - loss: 1.9219 - regression_loss: 1.4349 - classification_loss: 0.4871\n",
      "389/500 [======================>.......] - ETA: 1:55 - loss: 1.9245 - regression_loss: 1.4373 - classification_loss: 0.4872\n",
      "390/500 [======================>.......] - ETA: 1:54 - loss: 1.9259 - regression_loss: 1.4384 - classification_loss: 0.4875\n",
      "391/500 [======================>.......] - ETA: 1:53 - loss: 1.9249 - regression_loss: 1.4379 - classification_loss: 0.4871\n",
      "392/500 [======================>.......] - ETA: 1:52 - loss: 1.9271 - regression_loss: 1.4394 - classification_loss: 0.4877\n",
      "393/500 [======================>.......] - ETA: 1:51 - loss: 1.9279 - regression_loss: 1.4396 - classification_loss: 0.4883\n",
      "394/500 [======================>.......] - ETA: 1:50 - loss: 1.9288 - regression_loss: 1.4404 - classification_loss: 0.4884\n",
      "395/500 [======================>.......] - ETA: 1:49 - loss: 1.9307 - regression_loss: 1.4416 - classification_loss: 0.4891\n",
      "396/500 [======================>.......] - ETA: 1:48 - loss: 1.9297 - regression_loss: 1.4410 - classification_loss: 0.4886\n",
      "397/500 [======================>.......] - ETA: 1:47 - loss: 1.9316 - regression_loss: 1.4420 - classification_loss: 0.4895\n",
      "398/500 [======================>.......] - ETA: 1:46 - loss: 1.9313 - regression_loss: 1.4422 - classification_loss: 0.4892\n",
      "399/500 [======================>.......] - ETA: 1:45 - loss: 1.9320 - regression_loss: 1.4427 - classification_loss: 0.4893\n",
      "400/500 [=======================>......] - ETA: 1:44 - loss: 1.9330 - regression_loss: 1.4437 - classification_loss: 0.4894\n",
      "401/500 [=======================>......] - ETA: 1:42 - loss: 1.9317 - regression_loss: 1.4427 - classification_loss: 0.4890\n",
      "402/500 [=======================>......] - ETA: 1:41 - loss: 1.9320 - regression_loss: 1.4426 - classification_loss: 0.4893\n",
      "403/500 [=======================>......] - ETA: 1:40 - loss: 1.9313 - regression_loss: 1.4415 - classification_loss: 0.4897\n",
      "404/500 [=======================>......] - ETA: 1:39 - loss: 1.9298 - regression_loss: 1.4404 - classification_loss: 0.4894\n",
      "405/500 [=======================>......] - ETA: 1:38 - loss: 1.9284 - regression_loss: 1.4393 - classification_loss: 0.4891\n",
      "406/500 [=======================>......] - ETA: 1:37 - loss: 1.9268 - regression_loss: 1.4383 - classification_loss: 0.4885\n",
      "407/500 [=======================>......] - ETA: 1:36 - loss: 1.9272 - regression_loss: 1.4385 - classification_loss: 0.4887\n",
      "408/500 [=======================>......] - ETA: 1:35 - loss: 1.9280 - regression_loss: 1.4390 - classification_loss: 0.4890\n",
      "409/500 [=======================>......] - ETA: 1:34 - loss: 1.9291 - regression_loss: 1.4395 - classification_loss: 0.4896\n",
      "410/500 [=======================>......] - ETA: 1:33 - loss: 1.9284 - regression_loss: 1.4390 - classification_loss: 0.4894\n",
      "411/500 [=======================>......] - ETA: 1:32 - loss: 1.9300 - regression_loss: 1.4402 - classification_loss: 0.4897\n",
      "412/500 [=======================>......] - ETA: 1:31 - loss: 1.9283 - regression_loss: 1.4391 - classification_loss: 0.4892\n",
      "413/500 [=======================>......] - ETA: 1:30 - loss: 1.9275 - regression_loss: 1.4386 - classification_loss: 0.4889\n",
      "414/500 [=======================>......] - ETA: 1:29 - loss: 1.9285 - regression_loss: 1.4397 - classification_loss: 0.4888\n",
      "415/500 [=======================>......] - ETA: 1:28 - loss: 1.9298 - regression_loss: 1.4403 - classification_loss: 0.4895\n",
      "416/500 [=======================>......] - ETA: 1:27 - loss: 1.9306 - regression_loss: 1.4413 - classification_loss: 0.4893\n",
      "417/500 [========================>.....] - ETA: 1:26 - loss: 1.9291 - regression_loss: 1.4404 - classification_loss: 0.4887\n",
      "418/500 [========================>.....] - ETA: 1:25 - loss: 1.9288 - regression_loss: 1.4404 - classification_loss: 0.4884\n",
      "419/500 [========================>.....] - ETA: 1:24 - loss: 1.9309 - regression_loss: 1.4418 - classification_loss: 0.4891\n",
      "420/500 [========================>.....] - ETA: 1:23 - loss: 1.9300 - regression_loss: 1.4409 - classification_loss: 0.4891\n",
      "421/500 [========================>.....] - ETA: 1:22 - loss: 1.9288 - regression_loss: 1.4401 - classification_loss: 0.4888\n",
      "422/500 [========================>.....] - ETA: 1:21 - loss: 1.9286 - regression_loss: 1.4399 - classification_loss: 0.4887\n",
      "423/500 [========================>.....] - ETA: 1:20 - loss: 1.9303 - regression_loss: 1.4414 - classification_loss: 0.4889\n",
      "424/500 [========================>.....] - ETA: 1:19 - loss: 1.9320 - regression_loss: 1.4423 - classification_loss: 0.4897\n",
      "425/500 [========================>.....] - ETA: 1:17 - loss: 1.9318 - regression_loss: 1.4420 - classification_loss: 0.4898\n",
      "426/500 [========================>.....] - ETA: 1:16 - loss: 1.9320 - regression_loss: 1.4422 - classification_loss: 0.4898\n",
      "427/500 [========================>.....] - ETA: 1:15 - loss: 1.9308 - regression_loss: 1.4416 - classification_loss: 0.4892\n",
      "428/500 [========================>.....] - ETA: 1:14 - loss: 1.9283 - regression_loss: 1.4398 - classification_loss: 0.4885\n",
      "429/500 [========================>.....] - ETA: 1:13 - loss: 1.9293 - regression_loss: 1.4404 - classification_loss: 0.4888\n",
      "430/500 [========================>.....] - ETA: 1:12 - loss: 1.9281 - regression_loss: 1.4396 - classification_loss: 0.4884\n",
      "431/500 [========================>.....] - ETA: 1:11 - loss: 1.9274 - regression_loss: 1.4388 - classification_loss: 0.4887\n",
      "432/500 [========================>.....] - ETA: 1:10 - loss: 1.9292 - regression_loss: 1.4404 - classification_loss: 0.4888\n",
      "433/500 [========================>.....] - ETA: 1:09 - loss: 1.9292 - regression_loss: 1.4407 - classification_loss: 0.4886\n",
      "434/500 [=========================>....] - ETA: 1:08 - loss: 1.9280 - regression_loss: 1.4397 - classification_loss: 0.4883\n",
      "435/500 [=========================>....] - ETA: 1:07 - loss: 1.9274 - regression_loss: 1.4393 - classification_loss: 0.4881\n",
      "436/500 [=========================>....] - ETA: 1:06 - loss: 1.9273 - regression_loss: 1.4392 - classification_loss: 0.4880\n",
      "437/500 [=========================>....] - ETA: 1:05 - loss: 1.9271 - regression_loss: 1.4391 - classification_loss: 0.4879\n",
      "438/500 [=========================>....] - ETA: 1:04 - loss: 1.9262 - regression_loss: 1.4388 - classification_loss: 0.4874\n",
      "439/500 [=========================>....] - ETA: 1:03 - loss: 1.9267 - regression_loss: 1.4391 - classification_loss: 0.4876\n",
      "440/500 [=========================>....] - ETA: 1:02 - loss: 1.9252 - regression_loss: 1.4381 - classification_loss: 0.4871\n",
      "441/500 [=========================>....] - ETA: 1:01 - loss: 1.9240 - regression_loss: 1.4371 - classification_loss: 0.4869\n",
      "442/500 [=========================>....] - ETA: 1:00 - loss: 1.9222 - regression_loss: 1.4357 - classification_loss: 0.4865\n",
      "443/500 [=========================>....] - ETA: 59s - loss: 1.9201 - regression_loss: 1.4342 - classification_loss: 0.4859 \n",
      "444/500 [=========================>....] - ETA: 58s - loss: 1.9218 - regression_loss: 1.4358 - classification_loss: 0.4860\n",
      "445/500 [=========================>....] - ETA: 57s - loss: 1.9235 - regression_loss: 1.4373 - classification_loss: 0.4862\n",
      "446/500 [=========================>....] - ETA: 56s - loss: 1.9253 - regression_loss: 1.4383 - classification_loss: 0.4870\n",
      "447/500 [=========================>....] - ETA: 55s - loss: 1.9248 - regression_loss: 1.4381 - classification_loss: 0.4867\n",
      "448/500 [=========================>....] - ETA: 54s - loss: 1.9255 - regression_loss: 1.4390 - classification_loss: 0.4865\n",
      "449/500 [=========================>....] - ETA: 52s - loss: 1.9231 - regression_loss: 1.4372 - classification_loss: 0.4859\n",
      "450/500 [==========================>...] - ETA: 51s - loss: 1.9221 - regression_loss: 1.4367 - classification_loss: 0.4855\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 1.9232 - regression_loss: 1.4373 - classification_loss: 0.4859\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 1.9234 - regression_loss: 1.4375 - classification_loss: 0.4859\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.9238 - regression_loss: 1.4378 - classification_loss: 0.4860\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.9258 - regression_loss: 1.4388 - classification_loss: 0.4870\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.9237 - regression_loss: 1.4374 - classification_loss: 0.4863\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.9235 - regression_loss: 1.4370 - classification_loss: 0.4864\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.9237 - regression_loss: 1.4370 - classification_loss: 0.4867\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.9261 - regression_loss: 1.4384 - classification_loss: 0.4877\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.9255 - regression_loss: 1.4382 - classification_loss: 0.4873\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.9250 - regression_loss: 1.4379 - classification_loss: 0.4871\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 1.9247 - regression_loss: 1.4376 - classification_loss: 0.4870\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 1.9253 - regression_loss: 1.4380 - classification_loss: 0.4873\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 1.9248 - regression_loss: 1.4375 - classification_loss: 0.4873\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 1.9257 - regression_loss: 1.4381 - classification_loss: 0.4876\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 1.9262 - regression_loss: 1.4380 - classification_loss: 0.4882\n",
      "466/500 [==========================>...] - ETA: 35s - loss: 1.9266 - regression_loss: 1.4383 - classification_loss: 0.4884\n",
      "467/500 [===========================>..] - ETA: 34s - loss: 1.9248 - regression_loss: 1.4367 - classification_loss: 0.4880\n",
      "468/500 [===========================>..] - ETA: 33s - loss: 1.9250 - regression_loss: 1.4371 - classification_loss: 0.4879\n",
      "469/500 [===========================>..] - ETA: 32s - loss: 1.9234 - regression_loss: 1.4358 - classification_loss: 0.4877\n",
      "470/500 [===========================>..] - ETA: 31s - loss: 1.9228 - regression_loss: 1.4356 - classification_loss: 0.4873\n",
      "471/500 [===========================>..] - ETA: 30s - loss: 1.9227 - regression_loss: 1.4356 - classification_loss: 0.4870\n",
      "472/500 [===========================>..] - ETA: 29s - loss: 1.9231 - regression_loss: 1.4359 - classification_loss: 0.4872\n",
      "473/500 [===========================>..] - ETA: 28s - loss: 1.9223 - regression_loss: 1.4354 - classification_loss: 0.4869\n",
      "474/500 [===========================>..] - ETA: 27s - loss: 1.9208 - regression_loss: 1.4343 - classification_loss: 0.4865\n",
      "475/500 [===========================>..] - ETA: 25s - loss: 1.9210 - regression_loss: 1.4345 - classification_loss: 0.4864\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 1.9198 - regression_loss: 1.4336 - classification_loss: 0.4862\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.9197 - regression_loss: 1.4336 - classification_loss: 0.4861\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.9219 - regression_loss: 1.4351 - classification_loss: 0.4868\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.9218 - regression_loss: 1.4349 - classification_loss: 0.4868\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.9209 - regression_loss: 1.4346 - classification_loss: 0.4863\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.9217 - regression_loss: 1.4352 - classification_loss: 0.4865\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.9227 - regression_loss: 1.4361 - classification_loss: 0.4866\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.9216 - regression_loss: 1.4352 - classification_loss: 0.4864\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.9220 - regression_loss: 1.4357 - classification_loss: 0.4863\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.9215 - regression_loss: 1.4350 - classification_loss: 0.4865\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.9214 - regression_loss: 1.4348 - classification_loss: 0.4866\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.9212 - regression_loss: 1.4348 - classification_loss: 0.4864\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.9210 - regression_loss: 1.4346 - classification_loss: 0.4864\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.9229 - regression_loss: 1.4362 - classification_loss: 0.4868\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.9226 - regression_loss: 1.4362 - classification_loss: 0.4864\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.9211 - regression_loss: 1.4351 - classification_loss: 0.4860 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.9210 - regression_loss: 1.4353 - classification_loss: 0.4857\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.9228 - regression_loss: 1.4362 - classification_loss: 0.4866\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.9242 - regression_loss: 1.4372 - classification_loss: 0.4870\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.9231 - regression_loss: 1.4366 - classification_loss: 0.4865\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.9249 - regression_loss: 1.4379 - classification_loss: 0.4870\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.9230 - regression_loss: 1.4363 - classification_loss: 0.4867\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.9234 - regression_loss: 1.4365 - classification_loss: 0.4869\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.9246 - regression_loss: 1.4374 - classification_loss: 0.4872\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9247 - regression_loss: 1.4377 - classification_loss: 0.4870\n",
      "Epoch 00015: saving model to ./snapshots\\resnet50_csv_15.h5\n",
      "\n",
      "500/500 [==============================] - 521s 1s/step - loss: 1.9247 - regression_loss: 1.4377 - classification_loss: 0.4870\n",
      "Epoch 16/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 2.1804 - regression_loss: 1.5697 - classification_loss: 0.6107\n",
      "  2/500 [..............................] - ETA: 4:34 - loss: 1.9443 - regression_loss: 1.3919 - classification_loss: 0.5524\n",
      "  3/500 [..............................] - ETA: 5:44 - loss: 1.9654 - regression_loss: 1.4648 - classification_loss: 0.5007\n",
      "  4/500 [..............................] - ETA: 6:30 - loss: 1.9336 - regression_loss: 1.4678 - classification_loss: 0.4658\n",
      "  5/500 [..............................] - ETA: 6:48 - loss: 1.8121 - regression_loss: 1.3698 - classification_loss: 0.4423\n",
      "  6/500 [..............................] - ETA: 12:10 - loss: 1.9227 - regression_loss: 1.4087 - classification_loss: 0.5140\n",
      "  7/500 [..............................] - ETA: 14:18 - loss: 1.9275 - regression_loss: 1.4279 - classification_loss: 0.4996\n",
      "  8/500 [..............................] - ETA: 13:35 - loss: 1.9628 - regression_loss: 1.4552 - classification_loss: 0.5076\n",
      "  9/500 [..............................] - ETA: 13:08 - loss: 1.9300 - regression_loss: 1.4192 - classification_loss: 0.5108\n",
      " 10/500 [..............................] - ETA: 12:40 - loss: 1.9422 - regression_loss: 1.4354 - classification_loss: 0.5068\n",
      " 11/500 [..............................] - ETA: 12:18 - loss: 1.8738 - regression_loss: 1.3879 - classification_loss: 0.4859\n",
      " 12/500 [..............................] - ETA: 11:55 - loss: 1.7755 - regression_loss: 1.3165 - classification_loss: 0.4590\n",
      " 13/500 [..............................] - ETA: 11:35 - loss: 1.7607 - regression_loss: 1.3147 - classification_loss: 0.4459\n",
      " 14/500 [..............................] - ETA: 11:22 - loss: 1.7200 - regression_loss: 1.2874 - classification_loss: 0.4326\n",
      " 15/500 [..............................] - ETA: 11:07 - loss: 1.7670 - regression_loss: 1.3242 - classification_loss: 0.4427\n",
      " 16/500 [..............................] - ETA: 10:55 - loss: 1.8235 - regression_loss: 1.3730 - classification_loss: 0.4506\n",
      " 17/500 [>.............................] - ETA: 10:43 - loss: 1.8315 - regression_loss: 1.3629 - classification_loss: 0.4686\n",
      " 18/500 [>.............................] - ETA: 10:32 - loss: 1.7957 - regression_loss: 1.3354 - classification_loss: 0.4603\n",
      " 19/500 [>.............................] - ETA: 10:23 - loss: 1.7907 - regression_loss: 1.3359 - classification_loss: 0.4548\n",
      " 20/500 [>.............................] - ETA: 10:14 - loss: 1.8076 - regression_loss: 1.3509 - classification_loss: 0.4567\n",
      " 21/500 [>.............................] - ETA: 10:01 - loss: 1.8192 - regression_loss: 1.3583 - classification_loss: 0.4609\n",
      " 22/500 [>.............................] - ETA: 9:59 - loss: 1.8216 - regression_loss: 1.3652 - classification_loss: 0.4564 \n",
      " 23/500 [>.............................] - ETA: 9:52 - loss: 1.8056 - regression_loss: 1.3540 - classification_loss: 0.4516\n",
      " 24/500 [>.............................] - ETA: 9:46 - loss: 1.7824 - regression_loss: 1.3405 - classification_loss: 0.4419\n",
      " 25/500 [>.............................] - ETA: 9:40 - loss: 1.7791 - regression_loss: 1.3433 - classification_loss: 0.4357\n",
      " 26/500 [>.............................] - ETA: 9:36 - loss: 1.7642 - regression_loss: 1.3323 - classification_loss: 0.4319\n",
      " 27/500 [>.............................] - ETA: 9:33 - loss: 1.7857 - regression_loss: 1.3476 - classification_loss: 0.4381\n",
      " 28/500 [>.............................] - ETA: 9:28 - loss: 1.7870 - regression_loss: 1.3501 - classification_loss: 0.4368\n",
      " 29/500 [>.............................] - ETA: 9:23 - loss: 1.7995 - regression_loss: 1.3576 - classification_loss: 0.4419\n",
      " 30/500 [>.............................] - ETA: 9:20 - loss: 1.7911 - regression_loss: 1.3544 - classification_loss: 0.4367\n",
      " 31/500 [>.............................] - ETA: 9:15 - loss: 1.8399 - regression_loss: 1.3859 - classification_loss: 0.4540\n",
      " 32/500 [>.............................] - ETA: 9:11 - loss: 1.8640 - regression_loss: 1.4046 - classification_loss: 0.4593\n",
      " 33/500 [>.............................] - ETA: 9:07 - loss: 1.8601 - regression_loss: 1.4011 - classification_loss: 0.4590\n",
      " 34/500 [=>............................] - ETA: 9:05 - loss: 1.8706 - regression_loss: 1.4131 - classification_loss: 0.4574\n",
      " 35/500 [=>............................] - ETA: 9:01 - loss: 1.8766 - regression_loss: 1.4148 - classification_loss: 0.4619\n",
      " 36/500 [=>............................] - ETA: 8:57 - loss: 1.8659 - regression_loss: 1.4068 - classification_loss: 0.4591\n",
      " 37/500 [=>............................] - ETA: 8:55 - loss: 1.8548 - regression_loss: 1.3984 - classification_loss: 0.4565\n",
      " 38/500 [=>............................] - ETA: 8:52 - loss: 1.8796 - regression_loss: 1.4181 - classification_loss: 0.4615\n",
      " 39/500 [=>............................] - ETA: 8:50 - loss: 1.8893 - regression_loss: 1.4272 - classification_loss: 0.4622\n",
      " 40/500 [=>............................] - ETA: 8:47 - loss: 1.9152 - regression_loss: 1.4469 - classification_loss: 0.4684\n",
      " 41/500 [=>............................] - ETA: 8:46 - loss: 1.9231 - regression_loss: 1.4512 - classification_loss: 0.4720\n",
      " 42/500 [=>............................] - ETA: 8:43 - loss: 1.9117 - regression_loss: 1.4417 - classification_loss: 0.4700\n",
      " 43/500 [=>............................] - ETA: 8:42 - loss: 1.8930 - regression_loss: 1.4280 - classification_loss: 0.4650\n",
      " 44/500 [=>............................] - ETA: 8:39 - loss: 1.9076 - regression_loss: 1.4350 - classification_loss: 0.4726\n",
      " 45/500 [=>............................] - ETA: 8:37 - loss: 1.8960 - regression_loss: 1.4281 - classification_loss: 0.4679\n",
      " 46/500 [=>............................] - ETA: 8:34 - loss: 1.9210 - regression_loss: 1.4494 - classification_loss: 0.4716\n",
      " 47/500 [=>............................] - ETA: 8:33 - loss: 1.9212 - regression_loss: 1.4488 - classification_loss: 0.4724\n",
      " 48/500 [=>............................] - ETA: 8:29 - loss: 1.9434 - regression_loss: 1.4625 - classification_loss: 0.4809\n",
      " 49/500 [=>............................] - ETA: 8:28 - loss: 1.9580 - regression_loss: 1.4721 - classification_loss: 0.4859\n",
      " 50/500 [==>...........................] - ETA: 8:26 - loss: 1.9567 - regression_loss: 1.4722 - classification_loss: 0.4845\n",
      " 51/500 [==>...........................] - ETA: 8:24 - loss: 1.9459 - regression_loss: 1.4641 - classification_loss: 0.4818\n",
      " 52/500 [==>...........................] - ETA: 8:22 - loss: 1.9391 - regression_loss: 1.4605 - classification_loss: 0.4786\n",
      " 53/500 [==>...........................] - ETA: 8:20 - loss: 1.9268 - regression_loss: 1.4522 - classification_loss: 0.4746\n",
      " 54/500 [==>...........................] - ETA: 8:18 - loss: 1.9394 - regression_loss: 1.4624 - classification_loss: 0.4770\n",
      " 55/500 [==>...........................] - ETA: 8:16 - loss: 1.9400 - regression_loss: 1.4637 - classification_loss: 0.4764\n",
      " 56/500 [==>...........................] - ETA: 8:14 - loss: 1.9293 - regression_loss: 1.4550 - classification_loss: 0.4743\n",
      " 57/500 [==>...........................] - ETA: 8:12 - loss: 1.9273 - regression_loss: 1.4555 - classification_loss: 0.4719\n",
      " 58/500 [==>...........................] - ETA: 8:10 - loss: 1.9249 - regression_loss: 1.4502 - classification_loss: 0.4746\n",
      " 59/500 [==>...........................] - ETA: 8:09 - loss: 1.9413 - regression_loss: 1.4618 - classification_loss: 0.4795\n",
      " 60/500 [==>...........................] - ETA: 8:07 - loss: 1.9304 - regression_loss: 1.4538 - classification_loss: 0.4766\n",
      " 61/500 [==>...........................] - ETA: 8:06 - loss: 1.9272 - regression_loss: 1.4499 - classification_loss: 0.4773\n",
      " 62/500 [==>...........................] - ETA: 8:04 - loss: 1.9216 - regression_loss: 1.4474 - classification_loss: 0.4743\n",
      " 63/500 [==>...........................] - ETA: 8:01 - loss: 1.9207 - regression_loss: 1.4484 - classification_loss: 0.4723\n",
      " 64/500 [==>...........................] - ETA: 8:00 - loss: 1.9136 - regression_loss: 1.4420 - classification_loss: 0.4716\n",
      " 65/500 [==>...........................] - ETA: 7:58 - loss: 1.9171 - regression_loss: 1.4447 - classification_loss: 0.4724\n",
      " 66/500 [==>...........................] - ETA: 7:57 - loss: 1.9327 - regression_loss: 1.4568 - classification_loss: 0.4759\n",
      " 67/500 [===>..........................] - ETA: 7:54 - loss: 1.9376 - regression_loss: 1.4589 - classification_loss: 0.4787\n",
      " 68/500 [===>..........................] - ETA: 7:53 - loss: 1.9336 - regression_loss: 1.4570 - classification_loss: 0.4766\n",
      " 69/500 [===>..........................] - ETA: 7:51 - loss: 1.9291 - regression_loss: 1.4545 - classification_loss: 0.4746\n",
      " 70/500 [===>..........................] - ETA: 7:50 - loss: 1.9516 - regression_loss: 1.4710 - classification_loss: 0.4806\n",
      " 71/500 [===>..........................] - ETA: 7:49 - loss: 1.9492 - regression_loss: 1.4689 - classification_loss: 0.4803\n",
      " 72/500 [===>..........................] - ETA: 7:47 - loss: 1.9467 - regression_loss: 1.4680 - classification_loss: 0.4786\n",
      " 73/500 [===>..........................] - ETA: 7:44 - loss: 1.9619 - regression_loss: 1.4780 - classification_loss: 0.4839\n",
      " 74/500 [===>..........................] - ETA: 7:42 - loss: 1.9603 - regression_loss: 1.4747 - classification_loss: 0.4856\n",
      " 75/500 [===>..........................] - ETA: 7:40 - loss: 1.9647 - regression_loss: 1.4782 - classification_loss: 0.4865\n",
      " 76/500 [===>..........................] - ETA: 7:39 - loss: 1.9782 - regression_loss: 1.4868 - classification_loss: 0.4913\n",
      " 77/500 [===>..........................] - ETA: 7:37 - loss: 1.9770 - regression_loss: 1.4873 - classification_loss: 0.4897\n",
      " 78/500 [===>..........................] - ETA: 7:36 - loss: 1.9863 - regression_loss: 1.4922 - classification_loss: 0.4941\n",
      " 79/500 [===>..........................] - ETA: 7:35 - loss: 1.9840 - regression_loss: 1.4913 - classification_loss: 0.4927\n",
      " 80/500 [===>..........................] - ETA: 7:34 - loss: 1.9843 - regression_loss: 1.4916 - classification_loss: 0.4927\n",
      " 81/500 [===>..........................] - ETA: 7:33 - loss: 1.9904 - regression_loss: 1.4971 - classification_loss: 0.4933\n",
      " 82/500 [===>..........................] - ETA: 7:32 - loss: 1.9821 - regression_loss: 1.4915 - classification_loss: 0.4907\n",
      " 83/500 [===>..........................] - ETA: 7:30 - loss: 1.9853 - regression_loss: 1.4946 - classification_loss: 0.4907\n",
      " 84/500 [====>.........................] - ETA: 7:29 - loss: 1.9861 - regression_loss: 1.4944 - classification_loss: 0.4917\n",
      " 85/500 [====>.........................] - ETA: 7:27 - loss: 1.9786 - regression_loss: 1.4893 - classification_loss: 0.4893\n",
      " 86/500 [====>.........................] - ETA: 7:26 - loss: 1.9813 - regression_loss: 1.4931 - classification_loss: 0.4882\n",
      " 87/500 [====>.........................] - ETA: 7:24 - loss: 1.9742 - regression_loss: 1.4882 - classification_loss: 0.4860\n",
      " 88/500 [====>.........................] - ETA: 7:23 - loss: 1.9725 - regression_loss: 1.4878 - classification_loss: 0.4848\n",
      " 89/500 [====>.........................] - ETA: 7:21 - loss: 1.9721 - regression_loss: 1.4858 - classification_loss: 0.4863\n",
      " 90/500 [====>.........................] - ETA: 7:20 - loss: 1.9646 - regression_loss: 1.4796 - classification_loss: 0.4850\n",
      " 91/500 [====>.........................] - ETA: 7:18 - loss: 1.9654 - regression_loss: 1.4798 - classification_loss: 0.4855\n",
      " 92/500 [====>.........................] - ETA: 7:17 - loss: 1.9607 - regression_loss: 1.4773 - classification_loss: 0.4834\n",
      " 93/500 [====>.........................] - ETA: 7:16 - loss: 1.9612 - regression_loss: 1.4768 - classification_loss: 0.4843\n",
      " 94/500 [====>.........................] - ETA: 7:15 - loss: 1.9589 - regression_loss: 1.4746 - classification_loss: 0.4844\n",
      " 95/500 [====>.........................] - ETA: 7:13 - loss: 1.9568 - regression_loss: 1.4735 - classification_loss: 0.4833\n",
      " 96/500 [====>.........................] - ETA: 7:12 - loss: 1.9560 - regression_loss: 1.4719 - classification_loss: 0.4841\n",
      " 97/500 [====>.........................] - ETA: 7:11 - loss: 1.9558 - regression_loss: 1.4703 - classification_loss: 0.4855\n",
      " 98/500 [====>.........................] - ETA: 7:10 - loss: 1.9566 - regression_loss: 1.4710 - classification_loss: 0.4856\n",
      " 99/500 [====>.........................] - ETA: 7:09 - loss: 1.9536 - regression_loss: 1.4700 - classification_loss: 0.4836\n",
      "100/500 [=====>........................] - ETA: 7:08 - loss: 1.9568 - regression_loss: 1.4735 - classification_loss: 0.4833\n",
      "101/500 [=====>........................] - ETA: 7:07 - loss: 1.9619 - regression_loss: 1.4792 - classification_loss: 0.4828\n",
      "102/500 [=====>........................] - ETA: 7:06 - loss: 1.9670 - regression_loss: 1.4804 - classification_loss: 0.4866\n",
      "103/500 [=====>........................] - ETA: 7:05 - loss: 1.9648 - regression_loss: 1.4775 - classification_loss: 0.4873\n",
      "104/500 [=====>........................] - ETA: 7:03 - loss: 1.9640 - regression_loss: 1.4742 - classification_loss: 0.4898\n",
      "105/500 [=====>........................] - ETA: 7:02 - loss: 1.9536 - regression_loss: 1.4672 - classification_loss: 0.4863\n",
      "106/500 [=====>........................] - ETA: 7:01 - loss: 1.9478 - regression_loss: 1.4627 - classification_loss: 0.4851\n",
      "107/500 [=====>........................] - ETA: 6:59 - loss: 1.9518 - regression_loss: 1.4663 - classification_loss: 0.4855\n",
      "108/500 [=====>........................] - ETA: 6:58 - loss: 1.9602 - regression_loss: 1.4727 - classification_loss: 0.4875\n",
      "109/500 [=====>........................] - ETA: 6:57 - loss: 1.9588 - regression_loss: 1.4719 - classification_loss: 0.4869\n",
      "110/500 [=====>........................] - ETA: 6:55 - loss: 1.9533 - regression_loss: 1.4672 - classification_loss: 0.4861\n",
      "111/500 [=====>........................] - ETA: 6:54 - loss: 1.9490 - regression_loss: 1.4622 - classification_loss: 0.4868\n",
      "112/500 [=====>........................] - ETA: 6:52 - loss: 1.9481 - regression_loss: 1.4606 - classification_loss: 0.4875\n",
      "113/500 [=====>........................] - ETA: 6:52 - loss: 1.9533 - regression_loss: 1.4647 - classification_loss: 0.4886\n",
      "114/500 [=====>........................] - ETA: 6:51 - loss: 1.9495 - regression_loss: 1.4618 - classification_loss: 0.4877\n",
      "115/500 [=====>........................] - ETA: 6:49 - loss: 1.9454 - regression_loss: 1.4585 - classification_loss: 0.4869\n",
      "116/500 [=====>........................] - ETA: 6:48 - loss: 1.9458 - regression_loss: 1.4586 - classification_loss: 0.4872\n",
      "117/500 [======>.......................] - ETA: 6:47 - loss: 1.9430 - regression_loss: 1.4564 - classification_loss: 0.4865\n",
      "118/500 [======>.......................] - ETA: 6:45 - loss: 1.9359 - regression_loss: 1.4509 - classification_loss: 0.4850\n",
      "119/500 [======>.......................] - ETA: 6:44 - loss: 1.9314 - regression_loss: 1.4471 - classification_loss: 0.4843\n",
      "120/500 [======>.......................] - ETA: 6:42 - loss: 1.9268 - regression_loss: 1.4447 - classification_loss: 0.4821\n",
      "121/500 [======>.......................] - ETA: 6:41 - loss: 1.9330 - regression_loss: 1.4493 - classification_loss: 0.4837\n",
      "122/500 [======>.......................] - ETA: 6:40 - loss: 1.9304 - regression_loss: 1.4482 - classification_loss: 0.4822\n",
      "123/500 [======>.......................] - ETA: 6:39 - loss: 1.9432 - regression_loss: 1.4537 - classification_loss: 0.4894\n",
      "124/500 [======>.......................] - ETA: 6:38 - loss: 1.9506 - regression_loss: 1.4582 - classification_loss: 0.4923\n",
      "125/500 [======>.......................] - ETA: 6:36 - loss: 1.9533 - regression_loss: 1.4593 - classification_loss: 0.4940\n",
      "126/500 [======>.......................] - ETA: 6:35 - loss: 1.9522 - regression_loss: 1.4595 - classification_loss: 0.4927\n",
      "127/500 [======>.......................] - ETA: 6:34 - loss: 1.9512 - regression_loss: 1.4579 - classification_loss: 0.4933\n",
      "128/500 [======>.......................] - ETA: 6:33 - loss: 1.9446 - regression_loss: 1.4521 - classification_loss: 0.4924\n",
      "129/500 [======>.......................] - ETA: 6:32 - loss: 1.9414 - regression_loss: 1.4504 - classification_loss: 0.4910\n",
      "130/500 [======>.......................] - ETA: 6:31 - loss: 1.9400 - regression_loss: 1.4495 - classification_loss: 0.4906\n",
      "131/500 [======>.......................] - ETA: 6:30 - loss: 1.9359 - regression_loss: 1.4446 - classification_loss: 0.4913\n",
      "132/500 [======>.......................] - ETA: 6:28 - loss: 1.9368 - regression_loss: 1.4453 - classification_loss: 0.4915\n",
      "133/500 [======>.......................] - ETA: 6:27 - loss: 1.9408 - regression_loss: 1.4472 - classification_loss: 0.4936\n",
      "134/500 [=======>......................] - ETA: 6:26 - loss: 1.9419 - regression_loss: 1.4479 - classification_loss: 0.4940\n",
      "135/500 [=======>......................] - ETA: 6:24 - loss: 1.9449 - regression_loss: 1.4509 - classification_loss: 0.4940\n",
      "136/500 [=======>......................] - ETA: 6:24 - loss: 1.9431 - regression_loss: 1.4502 - classification_loss: 0.4929\n",
      "137/500 [=======>......................] - ETA: 6:23 - loss: 1.9451 - regression_loss: 1.4526 - classification_loss: 0.4925\n",
      "138/500 [=======>......................] - ETA: 6:22 - loss: 1.9487 - regression_loss: 1.4545 - classification_loss: 0.4942\n",
      "139/500 [=======>......................] - ETA: 6:21 - loss: 1.9571 - regression_loss: 1.4605 - classification_loss: 0.4966\n",
      "140/500 [=======>......................] - ETA: 6:19 - loss: 1.9550 - regression_loss: 1.4588 - classification_loss: 0.4963\n",
      "141/500 [=======>......................] - ETA: 6:18 - loss: 1.9516 - regression_loss: 1.4566 - classification_loss: 0.4950\n",
      "142/500 [=======>......................] - ETA: 6:17 - loss: 1.9470 - regression_loss: 1.4539 - classification_loss: 0.4931\n",
      "143/500 [=======>......................] - ETA: 6:16 - loss: 1.9488 - regression_loss: 1.4553 - classification_loss: 0.4935\n",
      "144/500 [=======>......................] - ETA: 6:15 - loss: 1.9512 - regression_loss: 1.4567 - classification_loss: 0.4945\n",
      "145/500 [=======>......................] - ETA: 6:14 - loss: 1.9468 - regression_loss: 1.4529 - classification_loss: 0.4940\n",
      "146/500 [=======>......................] - ETA: 6:13 - loss: 1.9500 - regression_loss: 1.4540 - classification_loss: 0.4961\n",
      "147/500 [=======>......................] - ETA: 6:11 - loss: 1.9446 - regression_loss: 1.4499 - classification_loss: 0.4947\n",
      "148/500 [=======>......................] - ETA: 6:10 - loss: 1.9431 - regression_loss: 1.4485 - classification_loss: 0.4946\n",
      "149/500 [=======>......................] - ETA: 6:09 - loss: 1.9418 - regression_loss: 1.4481 - classification_loss: 0.4937\n",
      "150/500 [========>.....................] - ETA: 6:08 - loss: 1.9473 - regression_loss: 1.4512 - classification_loss: 0.4961\n",
      "151/500 [========>.....................] - ETA: 6:07 - loss: 1.9492 - regression_loss: 1.4530 - classification_loss: 0.4961\n",
      "152/500 [========>.....................] - ETA: 6:06 - loss: 1.9453 - regression_loss: 1.4497 - classification_loss: 0.4955\n",
      "153/500 [========>.....................] - ETA: 6:07 - loss: 1.9543 - regression_loss: 1.4559 - classification_loss: 0.4984\n",
      "154/500 [========>.....................] - ETA: 6:06 - loss: 1.9563 - regression_loss: 1.4572 - classification_loss: 0.4991\n",
      "155/500 [========>.....................] - ETA: 6:05 - loss: 1.9559 - regression_loss: 1.4567 - classification_loss: 0.4991\n",
      "156/500 [========>.....................] - ETA: 6:04 - loss: 1.9565 - regression_loss: 1.4578 - classification_loss: 0.4987\n",
      "157/500 [========>.....................] - ETA: 6:03 - loss: 1.9519 - regression_loss: 1.4545 - classification_loss: 0.4974\n",
      "158/500 [========>.....................] - ETA: 6:01 - loss: 1.9590 - regression_loss: 1.4594 - classification_loss: 0.4996\n",
      "159/500 [========>.....................] - ETA: 6:00 - loss: 1.9569 - regression_loss: 1.4584 - classification_loss: 0.4985\n",
      "160/500 [========>.....................] - ETA: 5:59 - loss: 1.9554 - regression_loss: 1.4576 - classification_loss: 0.4978\n",
      "161/500 [========>.....................] - ETA: 5:58 - loss: 1.9578 - regression_loss: 1.4588 - classification_loss: 0.4990\n",
      "162/500 [========>.....................] - ETA: 5:57 - loss: 1.9632 - regression_loss: 1.4609 - classification_loss: 0.5023\n",
      "163/500 [========>.....................] - ETA: 5:56 - loss: 1.9593 - regression_loss: 1.4577 - classification_loss: 0.5016\n",
      "164/500 [========>.....................] - ETA: 5:55 - loss: 1.9637 - regression_loss: 1.4616 - classification_loss: 0.5020\n",
      "165/500 [========>.....................] - ETA: 5:54 - loss: 1.9662 - regression_loss: 1.4629 - classification_loss: 0.5033\n",
      "166/500 [========>.....................] - ETA: 5:53 - loss: 1.9632 - regression_loss: 1.4608 - classification_loss: 0.5024\n",
      "167/500 [=========>....................] - ETA: 5:51 - loss: 1.9577 - regression_loss: 1.4563 - classification_loss: 0.5014\n",
      "168/500 [=========>....................] - ETA: 5:50 - loss: 1.9560 - regression_loss: 1.4551 - classification_loss: 0.5009\n",
      "169/500 [=========>....................] - ETA: 5:49 - loss: 1.9533 - regression_loss: 1.4531 - classification_loss: 0.5002\n",
      "170/500 [=========>....................] - ETA: 5:48 - loss: 1.9504 - regression_loss: 1.4512 - classification_loss: 0.4992\n",
      "171/500 [=========>....................] - ETA: 5:47 - loss: 1.9477 - regression_loss: 1.4495 - classification_loss: 0.4982\n",
      "172/500 [=========>....................] - ETA: 5:46 - loss: 1.9451 - regression_loss: 1.4469 - classification_loss: 0.4981\n",
      "173/500 [=========>....................] - ETA: 5:45 - loss: 1.9474 - regression_loss: 1.4496 - classification_loss: 0.4978\n",
      "174/500 [=========>....................] - ETA: 5:44 - loss: 1.9488 - regression_loss: 1.4502 - classification_loss: 0.4986\n",
      "175/500 [=========>....................] - ETA: 5:43 - loss: 1.9488 - regression_loss: 1.4505 - classification_loss: 0.4983\n",
      "176/500 [=========>....................] - ETA: 5:42 - loss: 1.9498 - regression_loss: 1.4506 - classification_loss: 0.4992\n",
      "177/500 [=========>....................] - ETA: 5:41 - loss: 1.9444 - regression_loss: 1.4466 - classification_loss: 0.4979\n",
      "178/500 [=========>....................] - ETA: 5:40 - loss: 1.9484 - regression_loss: 1.4502 - classification_loss: 0.4982\n",
      "179/500 [=========>....................] - ETA: 5:39 - loss: 1.9471 - regression_loss: 1.4501 - classification_loss: 0.4970\n",
      "180/500 [=========>....................] - ETA: 5:38 - loss: 1.9476 - regression_loss: 1.4495 - classification_loss: 0.4981\n",
      "181/500 [=========>....................] - ETA: 5:36 - loss: 1.9479 - regression_loss: 1.4496 - classification_loss: 0.4983\n",
      "182/500 [=========>....................] - ETA: 5:35 - loss: 1.9498 - regression_loss: 1.4517 - classification_loss: 0.4981\n",
      "183/500 [=========>....................] - ETA: 5:34 - loss: 1.9469 - regression_loss: 1.4498 - classification_loss: 0.4971\n",
      "184/500 [==========>...................] - ETA: 5:33 - loss: 1.9495 - regression_loss: 1.4518 - classification_loss: 0.4976\n",
      "185/500 [==========>...................] - ETA: 5:32 - loss: 1.9519 - regression_loss: 1.4532 - classification_loss: 0.4988\n",
      "186/500 [==========>...................] - ETA: 5:31 - loss: 1.9496 - regression_loss: 1.4519 - classification_loss: 0.4977\n",
      "187/500 [==========>...................] - ETA: 5:30 - loss: 1.9517 - regression_loss: 1.4531 - classification_loss: 0.4986\n",
      "188/500 [==========>...................] - ETA: 5:29 - loss: 1.9534 - regression_loss: 1.4549 - classification_loss: 0.4985\n",
      "189/500 [==========>...................] - ETA: 5:28 - loss: 1.9527 - regression_loss: 1.4543 - classification_loss: 0.4984\n",
      "190/500 [==========>...................] - ETA: 5:27 - loss: 1.9518 - regression_loss: 1.4536 - classification_loss: 0.4982\n",
      "191/500 [==========>...................] - ETA: 5:26 - loss: 1.9495 - regression_loss: 1.4524 - classification_loss: 0.4972\n",
      "192/500 [==========>...................] - ETA: 5:25 - loss: 1.9479 - regression_loss: 1.4516 - classification_loss: 0.4963\n",
      "193/500 [==========>...................] - ETA: 5:24 - loss: 1.9466 - regression_loss: 1.4504 - classification_loss: 0.4963\n",
      "194/500 [==========>...................] - ETA: 5:23 - loss: 1.9436 - regression_loss: 1.4482 - classification_loss: 0.4954\n",
      "195/500 [==========>...................] - ETA: 5:22 - loss: 1.9421 - regression_loss: 1.4471 - classification_loss: 0.4950\n",
      "196/500 [==========>...................] - ETA: 5:20 - loss: 1.9384 - regression_loss: 1.4444 - classification_loss: 0.4940\n",
      "197/500 [==========>...................] - ETA: 5:19 - loss: 1.9387 - regression_loss: 1.4444 - classification_loss: 0.4943\n",
      "198/500 [==========>...................] - ETA: 5:18 - loss: 1.9399 - regression_loss: 1.4452 - classification_loss: 0.4947\n",
      "199/500 [==========>...................] - ETA: 5:17 - loss: 1.9387 - regression_loss: 1.4442 - classification_loss: 0.4945\n",
      "200/500 [===========>..................] - ETA: 5:16 - loss: 1.9372 - regression_loss: 1.4431 - classification_loss: 0.4941\n",
      "201/500 [===========>..................] - ETA: 5:15 - loss: 1.9351 - regression_loss: 1.4411 - classification_loss: 0.4940\n",
      "202/500 [===========>..................] - ETA: 5:14 - loss: 1.9383 - regression_loss: 1.4439 - classification_loss: 0.4944\n",
      "203/500 [===========>..................] - ETA: 5:13 - loss: 1.9353 - regression_loss: 1.4411 - classification_loss: 0.4941\n",
      "204/500 [===========>..................] - ETA: 5:12 - loss: 1.9344 - regression_loss: 1.4405 - classification_loss: 0.4940\n",
      "205/500 [===========>..................] - ETA: 5:11 - loss: 1.9323 - regression_loss: 1.4392 - classification_loss: 0.4931\n",
      "206/500 [===========>..................] - ETA: 5:10 - loss: 1.9309 - regression_loss: 1.4382 - classification_loss: 0.4927\n",
      "207/500 [===========>..................] - ETA: 5:08 - loss: 1.9339 - regression_loss: 1.4400 - classification_loss: 0.4939\n",
      "208/500 [===========>..................] - ETA: 5:07 - loss: 1.9328 - regression_loss: 1.4398 - classification_loss: 0.4929\n",
      "209/500 [===========>..................] - ETA: 5:06 - loss: 1.9359 - regression_loss: 1.4424 - classification_loss: 0.4935\n",
      "210/500 [===========>..................] - ETA: 5:05 - loss: 1.9382 - regression_loss: 1.4439 - classification_loss: 0.4943\n",
      "211/500 [===========>..................] - ETA: 5:04 - loss: 1.9385 - regression_loss: 1.4446 - classification_loss: 0.4938\n",
      "212/500 [===========>..................] - ETA: 5:03 - loss: 1.9409 - regression_loss: 1.4449 - classification_loss: 0.4961\n",
      "213/500 [===========>..................] - ETA: 5:02 - loss: 1.9403 - regression_loss: 1.4446 - classification_loss: 0.4957\n",
      "214/500 [===========>..................] - ETA: 5:01 - loss: 1.9394 - regression_loss: 1.4436 - classification_loss: 0.4957\n",
      "215/500 [===========>..................] - ETA: 4:59 - loss: 1.9386 - regression_loss: 1.4425 - classification_loss: 0.4961\n",
      "216/500 [===========>..................] - ETA: 4:58 - loss: 1.9404 - regression_loss: 1.4432 - classification_loss: 0.4972\n",
      "217/500 [============>.................] - ETA: 4:57 - loss: 1.9437 - regression_loss: 1.4462 - classification_loss: 0.4974\n",
      "218/500 [============>.................] - ETA: 4:56 - loss: 1.9410 - regression_loss: 1.4445 - classification_loss: 0.4965\n",
      "219/500 [============>.................] - ETA: 4:55 - loss: 1.9408 - regression_loss: 1.4433 - classification_loss: 0.4974\n",
      "220/500 [============>.................] - ETA: 4:54 - loss: 1.9417 - regression_loss: 1.4435 - classification_loss: 0.4982\n",
      "221/500 [============>.................] - ETA: 4:53 - loss: 1.9434 - regression_loss: 1.4447 - classification_loss: 0.4987\n",
      "222/500 [============>.................] - ETA: 4:51 - loss: 1.9437 - regression_loss: 1.4450 - classification_loss: 0.4987\n",
      "223/500 [============>.................] - ETA: 4:50 - loss: 1.9500 - regression_loss: 1.4492 - classification_loss: 0.5007\n",
      "224/500 [============>.................] - ETA: 4:49 - loss: 1.9527 - regression_loss: 1.4513 - classification_loss: 0.5015\n",
      "225/500 [============>.................] - ETA: 4:48 - loss: 1.9526 - regression_loss: 1.4513 - classification_loss: 0.5013\n",
      "226/500 [============>.................] - ETA: 4:47 - loss: 1.9522 - regression_loss: 1.4511 - classification_loss: 0.5011\n",
      "227/500 [============>.................] - ETA: 4:46 - loss: 1.9536 - regression_loss: 1.4524 - classification_loss: 0.5013\n",
      "228/500 [============>.................] - ETA: 4:45 - loss: 1.9524 - regression_loss: 1.4516 - classification_loss: 0.5007\n",
      "229/500 [============>.................] - ETA: 4:44 - loss: 1.9557 - regression_loss: 1.4540 - classification_loss: 0.5017\n",
      "230/500 [============>.................] - ETA: 4:43 - loss: 1.9547 - regression_loss: 1.4535 - classification_loss: 0.5012\n",
      "231/500 [============>.................] - ETA: 4:42 - loss: 1.9587 - regression_loss: 1.4566 - classification_loss: 0.5021\n",
      "232/500 [============>.................] - ETA: 4:41 - loss: 1.9571 - regression_loss: 1.4554 - classification_loss: 0.5017\n",
      "233/500 [============>.................] - ETA: 4:40 - loss: 1.9536 - regression_loss: 1.4530 - classification_loss: 0.5006\n",
      "234/500 [=============>................] - ETA: 4:39 - loss: 1.9561 - regression_loss: 1.4547 - classification_loss: 0.5015\n",
      "235/500 [=============>................] - ETA: 4:38 - loss: 1.9584 - regression_loss: 1.4567 - classification_loss: 0.5017\n",
      "236/500 [=============>................] - ETA: 4:37 - loss: 1.9601 - regression_loss: 1.4578 - classification_loss: 0.5023\n",
      "237/500 [=============>................] - ETA: 4:36 - loss: 1.9638 - regression_loss: 1.4605 - classification_loss: 0.5033\n",
      "238/500 [=============>................] - ETA: 4:35 - loss: 1.9662 - regression_loss: 1.4620 - classification_loss: 0.5043\n",
      "239/500 [=============>................] - ETA: 4:34 - loss: 1.9658 - regression_loss: 1.4616 - classification_loss: 0.5041\n",
      "240/500 [=============>................] - ETA: 4:32 - loss: 1.9654 - regression_loss: 1.4619 - classification_loss: 0.5036\n",
      "241/500 [=============>................] - ETA: 4:31 - loss: 1.9678 - regression_loss: 1.4643 - classification_loss: 0.5035\n",
      "242/500 [=============>................] - ETA: 4:30 - loss: 1.9679 - regression_loss: 1.4647 - classification_loss: 0.5033\n",
      "243/500 [=============>................] - ETA: 4:29 - loss: 1.9701 - regression_loss: 1.4666 - classification_loss: 0.5035\n",
      "244/500 [=============>................] - ETA: 4:28 - loss: 1.9697 - regression_loss: 1.4668 - classification_loss: 0.5029\n",
      "245/500 [=============>................] - ETA: 4:27 - loss: 1.9692 - regression_loss: 1.4670 - classification_loss: 0.5021\n",
      "246/500 [=============>................] - ETA: 4:26 - loss: 1.9701 - regression_loss: 1.4678 - classification_loss: 0.5023\n",
      "247/500 [=============>................] - ETA: 4:25 - loss: 1.9672 - regression_loss: 1.4660 - classification_loss: 0.5012\n",
      "248/500 [=============>................] - ETA: 4:24 - loss: 1.9662 - regression_loss: 1.4648 - classification_loss: 0.5014\n",
      "249/500 [=============>................] - ETA: 4:23 - loss: 1.9630 - regression_loss: 1.4626 - classification_loss: 0.5004\n",
      "250/500 [==============>...............] - ETA: 4:22 - loss: 1.9617 - regression_loss: 1.4620 - classification_loss: 0.4998\n",
      "251/500 [==============>...............] - ETA: 4:21 - loss: 1.9586 - regression_loss: 1.4593 - classification_loss: 0.4993\n",
      "252/500 [==============>...............] - ETA: 4:20 - loss: 1.9565 - regression_loss: 1.4578 - classification_loss: 0.4987\n",
      "253/500 [==============>...............] - ETA: 4:19 - loss: 1.9556 - regression_loss: 1.4577 - classification_loss: 0.4979\n",
      "254/500 [==============>...............] - ETA: 4:17 - loss: 1.9581 - regression_loss: 1.4596 - classification_loss: 0.4985\n",
      "255/500 [==============>...............] - ETA: 4:16 - loss: 1.9560 - regression_loss: 1.4584 - classification_loss: 0.4976\n",
      "256/500 [==============>...............] - ETA: 4:15 - loss: 1.9530 - regression_loss: 1.4565 - classification_loss: 0.4966\n",
      "257/500 [==============>...............] - ETA: 4:14 - loss: 1.9557 - regression_loss: 1.4583 - classification_loss: 0.4974\n",
      "258/500 [==============>...............] - ETA: 4:13 - loss: 1.9545 - regression_loss: 1.4579 - classification_loss: 0.4967\n",
      "259/500 [==============>...............] - ETA: 4:12 - loss: 1.9554 - regression_loss: 1.4587 - classification_loss: 0.4967\n",
      "260/500 [==============>...............] - ETA: 4:11 - loss: 1.9516 - regression_loss: 1.4558 - classification_loss: 0.4957\n",
      "261/500 [==============>...............] - ETA: 4:10 - loss: 1.9484 - regression_loss: 1.4534 - classification_loss: 0.4950\n",
      "262/500 [==============>...............] - ETA: 4:09 - loss: 1.9490 - regression_loss: 1.4535 - classification_loss: 0.4955\n",
      "263/500 [==============>...............] - ETA: 4:08 - loss: 1.9474 - regression_loss: 1.4521 - classification_loss: 0.4953\n",
      "264/500 [==============>...............] - ETA: 4:07 - loss: 1.9482 - regression_loss: 1.4530 - classification_loss: 0.4952\n",
      "265/500 [==============>...............] - ETA: 4:06 - loss: 1.9465 - regression_loss: 1.4520 - classification_loss: 0.4945\n",
      "266/500 [==============>...............] - ETA: 4:05 - loss: 1.9484 - regression_loss: 1.4532 - classification_loss: 0.4952\n",
      "267/500 [===============>..............] - ETA: 4:04 - loss: 1.9469 - regression_loss: 1.4524 - classification_loss: 0.4946\n",
      "268/500 [===============>..............] - ETA: 4:02 - loss: 1.9474 - regression_loss: 1.4530 - classification_loss: 0.4943\n",
      "269/500 [===============>..............] - ETA: 4:01 - loss: 1.9457 - regression_loss: 1.4521 - classification_loss: 0.4936\n",
      "270/500 [===============>..............] - ETA: 4:00 - loss: 1.9483 - regression_loss: 1.4536 - classification_loss: 0.4946\n",
      "271/500 [===============>..............] - ETA: 3:59 - loss: 1.9473 - regression_loss: 1.4531 - classification_loss: 0.4942\n",
      "272/500 [===============>..............] - ETA: 3:58 - loss: 1.9453 - regression_loss: 1.4516 - classification_loss: 0.4937\n",
      "273/500 [===============>..............] - ETA: 3:57 - loss: 1.9494 - regression_loss: 1.4543 - classification_loss: 0.4952\n",
      "274/500 [===============>..............] - ETA: 3:56 - loss: 1.9502 - regression_loss: 1.4554 - classification_loss: 0.4948\n",
      "275/500 [===============>..............] - ETA: 3:55 - loss: 1.9501 - regression_loss: 1.4555 - classification_loss: 0.4945\n",
      "276/500 [===============>..............] - ETA: 3:54 - loss: 1.9469 - regression_loss: 1.4531 - classification_loss: 0.4937\n",
      "277/500 [===============>..............] - ETA: 3:53 - loss: 1.9484 - regression_loss: 1.4539 - classification_loss: 0.4945\n",
      "278/500 [===============>..............] - ETA: 3:52 - loss: 1.9492 - regression_loss: 1.4543 - classification_loss: 0.4949\n",
      "279/500 [===============>..............] - ETA: 3:51 - loss: 1.9470 - regression_loss: 1.4530 - classification_loss: 0.4940\n",
      "280/500 [===============>..............] - ETA: 3:50 - loss: 1.9473 - regression_loss: 1.4536 - classification_loss: 0.4938\n",
      "281/500 [===============>..............] - ETA: 3:49 - loss: 1.9491 - regression_loss: 1.4555 - classification_loss: 0.4937\n",
      "282/500 [===============>..............] - ETA: 3:47 - loss: 1.9514 - regression_loss: 1.4570 - classification_loss: 0.4944\n",
      "283/500 [===============>..............] - ETA: 3:46 - loss: 1.9521 - regression_loss: 1.4581 - classification_loss: 0.4940\n",
      "284/500 [================>.............] - ETA: 3:45 - loss: 1.9505 - regression_loss: 1.4571 - classification_loss: 0.4935\n",
      "285/500 [================>.............] - ETA: 3:44 - loss: 1.9526 - regression_loss: 1.4581 - classification_loss: 0.4944\n",
      "286/500 [================>.............] - ETA: 3:43 - loss: 1.9512 - regression_loss: 1.4574 - classification_loss: 0.4939\n",
      "287/500 [================>.............] - ETA: 3:42 - loss: 1.9494 - regression_loss: 1.4564 - classification_loss: 0.4930\n",
      "288/500 [================>.............] - ETA: 3:41 - loss: 1.9500 - regression_loss: 1.4569 - classification_loss: 0.4931\n",
      "289/500 [================>.............] - ETA: 3:40 - loss: 1.9521 - regression_loss: 1.4583 - classification_loss: 0.4939\n",
      "290/500 [================>.............] - ETA: 3:39 - loss: 1.9496 - regression_loss: 1.4564 - classification_loss: 0.4932\n",
      "291/500 [================>.............] - ETA: 3:38 - loss: 1.9494 - regression_loss: 1.4563 - classification_loss: 0.4931\n",
      "292/500 [================>.............] - ETA: 3:37 - loss: 1.9474 - regression_loss: 1.4545 - classification_loss: 0.4929\n",
      "293/500 [================>.............] - ETA: 3:36 - loss: 1.9470 - regression_loss: 1.4543 - classification_loss: 0.4927\n",
      "294/500 [================>.............] - ETA: 3:35 - loss: 1.9457 - regression_loss: 1.4534 - classification_loss: 0.4922\n",
      "295/500 [================>.............] - ETA: 3:34 - loss: 1.9495 - regression_loss: 1.4560 - classification_loss: 0.4935\n",
      "296/500 [================>.............] - ETA: 3:33 - loss: 1.9486 - regression_loss: 1.4553 - classification_loss: 0.4933\n",
      "297/500 [================>.............] - ETA: 3:32 - loss: 1.9480 - regression_loss: 1.4545 - classification_loss: 0.4935\n",
      "298/500 [================>.............] - ETA: 3:31 - loss: 1.9484 - regression_loss: 1.4546 - classification_loss: 0.4937\n",
      "299/500 [================>.............] - ETA: 3:30 - loss: 1.9461 - regression_loss: 1.4528 - classification_loss: 0.4933\n",
      "300/500 [=================>............] - ETA: 3:29 - loss: 1.9451 - regression_loss: 1.4516 - classification_loss: 0.4935\n",
      "301/500 [=================>............] - ETA: 3:28 - loss: 1.9439 - regression_loss: 1.4510 - classification_loss: 0.4929\n",
      "302/500 [=================>............] - ETA: 3:27 - loss: 1.9434 - regression_loss: 1.4506 - classification_loss: 0.4928\n",
      "303/500 [=================>............] - ETA: 3:26 - loss: 1.9449 - regression_loss: 1.4523 - classification_loss: 0.4925\n",
      "304/500 [=================>............] - ETA: 3:25 - loss: 1.9440 - regression_loss: 1.4515 - classification_loss: 0.4925\n",
      "305/500 [=================>............] - ETA: 3:23 - loss: 1.9457 - regression_loss: 1.4525 - classification_loss: 0.4932\n",
      "306/500 [=================>............] - ETA: 3:22 - loss: 1.9492 - regression_loss: 1.4549 - classification_loss: 0.4943\n",
      "307/500 [=================>............] - ETA: 3:21 - loss: 1.9485 - regression_loss: 1.4546 - classification_loss: 0.4940\n",
      "308/500 [=================>............] - ETA: 3:20 - loss: 1.9506 - regression_loss: 1.4557 - classification_loss: 0.4949\n",
      "309/500 [=================>............] - ETA: 3:19 - loss: 1.9501 - regression_loss: 1.4554 - classification_loss: 0.4947\n",
      "310/500 [=================>............] - ETA: 3:18 - loss: 1.9494 - regression_loss: 1.4546 - classification_loss: 0.4948\n",
      "311/500 [=================>............] - ETA: 3:17 - loss: 1.9495 - regression_loss: 1.4547 - classification_loss: 0.4948\n",
      "312/500 [=================>............] - ETA: 3:16 - loss: 1.9491 - regression_loss: 1.4540 - classification_loss: 0.4950\n",
      "313/500 [=================>............] - ETA: 3:15 - loss: 1.9478 - regression_loss: 1.4531 - classification_loss: 0.4947\n",
      "314/500 [=================>............] - ETA: 3:14 - loss: 1.9475 - regression_loss: 1.4528 - classification_loss: 0.4947\n",
      "315/500 [=================>............] - ETA: 3:13 - loss: 1.9466 - regression_loss: 1.4522 - classification_loss: 0.4944\n",
      "316/500 [=================>............] - ETA: 3:12 - loss: 1.9465 - regression_loss: 1.4519 - classification_loss: 0.4945\n",
      "317/500 [==================>...........] - ETA: 3:11 - loss: 1.9440 - regression_loss: 1.4502 - classification_loss: 0.4938\n",
      "318/500 [==================>...........] - ETA: 3:09 - loss: 1.9442 - regression_loss: 1.4502 - classification_loss: 0.4940\n",
      "319/500 [==================>...........] - ETA: 3:08 - loss: 1.9434 - regression_loss: 1.4496 - classification_loss: 0.4938\n",
      "320/500 [==================>...........] - ETA: 3:07 - loss: 1.9437 - regression_loss: 1.4500 - classification_loss: 0.4937\n",
      "321/500 [==================>...........] - ETA: 3:06 - loss: 1.9422 - regression_loss: 1.4491 - classification_loss: 0.4931\n",
      "322/500 [==================>...........] - ETA: 3:05 - loss: 1.9423 - regression_loss: 1.4492 - classification_loss: 0.4930\n",
      "323/500 [==================>...........] - ETA: 3:04 - loss: 1.9407 - regression_loss: 1.4484 - classification_loss: 0.4923\n",
      "324/500 [==================>...........] - ETA: 3:03 - loss: 1.9418 - regression_loss: 1.4497 - classification_loss: 0.4921\n",
      "325/500 [==================>...........] - ETA: 3:02 - loss: 1.9423 - regression_loss: 1.4502 - classification_loss: 0.4921\n",
      "326/500 [==================>...........] - ETA: 3:01 - loss: 1.9402 - regression_loss: 1.4489 - classification_loss: 0.4914\n",
      "327/500 [==================>...........] - ETA: 3:00 - loss: 1.9397 - regression_loss: 1.4485 - classification_loss: 0.4912\n",
      "328/500 [==================>...........] - ETA: 2:59 - loss: 1.9403 - regression_loss: 1.4491 - classification_loss: 0.4913\n",
      "329/500 [==================>...........] - ETA: 2:58 - loss: 1.9379 - regression_loss: 1.4470 - classification_loss: 0.4909\n",
      "330/500 [==================>...........] - ETA: 2:57 - loss: 1.9377 - regression_loss: 1.4468 - classification_loss: 0.4909\n",
      "331/500 [==================>...........] - ETA: 2:56 - loss: 1.9351 - regression_loss: 1.4449 - classification_loss: 0.4901\n",
      "332/500 [==================>...........] - ETA: 2:55 - loss: 1.9343 - regression_loss: 1.4444 - classification_loss: 0.4899\n",
      "333/500 [==================>...........] - ETA: 2:53 - loss: 1.9342 - regression_loss: 1.4444 - classification_loss: 0.4898\n",
      "334/500 [===================>..........] - ETA: 2:52 - loss: 1.9367 - regression_loss: 1.4462 - classification_loss: 0.4905\n",
      "335/500 [===================>..........] - ETA: 2:51 - loss: 1.9390 - regression_loss: 1.4479 - classification_loss: 0.4911\n",
      "336/500 [===================>..........] - ETA: 2:50 - loss: 1.9395 - regression_loss: 1.4480 - classification_loss: 0.4915\n",
      "337/500 [===================>..........] - ETA: 2:49 - loss: 1.9394 - regression_loss: 1.4477 - classification_loss: 0.4917\n",
      "338/500 [===================>..........] - ETA: 2:48 - loss: 1.9394 - regression_loss: 1.4482 - classification_loss: 0.4912\n",
      "339/500 [===================>..........] - ETA: 2:47 - loss: 1.9391 - regression_loss: 1.4482 - classification_loss: 0.4909\n",
      "340/500 [===================>..........] - ETA: 2:46 - loss: 1.9378 - regression_loss: 1.4473 - classification_loss: 0.4904\n",
      "341/500 [===================>..........] - ETA: 2:45 - loss: 1.9353 - regression_loss: 1.4455 - classification_loss: 0.4898\n",
      "342/500 [===================>..........] - ETA: 2:44 - loss: 1.9349 - regression_loss: 1.4451 - classification_loss: 0.4898\n",
      "343/500 [===================>..........] - ETA: 2:43 - loss: 1.9350 - regression_loss: 1.4446 - classification_loss: 0.4905\n",
      "344/500 [===================>..........] - ETA: 2:42 - loss: 1.9340 - regression_loss: 1.4438 - classification_loss: 0.4903\n",
      "345/500 [===================>..........] - ETA: 2:41 - loss: 1.9334 - regression_loss: 1.4432 - classification_loss: 0.4903\n",
      "346/500 [===================>..........] - ETA: 2:40 - loss: 1.9313 - regression_loss: 1.4417 - classification_loss: 0.4896\n",
      "347/500 [===================>..........] - ETA: 2:39 - loss: 1.9290 - regression_loss: 1.4401 - classification_loss: 0.4889\n",
      "348/500 [===================>..........] - ETA: 2:38 - loss: 1.9294 - regression_loss: 1.4406 - classification_loss: 0.4887\n",
      "349/500 [===================>..........] - ETA: 2:37 - loss: 1.9301 - regression_loss: 1.4415 - classification_loss: 0.4886\n",
      "350/500 [====================>.........] - ETA: 2:36 - loss: 1.9305 - regression_loss: 1.4417 - classification_loss: 0.4888\n",
      "351/500 [====================>.........] - ETA: 2:35 - loss: 1.9280 - regression_loss: 1.4401 - classification_loss: 0.4879\n",
      "352/500 [====================>.........] - ETA: 2:34 - loss: 1.9311 - regression_loss: 1.4423 - classification_loss: 0.4888\n",
      "353/500 [====================>.........] - ETA: 2:33 - loss: 1.9312 - regression_loss: 1.4425 - classification_loss: 0.4887\n",
      "354/500 [====================>.........] - ETA: 2:32 - loss: 1.9302 - regression_loss: 1.4420 - classification_loss: 0.4882\n",
      "355/500 [====================>.........] - ETA: 2:31 - loss: 1.9311 - regression_loss: 1.4422 - classification_loss: 0.4889\n",
      "356/500 [====================>.........] - ETA: 2:30 - loss: 1.9300 - regression_loss: 1.4412 - classification_loss: 0.4888\n",
      "357/500 [====================>.........] - ETA: 2:29 - loss: 1.9285 - regression_loss: 1.4401 - classification_loss: 0.4884\n",
      "358/500 [====================>.........] - ETA: 2:27 - loss: 1.9286 - regression_loss: 1.4401 - classification_loss: 0.4886\n",
      "359/500 [====================>.........] - ETA: 2:26 - loss: 1.9267 - regression_loss: 1.4385 - classification_loss: 0.4883\n",
      "360/500 [====================>.........] - ETA: 2:25 - loss: 1.9253 - regression_loss: 1.4374 - classification_loss: 0.4879\n",
      "361/500 [====================>.........] - ETA: 2:24 - loss: 1.9265 - regression_loss: 1.4384 - classification_loss: 0.4881\n",
      "362/500 [====================>.........] - ETA: 2:23 - loss: 1.9280 - regression_loss: 1.4394 - classification_loss: 0.4886\n",
      "363/500 [====================>.........] - ETA: 2:22 - loss: 1.9277 - regression_loss: 1.4391 - classification_loss: 0.4886\n",
      "364/500 [====================>.........] - ETA: 2:21 - loss: 1.9277 - regression_loss: 1.4392 - classification_loss: 0.4885\n",
      "365/500 [====================>.........] - ETA: 2:20 - loss: 1.9256 - regression_loss: 1.4376 - classification_loss: 0.4880\n",
      "366/500 [====================>.........] - ETA: 2:19 - loss: 1.9279 - regression_loss: 1.4396 - classification_loss: 0.4883\n",
      "367/500 [=====================>........] - ETA: 2:18 - loss: 1.9268 - regression_loss: 1.4389 - classification_loss: 0.4879\n",
      "368/500 [=====================>........] - ETA: 2:17 - loss: 1.9269 - regression_loss: 1.4387 - classification_loss: 0.4883\n",
      "369/500 [=====================>........] - ETA: 2:16 - loss: 1.9283 - regression_loss: 1.4400 - classification_loss: 0.4883\n",
      "370/500 [=====================>........] - ETA: 2:15 - loss: 1.9279 - regression_loss: 1.4395 - classification_loss: 0.4883\n",
      "371/500 [=====================>........] - ETA: 2:14 - loss: 1.9304 - regression_loss: 1.4414 - classification_loss: 0.4889\n",
      "372/500 [=====================>........] - ETA: 2:13 - loss: 1.9311 - regression_loss: 1.4418 - classification_loss: 0.4892\n",
      "373/500 [=====================>........] - ETA: 2:12 - loss: 1.9304 - regression_loss: 1.4414 - classification_loss: 0.4889\n",
      "374/500 [=====================>........] - ETA: 2:11 - loss: 1.9273 - regression_loss: 1.4391 - classification_loss: 0.4882\n",
      "375/500 [=====================>........] - ETA: 2:10 - loss: 1.9268 - regression_loss: 1.4388 - classification_loss: 0.4880\n",
      "376/500 [=====================>........] - ETA: 2:09 - loss: 1.9274 - regression_loss: 1.4389 - classification_loss: 0.4884\n",
      "377/500 [=====================>........] - ETA: 2:07 - loss: 1.9287 - regression_loss: 1.4397 - classification_loss: 0.4890\n",
      "378/500 [=====================>........] - ETA: 2:06 - loss: 1.9260 - regression_loss: 1.4376 - classification_loss: 0.4885\n",
      "379/500 [=====================>........] - ETA: 2:05 - loss: 1.9257 - regression_loss: 1.4376 - classification_loss: 0.4881\n",
      "380/500 [=====================>........] - ETA: 2:04 - loss: 1.9273 - regression_loss: 1.4383 - classification_loss: 0.4890\n",
      "381/500 [=====================>........] - ETA: 2:03 - loss: 1.9271 - regression_loss: 1.4381 - classification_loss: 0.4890\n",
      "382/500 [=====================>........] - ETA: 2:02 - loss: 1.9291 - regression_loss: 1.4395 - classification_loss: 0.4897\n",
      "383/500 [=====================>........] - ETA: 2:01 - loss: 1.9284 - regression_loss: 1.4389 - classification_loss: 0.4895\n",
      "384/500 [======================>.......] - ETA: 2:00 - loss: 1.9290 - regression_loss: 1.4389 - classification_loss: 0.4901\n",
      "385/500 [======================>.......] - ETA: 1:59 - loss: 1.9277 - regression_loss: 1.4380 - classification_loss: 0.4898\n",
      "386/500 [======================>.......] - ETA: 1:58 - loss: 1.9301 - regression_loss: 1.4389 - classification_loss: 0.4911\n",
      "387/500 [======================>.......] - ETA: 1:57 - loss: 1.9289 - regression_loss: 1.4382 - classification_loss: 0.4907\n",
      "388/500 [======================>.......] - ETA: 1:56 - loss: 1.9321 - regression_loss: 1.4398 - classification_loss: 0.4923\n",
      "389/500 [======================>.......] - ETA: 1:55 - loss: 1.9328 - regression_loss: 1.4406 - classification_loss: 0.4922\n",
      "390/500 [======================>.......] - ETA: 1:54 - loss: 1.9321 - regression_loss: 1.4400 - classification_loss: 0.4921\n",
      "391/500 [======================>.......] - ETA: 1:53 - loss: 1.9343 - regression_loss: 1.4414 - classification_loss: 0.4929\n",
      "392/500 [======================>.......] - ETA: 1:52 - loss: 1.9341 - regression_loss: 1.4413 - classification_loss: 0.4927\n",
      "393/500 [======================>.......] - ETA: 1:51 - loss: 1.9349 - regression_loss: 1.4420 - classification_loss: 0.4928\n",
      "394/500 [======================>.......] - ETA: 1:50 - loss: 1.9350 - regression_loss: 1.4419 - classification_loss: 0.4931\n",
      "395/500 [======================>.......] - ETA: 1:49 - loss: 1.9353 - regression_loss: 1.4421 - classification_loss: 0.4932\n",
      "396/500 [======================>.......] - ETA: 1:48 - loss: 1.9371 - regression_loss: 1.4437 - classification_loss: 0.4934\n",
      "397/500 [======================>.......] - ETA: 1:47 - loss: 1.9360 - regression_loss: 1.4430 - classification_loss: 0.4930\n",
      "398/500 [======================>.......] - ETA: 1:46 - loss: 1.9341 - regression_loss: 1.4417 - classification_loss: 0.4925\n",
      "399/500 [======================>.......] - ETA: 1:45 - loss: 1.9358 - regression_loss: 1.4428 - classification_loss: 0.4930\n",
      "400/500 [=======================>......] - ETA: 1:44 - loss: 1.9344 - regression_loss: 1.4415 - classification_loss: 0.4929\n",
      "401/500 [=======================>......] - ETA: 1:43 - loss: 1.9367 - regression_loss: 1.4428 - classification_loss: 0.4939\n",
      "402/500 [=======================>......] - ETA: 1:42 - loss: 1.9350 - regression_loss: 1.4416 - classification_loss: 0.4934\n",
      "403/500 [=======================>......] - ETA: 1:41 - loss: 1.9362 - regression_loss: 1.4425 - classification_loss: 0.4937\n",
      "404/500 [=======================>......] - ETA: 1:39 - loss: 1.9351 - regression_loss: 1.4420 - classification_loss: 0.4932\n",
      "405/500 [=======================>......] - ETA: 1:38 - loss: 1.9348 - regression_loss: 1.4416 - classification_loss: 0.4932\n",
      "406/500 [=======================>......] - ETA: 1:37 - loss: 1.9348 - regression_loss: 1.4417 - classification_loss: 0.4931\n",
      "407/500 [=======================>......] - ETA: 1:36 - loss: 1.9341 - regression_loss: 1.4412 - classification_loss: 0.4929\n",
      "408/500 [=======================>......] - ETA: 1:35 - loss: 1.9335 - regression_loss: 1.4410 - classification_loss: 0.4925\n",
      "409/500 [=======================>......] - ETA: 1:34 - loss: 1.9326 - regression_loss: 1.4403 - classification_loss: 0.4923\n",
      "410/500 [=======================>......] - ETA: 1:33 - loss: 1.9339 - regression_loss: 1.4408 - classification_loss: 0.4931\n",
      "411/500 [=======================>......] - ETA: 1:33 - loss: 1.9323 - regression_loss: 1.4397 - classification_loss: 0.4926\n",
      "412/500 [=======================>......] - ETA: 1:32 - loss: 1.9310 - regression_loss: 1.4388 - classification_loss: 0.4922\n",
      "413/500 [=======================>......] - ETA: 1:31 - loss: 1.9292 - regression_loss: 1.4375 - classification_loss: 0.4917\n",
      "414/500 [=======================>......] - ETA: 1:29 - loss: 1.9298 - regression_loss: 1.4382 - classification_loss: 0.4917\n",
      "415/500 [=======================>......] - ETA: 1:28 - loss: 1.9308 - regression_loss: 1.4389 - classification_loss: 0.4918\n",
      "416/500 [=======================>......] - ETA: 1:27 - loss: 1.9317 - regression_loss: 1.4396 - classification_loss: 0.4921\n",
      "417/500 [========================>.....] - ETA: 1:26 - loss: 1.9307 - regression_loss: 1.4391 - classification_loss: 0.4916\n",
      "418/500 [========================>.....] - ETA: 1:25 - loss: 1.9304 - regression_loss: 1.4389 - classification_loss: 0.4915\n",
      "419/500 [========================>.....] - ETA: 1:24 - loss: 1.9299 - regression_loss: 1.4386 - classification_loss: 0.4912\n",
      "420/500 [========================>.....] - ETA: 1:23 - loss: 1.9287 - regression_loss: 1.4376 - classification_loss: 0.4910\n",
      "421/500 [========================>.....] - ETA: 1:22 - loss: 1.9285 - regression_loss: 1.4377 - classification_loss: 0.4909\n",
      "422/500 [========================>.....] - ETA: 1:21 - loss: 1.9276 - regression_loss: 1.4370 - classification_loss: 0.4906\n",
      "423/500 [========================>.....] - ETA: 1:20 - loss: 1.9276 - regression_loss: 1.4373 - classification_loss: 0.4904\n",
      "424/500 [========================>.....] - ETA: 1:19 - loss: 1.9281 - regression_loss: 1.4375 - classification_loss: 0.4906\n",
      "425/500 [========================>.....] - ETA: 1:18 - loss: 1.9287 - regression_loss: 1.4381 - classification_loss: 0.4907\n",
      "426/500 [========================>.....] - ETA: 1:17 - loss: 1.9290 - regression_loss: 1.4384 - classification_loss: 0.4906\n",
      "427/500 [========================>.....] - ETA: 1:16 - loss: 1.9312 - regression_loss: 1.4397 - classification_loss: 0.4915\n",
      "428/500 [========================>.....] - ETA: 1:15 - loss: 1.9316 - regression_loss: 1.4395 - classification_loss: 0.4921\n",
      "429/500 [========================>.....] - ETA: 1:14 - loss: 1.9294 - regression_loss: 1.4380 - classification_loss: 0.4914\n",
      "430/500 [========================>.....] - ETA: 1:13 - loss: 1.9303 - regression_loss: 1.4388 - classification_loss: 0.4915\n",
      "431/500 [========================>.....] - ETA: 1:12 - loss: 1.9293 - regression_loss: 1.4381 - classification_loss: 0.4911\n",
      "432/500 [========================>.....] - ETA: 1:10 - loss: 1.9286 - regression_loss: 1.4375 - classification_loss: 0.4911\n",
      "433/500 [========================>.....] - ETA: 1:09 - loss: 1.9287 - regression_loss: 1.4374 - classification_loss: 0.4913\n",
      "434/500 [=========================>....] - ETA: 1:08 - loss: 1.9305 - regression_loss: 1.4388 - classification_loss: 0.4917\n",
      "435/500 [=========================>....] - ETA: 1:07 - loss: 1.9294 - regression_loss: 1.4380 - classification_loss: 0.4914\n",
      "436/500 [=========================>....] - ETA: 1:06 - loss: 1.9288 - regression_loss: 1.4378 - classification_loss: 0.4910\n",
      "437/500 [=========================>....] - ETA: 1:05 - loss: 1.9299 - regression_loss: 1.4389 - classification_loss: 0.4910\n",
      "438/500 [=========================>....] - ETA: 1:04 - loss: 1.9309 - regression_loss: 1.4400 - classification_loss: 0.4909\n",
      "439/500 [=========================>....] - ETA: 1:03 - loss: 1.9327 - regression_loss: 1.4413 - classification_loss: 0.4914\n",
      "440/500 [=========================>....] - ETA: 1:02 - loss: 1.9319 - regression_loss: 1.4407 - classification_loss: 0.4912\n",
      "441/500 [=========================>....] - ETA: 1:01 - loss: 1.9312 - regression_loss: 1.4400 - classification_loss: 0.4912\n",
      "442/500 [=========================>....] - ETA: 1:00 - loss: 1.9301 - regression_loss: 1.4392 - classification_loss: 0.4910\n",
      "443/500 [=========================>....] - ETA: 59s - loss: 1.9304 - regression_loss: 1.4390 - classification_loss: 0.4914 \n",
      "444/500 [=========================>....] - ETA: 58s - loss: 1.9312 - regression_loss: 1.4395 - classification_loss: 0.4916\n",
      "445/500 [=========================>....] - ETA: 57s - loss: 1.9297 - regression_loss: 1.4386 - classification_loss: 0.4911\n",
      "446/500 [=========================>....] - ETA: 56s - loss: 1.9283 - regression_loss: 1.4378 - classification_loss: 0.4905\n",
      "447/500 [=========================>....] - ETA: 55s - loss: 1.9279 - regression_loss: 1.4376 - classification_loss: 0.4903\n",
      "448/500 [=========================>....] - ETA: 54s - loss: 1.9288 - regression_loss: 1.4381 - classification_loss: 0.4907\n",
      "449/500 [=========================>....] - ETA: 53s - loss: 1.9315 - regression_loss: 1.4402 - classification_loss: 0.4913\n",
      "450/500 [==========================>...] - ETA: 52s - loss: 1.9313 - regression_loss: 1.4398 - classification_loss: 0.4915\n",
      "451/500 [==========================>...] - ETA: 51s - loss: 1.9322 - regression_loss: 1.4399 - classification_loss: 0.4923\n",
      "452/500 [==========================>...] - ETA: 50s - loss: 1.9325 - regression_loss: 1.4404 - classification_loss: 0.4921\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.9333 - regression_loss: 1.4409 - classification_loss: 0.4924\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.9324 - regression_loss: 1.4402 - classification_loss: 0.4923\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.9320 - regression_loss: 1.4399 - classification_loss: 0.4921\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.9313 - regression_loss: 1.4393 - classification_loss: 0.4921\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.9317 - regression_loss: 1.4397 - classification_loss: 0.4919\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.9315 - regression_loss: 1.4396 - classification_loss: 0.4919\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.9305 - regression_loss: 1.4385 - classification_loss: 0.4920\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.9315 - regression_loss: 1.4392 - classification_loss: 0.4923\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 1.9306 - regression_loss: 1.4384 - classification_loss: 0.4922\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 1.9287 - regression_loss: 1.4372 - classification_loss: 0.4916\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 1.9278 - regression_loss: 1.4368 - classification_loss: 0.4910\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 1.9264 - regression_loss: 1.4359 - classification_loss: 0.4906\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 1.9264 - regression_loss: 1.4358 - classification_loss: 0.4906\n",
      "466/500 [==========================>...] - ETA: 35s - loss: 1.9254 - regression_loss: 1.4351 - classification_loss: 0.4903\n",
      "467/500 [===========================>..] - ETA: 34s - loss: 1.9261 - regression_loss: 1.4355 - classification_loss: 0.4906\n",
      "468/500 [===========================>..] - ETA: 33s - loss: 1.9287 - regression_loss: 1.4373 - classification_loss: 0.4914\n",
      "469/500 [===========================>..] - ETA: 32s - loss: 1.9278 - regression_loss: 1.4367 - classification_loss: 0.4911\n",
      "470/500 [===========================>..] - ETA: 31s - loss: 1.9292 - regression_loss: 1.4375 - classification_loss: 0.4917\n",
      "471/500 [===========================>..] - ETA: 30s - loss: 1.9280 - regression_loss: 1.4367 - classification_loss: 0.4913\n",
      "472/500 [===========================>..] - ETA: 29s - loss: 1.9280 - regression_loss: 1.4367 - classification_loss: 0.4913\n",
      "473/500 [===========================>..] - ETA: 28s - loss: 1.9273 - regression_loss: 1.4363 - classification_loss: 0.4910\n",
      "474/500 [===========================>..] - ETA: 27s - loss: 1.9279 - regression_loss: 1.4363 - classification_loss: 0.4916\n",
      "475/500 [===========================>..] - ETA: 26s - loss: 1.9263 - regression_loss: 1.4351 - classification_loss: 0.4912\n",
      "476/500 [===========================>..] - ETA: 25s - loss: 1.9269 - regression_loss: 1.4358 - classification_loss: 0.4910\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.9298 - regression_loss: 1.4377 - classification_loss: 0.4921\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.9288 - regression_loss: 1.4369 - classification_loss: 0.4919\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.9286 - regression_loss: 1.4368 - classification_loss: 0.4919\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.9274 - regression_loss: 1.4357 - classification_loss: 0.4917\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.9275 - regression_loss: 1.4357 - classification_loss: 0.4918\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.9264 - regression_loss: 1.4351 - classification_loss: 0.4914\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.9284 - regression_loss: 1.4362 - classification_loss: 0.4922\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.9292 - regression_loss: 1.4369 - classification_loss: 0.4923\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.9288 - regression_loss: 1.4363 - classification_loss: 0.4925\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.9296 - regression_loss: 1.4371 - classification_loss: 0.4925\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.9295 - regression_loss: 1.4370 - classification_loss: 0.4925\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.9288 - regression_loss: 1.4365 - classification_loss: 0.4923\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.9312 - regression_loss: 1.4381 - classification_loss: 0.4931\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.9294 - regression_loss: 1.4365 - classification_loss: 0.4929\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.9311 - regression_loss: 1.4379 - classification_loss: 0.4931 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.9306 - regression_loss: 1.4377 - classification_loss: 0.4928\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.9300 - regression_loss: 1.4372 - classification_loss: 0.4928\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.9299 - regression_loss: 1.4370 - classification_loss: 0.4929\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.9284 - regression_loss: 1.4360 - classification_loss: 0.4925\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.9274 - regression_loss: 1.4350 - classification_loss: 0.4925\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.9270 - regression_loss: 1.4345 - classification_loss: 0.4925\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.9258 - regression_loss: 1.4337 - classification_loss: 0.4921\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.9255 - regression_loss: 1.4337 - classification_loss: 0.4918\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9267 - regression_loss: 1.4344 - classification_loss: 0.4923\n",
      "Epoch 00016: saving model to ./snapshots\\resnet50_csv_16.h5\n",
      "\n",
      "500/500 [==============================] - 522s 1s/step - loss: 1.9267 - regression_loss: 1.4344 - classification_loss: 0.4923\n",
      "Epoch 17/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 2.0071 - regression_loss: 1.4951 - classification_loss: 0.5119\n",
      "  2/500 [..............................] - ETA: 4:03 - loss: 1.8008 - regression_loss: 1.2117 - classification_loss: 0.5890\n",
      "  3/500 [..............................] - ETA: 5:41 - loss: 2.1900 - regression_loss: 1.5073 - classification_loss: 0.6828\n",
      "  4/500 [..............................] - ETA: 5:51 - loss: 1.8445 - regression_loss: 1.2727 - classification_loss: 0.5718\n",
      "  5/500 [..............................] - ETA: 6:40 - loss: 1.8262 - regression_loss: 1.2758 - classification_loss: 0.5504\n",
      "  6/500 [..............................] - ETA: 6:53 - loss: 1.9616 - regression_loss: 1.3659 - classification_loss: 0.5957\n",
      "  7/500 [..............................] - ETA: 7:10 - loss: 2.0311 - regression_loss: 1.4005 - classification_loss: 0.6305\n",
      "  8/500 [..............................] - ETA: 7:22 - loss: 2.1379 - regression_loss: 1.4256 - classification_loss: 0.7123\n",
      "  9/500 [..............................] - ETA: 7:36 - loss: 2.2047 - regression_loss: 1.4941 - classification_loss: 0.7106\n",
      " 10/500 [..............................] - ETA: 7:42 - loss: 2.1611 - regression_loss: 1.4840 - classification_loss: 0.6771\n",
      " 11/500 [..............................] - ETA: 7:43 - loss: 2.1173 - regression_loss: 1.4593 - classification_loss: 0.6580\n",
      " 12/500 [..............................] - ETA: 7:48 - loss: 2.1215 - regression_loss: 1.4694 - classification_loss: 0.6521\n",
      " 13/500 [..............................] - ETA: 7:49 - loss: 2.0665 - regression_loss: 1.4457 - classification_loss: 0.6208\n",
      " 14/500 [..............................] - ETA: 7:52 - loss: 2.0278 - regression_loss: 1.4267 - classification_loss: 0.6010\n",
      " 15/500 [..............................] - ETA: 7:54 - loss: 2.0273 - regression_loss: 1.4389 - classification_loss: 0.5884\n",
      " 16/500 [..............................] - ETA: 7:57 - loss: 2.0076 - regression_loss: 1.4258 - classification_loss: 0.5818\n",
      " 17/500 [>.............................] - ETA: 7:56 - loss: 1.9703 - regression_loss: 1.4088 - classification_loss: 0.5615\n",
      " 18/500 [>.............................] - ETA: 7:55 - loss: 1.9326 - regression_loss: 1.3871 - classification_loss: 0.5455\n",
      " 19/500 [>.............................] - ETA: 7:59 - loss: 1.8993 - regression_loss: 1.3702 - classification_loss: 0.5291\n",
      " 20/500 [>.............................] - ETA: 7:56 - loss: 1.8966 - regression_loss: 1.3723 - classification_loss: 0.5244\n",
      " 21/500 [>.............................] - ETA: 7:55 - loss: 1.9119 - regression_loss: 1.3836 - classification_loss: 0.5284\n",
      " 22/500 [>.............................] - ETA: 7:54 - loss: 1.8805 - regression_loss: 1.3607 - classification_loss: 0.5197\n",
      " 23/500 [>.............................] - ETA: 7:53 - loss: 1.8898 - regression_loss: 1.3695 - classification_loss: 0.5204\n",
      " 24/500 [>.............................] - ETA: 7:52 - loss: 1.8862 - regression_loss: 1.3722 - classification_loss: 0.5140\n",
      " 25/500 [>.............................] - ETA: 7:50 - loss: 1.8923 - regression_loss: 1.3755 - classification_loss: 0.5168\n",
      " 26/500 [>.............................] - ETA: 7:49 - loss: 1.8679 - regression_loss: 1.3577 - classification_loss: 0.5103\n",
      " 27/500 [>.............................] - ETA: 7:48 - loss: 1.8436 - regression_loss: 1.3401 - classification_loss: 0.5035\n",
      " 28/500 [>.............................] - ETA: 7:43 - loss: 1.8277 - regression_loss: 1.3293 - classification_loss: 0.4984\n",
      " 29/500 [>.............................] - ETA: 7:45 - loss: 1.8198 - regression_loss: 1.3252 - classification_loss: 0.4946\n",
      " 30/500 [>.............................] - ETA: 7:45 - loss: 1.8166 - regression_loss: 1.3256 - classification_loss: 0.4910\n",
      " 31/500 [>.............................] - ETA: 7:46 - loss: 1.8060 - regression_loss: 1.3186 - classification_loss: 0.4874\n",
      " 32/500 [>.............................] - ETA: 7:46 - loss: 1.8034 - regression_loss: 1.3198 - classification_loss: 0.4836\n",
      " 33/500 [>.............................] - ETA: 7:45 - loss: 1.8168 - regression_loss: 1.3282 - classification_loss: 0.4886\n",
      " 34/500 [=>............................] - ETA: 7:45 - loss: 1.8516 - regression_loss: 1.3612 - classification_loss: 0.4904\n",
      " 35/500 [=>............................] - ETA: 7:44 - loss: 1.8523 - regression_loss: 1.3622 - classification_loss: 0.4901\n",
      " 36/500 [=>............................] - ETA: 7:43 - loss: 1.8541 - regression_loss: 1.3671 - classification_loss: 0.4870\n",
      " 37/500 [=>............................] - ETA: 7:41 - loss: 1.8701 - regression_loss: 1.3752 - classification_loss: 0.4949\n",
      " 38/500 [=>............................] - ETA: 7:41 - loss: 1.8645 - regression_loss: 1.3714 - classification_loss: 0.4931\n",
      " 39/500 [=>............................] - ETA: 7:41 - loss: 1.8655 - regression_loss: 1.3706 - classification_loss: 0.4949\n",
      " 40/500 [=>............................] - ETA: 7:40 - loss: 1.8749 - regression_loss: 1.3777 - classification_loss: 0.4972\n",
      " 41/500 [=>............................] - ETA: 7:40 - loss: 1.8775 - regression_loss: 1.3766 - classification_loss: 0.5009\n",
      " 42/500 [=>............................] - ETA: 7:39 - loss: 1.8815 - regression_loss: 1.3806 - classification_loss: 0.5009\n",
      " 43/500 [=>............................] - ETA: 7:36 - loss: 1.8890 - regression_loss: 1.3865 - classification_loss: 0.5025\n",
      " 44/500 [=>............................] - ETA: 7:37 - loss: 1.8805 - regression_loss: 1.3834 - classification_loss: 0.4971\n",
      " 45/500 [=>............................] - ETA: 7:37 - loss: 1.8995 - regression_loss: 1.3962 - classification_loss: 0.5033\n",
      " 46/500 [=>............................] - ETA: 7:35 - loss: 1.9025 - regression_loss: 1.4015 - classification_loss: 0.5010\n",
      " 47/500 [=>............................] - ETA: 7:34 - loss: 1.8916 - regression_loss: 1.3923 - classification_loss: 0.4992\n",
      " 48/500 [=>............................] - ETA: 7:32 - loss: 1.8847 - regression_loss: 1.3885 - classification_loss: 0.4962\n",
      " 49/500 [=>............................] - ETA: 7:32 - loss: 1.8853 - regression_loss: 1.3903 - classification_loss: 0.4950\n",
      " 50/500 [==>...........................] - ETA: 7:31 - loss: 1.8742 - regression_loss: 1.3819 - classification_loss: 0.4922\n",
      " 51/500 [==>...........................] - ETA: 7:31 - loss: 1.8633 - regression_loss: 1.3729 - classification_loss: 0.4904\n",
      " 52/500 [==>...........................] - ETA: 7:30 - loss: 1.8639 - regression_loss: 1.3775 - classification_loss: 0.4864\n",
      " 53/500 [==>...........................] - ETA: 7:30 - loss: 1.8547 - regression_loss: 1.3718 - classification_loss: 0.4829\n",
      " 54/500 [==>...........................] - ETA: 7:29 - loss: 1.8515 - regression_loss: 1.3711 - classification_loss: 0.4804\n",
      " 55/500 [==>...........................] - ETA: 7:28 - loss: 1.8627 - regression_loss: 1.3795 - classification_loss: 0.4832\n",
      " 56/500 [==>...........................] - ETA: 7:27 - loss: 1.8779 - regression_loss: 1.3906 - classification_loss: 0.4872\n",
      " 57/500 [==>...........................] - ETA: 7:26 - loss: 1.8740 - regression_loss: 1.3877 - classification_loss: 0.4863\n",
      " 58/500 [==>...........................] - ETA: 7:25 - loss: 1.8857 - regression_loss: 1.3961 - classification_loss: 0.4897\n",
      " 59/500 [==>...........................] - ETA: 7:25 - loss: 1.8873 - regression_loss: 1.3971 - classification_loss: 0.4902\n",
      " 60/500 [==>...........................] - ETA: 7:24 - loss: 1.8965 - regression_loss: 1.4051 - classification_loss: 0.4915\n",
      " 61/500 [==>...........................] - ETA: 7:23 - loss: 1.8846 - regression_loss: 1.3970 - classification_loss: 0.4876\n",
      " 62/500 [==>...........................] - ETA: 7:23 - loss: 1.8900 - regression_loss: 1.3988 - classification_loss: 0.4912\n",
      " 63/500 [==>...........................] - ETA: 7:21 - loss: 1.8791 - regression_loss: 1.3917 - classification_loss: 0.4875\n",
      " 64/500 [==>...........................] - ETA: 7:20 - loss: 1.8813 - regression_loss: 1.3914 - classification_loss: 0.4899\n",
      " 65/500 [==>...........................] - ETA: 7:20 - loss: 1.8839 - regression_loss: 1.3937 - classification_loss: 0.4902\n",
      " 66/500 [==>...........................] - ETA: 7:19 - loss: 1.8772 - regression_loss: 1.3885 - classification_loss: 0.4887\n",
      " 67/500 [===>..........................] - ETA: 7:19 - loss: 1.8842 - regression_loss: 1.3939 - classification_loss: 0.4903\n",
      " 68/500 [===>..........................] - ETA: 7:17 - loss: 1.8794 - regression_loss: 1.3915 - classification_loss: 0.4879\n",
      " 69/500 [===>..........................] - ETA: 7:16 - loss: 1.8827 - regression_loss: 1.3942 - classification_loss: 0.4886\n",
      " 70/500 [===>..........................] - ETA: 7:15 - loss: 1.8712 - regression_loss: 1.3869 - classification_loss: 0.4843\n",
      " 71/500 [===>..........................] - ETA: 7:14 - loss: 1.8695 - regression_loss: 1.3865 - classification_loss: 0.4830\n",
      " 72/500 [===>..........................] - ETA: 7:13 - loss: 1.8760 - regression_loss: 1.3900 - classification_loss: 0.4860\n",
      " 73/500 [===>..........................] - ETA: 7:12 - loss: 1.8758 - regression_loss: 1.3902 - classification_loss: 0.4856\n",
      " 74/500 [===>..........................] - ETA: 7:11 - loss: 1.8652 - regression_loss: 1.3822 - classification_loss: 0.4830\n",
      " 75/500 [===>..........................] - ETA: 7:10 - loss: 1.8628 - regression_loss: 1.3810 - classification_loss: 0.4818\n",
      " 76/500 [===>..........................] - ETA: 7:09 - loss: 1.8753 - regression_loss: 1.3881 - classification_loss: 0.4872\n",
      " 77/500 [===>..........................] - ETA: 7:09 - loss: 1.8839 - regression_loss: 1.3962 - classification_loss: 0.4877\n",
      " 78/500 [===>..........................] - ETA: 7:08 - loss: 1.8843 - regression_loss: 1.3959 - classification_loss: 0.4883\n",
      " 79/500 [===>..........................] - ETA: 7:07 - loss: 1.8891 - regression_loss: 1.3993 - classification_loss: 0.4899\n",
      " 80/500 [===>..........................] - ETA: 7:06 - loss: 1.8851 - regression_loss: 1.3949 - classification_loss: 0.4902\n",
      " 81/500 [===>..........................] - ETA: 7:04 - loss: 1.8849 - regression_loss: 1.3949 - classification_loss: 0.4900\n",
      " 82/500 [===>..........................] - ETA: 7:03 - loss: 1.8927 - regression_loss: 1.4030 - classification_loss: 0.4897\n",
      " 83/500 [===>..........................] - ETA: 7:03 - loss: 1.8911 - regression_loss: 1.4021 - classification_loss: 0.4891\n",
      " 84/500 [====>.........................] - ETA: 7:01 - loss: 1.8857 - regression_loss: 1.3981 - classification_loss: 0.4876\n",
      " 85/500 [====>.........................] - ETA: 7:00 - loss: 1.8881 - regression_loss: 1.4003 - classification_loss: 0.4877\n",
      " 86/500 [====>.........................] - ETA: 6:58 - loss: 1.8829 - regression_loss: 1.3957 - classification_loss: 0.4872\n",
      " 87/500 [====>.........................] - ETA: 6:58 - loss: 1.8885 - regression_loss: 1.3992 - classification_loss: 0.4893\n",
      " 88/500 [====>.........................] - ETA: 6:57 - loss: 1.8981 - regression_loss: 1.4077 - classification_loss: 0.4905\n",
      " 89/500 [====>.........................] - ETA: 6:55 - loss: 1.9150 - regression_loss: 1.4198 - classification_loss: 0.4953\n",
      " 90/500 [====>.........................] - ETA: 6:55 - loss: 1.9162 - regression_loss: 1.4214 - classification_loss: 0.4949\n",
      " 91/500 [====>.........................] - ETA: 6:53 - loss: 1.9158 - regression_loss: 1.4207 - classification_loss: 0.4952\n",
      " 92/500 [====>.........................] - ETA: 6:52 - loss: 1.9113 - regression_loss: 1.4170 - classification_loss: 0.4943\n",
      " 93/500 [====>.........................] - ETA: 6:52 - loss: 1.9137 - regression_loss: 1.4150 - classification_loss: 0.4987\n",
      " 94/500 [====>.........................] - ETA: 6:51 - loss: 1.9140 - regression_loss: 1.4165 - classification_loss: 0.4975\n",
      " 95/500 [====>.........................] - ETA: 6:49 - loss: 1.9107 - regression_loss: 1.4140 - classification_loss: 0.4967\n",
      " 96/500 [====>.........................] - ETA: 6:49 - loss: 1.9109 - regression_loss: 1.4147 - classification_loss: 0.4962\n",
      " 97/500 [====>.........................] - ETA: 6:48 - loss: 1.9170 - regression_loss: 1.4216 - classification_loss: 0.4955\n",
      " 98/500 [====>.........................] - ETA: 6:46 - loss: 1.9239 - regression_loss: 1.4266 - classification_loss: 0.4972\n",
      " 99/500 [====>.........................] - ETA: 6:46 - loss: 1.9220 - regression_loss: 1.4265 - classification_loss: 0.4955\n",
      "100/500 [=====>........................] - ETA: 6:44 - loss: 1.9157 - regression_loss: 1.4224 - classification_loss: 0.4932\n",
      "101/500 [=====>........................] - ETA: 6:43 - loss: 1.9168 - regression_loss: 1.4239 - classification_loss: 0.4929\n",
      "102/500 [=====>........................] - ETA: 6:42 - loss: 1.9127 - regression_loss: 1.4208 - classification_loss: 0.4919\n",
      "103/500 [=====>........................] - ETA: 6:41 - loss: 1.9168 - regression_loss: 1.4235 - classification_loss: 0.4934\n",
      "104/500 [=====>........................] - ETA: 6:41 - loss: 1.9152 - regression_loss: 1.4228 - classification_loss: 0.4924\n",
      "105/500 [=====>........................] - ETA: 6:40 - loss: 1.9235 - regression_loss: 1.4283 - classification_loss: 0.4952\n",
      "106/500 [=====>........................] - ETA: 6:39 - loss: 1.9317 - regression_loss: 1.4350 - classification_loss: 0.4967\n",
      "107/500 [=====>........................] - ETA: 6:38 - loss: 1.9323 - regression_loss: 1.4356 - classification_loss: 0.4967\n",
      "108/500 [=====>........................] - ETA: 6:37 - loss: 1.9303 - regression_loss: 1.4342 - classification_loss: 0.4960\n",
      "109/500 [=====>........................] - ETA: 6:36 - loss: 1.9329 - regression_loss: 1.4364 - classification_loss: 0.4965\n",
      "110/500 [=====>........................] - ETA: 6:36 - loss: 1.9307 - regression_loss: 1.4352 - classification_loss: 0.4955\n",
      "111/500 [=====>........................] - ETA: 6:35 - loss: 1.9285 - regression_loss: 1.4321 - classification_loss: 0.4965\n",
      "112/500 [=====>........................] - ETA: 6:34 - loss: 1.9313 - regression_loss: 1.4347 - classification_loss: 0.4966\n",
      "113/500 [=====>........................] - ETA: 6:33 - loss: 1.9298 - regression_loss: 1.4336 - classification_loss: 0.4962\n",
      "114/500 [=====>........................] - ETA: 6:32 - loss: 1.9250 - regression_loss: 1.4305 - classification_loss: 0.4945\n",
      "115/500 [=====>........................] - ETA: 6:31 - loss: 1.9208 - regression_loss: 1.4270 - classification_loss: 0.4938\n",
      "116/500 [=====>........................] - ETA: 6:30 - loss: 1.9223 - regression_loss: 1.4281 - classification_loss: 0.4942\n",
      "117/500 [======>.......................] - ETA: 6:29 - loss: 1.9287 - regression_loss: 1.4315 - classification_loss: 0.4972\n",
      "118/500 [======>.......................] - ETA: 6:28 - loss: 1.9374 - regression_loss: 1.4382 - classification_loss: 0.4992\n",
      "119/500 [======>.......................] - ETA: 6:27 - loss: 1.9337 - regression_loss: 1.4353 - classification_loss: 0.4984\n",
      "120/500 [======>.......................] - ETA: 6:26 - loss: 1.9303 - regression_loss: 1.4319 - classification_loss: 0.4984\n",
      "121/500 [======>.......................] - ETA: 6:25 - loss: 1.9290 - regression_loss: 1.4312 - classification_loss: 0.4978\n",
      "122/500 [======>.......................] - ETA: 6:24 - loss: 1.9293 - regression_loss: 1.4315 - classification_loss: 0.4978\n",
      "123/500 [======>.......................] - ETA: 6:23 - loss: 1.9252 - regression_loss: 1.4296 - classification_loss: 0.4956\n",
      "124/500 [======>.......................] - ETA: 6:22 - loss: 1.9189 - regression_loss: 1.4241 - classification_loss: 0.4948\n",
      "125/500 [======>.......................] - ETA: 6:21 - loss: 1.9251 - regression_loss: 1.4277 - classification_loss: 0.4974\n",
      "126/500 [======>.......................] - ETA: 6:20 - loss: 1.9330 - regression_loss: 1.4334 - classification_loss: 0.4996\n",
      "127/500 [======>.......................] - ETA: 6:19 - loss: 1.9285 - regression_loss: 1.4297 - classification_loss: 0.4988\n",
      "128/500 [======>.......................] - ETA: 6:18 - loss: 1.9322 - regression_loss: 1.4318 - classification_loss: 0.5004\n",
      "129/500 [======>.......................] - ETA: 6:17 - loss: 1.9299 - regression_loss: 1.4300 - classification_loss: 0.4999\n",
      "130/500 [======>.......................] - ETA: 6:16 - loss: 1.9340 - regression_loss: 1.4316 - classification_loss: 0.5024\n",
      "131/500 [======>.......................] - ETA: 6:15 - loss: 1.9333 - regression_loss: 1.4307 - classification_loss: 0.5026\n",
      "132/500 [======>.......................] - ETA: 6:14 - loss: 1.9332 - regression_loss: 1.4312 - classification_loss: 0.5021\n",
      "133/500 [======>.......................] - ETA: 6:13 - loss: 1.9400 - regression_loss: 1.4361 - classification_loss: 0.5039\n",
      "134/500 [=======>......................] - ETA: 6:12 - loss: 1.9465 - regression_loss: 1.4411 - classification_loss: 0.5054\n",
      "135/500 [=======>......................] - ETA: 6:11 - loss: 1.9436 - regression_loss: 1.4385 - classification_loss: 0.5050\n",
      "136/500 [=======>......................] - ETA: 6:11 - loss: 1.9420 - regression_loss: 1.4384 - classification_loss: 0.5036\n",
      "137/500 [=======>......................] - ETA: 6:10 - loss: 1.9399 - regression_loss: 1.4368 - classification_loss: 0.5031\n",
      "138/500 [=======>......................] - ETA: 6:09 - loss: 1.9450 - regression_loss: 1.4411 - classification_loss: 0.5039\n",
      "139/500 [=======>......................] - ETA: 6:08 - loss: 1.9488 - regression_loss: 1.4429 - classification_loss: 0.5059\n",
      "140/500 [=======>......................] - ETA: 6:06 - loss: 1.9456 - regression_loss: 1.4399 - classification_loss: 0.5057\n",
      "141/500 [=======>......................] - ETA: 6:06 - loss: 1.9434 - regression_loss: 1.4385 - classification_loss: 0.5049\n",
      "142/500 [=======>......................] - ETA: 6:05 - loss: 1.9428 - regression_loss: 1.4373 - classification_loss: 0.5056\n",
      "143/500 [=======>......................] - ETA: 6:04 - loss: 1.9424 - regression_loss: 1.4374 - classification_loss: 0.5050\n",
      "144/500 [=======>......................] - ETA: 6:03 - loss: 1.9376 - regression_loss: 1.4338 - classification_loss: 0.5039\n",
      "145/500 [=======>......................] - ETA: 6:02 - loss: 1.9410 - regression_loss: 1.4362 - classification_loss: 0.5048\n",
      "146/500 [=======>......................] - ETA: 6:01 - loss: 1.9447 - regression_loss: 1.4391 - classification_loss: 0.5056\n",
      "147/500 [=======>......................] - ETA: 6:00 - loss: 1.9406 - regression_loss: 1.4366 - classification_loss: 0.5040\n",
      "148/500 [=======>......................] - ETA: 5:59 - loss: 1.9400 - regression_loss: 1.4367 - classification_loss: 0.5033\n",
      "149/500 [=======>......................] - ETA: 5:59 - loss: 1.9366 - regression_loss: 1.4347 - classification_loss: 0.5019\n",
      "150/500 [========>.....................] - ETA: 5:58 - loss: 1.9392 - regression_loss: 1.4362 - classification_loss: 0.5030\n",
      "151/500 [========>.....................] - ETA: 5:57 - loss: 1.9333 - regression_loss: 1.4320 - classification_loss: 0.5013\n",
      "152/500 [========>.....................] - ETA: 5:55 - loss: 1.9327 - regression_loss: 1.4322 - classification_loss: 0.5005\n",
      "153/500 [========>.....................] - ETA: 5:54 - loss: 1.9294 - regression_loss: 1.4297 - classification_loss: 0.4997\n",
      "154/500 [========>.....................] - ETA: 5:53 - loss: 1.9236 - regression_loss: 1.4255 - classification_loss: 0.4982\n",
      "155/500 [========>.....................] - ETA: 5:52 - loss: 1.9189 - regression_loss: 1.4215 - classification_loss: 0.4975\n",
      "156/500 [========>.....................] - ETA: 5:51 - loss: 1.9176 - regression_loss: 1.4199 - classification_loss: 0.4978\n",
      "157/500 [========>.....................] - ETA: 5:50 - loss: 1.9167 - regression_loss: 1.4188 - classification_loss: 0.4979\n",
      "158/500 [========>.....................] - ETA: 5:49 - loss: 1.9185 - regression_loss: 1.4193 - classification_loss: 0.4991\n",
      "159/500 [========>.....................] - ETA: 5:48 - loss: 1.9224 - regression_loss: 1.4223 - classification_loss: 0.5001\n",
      "160/500 [========>.....................] - ETA: 5:47 - loss: 1.9294 - regression_loss: 1.4264 - classification_loss: 0.5030\n",
      "161/500 [========>.....................] - ETA: 5:46 - loss: 1.9264 - regression_loss: 1.4236 - classification_loss: 0.5028\n",
      "162/500 [========>.....................] - ETA: 5:45 - loss: 1.9257 - regression_loss: 1.4238 - classification_loss: 0.5019\n",
      "163/500 [========>.....................] - ETA: 5:44 - loss: 1.9250 - regression_loss: 1.4225 - classification_loss: 0.5025\n",
      "164/500 [========>.....................] - ETA: 5:43 - loss: 1.9251 - regression_loss: 1.4225 - classification_loss: 0.5026\n",
      "165/500 [========>.....................] - ETA: 5:42 - loss: 1.9226 - regression_loss: 1.4209 - classification_loss: 0.5016\n",
      "166/500 [========>.....................] - ETA: 5:41 - loss: 1.9187 - regression_loss: 1.4180 - classification_loss: 0.5007\n",
      "167/500 [=========>....................] - ETA: 5:40 - loss: 1.9125 - regression_loss: 1.4131 - classification_loss: 0.4994\n",
      "168/500 [=========>....................] - ETA: 5:39 - loss: 1.9123 - regression_loss: 1.4129 - classification_loss: 0.4993\n",
      "169/500 [=========>....................] - ETA: 5:38 - loss: 1.9094 - regression_loss: 1.4107 - classification_loss: 0.4987\n",
      "170/500 [=========>....................] - ETA: 5:37 - loss: 1.9051 - regression_loss: 1.4076 - classification_loss: 0.4975\n",
      "171/500 [=========>....................] - ETA: 5:36 - loss: 1.9055 - regression_loss: 1.4079 - classification_loss: 0.4976\n",
      "172/500 [=========>....................] - ETA: 5:35 - loss: 1.9083 - regression_loss: 1.4104 - classification_loss: 0.4979\n",
      "173/500 [=========>....................] - ETA: 5:34 - loss: 1.9047 - regression_loss: 1.4078 - classification_loss: 0.4970\n",
      "174/500 [=========>....................] - ETA: 5:33 - loss: 1.9046 - regression_loss: 1.4069 - classification_loss: 0.4978\n",
      "175/500 [=========>....................] - ETA: 5:32 - loss: 1.9033 - regression_loss: 1.4062 - classification_loss: 0.4971\n",
      "176/500 [=========>....................] - ETA: 5:31 - loss: 1.9040 - regression_loss: 1.4071 - classification_loss: 0.4969\n",
      "177/500 [=========>....................] - ETA: 5:29 - loss: 1.9028 - regression_loss: 1.4055 - classification_loss: 0.4973\n",
      "178/500 [=========>....................] - ETA: 5:29 - loss: 1.9023 - regression_loss: 1.4056 - classification_loss: 0.4967\n",
      "179/500 [=========>....................] - ETA: 5:28 - loss: 1.9061 - regression_loss: 1.4082 - classification_loss: 0.4979\n",
      "180/500 [=========>....................] - ETA: 5:27 - loss: 1.9042 - regression_loss: 1.4071 - classification_loss: 0.4972\n",
      "181/500 [=========>....................] - ETA: 5:26 - loss: 1.9029 - regression_loss: 1.4056 - classification_loss: 0.4973\n",
      "182/500 [=========>....................] - ETA: 5:25 - loss: 1.9019 - regression_loss: 1.4053 - classification_loss: 0.4966\n",
      "183/500 [=========>....................] - ETA: 5:24 - loss: 1.8989 - regression_loss: 1.4036 - classification_loss: 0.4953\n",
      "184/500 [==========>...................] - ETA: 5:23 - loss: 1.8940 - regression_loss: 1.3999 - classification_loss: 0.4941\n",
      "185/500 [==========>...................] - ETA: 5:22 - loss: 1.8928 - regression_loss: 1.3992 - classification_loss: 0.4936\n",
      "186/500 [==========>...................] - ETA: 5:21 - loss: 1.8919 - regression_loss: 1.3988 - classification_loss: 0.4931\n",
      "187/500 [==========>...................] - ETA: 5:20 - loss: 1.8916 - regression_loss: 1.3989 - classification_loss: 0.4927\n",
      "188/500 [==========>...................] - ETA: 5:19 - loss: 1.8903 - regression_loss: 1.3984 - classification_loss: 0.4918\n",
      "189/500 [==========>...................] - ETA: 5:18 - loss: 1.8871 - regression_loss: 1.3961 - classification_loss: 0.4910\n",
      "190/500 [==========>...................] - ETA: 5:17 - loss: 1.8906 - regression_loss: 1.3980 - classification_loss: 0.4926\n",
      "191/500 [==========>...................] - ETA: 5:16 - loss: 1.8907 - regression_loss: 1.3979 - classification_loss: 0.4928\n",
      "192/500 [==========>...................] - ETA: 5:15 - loss: 1.8902 - regression_loss: 1.3970 - classification_loss: 0.4932\n",
      "193/500 [==========>...................] - ETA: 5:14 - loss: 1.8907 - regression_loss: 1.3962 - classification_loss: 0.4945\n",
      "194/500 [==========>...................] - ETA: 5:13 - loss: 1.8909 - regression_loss: 1.3972 - classification_loss: 0.4937\n",
      "195/500 [==========>...................] - ETA: 5:12 - loss: 1.8942 - regression_loss: 1.3997 - classification_loss: 0.4945\n",
      "196/500 [==========>...................] - ETA: 5:11 - loss: 1.8940 - regression_loss: 1.3999 - classification_loss: 0.4941\n",
      "197/500 [==========>...................] - ETA: 5:10 - loss: 1.8948 - regression_loss: 1.4010 - classification_loss: 0.4939\n",
      "198/500 [==========>...................] - ETA: 5:09 - loss: 1.8922 - regression_loss: 1.3988 - classification_loss: 0.4934\n",
      "199/500 [==========>...................] - ETA: 5:08 - loss: 1.8942 - regression_loss: 1.4007 - classification_loss: 0.4935\n",
      "200/500 [===========>..................] - ETA: 5:07 - loss: 1.8978 - regression_loss: 1.4041 - classification_loss: 0.4937\n",
      "201/500 [===========>..................] - ETA: 5:06 - loss: 1.9020 - regression_loss: 1.4071 - classification_loss: 0.4949\n",
      "202/500 [===========>..................] - ETA: 5:05 - loss: 1.9047 - regression_loss: 1.4090 - classification_loss: 0.4957\n",
      "203/500 [===========>..................] - ETA: 5:04 - loss: 1.9061 - regression_loss: 1.4098 - classification_loss: 0.4963\n",
      "204/500 [===========>..................] - ETA: 5:03 - loss: 1.9084 - regression_loss: 1.4118 - classification_loss: 0.4966\n",
      "205/500 [===========>..................] - ETA: 5:02 - loss: 1.9103 - regression_loss: 1.4131 - classification_loss: 0.4972\n",
      "206/500 [===========>..................] - ETA: 5:01 - loss: 1.9080 - regression_loss: 1.4112 - classification_loss: 0.4968\n",
      "207/500 [===========>..................] - ETA: 5:00 - loss: 1.9105 - regression_loss: 1.4135 - classification_loss: 0.4970\n",
      "208/500 [===========>..................] - ETA: 4:59 - loss: 1.9099 - regression_loss: 1.4129 - classification_loss: 0.4970\n",
      "209/500 [===========>..................] - ETA: 4:58 - loss: 1.9088 - regression_loss: 1.4124 - classification_loss: 0.4965\n",
      "210/500 [===========>..................] - ETA: 4:57 - loss: 1.9091 - regression_loss: 1.4131 - classification_loss: 0.4960\n",
      "211/500 [===========>..................] - ETA: 4:56 - loss: 1.9072 - regression_loss: 1.4120 - classification_loss: 0.4952\n",
      "212/500 [===========>..................] - ETA: 4:55 - loss: 1.9052 - regression_loss: 1.4106 - classification_loss: 0.4946\n",
      "213/500 [===========>..................] - ETA: 4:54 - loss: 1.9066 - regression_loss: 1.4117 - classification_loss: 0.4949\n",
      "214/500 [===========>..................] - ETA: 4:53 - loss: 1.9053 - regression_loss: 1.4108 - classification_loss: 0.4946\n",
      "215/500 [===========>..................] - ETA: 4:52 - loss: 1.9017 - regression_loss: 1.4082 - classification_loss: 0.4936\n",
      "216/500 [===========>..................] - ETA: 4:51 - loss: 1.9013 - regression_loss: 1.4086 - classification_loss: 0.4927\n",
      "217/500 [============>.................] - ETA: 4:50 - loss: 1.9017 - regression_loss: 1.4091 - classification_loss: 0.4926\n",
      "218/500 [============>.................] - ETA: 4:49 - loss: 1.9013 - regression_loss: 1.4094 - classification_loss: 0.4919\n",
      "219/500 [============>.................] - ETA: 4:48 - loss: 1.9008 - regression_loss: 1.4096 - classification_loss: 0.4913\n",
      "220/500 [============>.................] - ETA: 4:46 - loss: 1.9034 - regression_loss: 1.4118 - classification_loss: 0.4916\n",
      "221/500 [============>.................] - ETA: 4:45 - loss: 1.9045 - regression_loss: 1.4114 - classification_loss: 0.4931\n",
      "222/500 [============>.................] - ETA: 4:44 - loss: 1.9053 - regression_loss: 1.4117 - classification_loss: 0.4936\n",
      "223/500 [============>.................] - ETA: 4:43 - loss: 1.9062 - regression_loss: 1.4123 - classification_loss: 0.4939\n",
      "224/500 [============>.................] - ETA: 4:42 - loss: 1.9038 - regression_loss: 1.4110 - classification_loss: 0.4928\n",
      "225/500 [============>.................] - ETA: 4:41 - loss: 1.9053 - regression_loss: 1.4130 - classification_loss: 0.4923\n",
      "226/500 [============>.................] - ETA: 4:40 - loss: 1.9048 - regression_loss: 1.4133 - classification_loss: 0.4914\n",
      "227/500 [============>.................] - ETA: 4:39 - loss: 1.9041 - regression_loss: 1.4120 - classification_loss: 0.4922\n",
      "228/500 [============>.................] - ETA: 4:38 - loss: 1.9062 - regression_loss: 1.4137 - classification_loss: 0.4925\n",
      "229/500 [============>.................] - ETA: 4:37 - loss: 1.9079 - regression_loss: 1.4152 - classification_loss: 0.4927\n",
      "230/500 [============>.................] - ETA: 4:36 - loss: 1.9093 - regression_loss: 1.4147 - classification_loss: 0.4946\n",
      "231/500 [============>.................] - ETA: 4:35 - loss: 1.9113 - regression_loss: 1.4161 - classification_loss: 0.4952\n",
      "232/500 [============>.................] - ETA: 4:34 - loss: 1.9123 - regression_loss: 1.4163 - classification_loss: 0.4960\n",
      "233/500 [============>.................] - ETA: 4:33 - loss: 1.9122 - regression_loss: 1.4162 - classification_loss: 0.4960\n",
      "234/500 [=============>................] - ETA: 4:32 - loss: 1.9151 - regression_loss: 1.4185 - classification_loss: 0.4966\n",
      "235/500 [=============>................] - ETA: 4:31 - loss: 1.9161 - regression_loss: 1.4193 - classification_loss: 0.4967\n",
      "236/500 [=============>................] - ETA: 4:30 - loss: 1.9189 - regression_loss: 1.4210 - classification_loss: 0.4979\n",
      "237/500 [=============>................] - ETA: 4:29 - loss: 1.9159 - regression_loss: 1.4187 - classification_loss: 0.4972\n",
      "238/500 [=============>................] - ETA: 4:28 - loss: 1.9152 - regression_loss: 1.4187 - classification_loss: 0.4965\n",
      "239/500 [=============>................] - ETA: 4:27 - loss: 1.9186 - regression_loss: 1.4213 - classification_loss: 0.4973\n",
      "240/500 [=============>................] - ETA: 4:26 - loss: 1.9160 - regression_loss: 1.4189 - classification_loss: 0.4972\n",
      "241/500 [=============>................] - ETA: 4:25 - loss: 1.9181 - regression_loss: 1.4207 - classification_loss: 0.4974\n",
      "242/500 [=============>................] - ETA: 4:24 - loss: 1.9194 - regression_loss: 1.4217 - classification_loss: 0.4977\n",
      "243/500 [=============>................] - ETA: 4:23 - loss: 1.9222 - regression_loss: 1.4238 - classification_loss: 0.4984\n",
      "244/500 [=============>................] - ETA: 4:22 - loss: 1.9216 - regression_loss: 1.4231 - classification_loss: 0.4985\n",
      "245/500 [=============>................] - ETA: 4:21 - loss: 1.9208 - regression_loss: 1.4224 - classification_loss: 0.4984\n",
      "246/500 [=============>................] - ETA: 4:20 - loss: 1.9194 - regression_loss: 1.4217 - classification_loss: 0.4977\n",
      "247/500 [=============>................] - ETA: 4:19 - loss: 1.9227 - regression_loss: 1.4247 - classification_loss: 0.4980\n",
      "248/500 [=============>................] - ETA: 4:18 - loss: 1.9181 - regression_loss: 1.4213 - classification_loss: 0.4968\n",
      "249/500 [=============>................] - ETA: 4:17 - loss: 1.9157 - regression_loss: 1.4198 - classification_loss: 0.4960\n",
      "250/500 [==============>...............] - ETA: 4:16 - loss: 1.9134 - regression_loss: 1.4182 - classification_loss: 0.4952\n",
      "251/500 [==============>...............] - ETA: 4:15 - loss: 1.9097 - regression_loss: 1.4148 - classification_loss: 0.4949\n",
      "252/500 [==============>...............] - ETA: 4:14 - loss: 1.9125 - regression_loss: 1.4166 - classification_loss: 0.4959\n",
      "253/500 [==============>...............] - ETA: 4:13 - loss: 1.9148 - regression_loss: 1.4177 - classification_loss: 0.4971\n",
      "254/500 [==============>...............] - ETA: 4:12 - loss: 1.9135 - regression_loss: 1.4167 - classification_loss: 0.4967\n",
      "255/500 [==============>...............] - ETA: 4:11 - loss: 1.9121 - regression_loss: 1.4153 - classification_loss: 0.4968\n",
      "256/500 [==============>...............] - ETA: 4:10 - loss: 1.9092 - regression_loss: 1.4133 - classification_loss: 0.4958\n",
      "257/500 [==============>...............] - ETA: 4:09 - loss: 1.9089 - regression_loss: 1.4132 - classification_loss: 0.4957\n",
      "258/500 [==============>...............] - ETA: 4:08 - loss: 1.9087 - regression_loss: 1.4130 - classification_loss: 0.4957\n",
      "259/500 [==============>...............] - ETA: 4:07 - loss: 1.9096 - regression_loss: 1.4131 - classification_loss: 0.4965\n",
      "260/500 [==============>...............] - ETA: 4:06 - loss: 1.9075 - regression_loss: 1.4118 - classification_loss: 0.4958\n",
      "261/500 [==============>...............] - ETA: 4:05 - loss: 1.9078 - regression_loss: 1.4119 - classification_loss: 0.4959\n",
      "262/500 [==============>...............] - ETA: 4:04 - loss: 1.9062 - regression_loss: 1.4111 - classification_loss: 0.4952\n",
      "263/500 [==============>...............] - ETA: 4:02 - loss: 1.9060 - regression_loss: 1.4110 - classification_loss: 0.4950\n",
      "264/500 [==============>...............] - ETA: 4:02 - loss: 1.9080 - regression_loss: 1.4132 - classification_loss: 0.4948\n",
      "265/500 [==============>...............] - ETA: 4:00 - loss: 1.9087 - regression_loss: 1.4136 - classification_loss: 0.4951\n",
      "266/500 [==============>...............] - ETA: 3:59 - loss: 1.9117 - regression_loss: 1.4163 - classification_loss: 0.4954\n",
      "267/500 [===============>..............] - ETA: 3:59 - loss: 1.9099 - regression_loss: 1.4150 - classification_loss: 0.4948\n",
      "268/500 [===============>..............] - ETA: 3:57 - loss: 1.9107 - regression_loss: 1.4155 - classification_loss: 0.4953\n",
      "269/500 [===============>..............] - ETA: 3:56 - loss: 1.9109 - regression_loss: 1.4157 - classification_loss: 0.4952\n",
      "270/500 [===============>..............] - ETA: 3:55 - loss: 1.9097 - regression_loss: 1.4151 - classification_loss: 0.4947\n",
      "271/500 [===============>..............] - ETA: 3:54 - loss: 1.9086 - regression_loss: 1.4144 - classification_loss: 0.4942\n",
      "272/500 [===============>..............] - ETA: 3:53 - loss: 1.9053 - regression_loss: 1.4118 - classification_loss: 0.4935\n",
      "273/500 [===============>..............] - ETA: 3:52 - loss: 1.9103 - regression_loss: 1.4148 - classification_loss: 0.4955\n",
      "274/500 [===============>..............] - ETA: 3:51 - loss: 1.9098 - regression_loss: 1.4149 - classification_loss: 0.4949\n",
      "275/500 [===============>..............] - ETA: 3:50 - loss: 1.9080 - regression_loss: 1.4139 - classification_loss: 0.4941\n",
      "276/500 [===============>..............] - ETA: 3:49 - loss: 1.9093 - regression_loss: 1.4150 - classification_loss: 0.4944\n",
      "277/500 [===============>..............] - ETA: 3:48 - loss: 1.9122 - regression_loss: 1.4174 - classification_loss: 0.4948\n",
      "278/500 [===============>..............] - ETA: 3:47 - loss: 1.9103 - regression_loss: 1.4163 - classification_loss: 0.4940\n",
      "279/500 [===============>..............] - ETA: 3:46 - loss: 1.9096 - regression_loss: 1.4155 - classification_loss: 0.4941\n",
      "280/500 [===============>..............] - ETA: 3:45 - loss: 1.9089 - regression_loss: 1.4149 - classification_loss: 0.4940\n",
      "281/500 [===============>..............] - ETA: 3:44 - loss: 1.9091 - regression_loss: 1.4152 - classification_loss: 0.4939\n",
      "282/500 [===============>..............] - ETA: 3:43 - loss: 1.9070 - regression_loss: 1.4134 - classification_loss: 0.4936\n",
      "283/500 [===============>..............] - ETA: 3:42 - loss: 1.9094 - regression_loss: 1.4152 - classification_loss: 0.4942\n",
      "284/500 [================>.............] - ETA: 3:41 - loss: 1.9108 - regression_loss: 1.4161 - classification_loss: 0.4947\n",
      "285/500 [================>.............] - ETA: 3:40 - loss: 1.9107 - regression_loss: 1.4161 - classification_loss: 0.4946\n",
      "286/500 [================>.............] - ETA: 3:39 - loss: 1.9088 - regression_loss: 1.4147 - classification_loss: 0.4941\n",
      "287/500 [================>.............] - ETA: 3:38 - loss: 1.9093 - regression_loss: 1.4153 - classification_loss: 0.4941\n",
      "288/500 [================>.............] - ETA: 3:37 - loss: 1.9067 - regression_loss: 1.4132 - classification_loss: 0.4934\n",
      "289/500 [================>.............] - ETA: 3:36 - loss: 1.9085 - regression_loss: 1.4142 - classification_loss: 0.4943\n",
      "290/500 [================>.............] - ETA: 3:35 - loss: 1.9124 - regression_loss: 1.4166 - classification_loss: 0.4959\n",
      "291/500 [================>.............] - ETA: 3:34 - loss: 1.9137 - regression_loss: 1.4172 - classification_loss: 0.4965\n",
      "292/500 [================>.............] - ETA: 3:33 - loss: 1.9122 - regression_loss: 1.4161 - classification_loss: 0.4961\n",
      "293/500 [================>.............] - ETA: 3:32 - loss: 1.9106 - regression_loss: 1.4152 - classification_loss: 0.4954\n",
      "294/500 [================>.............] - ETA: 3:31 - loss: 1.9084 - regression_loss: 1.4136 - classification_loss: 0.4948\n",
      "295/500 [================>.............] - ETA: 3:30 - loss: 1.9084 - regression_loss: 1.4136 - classification_loss: 0.4949\n",
      "296/500 [================>.............] - ETA: 3:28 - loss: 1.9081 - regression_loss: 1.4137 - classification_loss: 0.4943\n",
      "297/500 [================>.............] - ETA: 3:27 - loss: 1.9113 - regression_loss: 1.4162 - classification_loss: 0.4951\n",
      "298/500 [================>.............] - ETA: 3:26 - loss: 1.9125 - regression_loss: 1.4169 - classification_loss: 0.4956\n",
      "299/500 [================>.............] - ETA: 3:25 - loss: 1.9115 - regression_loss: 1.4157 - classification_loss: 0.4958\n",
      "300/500 [=================>............] - ETA: 3:24 - loss: 1.9107 - regression_loss: 1.4149 - classification_loss: 0.4958\n",
      "301/500 [=================>............] - ETA: 3:23 - loss: 1.9123 - regression_loss: 1.4164 - classification_loss: 0.4958\n",
      "302/500 [=================>............] - ETA: 3:22 - loss: 1.9129 - regression_loss: 1.4175 - classification_loss: 0.4954\n",
      "303/500 [=================>............] - ETA: 3:21 - loss: 1.9115 - regression_loss: 1.4166 - classification_loss: 0.4949\n",
      "304/500 [=================>............] - ETA: 3:20 - loss: 1.9119 - regression_loss: 1.4171 - classification_loss: 0.4948\n",
      "305/500 [=================>............] - ETA: 3:19 - loss: 1.9152 - regression_loss: 1.4198 - classification_loss: 0.4955\n",
      "306/500 [=================>............] - ETA: 3:18 - loss: 1.9171 - regression_loss: 1.4211 - classification_loss: 0.4960\n",
      "307/500 [=================>............] - ETA: 3:17 - loss: 1.9179 - regression_loss: 1.4222 - classification_loss: 0.4958\n",
      "308/500 [=================>............] - ETA: 3:16 - loss: 1.9182 - regression_loss: 1.4223 - classification_loss: 0.4959\n",
      "309/500 [=================>............] - ETA: 3:15 - loss: 1.9169 - regression_loss: 1.4215 - classification_loss: 0.4954\n",
      "310/500 [=================>............] - ETA: 3:14 - loss: 1.9141 - regression_loss: 1.4194 - classification_loss: 0.4947\n",
      "311/500 [=================>............] - ETA: 3:13 - loss: 1.9161 - regression_loss: 1.4210 - classification_loss: 0.4952\n",
      "312/500 [=================>............] - ETA: 3:12 - loss: 1.9148 - regression_loss: 1.4200 - classification_loss: 0.4949\n",
      "313/500 [=================>............] - ETA: 3:11 - loss: 1.9147 - regression_loss: 1.4201 - classification_loss: 0.4946\n",
      "314/500 [=================>............] - ETA: 3:10 - loss: 1.9160 - regression_loss: 1.4210 - classification_loss: 0.4949\n",
      "315/500 [=================>............] - ETA: 3:09 - loss: 1.9156 - regression_loss: 1.4211 - classification_loss: 0.4945\n",
      "316/500 [=================>............] - ETA: 3:08 - loss: 1.9170 - regression_loss: 1.4228 - classification_loss: 0.4942\n",
      "317/500 [==================>...........] - ETA: 3:07 - loss: 1.9155 - regression_loss: 1.4217 - classification_loss: 0.4937\n",
      "318/500 [==================>...........] - ETA: 3:06 - loss: 1.9149 - regression_loss: 1.4211 - classification_loss: 0.4938\n",
      "319/500 [==================>...........] - ETA: 3:05 - loss: 1.9186 - regression_loss: 1.4229 - classification_loss: 0.4957\n",
      "320/500 [==================>...........] - ETA: 3:04 - loss: 1.9177 - regression_loss: 1.4223 - classification_loss: 0.4954\n",
      "321/500 [==================>...........] - ETA: 3:03 - loss: 1.9177 - regression_loss: 1.4219 - classification_loss: 0.4957\n",
      "322/500 [==================>...........] - ETA: 3:02 - loss: 1.9182 - regression_loss: 1.4224 - classification_loss: 0.4958\n",
      "323/500 [==================>...........] - ETA: 3:01 - loss: 1.9185 - regression_loss: 1.4230 - classification_loss: 0.4955\n",
      "324/500 [==================>...........] - ETA: 3:00 - loss: 1.9179 - regression_loss: 1.4225 - classification_loss: 0.4953\n",
      "325/500 [==================>...........] - ETA: 2:59 - loss: 1.9178 - regression_loss: 1.4225 - classification_loss: 0.4953\n",
      "326/500 [==================>...........] - ETA: 2:58 - loss: 1.9166 - regression_loss: 1.4220 - classification_loss: 0.4946\n",
      "327/500 [==================>...........] - ETA: 2:57 - loss: 1.9130 - regression_loss: 1.4194 - classification_loss: 0.4937\n",
      "328/500 [==================>...........] - ETA: 2:56 - loss: 1.9137 - regression_loss: 1.4199 - classification_loss: 0.4937\n",
      "329/500 [==================>...........] - ETA: 2:55 - loss: 1.9149 - regression_loss: 1.4210 - classification_loss: 0.4939\n",
      "330/500 [==================>...........] - ETA: 2:54 - loss: 1.9158 - regression_loss: 1.4218 - classification_loss: 0.4940\n",
      "331/500 [==================>...........] - ETA: 2:53 - loss: 1.9184 - regression_loss: 1.4241 - classification_loss: 0.4943\n",
      "332/500 [==================>...........] - ETA: 2:52 - loss: 1.9187 - regression_loss: 1.4243 - classification_loss: 0.4945\n",
      "333/500 [==================>...........] - ETA: 2:51 - loss: 1.9169 - regression_loss: 1.4228 - classification_loss: 0.4941\n",
      "334/500 [===================>..........] - ETA: 2:50 - loss: 1.9172 - regression_loss: 1.4231 - classification_loss: 0.4941\n",
      "335/500 [===================>..........] - ETA: 2:49 - loss: 1.9195 - regression_loss: 1.4251 - classification_loss: 0.4944\n",
      "336/500 [===================>..........] - ETA: 2:48 - loss: 1.9190 - regression_loss: 1.4249 - classification_loss: 0.4941\n",
      "337/500 [===================>..........] - ETA: 2:47 - loss: 1.9166 - regression_loss: 1.4231 - classification_loss: 0.4935\n",
      "338/500 [===================>..........] - ETA: 2:46 - loss: 1.9147 - regression_loss: 1.4219 - classification_loss: 0.4928\n",
      "339/500 [===================>..........] - ETA: 2:45 - loss: 1.9150 - regression_loss: 1.4225 - classification_loss: 0.4925\n",
      "340/500 [===================>..........] - ETA: 2:44 - loss: 1.9171 - regression_loss: 1.4233 - classification_loss: 0.4938\n",
      "341/500 [===================>..........] - ETA: 2:43 - loss: 1.9172 - regression_loss: 1.4237 - classification_loss: 0.4935\n",
      "342/500 [===================>..........] - ETA: 2:42 - loss: 1.9170 - regression_loss: 1.4235 - classification_loss: 0.4935\n",
      "343/500 [===================>..........] - ETA: 2:41 - loss: 1.9212 - regression_loss: 1.4262 - classification_loss: 0.4950\n",
      "344/500 [===================>..........] - ETA: 2:40 - loss: 1.9193 - regression_loss: 1.4248 - classification_loss: 0.4945\n",
      "345/500 [===================>..........] - ETA: 2:39 - loss: 1.9217 - regression_loss: 1.4261 - classification_loss: 0.4956\n",
      "346/500 [===================>..........] - ETA: 2:38 - loss: 1.9216 - regression_loss: 1.4260 - classification_loss: 0.4956\n",
      "347/500 [===================>..........] - ETA: 2:37 - loss: 1.9216 - regression_loss: 1.4266 - classification_loss: 0.4950\n",
      "348/500 [===================>..........] - ETA: 2:36 - loss: 1.9216 - regression_loss: 1.4270 - classification_loss: 0.4947\n",
      "349/500 [===================>..........] - ETA: 2:35 - loss: 1.9215 - regression_loss: 1.4265 - classification_loss: 0.4949\n",
      "350/500 [====================>.........] - ETA: 2:34 - loss: 1.9200 - regression_loss: 1.4254 - classification_loss: 0.4946\n",
      "351/500 [====================>.........] - ETA: 2:33 - loss: 1.9177 - regression_loss: 1.4236 - classification_loss: 0.4941\n",
      "352/500 [====================>.........] - ETA: 2:32 - loss: 1.9180 - regression_loss: 1.4238 - classification_loss: 0.4942\n",
      "353/500 [====================>.........] - ETA: 2:31 - loss: 1.9182 - regression_loss: 1.4242 - classification_loss: 0.4939\n",
      "354/500 [====================>.........] - ETA: 2:30 - loss: 1.9186 - regression_loss: 1.4251 - classification_loss: 0.4936\n",
      "355/500 [====================>.........] - ETA: 2:29 - loss: 1.9177 - regression_loss: 1.4246 - classification_loss: 0.4930\n",
      "356/500 [====================>.........] - ETA: 2:28 - loss: 1.9170 - regression_loss: 1.4245 - classification_loss: 0.4925\n",
      "357/500 [====================>.........] - ETA: 2:27 - loss: 1.9174 - regression_loss: 1.4250 - classification_loss: 0.4924\n",
      "358/500 [====================>.........] - ETA: 2:26 - loss: 1.9146 - regression_loss: 1.4230 - classification_loss: 0.4916\n",
      "359/500 [====================>.........] - ETA: 2:25 - loss: 1.9161 - regression_loss: 1.4237 - classification_loss: 0.4924\n",
      "360/500 [====================>.........] - ETA: 2:24 - loss: 1.9164 - regression_loss: 1.4235 - classification_loss: 0.4929\n",
      "361/500 [====================>.........] - ETA: 2:23 - loss: 1.9146 - regression_loss: 1.4221 - classification_loss: 0.4925\n",
      "362/500 [====================>.........] - ETA: 2:22 - loss: 1.9162 - regression_loss: 1.4228 - classification_loss: 0.4935\n",
      "363/500 [====================>.........] - ETA: 2:21 - loss: 1.9173 - regression_loss: 1.4237 - classification_loss: 0.4936\n",
      "364/500 [====================>.........] - ETA: 2:19 - loss: 1.9185 - regression_loss: 1.4243 - classification_loss: 0.4942\n",
      "365/500 [====================>.........] - ETA: 2:18 - loss: 1.9194 - regression_loss: 1.4248 - classification_loss: 0.4946\n",
      "366/500 [====================>.........] - ETA: 2:17 - loss: 1.9197 - regression_loss: 1.4252 - classification_loss: 0.4945\n",
      "367/500 [=====================>........] - ETA: 2:16 - loss: 1.9187 - regression_loss: 1.4243 - classification_loss: 0.4944\n",
      "368/500 [=====================>........] - ETA: 2:15 - loss: 1.9219 - regression_loss: 1.4265 - classification_loss: 0.4954\n",
      "369/500 [=====================>........] - ETA: 2:14 - loss: 1.9235 - regression_loss: 1.4269 - classification_loss: 0.4965\n",
      "370/500 [=====================>........] - ETA: 2:13 - loss: 1.9221 - regression_loss: 1.4260 - classification_loss: 0.4962\n",
      "371/500 [=====================>........] - ETA: 2:12 - loss: 1.9217 - regression_loss: 1.4260 - classification_loss: 0.4957\n",
      "372/500 [=====================>........] - ETA: 2:11 - loss: 1.9223 - regression_loss: 1.4260 - classification_loss: 0.4963\n",
      "373/500 [=====================>........] - ETA: 2:10 - loss: 1.9230 - regression_loss: 1.4266 - classification_loss: 0.4964\n",
      "374/500 [=====================>........] - ETA: 2:09 - loss: 1.9210 - regression_loss: 1.4252 - classification_loss: 0.4958\n",
      "375/500 [=====================>........] - ETA: 2:08 - loss: 1.9211 - regression_loss: 1.4254 - classification_loss: 0.4956\n",
      "376/500 [=====================>........] - ETA: 2:07 - loss: 1.9211 - regression_loss: 1.4259 - classification_loss: 0.4952\n",
      "377/500 [=====================>........] - ETA: 2:06 - loss: 1.9210 - regression_loss: 1.4255 - classification_loss: 0.4955\n",
      "378/500 [=====================>........] - ETA: 2:05 - loss: 1.9203 - regression_loss: 1.4249 - classification_loss: 0.4954\n",
      "379/500 [=====================>........] - ETA: 2:04 - loss: 1.9213 - regression_loss: 1.4254 - classification_loss: 0.4959\n",
      "380/500 [=====================>........] - ETA: 2:03 - loss: 1.9197 - regression_loss: 1.4242 - classification_loss: 0.4954\n",
      "381/500 [=====================>........] - ETA: 2:02 - loss: 1.9189 - regression_loss: 1.4231 - classification_loss: 0.4957\n",
      "382/500 [=====================>........] - ETA: 2:01 - loss: 1.9171 - regression_loss: 1.4220 - classification_loss: 0.4951\n",
      "383/500 [=====================>........] - ETA: 2:00 - loss: 1.9169 - regression_loss: 1.4220 - classification_loss: 0.4949\n",
      "384/500 [======================>.......] - ETA: 1:59 - loss: 1.9165 - regression_loss: 1.4220 - classification_loss: 0.4944\n",
      "385/500 [======================>.......] - ETA: 1:58 - loss: 1.9168 - regression_loss: 1.4223 - classification_loss: 0.4945\n",
      "386/500 [======================>.......] - ETA: 1:57 - loss: 1.9187 - regression_loss: 1.4233 - classification_loss: 0.4954\n",
      "387/500 [======================>.......] - ETA: 1:56 - loss: 1.9180 - regression_loss: 1.4231 - classification_loss: 0.4949\n",
      "388/500 [======================>.......] - ETA: 1:55 - loss: 1.9178 - regression_loss: 1.4228 - classification_loss: 0.4950\n",
      "389/500 [======================>.......] - ETA: 1:54 - loss: 1.9157 - regression_loss: 1.4214 - classification_loss: 0.4943\n",
      "390/500 [======================>.......] - ETA: 1:53 - loss: 1.9159 - regression_loss: 1.4218 - classification_loss: 0.4942\n",
      "391/500 [======================>.......] - ETA: 1:52 - loss: 1.9165 - regression_loss: 1.4225 - classification_loss: 0.4940\n",
      "392/500 [======================>.......] - ETA: 1:51 - loss: 1.9167 - regression_loss: 1.4226 - classification_loss: 0.4941\n",
      "393/500 [======================>.......] - ETA: 1:50 - loss: 1.9190 - regression_loss: 1.4234 - classification_loss: 0.4955\n",
      "394/500 [======================>.......] - ETA: 1:49 - loss: 1.9180 - regression_loss: 1.4229 - classification_loss: 0.4950\n",
      "395/500 [======================>.......] - ETA: 1:47 - loss: 1.9192 - regression_loss: 1.4240 - classification_loss: 0.4952\n",
      "396/500 [======================>.......] - ETA: 1:46 - loss: 1.9189 - regression_loss: 1.4237 - classification_loss: 0.4952\n",
      "397/500 [======================>.......] - ETA: 1:45 - loss: 1.9182 - regression_loss: 1.4231 - classification_loss: 0.4951\n",
      "398/500 [======================>.......] - ETA: 1:44 - loss: 1.9168 - regression_loss: 1.4218 - classification_loss: 0.4950\n",
      "399/500 [======================>.......] - ETA: 1:43 - loss: 1.9168 - regression_loss: 1.4218 - classification_loss: 0.4950\n",
      "400/500 [=======================>......] - ETA: 1:42 - loss: 1.9176 - regression_loss: 1.4220 - classification_loss: 0.4956\n",
      "401/500 [=======================>......] - ETA: 1:41 - loss: 1.9175 - regression_loss: 1.4211 - classification_loss: 0.4964\n",
      "402/500 [=======================>......] - ETA: 1:40 - loss: 1.9189 - regression_loss: 1.4220 - classification_loss: 0.4969\n",
      "403/500 [=======================>......] - ETA: 1:39 - loss: 1.9191 - regression_loss: 1.4220 - classification_loss: 0.4971\n",
      "404/500 [=======================>......] - ETA: 1:38 - loss: 1.9174 - regression_loss: 1.4207 - classification_loss: 0.4967\n",
      "405/500 [=======================>......] - ETA: 1:37 - loss: 1.9170 - regression_loss: 1.4204 - classification_loss: 0.4966\n",
      "406/500 [=======================>......] - ETA: 1:36 - loss: 1.9162 - regression_loss: 1.4201 - classification_loss: 0.4961\n",
      "407/500 [=======================>......] - ETA: 1:35 - loss: 1.9183 - regression_loss: 1.4218 - classification_loss: 0.4965\n",
      "408/500 [=======================>......] - ETA: 1:34 - loss: 1.9181 - regression_loss: 1.4214 - classification_loss: 0.4967\n",
      "409/500 [=======================>......] - ETA: 1:33 - loss: 1.9197 - regression_loss: 1.4225 - classification_loss: 0.4972\n",
      "410/500 [=======================>......] - ETA: 1:32 - loss: 1.9187 - regression_loss: 1.4216 - classification_loss: 0.4970\n",
      "411/500 [=======================>......] - ETA: 1:31 - loss: 1.9187 - regression_loss: 1.4218 - classification_loss: 0.4968\n",
      "412/500 [=======================>......] - ETA: 1:31 - loss: 1.9198 - regression_loss: 1.4231 - classification_loss: 0.4968\n",
      "413/500 [=======================>......] - ETA: 1:30 - loss: 1.9227 - regression_loss: 1.4252 - classification_loss: 0.4975\n",
      "414/500 [=======================>......] - ETA: 1:29 - loss: 1.9221 - regression_loss: 1.4249 - classification_loss: 0.4971\n",
      "415/500 [=======================>......] - ETA: 1:28 - loss: 1.9242 - regression_loss: 1.4262 - classification_loss: 0.4980\n",
      "416/500 [=======================>......] - ETA: 1:27 - loss: 1.9254 - regression_loss: 1.4270 - classification_loss: 0.4984\n",
      "417/500 [========================>.....] - ETA: 1:25 - loss: 1.9242 - regression_loss: 1.4262 - classification_loss: 0.4980\n",
      "418/500 [========================>.....] - ETA: 1:24 - loss: 1.9261 - regression_loss: 1.4269 - classification_loss: 0.4991\n",
      "419/500 [========================>.....] - ETA: 1:23 - loss: 1.9273 - regression_loss: 1.4280 - classification_loss: 0.4993\n",
      "420/500 [========================>.....] - ETA: 1:22 - loss: 1.9260 - regression_loss: 1.4268 - classification_loss: 0.4993\n",
      "421/500 [========================>.....] - ETA: 1:21 - loss: 1.9243 - regression_loss: 1.4257 - classification_loss: 0.4985\n",
      "422/500 [========================>.....] - ETA: 1:20 - loss: 1.9259 - regression_loss: 1.4274 - classification_loss: 0.4985\n",
      "423/500 [========================>.....] - ETA: 1:19 - loss: 1.9260 - regression_loss: 1.4272 - classification_loss: 0.4988\n",
      "424/500 [========================>.....] - ETA: 1:18 - loss: 1.9262 - regression_loss: 1.4272 - classification_loss: 0.4990\n",
      "425/500 [========================>.....] - ETA: 1:17 - loss: 1.9262 - regression_loss: 1.4276 - classification_loss: 0.4986\n",
      "426/500 [========================>.....] - ETA: 1:16 - loss: 1.9251 - regression_loss: 1.4270 - classification_loss: 0.4981\n",
      "427/500 [========================>.....] - ETA: 1:15 - loss: 1.9244 - regression_loss: 1.4265 - classification_loss: 0.4979\n",
      "428/500 [========================>.....] - ETA: 1:14 - loss: 1.9234 - regression_loss: 1.4256 - classification_loss: 0.4978\n",
      "429/500 [========================>.....] - ETA: 1:13 - loss: 1.9219 - regression_loss: 1.4246 - classification_loss: 0.4973\n",
      "430/500 [========================>.....] - ETA: 1:12 - loss: 1.9200 - regression_loss: 1.4233 - classification_loss: 0.4967\n",
      "431/500 [========================>.....] - ETA: 1:11 - loss: 1.9204 - regression_loss: 1.4235 - classification_loss: 0.4969\n",
      "432/500 [========================>.....] - ETA: 1:10 - loss: 1.9210 - regression_loss: 1.4245 - classification_loss: 0.4966\n",
      "433/500 [========================>.....] - ETA: 1:09 - loss: 1.9215 - regression_loss: 1.4247 - classification_loss: 0.4968\n",
      "434/500 [=========================>....] - ETA: 1:08 - loss: 1.9205 - regression_loss: 1.4241 - classification_loss: 0.4964\n",
      "435/500 [=========================>....] - ETA: 1:07 - loss: 1.9190 - regression_loss: 1.4229 - classification_loss: 0.4962\n",
      "436/500 [=========================>....] - ETA: 1:06 - loss: 1.9193 - regression_loss: 1.4228 - classification_loss: 0.4965\n",
      "437/500 [=========================>....] - ETA: 1:05 - loss: 1.9179 - regression_loss: 1.4217 - classification_loss: 0.4962\n",
      "438/500 [=========================>....] - ETA: 1:04 - loss: 1.9158 - regression_loss: 1.4202 - classification_loss: 0.4956\n",
      "439/500 [=========================>....] - ETA: 1:03 - loss: 1.9144 - regression_loss: 1.4195 - classification_loss: 0.4949\n",
      "440/500 [=========================>....] - ETA: 1:02 - loss: 1.9131 - regression_loss: 1.4184 - classification_loss: 0.4946\n",
      "441/500 [=========================>....] - ETA: 1:01 - loss: 1.9129 - regression_loss: 1.4186 - classification_loss: 0.4944\n",
      "442/500 [=========================>....] - ETA: 1:00 - loss: 1.9132 - regression_loss: 1.4186 - classification_loss: 0.4946\n",
      "443/500 [=========================>....] - ETA: 59s - loss: 1.9121 - regression_loss: 1.4176 - classification_loss: 0.4945 \n",
      "444/500 [=========================>....] - ETA: 58s - loss: 1.9123 - regression_loss: 1.4176 - classification_loss: 0.4947\n",
      "445/500 [=========================>....] - ETA: 56s - loss: 1.9140 - regression_loss: 1.4190 - classification_loss: 0.4950\n",
      "446/500 [=========================>....] - ETA: 55s - loss: 1.9138 - regression_loss: 1.4189 - classification_loss: 0.4949\n",
      "447/500 [=========================>....] - ETA: 54s - loss: 1.9125 - regression_loss: 1.4181 - classification_loss: 0.4944\n",
      "448/500 [=========================>....] - ETA: 53s - loss: 1.9133 - regression_loss: 1.4186 - classification_loss: 0.4947\n",
      "449/500 [=========================>....] - ETA: 52s - loss: 1.9140 - regression_loss: 1.4191 - classification_loss: 0.4949\n",
      "450/500 [==========================>...] - ETA: 51s - loss: 1.9139 - regression_loss: 1.4192 - classification_loss: 0.4947\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 1.9147 - regression_loss: 1.4199 - classification_loss: 0.4948\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 1.9156 - regression_loss: 1.4206 - classification_loss: 0.4951\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.9142 - regression_loss: 1.4196 - classification_loss: 0.4947\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.9161 - regression_loss: 1.4205 - classification_loss: 0.4957\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.9170 - regression_loss: 1.4208 - classification_loss: 0.4962\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.9160 - regression_loss: 1.4200 - classification_loss: 0.4960\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.9143 - regression_loss: 1.4187 - classification_loss: 0.4955\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.9137 - regression_loss: 1.4185 - classification_loss: 0.4952\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.9143 - regression_loss: 1.4194 - classification_loss: 0.4948\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.9136 - regression_loss: 1.4189 - classification_loss: 0.4947\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 1.9133 - regression_loss: 1.4187 - classification_loss: 0.4947\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 1.9125 - regression_loss: 1.4182 - classification_loss: 0.4944\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 1.9121 - regression_loss: 1.4181 - classification_loss: 0.4940\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 1.9139 - regression_loss: 1.4197 - classification_loss: 0.4942\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 1.9152 - regression_loss: 1.4208 - classification_loss: 0.4943\n",
      "466/500 [==========================>...] - ETA: 35s - loss: 1.9163 - regression_loss: 1.4217 - classification_loss: 0.4946\n",
      "467/500 [===========================>..] - ETA: 34s - loss: 1.9168 - regression_loss: 1.4215 - classification_loss: 0.4952\n",
      "468/500 [===========================>..] - ETA: 33s - loss: 1.9185 - regression_loss: 1.4223 - classification_loss: 0.4962\n",
      "469/500 [===========================>..] - ETA: 32s - loss: 1.9176 - regression_loss: 1.4217 - classification_loss: 0.4959\n",
      "470/500 [===========================>..] - ETA: 31s - loss: 1.9209 - regression_loss: 1.4241 - classification_loss: 0.4967\n",
      "471/500 [===========================>..] - ETA: 30s - loss: 1.9211 - regression_loss: 1.4241 - classification_loss: 0.4970\n",
      "472/500 [===========================>..] - ETA: 28s - loss: 1.9229 - regression_loss: 1.4255 - classification_loss: 0.4973\n",
      "473/500 [===========================>..] - ETA: 27s - loss: 1.9232 - regression_loss: 1.4259 - classification_loss: 0.4972\n",
      "474/500 [===========================>..] - ETA: 26s - loss: 1.9227 - regression_loss: 1.4257 - classification_loss: 0.4971\n",
      "475/500 [===========================>..] - ETA: 25s - loss: 1.9221 - regression_loss: 1.4252 - classification_loss: 0.4969\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 1.9214 - regression_loss: 1.4246 - classification_loss: 0.4967\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.9226 - regression_loss: 1.4255 - classification_loss: 0.4971\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.9223 - regression_loss: 1.4255 - classification_loss: 0.4969\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.9237 - regression_loss: 1.4265 - classification_loss: 0.4971\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.9230 - regression_loss: 1.4261 - classification_loss: 0.4969\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.9234 - regression_loss: 1.4263 - classification_loss: 0.4972\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.9254 - regression_loss: 1.4272 - classification_loss: 0.4982\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.9256 - regression_loss: 1.4274 - classification_loss: 0.4983\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.9249 - regression_loss: 1.4269 - classification_loss: 0.4980\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.9241 - regression_loss: 1.4264 - classification_loss: 0.4977\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.9229 - regression_loss: 1.4255 - classification_loss: 0.4975\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.9237 - regression_loss: 1.4263 - classification_loss: 0.4975\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.9241 - regression_loss: 1.4265 - classification_loss: 0.4976\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.9224 - regression_loss: 1.4252 - classification_loss: 0.4972\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.9222 - regression_loss: 1.4251 - classification_loss: 0.4971\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.9221 - regression_loss: 1.4250 - classification_loss: 0.4971 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.9223 - regression_loss: 1.4250 - classification_loss: 0.4972\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.9225 - regression_loss: 1.4250 - classification_loss: 0.4975\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.9227 - regression_loss: 1.4254 - classification_loss: 0.4974\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.9241 - regression_loss: 1.4266 - classification_loss: 0.4975\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.9235 - regression_loss: 1.4259 - classification_loss: 0.4976\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.9224 - regression_loss: 1.4251 - classification_loss: 0.4973\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.9228 - regression_loss: 1.4253 - classification_loss: 0.4974\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.9244 - regression_loss: 1.4264 - classification_loss: 0.4980\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9239 - regression_loss: 1.4262 - classification_loss: 0.4977\n",
      "Epoch 00017: saving model to ./snapshots\\resnet50_csv_17.h5\n",
      "\n",
      "500/500 [==============================] - 518s 1s/step - loss: 1.9239 - regression_loss: 1.4262 - classification_loss: 0.4977\n",
      "Epoch 18/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 2.0745 - regression_loss: 1.5481 - classification_loss: 0.5263\n",
      "  2/500 [..............................] - ETA: 4:38 - loss: 2.3339 - regression_loss: 1.6500 - classification_loss: 0.6839\n",
      "  3/500 [..............................] - ETA: 6:07 - loss: 2.1672 - regression_loss: 1.5662 - classification_loss: 0.6011\n",
      "  4/500 [..............................] - ETA: 6:23 - loss: 2.3720 - regression_loss: 1.7150 - classification_loss: 0.6570\n",
      "  5/500 [..............................] - ETA: 6:55 - loss: 2.3313 - regression_loss: 1.6940 - classification_loss: 0.6373\n",
      "  6/500 [..............................] - ETA: 7:07 - loss: 2.2634 - regression_loss: 1.6427 - classification_loss: 0.6207\n",
      "  7/500 [..............................] - ETA: 7:20 - loss: 2.2627 - regression_loss: 1.6604 - classification_loss: 0.6023\n",
      "  8/500 [..............................] - ETA: 7:32 - loss: 2.2381 - regression_loss: 1.6575 - classification_loss: 0.5806\n",
      "  9/500 [..............................] - ETA: 7:40 - loss: 2.1879 - regression_loss: 1.6364 - classification_loss: 0.5515\n",
      " 10/500 [..............................] - ETA: 7:41 - loss: 2.1866 - regression_loss: 1.6429 - classification_loss: 0.5437\n",
      " 11/500 [..............................] - ETA: 7:42 - loss: 2.1054 - regression_loss: 1.5945 - classification_loss: 0.5109\n",
      " 12/500 [..............................] - ETA: 7:42 - loss: 2.0913 - regression_loss: 1.5845 - classification_loss: 0.5068\n",
      " 13/500 [..............................] - ETA: 7:46 - loss: 2.0747 - regression_loss: 1.5748 - classification_loss: 0.4999\n",
      " 14/500 [..............................] - ETA: 7:46 - loss: 2.0771 - regression_loss: 1.5750 - classification_loss: 0.5021\n",
      " 15/500 [..............................] - ETA: 7:48 - loss: 2.0112 - regression_loss: 1.5178 - classification_loss: 0.4934\n",
      " 16/500 [..............................] - ETA: 7:48 - loss: 1.9571 - regression_loss: 1.4766 - classification_loss: 0.4806\n",
      " 17/500 [>.............................] - ETA: 7:48 - loss: 1.9148 - regression_loss: 1.4433 - classification_loss: 0.4716\n",
      " 18/500 [>.............................] - ETA: 7:50 - loss: 1.9157 - regression_loss: 1.4380 - classification_loss: 0.4778\n",
      " 19/500 [>.............................] - ETA: 7:44 - loss: 1.8894 - regression_loss: 1.4197 - classification_loss: 0.4697\n",
      " 20/500 [>.............................] - ETA: 7:49 - loss: 1.8790 - regression_loss: 1.4076 - classification_loss: 0.4713\n",
      " 21/500 [>.............................] - ETA: 7:48 - loss: 1.8778 - regression_loss: 1.4076 - classification_loss: 0.4703\n",
      " 22/500 [>.............................] - ETA: 7:49 - loss: 1.8404 - regression_loss: 1.3795 - classification_loss: 0.4609\n",
      " 23/500 [>.............................] - ETA: 7:50 - loss: 1.8944 - regression_loss: 1.4120 - classification_loss: 0.4824\n",
      " 24/500 [>.............................] - ETA: 7:50 - loss: 1.9001 - regression_loss: 1.4035 - classification_loss: 0.4965\n",
      " 25/500 [>.............................] - ETA: 7:51 - loss: 1.9144 - regression_loss: 1.4163 - classification_loss: 0.4981\n",
      " 26/500 [>.............................] - ETA: 7:50 - loss: 1.9200 - regression_loss: 1.4216 - classification_loss: 0.4983\n",
      " 27/500 [>.............................] - ETA: 7:51 - loss: 1.9089 - regression_loss: 1.4158 - classification_loss: 0.4930\n",
      " 28/500 [>.............................] - ETA: 7:51 - loss: 1.9242 - regression_loss: 1.4277 - classification_loss: 0.4965\n",
      " 29/500 [>.............................] - ETA: 7:50 - loss: 1.9469 - regression_loss: 1.4439 - classification_loss: 0.5030\n",
      " 30/500 [>.............................] - ETA: 7:46 - loss: 1.9406 - regression_loss: 1.4370 - classification_loss: 0.5036\n",
      " 31/500 [>.............................] - ETA: 7:49 - loss: 1.9460 - regression_loss: 1.4436 - classification_loss: 0.5024\n",
      " 32/500 [>.............................] - ETA: 7:48 - loss: 1.9234 - regression_loss: 1.4266 - classification_loss: 0.4968\n",
      " 33/500 [>.............................] - ETA: 7:43 - loss: 1.8943 - regression_loss: 1.4033 - classification_loss: 0.4910\n",
      " 34/500 [=>............................] - ETA: 7:46 - loss: 1.8838 - regression_loss: 1.3959 - classification_loss: 0.4878\n",
      " 35/500 [=>............................] - ETA: 7:46 - loss: 1.8901 - regression_loss: 1.4058 - classification_loss: 0.4842\n",
      " 36/500 [=>............................] - ETA: 7:45 - loss: 1.8810 - regression_loss: 1.3990 - classification_loss: 0.4819\n",
      " 37/500 [=>............................] - ETA: 7:45 - loss: 1.8909 - regression_loss: 1.4064 - classification_loss: 0.4845\n",
      " 38/500 [=>............................] - ETA: 7:44 - loss: 1.9025 - regression_loss: 1.4134 - classification_loss: 0.4891\n",
      " 39/500 [=>............................] - ETA: 7:44 - loss: 1.8867 - regression_loss: 1.4031 - classification_loss: 0.4837\n",
      " 40/500 [=>............................] - ETA: 7:40 - loss: 1.9052 - regression_loss: 1.4213 - classification_loss: 0.4840\n",
      " 41/500 [=>............................] - ETA: 7:41 - loss: 1.8882 - regression_loss: 1.4086 - classification_loss: 0.4796\n",
      " 42/500 [=>............................] - ETA: 7:41 - loss: 1.8774 - regression_loss: 1.4003 - classification_loss: 0.4771\n",
      " 43/500 [=>............................] - ETA: 7:40 - loss: 1.8770 - regression_loss: 1.3993 - classification_loss: 0.4777\n",
      " 44/500 [=>............................] - ETA: 7:38 - loss: 1.8863 - regression_loss: 1.4092 - classification_loss: 0.4771\n",
      " 45/500 [=>............................] - ETA: 7:38 - loss: 1.8912 - regression_loss: 1.4142 - classification_loss: 0.4770\n",
      " 46/500 [=>............................] - ETA: 7:37 - loss: 1.8944 - regression_loss: 1.4183 - classification_loss: 0.4761\n",
      " 47/500 [=>............................] - ETA: 7:36 - loss: 1.8748 - regression_loss: 1.4037 - classification_loss: 0.4711\n",
      " 48/500 [=>............................] - ETA: 7:35 - loss: 1.8720 - regression_loss: 1.4023 - classification_loss: 0.4697\n",
      " 49/500 [=>............................] - ETA: 7:35 - loss: 1.8763 - regression_loss: 1.4048 - classification_loss: 0.4715\n",
      " 50/500 [==>...........................] - ETA: 7:34 - loss: 1.8552 - regression_loss: 1.3886 - classification_loss: 0.4666\n",
      " 51/500 [==>...........................] - ETA: 7:32 - loss: 1.8591 - regression_loss: 1.3897 - classification_loss: 0.4694\n",
      " 52/500 [==>...........................] - ETA: 7:32 - loss: 1.8661 - regression_loss: 1.3948 - classification_loss: 0.4713\n",
      " 53/500 [==>...........................] - ETA: 7:31 - loss: 1.8887 - regression_loss: 1.4081 - classification_loss: 0.4805\n",
      " 54/500 [==>...........................] - ETA: 7:31 - loss: 1.8954 - regression_loss: 1.4106 - classification_loss: 0.4848\n",
      " 55/500 [==>...........................] - ETA: 7:30 - loss: 1.9030 - regression_loss: 1.4186 - classification_loss: 0.4845\n",
      " 56/500 [==>...........................] - ETA: 7:29 - loss: 1.8916 - regression_loss: 1.4118 - classification_loss: 0.4798\n",
      " 57/500 [==>...........................] - ETA: 7:28 - loss: 1.8883 - regression_loss: 1.4107 - classification_loss: 0.4776\n",
      " 58/500 [==>...........................] - ETA: 7:25 - loss: 1.8807 - regression_loss: 1.4040 - classification_loss: 0.4767\n",
      " 59/500 [==>...........................] - ETA: 7:25 - loss: 1.8848 - regression_loss: 1.4074 - classification_loss: 0.4775\n",
      " 60/500 [==>...........................] - ETA: 7:24 - loss: 1.8807 - regression_loss: 1.4057 - classification_loss: 0.4750\n",
      " 61/500 [==>...........................] - ETA: 7:23 - loss: 1.8787 - regression_loss: 1.4050 - classification_loss: 0.4737\n",
      " 62/500 [==>...........................] - ETA: 7:23 - loss: 1.8741 - regression_loss: 1.3994 - classification_loss: 0.4746\n",
      " 63/500 [==>...........................] - ETA: 7:21 - loss: 1.8757 - regression_loss: 1.3996 - classification_loss: 0.4761\n",
      " 64/500 [==>...........................] - ETA: 7:19 - loss: 1.8664 - regression_loss: 1.3926 - classification_loss: 0.4739\n",
      " 65/500 [==>...........................] - ETA: 7:18 - loss: 1.8647 - regression_loss: 1.3918 - classification_loss: 0.4728\n",
      " 66/500 [==>...........................] - ETA: 7:18 - loss: 1.8692 - regression_loss: 1.3934 - classification_loss: 0.4757\n",
      " 67/500 [===>..........................] - ETA: 7:17 - loss: 1.8612 - regression_loss: 1.3880 - classification_loss: 0.4732\n",
      " 68/500 [===>..........................] - ETA: 7:17 - loss: 1.8626 - regression_loss: 1.3910 - classification_loss: 0.4716\n",
      " 69/500 [===>..........................] - ETA: 7:16 - loss: 1.8561 - regression_loss: 1.3870 - classification_loss: 0.4691\n",
      " 70/500 [===>..........................] - ETA: 7:15 - loss: 1.8636 - regression_loss: 1.3908 - classification_loss: 0.4728\n",
      " 71/500 [===>..........................] - ETA: 7:15 - loss: 1.8552 - regression_loss: 1.3847 - classification_loss: 0.4705\n",
      " 72/500 [===>..........................] - ETA: 7:15 - loss: 1.8613 - regression_loss: 1.3884 - classification_loss: 0.4728\n",
      " 73/500 [===>..........................] - ETA: 7:13 - loss: 1.8720 - regression_loss: 1.3950 - classification_loss: 0.4770\n",
      " 74/500 [===>..........................] - ETA: 7:12 - loss: 1.8811 - regression_loss: 1.4006 - classification_loss: 0.4805\n",
      " 75/500 [===>..........................] - ETA: 7:12 - loss: 1.8811 - regression_loss: 1.4016 - classification_loss: 0.4795\n",
      " 76/500 [===>..........................] - ETA: 7:10 - loss: 1.8889 - regression_loss: 1.4079 - classification_loss: 0.4810\n",
      " 77/500 [===>..........................] - ETA: 7:09 - loss: 1.8894 - regression_loss: 1.4074 - classification_loss: 0.4820\n",
      " 78/500 [===>..........................] - ETA: 7:08 - loss: 1.9005 - regression_loss: 1.4174 - classification_loss: 0.4830\n",
      " 79/500 [===>..........................] - ETA: 7:07 - loss: 1.9067 - regression_loss: 1.4219 - classification_loss: 0.4848\n",
      " 80/500 [===>..........................] - ETA: 7:06 - loss: 1.8968 - regression_loss: 1.4154 - classification_loss: 0.4814\n",
      " 81/500 [===>..........................] - ETA: 7:04 - loss: 1.8954 - regression_loss: 1.4145 - classification_loss: 0.4810\n",
      " 82/500 [===>..........................] - ETA: 7:04 - loss: 1.8938 - regression_loss: 1.4145 - classification_loss: 0.4793\n",
      " 83/500 [===>..........................] - ETA: 7:01 - loss: 1.8904 - regression_loss: 1.4136 - classification_loss: 0.4767\n",
      " 84/500 [====>.........................] - ETA: 7:01 - loss: 1.8946 - regression_loss: 1.4141 - classification_loss: 0.4806\n",
      " 85/500 [====>.........................] - ETA: 7:01 - loss: 1.9002 - regression_loss: 1.4187 - classification_loss: 0.4815\n",
      " 86/500 [====>.........................] - ETA: 7:00 - loss: 1.9010 - regression_loss: 1.4181 - classification_loss: 0.4829\n",
      " 87/500 [====>.........................] - ETA: 6:58 - loss: 1.9045 - regression_loss: 1.4188 - classification_loss: 0.4857\n",
      " 88/500 [====>.........................] - ETA: 6:57 - loss: 1.9140 - regression_loss: 1.4234 - classification_loss: 0.4906\n",
      " 89/500 [====>.........................] - ETA: 6:56 - loss: 1.9225 - regression_loss: 1.4292 - classification_loss: 0.4933\n",
      " 90/500 [====>.........................] - ETA: 6:55 - loss: 1.9297 - regression_loss: 1.4368 - classification_loss: 0.4928\n",
      " 91/500 [====>.........................] - ETA: 6:55 - loss: 1.9427 - regression_loss: 1.4476 - classification_loss: 0.4951\n",
      " 92/500 [====>.........................] - ETA: 6:54 - loss: 1.9310 - regression_loss: 1.4388 - classification_loss: 0.4922\n",
      " 93/500 [====>.........................] - ETA: 6:53 - loss: 1.9399 - regression_loss: 1.4443 - classification_loss: 0.4957\n",
      " 94/500 [====>.........................] - ETA: 6:52 - loss: 1.9424 - regression_loss: 1.4460 - classification_loss: 0.4964\n",
      " 95/500 [====>.........................] - ETA: 6:51 - loss: 1.9508 - regression_loss: 1.4493 - classification_loss: 0.5014\n",
      " 96/500 [====>.........................] - ETA: 6:50 - loss: 1.9488 - regression_loss: 1.4479 - classification_loss: 0.5008\n",
      " 97/500 [====>.........................] - ETA: 6:49 - loss: 1.9478 - regression_loss: 1.4473 - classification_loss: 0.5005\n",
      " 98/500 [====>.........................] - ETA: 6:48 - loss: 1.9429 - regression_loss: 1.4449 - classification_loss: 0.4980\n",
      " 99/500 [====>.........................] - ETA: 6:47 - loss: 1.9416 - regression_loss: 1.4444 - classification_loss: 0.4972\n",
      "100/500 [=====>........................] - ETA: 6:46 - loss: 1.9335 - regression_loss: 1.4388 - classification_loss: 0.4947\n",
      "101/500 [=====>........................] - ETA: 6:45 - loss: 1.9330 - regression_loss: 1.4384 - classification_loss: 0.4946\n",
      "102/500 [=====>........................] - ETA: 6:44 - loss: 1.9409 - regression_loss: 1.4441 - classification_loss: 0.4968\n",
      "103/500 [=====>........................] - ETA: 6:42 - loss: 1.9449 - regression_loss: 1.4459 - classification_loss: 0.4990\n",
      "104/500 [=====>........................] - ETA: 6:42 - loss: 1.9471 - regression_loss: 1.4455 - classification_loss: 0.5016\n",
      "105/500 [=====>........................] - ETA: 6:41 - loss: 1.9435 - regression_loss: 1.4436 - classification_loss: 0.4999\n",
      "106/500 [=====>........................] - ETA: 6:40 - loss: 1.9444 - regression_loss: 1.4444 - classification_loss: 0.5000\n",
      "107/500 [=====>........................] - ETA: 6:39 - loss: 1.9531 - regression_loss: 1.4504 - classification_loss: 0.5027\n",
      "108/500 [=====>........................] - ETA: 6:38 - loss: 1.9521 - regression_loss: 1.4498 - classification_loss: 0.5024\n",
      "109/500 [=====>........................] - ETA: 6:37 - loss: 1.9525 - regression_loss: 1.4497 - classification_loss: 0.5027\n",
      "110/500 [=====>........................] - ETA: 6:36 - loss: 1.9575 - regression_loss: 1.4529 - classification_loss: 0.5046\n",
      "111/500 [=====>........................] - ETA: 6:35 - loss: 1.9548 - regression_loss: 1.4508 - classification_loss: 0.5040\n",
      "112/500 [=====>........................] - ETA: 6:34 - loss: 1.9606 - regression_loss: 1.4561 - classification_loss: 0.5045\n",
      "113/500 [=====>........................] - ETA: 6:33 - loss: 1.9640 - regression_loss: 1.4597 - classification_loss: 0.5043\n",
      "114/500 [=====>........................] - ETA: 6:32 - loss: 1.9595 - regression_loss: 1.4569 - classification_loss: 0.5026\n",
      "115/500 [=====>........................] - ETA: 6:31 - loss: 1.9599 - regression_loss: 1.4584 - classification_loss: 0.5015\n",
      "116/500 [=====>........................] - ETA: 6:30 - loss: 1.9554 - regression_loss: 1.4550 - classification_loss: 0.5004\n",
      "117/500 [======>.......................] - ETA: 6:29 - loss: 1.9597 - regression_loss: 1.4588 - classification_loss: 0.5009\n",
      "118/500 [======>.......................] - ETA: 6:28 - loss: 1.9564 - regression_loss: 1.4571 - classification_loss: 0.4994\n",
      "119/500 [======>.......................] - ETA: 6:27 - loss: 1.9605 - regression_loss: 1.4612 - classification_loss: 0.4994\n",
      "120/500 [======>.......................] - ETA: 6:26 - loss: 1.9626 - regression_loss: 1.4640 - classification_loss: 0.4986\n",
      "121/500 [======>.......................] - ETA: 6:25 - loss: 1.9696 - regression_loss: 1.4680 - classification_loss: 0.5016\n",
      "122/500 [======>.......................] - ETA: 6:24 - loss: 1.9690 - regression_loss: 1.4679 - classification_loss: 0.5011\n",
      "123/500 [======>.......................] - ETA: 6:23 - loss: 1.9675 - regression_loss: 1.4677 - classification_loss: 0.4998\n",
      "124/500 [======>.......................] - ETA: 6:22 - loss: 1.9709 - regression_loss: 1.4712 - classification_loss: 0.4998\n",
      "125/500 [======>.......................] - ETA: 6:21 - loss: 1.9655 - regression_loss: 1.4668 - classification_loss: 0.4987\n",
      "126/500 [======>.......................] - ETA: 6:20 - loss: 1.9719 - regression_loss: 1.4720 - classification_loss: 0.4999\n",
      "127/500 [======>.......................] - ETA: 6:19 - loss: 1.9787 - regression_loss: 1.4757 - classification_loss: 0.5030\n",
      "128/500 [======>.......................] - ETA: 6:18 - loss: 1.9792 - regression_loss: 1.4766 - classification_loss: 0.5027\n",
      "129/500 [======>.......................] - ETA: 6:17 - loss: 1.9735 - regression_loss: 1.4725 - classification_loss: 0.5010\n",
      "130/500 [======>.......................] - ETA: 6:16 - loss: 1.9731 - regression_loss: 1.4715 - classification_loss: 0.5016\n",
      "131/500 [======>.......................] - ETA: 6:15 - loss: 1.9669 - regression_loss: 1.4674 - classification_loss: 0.4995\n",
      "132/500 [======>.......................] - ETA: 6:14 - loss: 1.9651 - regression_loss: 1.4666 - classification_loss: 0.4985\n",
      "133/500 [======>.......................] - ETA: 6:12 - loss: 1.9621 - regression_loss: 1.4643 - classification_loss: 0.4979\n",
      "134/500 [=======>......................] - ETA: 6:12 - loss: 1.9632 - regression_loss: 1.4656 - classification_loss: 0.4976\n",
      "135/500 [=======>......................] - ETA: 6:11 - loss: 1.9610 - regression_loss: 1.4641 - classification_loss: 0.4968\n",
      "136/500 [=======>......................] - ETA: 6:10 - loss: 1.9601 - regression_loss: 1.4639 - classification_loss: 0.4961\n",
      "137/500 [=======>......................] - ETA: 6:09 - loss: 1.9641 - regression_loss: 1.4662 - classification_loss: 0.4979\n",
      "138/500 [=======>......................] - ETA: 6:08 - loss: 1.9625 - regression_loss: 1.4652 - classification_loss: 0.4972\n",
      "139/500 [=======>......................] - ETA: 6:07 - loss: 1.9609 - regression_loss: 1.4643 - classification_loss: 0.4967\n",
      "140/500 [=======>......................] - ETA: 6:06 - loss: 1.9599 - regression_loss: 1.4627 - classification_loss: 0.4972\n",
      "141/500 [=======>......................] - ETA: 6:05 - loss: 1.9567 - regression_loss: 1.4601 - classification_loss: 0.4966\n",
      "142/500 [=======>......................] - ETA: 6:04 - loss: 1.9577 - regression_loss: 1.4615 - classification_loss: 0.4962\n",
      "143/500 [=======>......................] - ETA: 6:03 - loss: 1.9603 - regression_loss: 1.4615 - classification_loss: 0.4988\n",
      "144/500 [=======>......................] - ETA: 6:02 - loss: 1.9631 - regression_loss: 1.4639 - classification_loss: 0.4992\n",
      "145/500 [=======>......................] - ETA: 6:01 - loss: 1.9559 - regression_loss: 1.4587 - classification_loss: 0.4972\n",
      "146/500 [=======>......................] - ETA: 6:00 - loss: 1.9568 - regression_loss: 1.4595 - classification_loss: 0.4972\n",
      "147/500 [=======>......................] - ETA: 5:59 - loss: 1.9606 - regression_loss: 1.4619 - classification_loss: 0.4987\n",
      "148/500 [=======>......................] - ETA: 5:58 - loss: 1.9592 - regression_loss: 1.4606 - classification_loss: 0.4986\n",
      "149/500 [=======>......................] - ETA: 5:57 - loss: 1.9567 - regression_loss: 1.4583 - classification_loss: 0.4984\n",
      "150/500 [========>.....................] - ETA: 5:56 - loss: 1.9541 - regression_loss: 1.4564 - classification_loss: 0.4977\n",
      "151/500 [========>.....................] - ETA: 5:55 - loss: 1.9500 - regression_loss: 1.4529 - classification_loss: 0.4971\n",
      "152/500 [========>.....................] - ETA: 5:54 - loss: 1.9509 - regression_loss: 1.4540 - classification_loss: 0.4969\n",
      "153/500 [========>.....................] - ETA: 5:53 - loss: 1.9531 - regression_loss: 1.4564 - classification_loss: 0.4967\n",
      "154/500 [========>.....................] - ETA: 5:52 - loss: 1.9550 - regression_loss: 1.4590 - classification_loss: 0.4960\n",
      "155/500 [========>.....................] - ETA: 5:51 - loss: 1.9536 - regression_loss: 1.4573 - classification_loss: 0.4963\n",
      "156/500 [========>.....................] - ETA: 5:50 - loss: 1.9598 - regression_loss: 1.4615 - classification_loss: 0.4983\n",
      "157/500 [========>.....................] - ETA: 5:49 - loss: 1.9545 - regression_loss: 1.4577 - classification_loss: 0.4968\n",
      "158/500 [========>.....................] - ETA: 5:48 - loss: 1.9535 - regression_loss: 1.4559 - classification_loss: 0.4976\n",
      "159/500 [========>.....................] - ETA: 5:47 - loss: 1.9547 - regression_loss: 1.4572 - classification_loss: 0.4975\n",
      "160/500 [========>.....................] - ETA: 5:46 - loss: 1.9543 - regression_loss: 1.4562 - classification_loss: 0.4981\n",
      "161/500 [========>.....................] - ETA: 5:45 - loss: 1.9565 - regression_loss: 1.4588 - classification_loss: 0.4976\n",
      "162/500 [========>.....................] - ETA: 5:44 - loss: 1.9598 - regression_loss: 1.4617 - classification_loss: 0.4981\n",
      "163/500 [========>.....................] - ETA: 5:43 - loss: 1.9577 - regression_loss: 1.4601 - classification_loss: 0.4976\n",
      "164/500 [========>.....................] - ETA: 5:42 - loss: 1.9632 - regression_loss: 1.4638 - classification_loss: 0.4994\n",
      "165/500 [========>.....................] - ETA: 5:41 - loss: 1.9634 - regression_loss: 1.4639 - classification_loss: 0.4996\n",
      "166/500 [========>.....................] - ETA: 5:40 - loss: 1.9697 - regression_loss: 1.4684 - classification_loss: 0.5013\n",
      "167/500 [=========>....................] - ETA: 5:39 - loss: 1.9689 - regression_loss: 1.4678 - classification_loss: 0.5011\n",
      "168/500 [=========>....................] - ETA: 5:38 - loss: 1.9682 - regression_loss: 1.4679 - classification_loss: 0.5003\n",
      "169/500 [=========>....................] - ETA: 5:37 - loss: 1.9695 - regression_loss: 1.4685 - classification_loss: 0.5010\n",
      "170/500 [=========>....................] - ETA: 5:36 - loss: 1.9664 - regression_loss: 1.4662 - classification_loss: 0.5002\n",
      "171/500 [=========>....................] - ETA: 5:35 - loss: 1.9685 - regression_loss: 1.4671 - classification_loss: 0.5014\n",
      "172/500 [=========>....................] - ETA: 5:34 - loss: 1.9703 - regression_loss: 1.4681 - classification_loss: 0.5023\n",
      "173/500 [=========>....................] - ETA: 5:33 - loss: 1.9738 - regression_loss: 1.4712 - classification_loss: 0.5026\n",
      "174/500 [=========>....................] - ETA: 5:32 - loss: 1.9753 - regression_loss: 1.4722 - classification_loss: 0.5031\n",
      "175/500 [=========>....................] - ETA: 5:31 - loss: 1.9775 - regression_loss: 1.4735 - classification_loss: 0.5040\n",
      "176/500 [=========>....................] - ETA: 5:29 - loss: 1.9746 - regression_loss: 1.4707 - classification_loss: 0.5039\n",
      "177/500 [=========>....................] - ETA: 5:29 - loss: 1.9755 - regression_loss: 1.4706 - classification_loss: 0.5048\n",
      "178/500 [=========>....................] - ETA: 5:27 - loss: 1.9743 - regression_loss: 1.4697 - classification_loss: 0.5046\n",
      "179/500 [=========>....................] - ETA: 5:27 - loss: 1.9742 - regression_loss: 1.4702 - classification_loss: 0.5040\n",
      "180/500 [=========>....................] - ETA: 5:25 - loss: 1.9783 - regression_loss: 1.4735 - classification_loss: 0.5049\n",
      "181/500 [=========>....................] - ETA: 5:25 - loss: 1.9740 - regression_loss: 1.4706 - classification_loss: 0.5034\n",
      "182/500 [=========>....................] - ETA: 5:24 - loss: 1.9743 - regression_loss: 1.4711 - classification_loss: 0.5032\n",
      "183/500 [=========>....................] - ETA: 5:23 - loss: 1.9767 - regression_loss: 1.4730 - classification_loss: 0.5037\n",
      "184/500 [==========>...................] - ETA: 5:21 - loss: 1.9782 - regression_loss: 1.4740 - classification_loss: 0.5042\n",
      "185/500 [==========>...................] - ETA: 5:20 - loss: 1.9784 - regression_loss: 1.4745 - classification_loss: 0.5039\n",
      "186/500 [==========>...................] - ETA: 5:20 - loss: 1.9744 - regression_loss: 1.4719 - classification_loss: 0.5024\n",
      "187/500 [==========>...................] - ETA: 5:19 - loss: 1.9777 - regression_loss: 1.4749 - classification_loss: 0.5028\n",
      "188/500 [==========>...................] - ETA: 5:18 - loss: 1.9775 - regression_loss: 1.4755 - classification_loss: 0.5020\n",
      "189/500 [==========>...................] - ETA: 5:17 - loss: 1.9774 - regression_loss: 1.4743 - classification_loss: 0.5032\n",
      "190/500 [==========>...................] - ETA: 5:16 - loss: 1.9812 - regression_loss: 1.4764 - classification_loss: 0.5048\n",
      "191/500 [==========>...................] - ETA: 5:15 - loss: 1.9793 - regression_loss: 1.4753 - classification_loss: 0.5040\n",
      "192/500 [==========>...................] - ETA: 5:14 - loss: 1.9806 - regression_loss: 1.4754 - classification_loss: 0.5051\n",
      "193/500 [==========>...................] - ETA: 5:13 - loss: 1.9822 - regression_loss: 1.4765 - classification_loss: 0.5057\n",
      "194/500 [==========>...................] - ETA: 5:12 - loss: 1.9837 - regression_loss: 1.4774 - classification_loss: 0.5063\n",
      "195/500 [==========>...................] - ETA: 5:11 - loss: 1.9800 - regression_loss: 1.4746 - classification_loss: 0.5054\n",
      "196/500 [==========>...................] - ETA: 5:10 - loss: 1.9772 - regression_loss: 1.4725 - classification_loss: 0.5047\n",
      "197/500 [==========>...................] - ETA: 5:09 - loss: 1.9762 - regression_loss: 1.4718 - classification_loss: 0.5044\n",
      "198/500 [==========>...................] - ETA: 5:08 - loss: 1.9728 - regression_loss: 1.4694 - classification_loss: 0.5034\n",
      "199/500 [==========>...................] - ETA: 5:06 - loss: 1.9764 - regression_loss: 1.4706 - classification_loss: 0.5058\n",
      "200/500 [===========>..................] - ETA: 5:06 - loss: 1.9717 - regression_loss: 1.4673 - classification_loss: 0.5043\n",
      "201/500 [===========>..................] - ETA: 5:05 - loss: 1.9736 - regression_loss: 1.4686 - classification_loss: 0.5050\n",
      "202/500 [===========>..................] - ETA: 5:04 - loss: 1.9710 - regression_loss: 1.4665 - classification_loss: 0.5045\n",
      "203/500 [===========>..................] - ETA: 5:03 - loss: 1.9717 - regression_loss: 1.4661 - classification_loss: 0.5057\n",
      "204/500 [===========>..................] - ETA: 5:02 - loss: 1.9713 - regression_loss: 1.4662 - classification_loss: 0.5051\n",
      "205/500 [===========>..................] - ETA: 5:01 - loss: 1.9735 - regression_loss: 1.4684 - classification_loss: 0.5052\n",
      "206/500 [===========>..................] - ETA: 4:59 - loss: 1.9698 - regression_loss: 1.4660 - classification_loss: 0.5038\n",
      "207/500 [===========>..................] - ETA: 5:02 - loss: 1.9679 - regression_loss: 1.4648 - classification_loss: 0.5031\n",
      "208/500 [===========>..................] - ETA: 5:01 - loss: 1.9671 - regression_loss: 1.4640 - classification_loss: 0.5031\n",
      "209/500 [===========>..................] - ETA: 5:00 - loss: 1.9682 - regression_loss: 1.4646 - classification_loss: 0.5036\n",
      "210/500 [===========>..................] - ETA: 4:59 - loss: 1.9704 - regression_loss: 1.4667 - classification_loss: 0.5037\n",
      "211/500 [===========>..................] - ETA: 4:58 - loss: 1.9673 - regression_loss: 1.4643 - classification_loss: 0.5030\n",
      "212/500 [===========>..................] - ETA: 4:57 - loss: 1.9657 - regression_loss: 1.4634 - classification_loss: 0.5023\n",
      "213/500 [===========>..................] - ETA: 4:55 - loss: 1.9633 - regression_loss: 1.4617 - classification_loss: 0.5016\n",
      "214/500 [===========>..................] - ETA: 4:55 - loss: 1.9667 - regression_loss: 1.4642 - classification_loss: 0.5025\n",
      "215/500 [===========>..................] - ETA: 4:53 - loss: 1.9677 - regression_loss: 1.4649 - classification_loss: 0.5028\n",
      "216/500 [===========>..................] - ETA: 4:52 - loss: 1.9676 - regression_loss: 1.4651 - classification_loss: 0.5025\n",
      "217/500 [============>.................] - ETA: 4:51 - loss: 1.9682 - regression_loss: 1.4648 - classification_loss: 0.5034\n",
      "218/500 [============>.................] - ETA: 4:50 - loss: 1.9655 - regression_loss: 1.4625 - classification_loss: 0.5030\n",
      "219/500 [============>.................] - ETA: 4:49 - loss: 1.9658 - regression_loss: 1.4634 - classification_loss: 0.5024\n",
      "220/500 [============>.................] - ETA: 4:48 - loss: 1.9665 - regression_loss: 1.4632 - classification_loss: 0.5033\n",
      "221/500 [============>.................] - ETA: 4:47 - loss: 1.9670 - regression_loss: 1.4635 - classification_loss: 0.5034\n",
      "222/500 [============>.................] - ETA: 4:46 - loss: 1.9665 - regression_loss: 1.4628 - classification_loss: 0.5037\n",
      "223/500 [============>.................] - ETA: 4:45 - loss: 1.9669 - regression_loss: 1.4629 - classification_loss: 0.5041\n",
      "224/500 [============>.................] - ETA: 4:44 - loss: 1.9657 - regression_loss: 1.4621 - classification_loss: 0.5035\n",
      "225/500 [============>.................] - ETA: 4:43 - loss: 1.9636 - regression_loss: 1.4598 - classification_loss: 0.5037\n",
      "226/500 [============>.................] - ETA: 4:42 - loss: 1.9656 - regression_loss: 1.4616 - classification_loss: 0.5041\n",
      "227/500 [============>.................] - ETA: 4:41 - loss: 1.9650 - regression_loss: 1.4611 - classification_loss: 0.5039\n",
      "228/500 [============>.................] - ETA: 4:40 - loss: 1.9668 - regression_loss: 1.4627 - classification_loss: 0.5041\n",
      "229/500 [============>.................] - ETA: 4:39 - loss: 1.9648 - regression_loss: 1.4615 - classification_loss: 0.5033\n",
      "230/500 [============>.................] - ETA: 4:38 - loss: 1.9658 - regression_loss: 1.4620 - classification_loss: 0.5039\n",
      "231/500 [============>.................] - ETA: 4:37 - loss: 1.9643 - regression_loss: 1.4608 - classification_loss: 0.5035\n",
      "232/500 [============>.................] - ETA: 4:36 - loss: 1.9606 - regression_loss: 1.4578 - classification_loss: 0.5029\n",
      "233/500 [============>.................] - ETA: 4:35 - loss: 1.9601 - regression_loss: 1.4579 - classification_loss: 0.5022\n",
      "234/500 [=============>................] - ETA: 4:34 - loss: 1.9632 - regression_loss: 1.4604 - classification_loss: 0.5028\n",
      "235/500 [=============>................] - ETA: 4:33 - loss: 1.9606 - regression_loss: 1.4586 - classification_loss: 0.5021\n",
      "236/500 [=============>................] - ETA: 4:32 - loss: 1.9611 - regression_loss: 1.4588 - classification_loss: 0.5024\n",
      "237/500 [=============>................] - ETA: 4:31 - loss: 1.9607 - regression_loss: 1.4586 - classification_loss: 0.5021\n",
      "238/500 [=============>................] - ETA: 4:30 - loss: 1.9597 - regression_loss: 1.4578 - classification_loss: 0.5019\n",
      "239/500 [=============>................] - ETA: 4:29 - loss: 1.9628 - regression_loss: 1.4574 - classification_loss: 0.5054\n",
      "240/500 [=============>................] - ETA: 4:28 - loss: 1.9602 - regression_loss: 1.4557 - classification_loss: 0.5045\n",
      "241/500 [=============>................] - ETA: 4:27 - loss: 1.9597 - regression_loss: 1.4556 - classification_loss: 0.5041\n",
      "242/500 [=============>................] - ETA: 4:26 - loss: 1.9612 - regression_loss: 1.4568 - classification_loss: 0.5043\n",
      "243/500 [=============>................] - ETA: 4:25 - loss: 1.9587 - regression_loss: 1.4539 - classification_loss: 0.5048\n",
      "244/500 [=============>................] - ETA: 4:24 - loss: 1.9568 - regression_loss: 1.4520 - classification_loss: 0.5048\n",
      "245/500 [=============>................] - ETA: 4:23 - loss: 1.9551 - regression_loss: 1.4512 - classification_loss: 0.5039\n",
      "246/500 [=============>................] - ETA: 4:22 - loss: 1.9575 - regression_loss: 1.4530 - classification_loss: 0.5045\n",
      "247/500 [=============>................] - ETA: 4:21 - loss: 1.9548 - regression_loss: 1.4507 - classification_loss: 0.5040\n",
      "248/500 [=============>................] - ETA: 4:20 - loss: 1.9525 - regression_loss: 1.4490 - classification_loss: 0.5035\n",
      "249/500 [=============>................] - ETA: 4:19 - loss: 1.9510 - regression_loss: 1.4479 - classification_loss: 0.5031\n",
      "250/500 [==============>...............] - ETA: 4:18 - loss: 1.9532 - regression_loss: 1.4489 - classification_loss: 0.5043\n",
      "251/500 [==============>...............] - ETA: 4:17 - loss: 1.9519 - regression_loss: 1.4482 - classification_loss: 0.5037\n",
      "252/500 [==============>...............] - ETA: 4:16 - loss: 1.9563 - regression_loss: 1.4516 - classification_loss: 0.5046\n",
      "253/500 [==============>...............] - ETA: 4:15 - loss: 1.9538 - regression_loss: 1.4500 - classification_loss: 0.5038\n",
      "254/500 [==============>...............] - ETA: 4:14 - loss: 1.9556 - regression_loss: 1.4512 - classification_loss: 0.5044\n",
      "255/500 [==============>...............] - ETA: 4:13 - loss: 1.9541 - regression_loss: 1.4505 - classification_loss: 0.5036\n",
      "256/500 [==============>...............] - ETA: 4:12 - loss: 1.9594 - regression_loss: 1.4535 - classification_loss: 0.5058\n",
      "257/500 [==============>...............] - ETA: 4:11 - loss: 1.9602 - regression_loss: 1.4544 - classification_loss: 0.5058\n",
      "258/500 [==============>...............] - ETA: 4:09 - loss: 1.9616 - regression_loss: 1.4556 - classification_loss: 0.5060\n",
      "259/500 [==============>...............] - ETA: 4:08 - loss: 1.9638 - regression_loss: 1.4574 - classification_loss: 0.5064\n",
      "260/500 [==============>...............] - ETA: 4:07 - loss: 1.9607 - regression_loss: 1.4551 - classification_loss: 0.5056\n",
      "261/500 [==============>...............] - ETA: 4:06 - loss: 1.9605 - regression_loss: 1.4550 - classification_loss: 0.5055\n",
      "262/500 [==============>...............] - ETA: 4:05 - loss: 1.9586 - regression_loss: 1.4541 - classification_loss: 0.5045\n",
      "263/500 [==============>...............] - ETA: 4:04 - loss: 1.9583 - regression_loss: 1.4538 - classification_loss: 0.5044\n",
      "264/500 [==============>...............] - ETA: 4:03 - loss: 1.9592 - regression_loss: 1.4551 - classification_loss: 0.5041\n",
      "265/500 [==============>...............] - ETA: 4:02 - loss: 1.9594 - regression_loss: 1.4554 - classification_loss: 0.5041\n",
      "266/500 [==============>...............] - ETA: 4:01 - loss: 1.9575 - regression_loss: 1.4537 - classification_loss: 0.5037\n",
      "267/500 [===============>..............] - ETA: 4:00 - loss: 1.9571 - regression_loss: 1.4538 - classification_loss: 0.5033\n",
      "268/500 [===============>..............] - ETA: 3:59 - loss: 1.9604 - regression_loss: 1.4557 - classification_loss: 0.5047\n",
      "269/500 [===============>..............] - ETA: 3:58 - loss: 1.9603 - regression_loss: 1.4559 - classification_loss: 0.5044\n",
      "270/500 [===============>..............] - ETA: 3:57 - loss: 1.9577 - regression_loss: 1.4544 - classification_loss: 0.5033\n",
      "271/500 [===============>..............] - ETA: 3:56 - loss: 1.9543 - regression_loss: 1.4515 - classification_loss: 0.5028\n",
      "272/500 [===============>..............] - ETA: 3:55 - loss: 1.9519 - regression_loss: 1.4498 - classification_loss: 0.5021\n",
      "273/500 [===============>..............] - ETA: 3:54 - loss: 1.9523 - regression_loss: 1.4496 - classification_loss: 0.5027\n",
      "274/500 [===============>..............] - ETA: 3:53 - loss: 1.9516 - regression_loss: 1.4489 - classification_loss: 0.5028\n",
      "275/500 [===============>..............] - ETA: 3:52 - loss: 1.9520 - regression_loss: 1.4488 - classification_loss: 0.5033\n",
      "276/500 [===============>..............] - ETA: 3:51 - loss: 1.9502 - regression_loss: 1.4473 - classification_loss: 0.5029\n",
      "277/500 [===============>..............] - ETA: 3:49 - loss: 1.9499 - regression_loss: 1.4472 - classification_loss: 0.5027\n",
      "278/500 [===============>..............] - ETA: 3:48 - loss: 1.9494 - regression_loss: 1.4470 - classification_loss: 0.5024\n",
      "279/500 [===============>..............] - ETA: 3:47 - loss: 1.9505 - regression_loss: 1.4481 - classification_loss: 0.5025\n",
      "280/500 [===============>..............] - ETA: 3:46 - loss: 1.9500 - regression_loss: 1.4480 - classification_loss: 0.5020\n",
      "281/500 [===============>..............] - ETA: 3:45 - loss: 1.9497 - regression_loss: 1.4479 - classification_loss: 0.5017\n",
      "282/500 [===============>..............] - ETA: 3:45 - loss: 1.9508 - regression_loss: 1.4490 - classification_loss: 0.5018\n",
      "283/500 [===============>..............] - ETA: 3:43 - loss: 1.9513 - regression_loss: 1.4494 - classification_loss: 0.5019\n",
      "284/500 [================>.............] - ETA: 3:42 - loss: 1.9502 - regression_loss: 1.4485 - classification_loss: 0.5016\n",
      "285/500 [================>.............] - ETA: 3:41 - loss: 1.9501 - regression_loss: 1.4483 - classification_loss: 0.5018\n",
      "286/500 [================>.............] - ETA: 3:41 - loss: 1.9485 - regression_loss: 1.4473 - classification_loss: 0.5012\n",
      "287/500 [================>.............] - ETA: 3:40 - loss: 1.9473 - regression_loss: 1.4467 - classification_loss: 0.5006\n",
      "288/500 [================>.............] - ETA: 3:39 - loss: 1.9469 - regression_loss: 1.4468 - classification_loss: 0.5000\n",
      "289/500 [================>.............] - ETA: 3:37 - loss: 1.9473 - regression_loss: 1.4473 - classification_loss: 0.4999\n",
      "290/500 [================>.............] - ETA: 3:36 - loss: 1.9464 - regression_loss: 1.4468 - classification_loss: 0.4996\n",
      "291/500 [================>.............] - ETA: 3:35 - loss: 1.9474 - regression_loss: 1.4476 - classification_loss: 0.4998\n",
      "292/500 [================>.............] - ETA: 3:34 - loss: 1.9478 - regression_loss: 1.4479 - classification_loss: 0.4998\n",
      "293/500 [================>.............] - ETA: 3:33 - loss: 1.9467 - regression_loss: 1.4476 - classification_loss: 0.4992\n",
      "294/500 [================>.............] - ETA: 3:32 - loss: 1.9449 - regression_loss: 1.4463 - classification_loss: 0.4986\n",
      "295/500 [================>.............] - ETA: 3:31 - loss: 1.9430 - regression_loss: 1.4449 - classification_loss: 0.4981\n",
      "296/500 [================>.............] - ETA: 3:30 - loss: 1.9448 - regression_loss: 1.4463 - classification_loss: 0.4985\n",
      "297/500 [================>.............] - ETA: 3:29 - loss: 1.9470 - regression_loss: 1.4478 - classification_loss: 0.4991\n",
      "298/500 [================>.............] - ETA: 3:28 - loss: 1.9471 - regression_loss: 1.4479 - classification_loss: 0.4992\n",
      "299/500 [================>.............] - ETA: 3:27 - loss: 1.9474 - regression_loss: 1.4480 - classification_loss: 0.4993\n",
      "300/500 [=================>............] - ETA: 3:26 - loss: 1.9477 - regression_loss: 1.4482 - classification_loss: 0.4995\n",
      "301/500 [=================>............] - ETA: 3:25 - loss: 1.9476 - regression_loss: 1.4479 - classification_loss: 0.4997\n",
      "302/500 [=================>............] - ETA: 3:24 - loss: 1.9499 - regression_loss: 1.4484 - classification_loss: 0.5015\n",
      "303/500 [=================>............] - ETA: 3:23 - loss: 1.9500 - regression_loss: 1.4489 - classification_loss: 0.5011\n",
      "304/500 [=================>............] - ETA: 3:22 - loss: 1.9471 - regression_loss: 1.4464 - classification_loss: 0.5007\n",
      "305/500 [=================>............] - ETA: 3:21 - loss: 1.9454 - regression_loss: 1.4452 - classification_loss: 0.5002\n",
      "306/500 [=================>............] - ETA: 3:20 - loss: 1.9448 - regression_loss: 1.4451 - classification_loss: 0.4997\n",
      "307/500 [=================>............] - ETA: 3:19 - loss: 1.9445 - regression_loss: 1.4450 - classification_loss: 0.4995\n",
      "308/500 [=================>............] - ETA: 3:18 - loss: 1.9473 - regression_loss: 1.4471 - classification_loss: 0.5002\n",
      "309/500 [=================>............] - ETA: 3:17 - loss: 1.9486 - regression_loss: 1.4484 - classification_loss: 0.5002\n",
      "310/500 [=================>............] - ETA: 3:16 - loss: 1.9489 - regression_loss: 1.4490 - classification_loss: 0.4999\n",
      "311/500 [=================>............] - ETA: 3:14 - loss: 1.9485 - regression_loss: 1.4486 - classification_loss: 0.4998\n",
      "312/500 [=================>............] - ETA: 3:13 - loss: 1.9498 - regression_loss: 1.4494 - classification_loss: 0.5004\n",
      "313/500 [=================>............] - ETA: 3:12 - loss: 1.9480 - regression_loss: 1.4484 - classification_loss: 0.4996\n",
      "314/500 [=================>............] - ETA: 3:11 - loss: 1.9470 - regression_loss: 1.4474 - classification_loss: 0.4995\n",
      "315/500 [=================>............] - ETA: 3:11 - loss: 1.9466 - regression_loss: 1.4471 - classification_loss: 0.4995\n",
      "316/500 [=================>............] - ETA: 3:09 - loss: 1.9458 - regression_loss: 1.4466 - classification_loss: 0.4992\n",
      "317/500 [==================>...........] - ETA: 3:08 - loss: 1.9470 - regression_loss: 1.4473 - classification_loss: 0.4997\n",
      "318/500 [==================>...........] - ETA: 3:07 - loss: 1.9469 - regression_loss: 1.4476 - classification_loss: 0.4992\n",
      "319/500 [==================>...........] - ETA: 3:06 - loss: 1.9457 - regression_loss: 1.4463 - classification_loss: 0.4993\n",
      "320/500 [==================>...........] - ETA: 3:05 - loss: 1.9455 - regression_loss: 1.4463 - classification_loss: 0.4992\n",
      "321/500 [==================>...........] - ETA: 3:04 - loss: 1.9458 - regression_loss: 1.4472 - classification_loss: 0.4986\n",
      "322/500 [==================>...........] - ETA: 3:03 - loss: 1.9439 - regression_loss: 1.4459 - classification_loss: 0.4980\n",
      "323/500 [==================>...........] - ETA: 3:02 - loss: 1.9444 - regression_loss: 1.4464 - classification_loss: 0.4980\n",
      "324/500 [==================>...........] - ETA: 3:01 - loss: 1.9440 - regression_loss: 1.4461 - classification_loss: 0.4979\n",
      "325/500 [==================>...........] - ETA: 3:00 - loss: 1.9457 - regression_loss: 1.4474 - classification_loss: 0.4983\n",
      "326/500 [==================>...........] - ETA: 2:59 - loss: 1.9460 - regression_loss: 1.4481 - classification_loss: 0.4979\n",
      "327/500 [==================>...........] - ETA: 2:58 - loss: 1.9441 - regression_loss: 1.4467 - classification_loss: 0.4974\n",
      "328/500 [==================>...........] - ETA: 2:57 - loss: 1.9427 - regression_loss: 1.4457 - classification_loss: 0.4969\n",
      "329/500 [==================>...........] - ETA: 2:56 - loss: 1.9417 - regression_loss: 1.4451 - classification_loss: 0.4966\n",
      "330/500 [==================>...........] - ETA: 2:55 - loss: 1.9415 - regression_loss: 1.4451 - classification_loss: 0.4964\n",
      "331/500 [==================>...........] - ETA: 2:54 - loss: 1.9416 - regression_loss: 1.4454 - classification_loss: 0.4962\n",
      "332/500 [==================>...........] - ETA: 2:53 - loss: 1.9431 - regression_loss: 1.4467 - classification_loss: 0.4964\n",
      "333/500 [==================>...........] - ETA: 2:52 - loss: 1.9429 - regression_loss: 1.4463 - classification_loss: 0.4966\n",
      "334/500 [===================>..........] - ETA: 2:51 - loss: 1.9431 - regression_loss: 1.4464 - classification_loss: 0.4967\n",
      "335/500 [===================>..........] - ETA: 2:50 - loss: 1.9442 - regression_loss: 1.4474 - classification_loss: 0.4968\n",
      "336/500 [===================>..........] - ETA: 2:49 - loss: 1.9426 - regression_loss: 1.4465 - classification_loss: 0.4961\n",
      "337/500 [===================>..........] - ETA: 2:48 - loss: 1.9431 - regression_loss: 1.4470 - classification_loss: 0.4961\n",
      "338/500 [===================>..........] - ETA: 2:47 - loss: 1.9405 - regression_loss: 1.4452 - classification_loss: 0.4954\n",
      "339/500 [===================>..........] - ETA: 2:46 - loss: 1.9377 - regression_loss: 1.4428 - classification_loss: 0.4948\n",
      "340/500 [===================>..........] - ETA: 2:45 - loss: 1.9383 - regression_loss: 1.4433 - classification_loss: 0.4950\n",
      "341/500 [===================>..........] - ETA: 2:44 - loss: 1.9371 - regression_loss: 1.4425 - classification_loss: 0.4946\n",
      "342/500 [===================>..........] - ETA: 2:43 - loss: 1.9364 - regression_loss: 1.4418 - classification_loss: 0.4947\n",
      "343/500 [===================>..........] - ETA: 2:42 - loss: 1.9346 - regression_loss: 1.4406 - classification_loss: 0.4939\n",
      "344/500 [===================>..........] - ETA: 2:41 - loss: 1.9324 - regression_loss: 1.4391 - classification_loss: 0.4933\n",
      "345/500 [===================>..........] - ETA: 2:40 - loss: 1.9310 - regression_loss: 1.4381 - classification_loss: 0.4930\n",
      "346/500 [===================>..........] - ETA: 2:39 - loss: 1.9287 - regression_loss: 1.4363 - classification_loss: 0.4924\n",
      "347/500 [===================>..........] - ETA: 2:38 - loss: 1.9289 - regression_loss: 1.4364 - classification_loss: 0.4925\n",
      "348/500 [===================>..........] - ETA: 2:36 - loss: 1.9281 - regression_loss: 1.4354 - classification_loss: 0.4927\n",
      "349/500 [===================>..........] - ETA: 2:35 - loss: 1.9258 - regression_loss: 1.4338 - classification_loss: 0.4920\n",
      "350/500 [====================>.........] - ETA: 2:34 - loss: 1.9232 - regression_loss: 1.4318 - classification_loss: 0.4914\n",
      "351/500 [====================>.........] - ETA: 2:33 - loss: 1.9254 - regression_loss: 1.4331 - classification_loss: 0.4923\n",
      "352/500 [====================>.........] - ETA: 2:32 - loss: 1.9260 - regression_loss: 1.4339 - classification_loss: 0.4922\n",
      "353/500 [====================>.........] - ETA: 2:31 - loss: 1.9261 - regression_loss: 1.4336 - classification_loss: 0.4924\n",
      "354/500 [====================>.........] - ETA: 2:30 - loss: 1.9267 - regression_loss: 1.4341 - classification_loss: 0.4926\n",
      "355/500 [====================>.........] - ETA: 2:29 - loss: 1.9262 - regression_loss: 1.4338 - classification_loss: 0.4924\n",
      "356/500 [====================>.........] - ETA: 2:28 - loss: 1.9250 - regression_loss: 1.4331 - classification_loss: 0.4919\n",
      "357/500 [====================>.........] - ETA: 2:27 - loss: 1.9274 - regression_loss: 1.4347 - classification_loss: 0.4927\n",
      "358/500 [====================>.........] - ETA: 2:26 - loss: 1.9285 - regression_loss: 1.4352 - classification_loss: 0.4933\n",
      "359/500 [====================>.........] - ETA: 2:25 - loss: 1.9284 - regression_loss: 1.4356 - classification_loss: 0.4928\n",
      "360/500 [====================>.........] - ETA: 2:24 - loss: 1.9299 - regression_loss: 1.4366 - classification_loss: 0.4933\n",
      "361/500 [====================>.........] - ETA: 2:23 - loss: 1.9302 - regression_loss: 1.4365 - classification_loss: 0.4936\n",
      "362/500 [====================>.........] - ETA: 2:22 - loss: 1.9295 - regression_loss: 1.4360 - classification_loss: 0.4935\n",
      "363/500 [====================>.........] - ETA: 2:21 - loss: 1.9306 - regression_loss: 1.4365 - classification_loss: 0.4941\n",
      "364/500 [====================>.........] - ETA: 2:20 - loss: 1.9317 - regression_loss: 1.4377 - classification_loss: 0.4941\n",
      "365/500 [====================>.........] - ETA: 2:19 - loss: 1.9315 - regression_loss: 1.4376 - classification_loss: 0.4940\n",
      "366/500 [====================>.........] - ETA: 2:18 - loss: 1.9306 - regression_loss: 1.4369 - classification_loss: 0.4937\n",
      "367/500 [=====================>........] - ETA: 2:17 - loss: 1.9294 - regression_loss: 1.4359 - classification_loss: 0.4935\n",
      "368/500 [=====================>........] - ETA: 2:16 - loss: 1.9275 - regression_loss: 1.4346 - classification_loss: 0.4929\n",
      "369/500 [=====================>........] - ETA: 2:14 - loss: 1.9259 - regression_loss: 1.4334 - classification_loss: 0.4925\n",
      "370/500 [=====================>........] - ETA: 2:14 - loss: 1.9254 - regression_loss: 1.4327 - classification_loss: 0.4927\n",
      "371/500 [=====================>........] - ETA: 2:12 - loss: 1.9262 - regression_loss: 1.4332 - classification_loss: 0.4929\n",
      "372/500 [=====================>........] - ETA: 2:11 - loss: 1.9256 - regression_loss: 1.4328 - classification_loss: 0.4928\n",
      "373/500 [=====================>........] - ETA: 2:10 - loss: 1.9232 - regression_loss: 1.4309 - classification_loss: 0.4923\n",
      "374/500 [=====================>........] - ETA: 2:09 - loss: 1.9213 - regression_loss: 1.4293 - classification_loss: 0.4920\n",
      "375/500 [=====================>........] - ETA: 2:08 - loss: 1.9192 - regression_loss: 1.4281 - classification_loss: 0.4911\n",
      "376/500 [=====================>........] - ETA: 2:07 - loss: 1.9182 - regression_loss: 1.4273 - classification_loss: 0.4909\n",
      "377/500 [=====================>........] - ETA: 2:06 - loss: 1.9206 - regression_loss: 1.4293 - classification_loss: 0.4913\n",
      "378/500 [=====================>........] - ETA: 2:05 - loss: 1.9210 - regression_loss: 1.4298 - classification_loss: 0.4912\n",
      "379/500 [=====================>........] - ETA: 2:04 - loss: 1.9223 - regression_loss: 1.4309 - classification_loss: 0.4913\n",
      "380/500 [=====================>........] - ETA: 2:03 - loss: 1.9213 - regression_loss: 1.4300 - classification_loss: 0.4912\n",
      "381/500 [=====================>........] - ETA: 2:02 - loss: 1.9191 - regression_loss: 1.4283 - classification_loss: 0.4907\n",
      "382/500 [=====================>........] - ETA: 2:01 - loss: 1.9193 - regression_loss: 1.4285 - classification_loss: 0.4907\n",
      "383/500 [=====================>........] - ETA: 2:00 - loss: 1.9185 - regression_loss: 1.4277 - classification_loss: 0.4908\n",
      "384/500 [======================>.......] - ETA: 1:59 - loss: 1.9169 - regression_loss: 1.4264 - classification_loss: 0.4905\n",
      "385/500 [======================>.......] - ETA: 1:58 - loss: 1.9142 - regression_loss: 1.4245 - classification_loss: 0.4897\n",
      "386/500 [======================>.......] - ETA: 1:57 - loss: 1.9128 - regression_loss: 1.4236 - classification_loss: 0.4891\n",
      "387/500 [======================>.......] - ETA: 1:56 - loss: 1.9131 - regression_loss: 1.4240 - classification_loss: 0.4891\n",
      "388/500 [======================>.......] - ETA: 1:55 - loss: 1.9138 - regression_loss: 1.4244 - classification_loss: 0.4894\n",
      "389/500 [======================>.......] - ETA: 1:54 - loss: 1.9118 - regression_loss: 1.4231 - classification_loss: 0.4887\n",
      "390/500 [======================>.......] - ETA: 1:53 - loss: 1.9124 - regression_loss: 1.4236 - classification_loss: 0.4889\n",
      "391/500 [======================>.......] - ETA: 1:52 - loss: 1.9099 - regression_loss: 1.4219 - classification_loss: 0.4881\n",
      "392/500 [======================>.......] - ETA: 1:51 - loss: 1.9089 - regression_loss: 1.4211 - classification_loss: 0.4878\n",
      "393/500 [======================>.......] - ETA: 1:50 - loss: 1.9094 - regression_loss: 1.4213 - classification_loss: 0.4881\n",
      "394/500 [======================>.......] - ETA: 1:49 - loss: 1.9113 - regression_loss: 1.4224 - classification_loss: 0.4889\n",
      "395/500 [======================>.......] - ETA: 1:48 - loss: 1.9115 - regression_loss: 1.4226 - classification_loss: 0.4890\n",
      "396/500 [======================>.......] - ETA: 1:47 - loss: 1.9096 - regression_loss: 1.4211 - classification_loss: 0.4885\n",
      "397/500 [======================>.......] - ETA: 1:46 - loss: 1.9075 - regression_loss: 1.4196 - classification_loss: 0.4879\n",
      "398/500 [======================>.......] - ETA: 1:45 - loss: 1.9075 - regression_loss: 1.4197 - classification_loss: 0.4878\n",
      "399/500 [======================>.......] - ETA: 1:44 - loss: 1.9072 - regression_loss: 1.4193 - classification_loss: 0.4879\n",
      "400/500 [=======================>......] - ETA: 1:42 - loss: 1.9055 - regression_loss: 1.4182 - classification_loss: 0.4873\n",
      "401/500 [=======================>......] - ETA: 1:41 - loss: 1.9071 - regression_loss: 1.4197 - classification_loss: 0.4873\n",
      "402/500 [=======================>......] - ETA: 1:40 - loss: 1.9081 - regression_loss: 1.4205 - classification_loss: 0.4876\n",
      "403/500 [=======================>......] - ETA: 1:39 - loss: 1.9062 - regression_loss: 1.4192 - classification_loss: 0.4871\n",
      "404/500 [=======================>......] - ETA: 1:38 - loss: 1.9054 - regression_loss: 1.4186 - classification_loss: 0.4868\n",
      "405/500 [=======================>......] - ETA: 1:37 - loss: 1.9079 - regression_loss: 1.4201 - classification_loss: 0.4878\n",
      "406/500 [=======================>......] - ETA: 1:36 - loss: 1.9055 - regression_loss: 1.4183 - classification_loss: 0.4871\n",
      "407/500 [=======================>......] - ETA: 1:35 - loss: 1.9061 - regression_loss: 1.4192 - classification_loss: 0.4869\n",
      "408/500 [=======================>......] - ETA: 1:34 - loss: 1.9061 - regression_loss: 1.4195 - classification_loss: 0.4866\n",
      "409/500 [=======================>......] - ETA: 1:33 - loss: 1.9068 - regression_loss: 1.4199 - classification_loss: 0.4869\n",
      "410/500 [=======================>......] - ETA: 1:32 - loss: 1.9074 - regression_loss: 1.4208 - classification_loss: 0.4866\n",
      "411/500 [=======================>......] - ETA: 1:31 - loss: 1.9102 - regression_loss: 1.4227 - classification_loss: 0.4875\n",
      "412/500 [=======================>......] - ETA: 1:30 - loss: 1.9121 - regression_loss: 1.4237 - classification_loss: 0.4884\n",
      "413/500 [=======================>......] - ETA: 1:29 - loss: 1.9120 - regression_loss: 1.4239 - classification_loss: 0.4882\n",
      "414/500 [=======================>......] - ETA: 1:28 - loss: 1.9131 - regression_loss: 1.4243 - classification_loss: 0.4888\n",
      "415/500 [=======================>......] - ETA: 1:27 - loss: 1.9132 - regression_loss: 1.4243 - classification_loss: 0.4889\n",
      "416/500 [=======================>......] - ETA: 1:26 - loss: 1.9125 - regression_loss: 1.4236 - classification_loss: 0.4889\n",
      "417/500 [========================>.....] - ETA: 1:25 - loss: 1.9128 - regression_loss: 1.4238 - classification_loss: 0.4891\n",
      "418/500 [========================>.....] - ETA: 1:24 - loss: 1.9128 - regression_loss: 1.4237 - classification_loss: 0.4891\n",
      "419/500 [========================>.....] - ETA: 1:23 - loss: 1.9131 - regression_loss: 1.4234 - classification_loss: 0.4897\n",
      "420/500 [========================>.....] - ETA: 1:22 - loss: 1.9128 - regression_loss: 1.4232 - classification_loss: 0.4896\n",
      "421/500 [========================>.....] - ETA: 1:21 - loss: 1.9139 - regression_loss: 1.4241 - classification_loss: 0.4897\n",
      "422/500 [========================>.....] - ETA: 1:20 - loss: 1.9144 - regression_loss: 1.4246 - classification_loss: 0.4898\n",
      "423/500 [========================>.....] - ETA: 1:19 - loss: 1.9133 - regression_loss: 1.4237 - classification_loss: 0.4896\n",
      "424/500 [========================>.....] - ETA: 1:18 - loss: 1.9137 - regression_loss: 1.4244 - classification_loss: 0.4893\n",
      "425/500 [========================>.....] - ETA: 1:17 - loss: 1.9111 - regression_loss: 1.4225 - classification_loss: 0.4886\n",
      "426/500 [========================>.....] - ETA: 1:16 - loss: 1.9113 - regression_loss: 1.4229 - classification_loss: 0.4884\n",
      "427/500 [========================>.....] - ETA: 1:15 - loss: 1.9103 - regression_loss: 1.4222 - classification_loss: 0.4881\n",
      "428/500 [========================>.....] - ETA: 1:14 - loss: 1.9121 - regression_loss: 1.4237 - classification_loss: 0.4884\n",
      "429/500 [========================>.....] - ETA: 1:13 - loss: 1.9136 - regression_loss: 1.4247 - classification_loss: 0.4889\n",
      "430/500 [========================>.....] - ETA: 1:12 - loss: 1.9133 - regression_loss: 1.4247 - classification_loss: 0.4886\n",
      "431/500 [========================>.....] - ETA: 1:11 - loss: 1.9139 - regression_loss: 1.4250 - classification_loss: 0.4889\n",
      "432/500 [========================>.....] - ETA: 1:10 - loss: 1.9138 - regression_loss: 1.4248 - classification_loss: 0.4891\n",
      "433/500 [========================>.....] - ETA: 1:09 - loss: 1.9139 - regression_loss: 1.4250 - classification_loss: 0.4889\n",
      "434/500 [=========================>....] - ETA: 1:08 - loss: 1.9147 - regression_loss: 1.4257 - classification_loss: 0.4891\n",
      "435/500 [=========================>....] - ETA: 1:07 - loss: 1.9146 - regression_loss: 1.4258 - classification_loss: 0.4888\n",
      "436/500 [=========================>....] - ETA: 1:06 - loss: 1.9152 - regression_loss: 1.4265 - classification_loss: 0.4887\n",
      "437/500 [=========================>....] - ETA: 1:04 - loss: 1.9139 - regression_loss: 1.4256 - classification_loss: 0.4883\n",
      "438/500 [=========================>....] - ETA: 1:03 - loss: 1.9133 - regression_loss: 1.4249 - classification_loss: 0.4884\n",
      "439/500 [=========================>....] - ETA: 1:02 - loss: 1.9114 - regression_loss: 1.4235 - classification_loss: 0.4879\n",
      "440/500 [=========================>....] - ETA: 1:01 - loss: 1.9127 - regression_loss: 1.4244 - classification_loss: 0.4883\n",
      "441/500 [=========================>....] - ETA: 1:00 - loss: 1.9122 - regression_loss: 1.4240 - classification_loss: 0.4882\n",
      "442/500 [=========================>....] - ETA: 59s - loss: 1.9123 - regression_loss: 1.4240 - classification_loss: 0.4883 \n",
      "443/500 [=========================>....] - ETA: 58s - loss: 1.9119 - regression_loss: 1.4236 - classification_loss: 0.4882\n",
      "444/500 [=========================>....] - ETA: 57s - loss: 1.9113 - regression_loss: 1.4232 - classification_loss: 0.4881\n",
      "445/500 [=========================>....] - ETA: 56s - loss: 1.9095 - regression_loss: 1.4218 - classification_loss: 0.4877\n",
      "446/500 [=========================>....] - ETA: 55s - loss: 1.9092 - regression_loss: 1.4215 - classification_loss: 0.4877\n",
      "447/500 [=========================>....] - ETA: 54s - loss: 1.9087 - regression_loss: 1.4214 - classification_loss: 0.4873\n",
      "448/500 [=========================>....] - ETA: 53s - loss: 1.9111 - regression_loss: 1.4231 - classification_loss: 0.4880\n",
      "449/500 [=========================>....] - ETA: 52s - loss: 1.9121 - regression_loss: 1.4235 - classification_loss: 0.4887\n",
      "450/500 [==========================>...] - ETA: 51s - loss: 1.9130 - regression_loss: 1.4242 - classification_loss: 0.4888\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 1.9135 - regression_loss: 1.4247 - classification_loss: 0.4888\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 1.9147 - regression_loss: 1.4254 - classification_loss: 0.4892\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.9153 - regression_loss: 1.4260 - classification_loss: 0.4893\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.9140 - regression_loss: 1.4253 - classification_loss: 0.4887\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.9131 - regression_loss: 1.4247 - classification_loss: 0.4884\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.9133 - regression_loss: 1.4251 - classification_loss: 0.4881\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.9126 - regression_loss: 1.4247 - classification_loss: 0.4880\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.9118 - regression_loss: 1.4242 - classification_loss: 0.4876\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.9140 - regression_loss: 1.4255 - classification_loss: 0.4885\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.9138 - regression_loss: 1.4257 - classification_loss: 0.4881\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 1.9142 - regression_loss: 1.4260 - classification_loss: 0.4882\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 1.9131 - regression_loss: 1.4251 - classification_loss: 0.4879\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 1.9138 - regression_loss: 1.4257 - classification_loss: 0.4881\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 1.9150 - regression_loss: 1.4265 - classification_loss: 0.4884\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 1.9147 - regression_loss: 1.4260 - classification_loss: 0.4886\n",
      "466/500 [==========================>...] - ETA: 35s - loss: 1.9165 - regression_loss: 1.4274 - classification_loss: 0.4891\n",
      "467/500 [===========================>..] - ETA: 34s - loss: 1.9147 - regression_loss: 1.4261 - classification_loss: 0.4885\n",
      "468/500 [===========================>..] - ETA: 33s - loss: 1.9177 - regression_loss: 1.4281 - classification_loss: 0.4896\n",
      "469/500 [===========================>..] - ETA: 31s - loss: 1.9186 - regression_loss: 1.4286 - classification_loss: 0.4899\n",
      "470/500 [===========================>..] - ETA: 30s - loss: 1.9201 - regression_loss: 1.4298 - classification_loss: 0.4903\n",
      "471/500 [===========================>..] - ETA: 29s - loss: 1.9209 - regression_loss: 1.4305 - classification_loss: 0.4904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472/500 [===========================>..] - ETA: 28s - loss: 1.9216 - regression_loss: 1.4311 - classification_loss: 0.4905\n",
      "473/500 [===========================>..] - ETA: 27s - loss: 1.9209 - regression_loss: 1.4308 - classification_loss: 0.4900\n",
      "474/500 [===========================>..] - ETA: 26s - loss: 1.9220 - regression_loss: 1.4316 - classification_loss: 0.4904\n",
      "475/500 [===========================>..] - ETA: 25s - loss: 1.9224 - regression_loss: 1.4320 - classification_loss: 0.4904\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 1.9226 - regression_loss: 1.4319 - classification_loss: 0.4907\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.9219 - regression_loss: 1.4314 - classification_loss: 0.4906\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.9215 - regression_loss: 1.4310 - classification_loss: 0.4905\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.9217 - regression_loss: 1.4311 - classification_loss: 0.4907\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.9209 - regression_loss: 1.4306 - classification_loss: 0.4903\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.9208 - regression_loss: 1.4306 - classification_loss: 0.4902\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.9202 - regression_loss: 1.4303 - classification_loss: 0.4899\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.9200 - regression_loss: 1.4304 - classification_loss: 0.4896\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.9197 - regression_loss: 1.4301 - classification_loss: 0.4896\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.9209 - regression_loss: 1.4314 - classification_loss: 0.4895\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.9206 - regression_loss: 1.4311 - classification_loss: 0.4895\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.9212 - regression_loss: 1.4314 - classification_loss: 0.4897\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.9229 - regression_loss: 1.4323 - classification_loss: 0.4905\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.9225 - regression_loss: 1.4320 - classification_loss: 0.4905\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.9219 - regression_loss: 1.4318 - classification_loss: 0.4901\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.9213 - regression_loss: 1.4313 - classification_loss: 0.4901 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.9201 - regression_loss: 1.4303 - classification_loss: 0.4898\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.9211 - regression_loss: 1.4308 - classification_loss: 0.4903\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.9201 - regression_loss: 1.4299 - classification_loss: 0.4901\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.9189 - regression_loss: 1.4291 - classification_loss: 0.4898\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.9197 - regression_loss: 1.4301 - classification_loss: 0.4897\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.9199 - regression_loss: 1.4303 - classification_loss: 0.4896\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.9205 - regression_loss: 1.4307 - classification_loss: 0.4898\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.9206 - regression_loss: 1.4307 - classification_loss: 0.4899\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9206 - regression_loss: 1.4310 - classification_loss: 0.4896\n",
      "Epoch 00018: saving model to ./snapshots\\resnet50_csv_18.h5\n",
      "\n",
      "500/500 [==============================] - 516s 1s/step - loss: 1.9206 - regression_loss: 1.4310 - classification_loss: 0.4896\n",
      "Epoch 19/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 1.0007 - regression_loss: 0.8113 - classification_loss: 0.1894\n",
      "  2/500 [..............................] - ETA: 3:39 - loss: 1.1155 - regression_loss: 0.8925 - classification_loss: 0.2230\n",
      "  3/500 [..............................] - ETA: 5:49 - loss: 1.3738 - regression_loss: 1.1093 - classification_loss: 0.2644\n",
      "  4/500 [..............................] - ETA: 6:23 - loss: 1.4320 - regression_loss: 1.1555 - classification_loss: 0.2765\n",
      "  5/500 [..............................] - ETA: 6:53 - loss: 1.5352 - regression_loss: 1.2094 - classification_loss: 0.3259\n",
      "  6/500 [..............................] - ETA: 7:13 - loss: 1.5828 - regression_loss: 1.2205 - classification_loss: 0.3623\n",
      "  7/500 [..............................] - ETA: 7:04 - loss: 1.4876 - regression_loss: 1.1466 - classification_loss: 0.3409\n",
      "  8/500 [..............................] - ETA: 7:22 - loss: 1.5292 - regression_loss: 1.1819 - classification_loss: 0.3473\n",
      "  9/500 [..............................] - ETA: 7:31 - loss: 1.5583 - regression_loss: 1.2022 - classification_loss: 0.3561\n",
      " 10/500 [..............................] - ETA: 7:39 - loss: 1.5884 - regression_loss: 1.2227 - classification_loss: 0.3656\n",
      " 11/500 [..............................] - ETA: 7:45 - loss: 1.6936 - regression_loss: 1.2943 - classification_loss: 0.3993\n",
      " 12/500 [..............................] - ETA: 7:50 - loss: 1.6817 - regression_loss: 1.2778 - classification_loss: 0.4038\n",
      " 13/500 [..............................] - ETA: 7:50 - loss: 1.6861 - regression_loss: 1.2820 - classification_loss: 0.4040\n",
      " 14/500 [..............................] - ETA: 7:50 - loss: 1.6979 - regression_loss: 1.2925 - classification_loss: 0.4055\n",
      " 15/500 [..............................] - ETA: 7:52 - loss: 1.7035 - regression_loss: 1.3056 - classification_loss: 0.3979\n",
      " 16/500 [..............................] - ETA: 7:44 - loss: 1.7967 - regression_loss: 1.3658 - classification_loss: 0.4309\n",
      " 17/500 [>.............................] - ETA: 7:50 - loss: 1.8116 - regression_loss: 1.3710 - classification_loss: 0.4406\n",
      " 18/500 [>.............................] - ETA: 7:49 - loss: 1.7871 - regression_loss: 1.3559 - classification_loss: 0.4312\n",
      " 19/500 [>.............................] - ETA: 7:51 - loss: 1.8218 - regression_loss: 1.3599 - classification_loss: 0.4619\n",
      " 20/500 [>.............................] - ETA: 7:46 - loss: 1.8012 - regression_loss: 1.3462 - classification_loss: 0.4550\n",
      " 21/500 [>.............................] - ETA: 7:47 - loss: 1.7750 - regression_loss: 1.3273 - classification_loss: 0.4477\n",
      " 22/500 [>.............................] - ETA: 7:49 - loss: 1.7664 - regression_loss: 1.3227 - classification_loss: 0.4437\n",
      " 23/500 [>.............................] - ETA: 7:50 - loss: 1.8042 - regression_loss: 1.3501 - classification_loss: 0.4541\n",
      " 24/500 [>.............................] - ETA: 7:50 - loss: 1.7961 - regression_loss: 1.3424 - classification_loss: 0.4537\n",
      " 25/500 [>.............................] - ETA: 7:51 - loss: 1.7909 - regression_loss: 1.3409 - classification_loss: 0.4500\n",
      " 26/500 [>.............................] - ETA: 7:52 - loss: 1.7986 - regression_loss: 1.3478 - classification_loss: 0.4508\n",
      " 27/500 [>.............................] - ETA: 7:51 - loss: 1.7856 - regression_loss: 1.3361 - classification_loss: 0.4495\n",
      " 28/500 [>.............................] - ETA: 7:51 - loss: 1.7763 - regression_loss: 1.3255 - classification_loss: 0.4508\n",
      " 29/500 [>.............................] - ETA: 7:52 - loss: 1.8033 - regression_loss: 1.3489 - classification_loss: 0.4544\n",
      " 30/500 [>.............................] - ETA: 7:49 - loss: 1.7901 - regression_loss: 1.3392 - classification_loss: 0.4510\n",
      " 31/500 [>.............................] - ETA: 7:50 - loss: 1.7618 - regression_loss: 1.3185 - classification_loss: 0.4432\n",
      " 32/500 [>.............................] - ETA: 7:47 - loss: 1.7607 - regression_loss: 1.3140 - classification_loss: 0.4467\n",
      " 33/500 [>.............................] - ETA: 7:47 - loss: 1.7919 - regression_loss: 1.3336 - classification_loss: 0.4583\n",
      " 34/500 [=>............................] - ETA: 7:47 - loss: 1.7714 - regression_loss: 1.3198 - classification_loss: 0.4517\n",
      " 35/500 [=>............................] - ETA: 7:46 - loss: 1.7774 - regression_loss: 1.3251 - classification_loss: 0.4523\n",
      " 36/500 [=>............................] - ETA: 7:46 - loss: 1.7753 - regression_loss: 1.3242 - classification_loss: 0.4511\n",
      " 37/500 [=>............................] - ETA: 7:47 - loss: 1.7791 - regression_loss: 1.3258 - classification_loss: 0.4533\n",
      " 38/500 [=>............................] - ETA: 7:45 - loss: 1.7832 - regression_loss: 1.3325 - classification_loss: 0.4507\n",
      " 39/500 [=>............................] - ETA: 7:44 - loss: 1.7795 - regression_loss: 1.3302 - classification_loss: 0.4493\n",
      " 40/500 [=>............................] - ETA: 7:44 - loss: 1.7996 - regression_loss: 1.3413 - classification_loss: 0.4583\n",
      " 41/500 [=>............................] - ETA: 7:45 - loss: 1.7902 - regression_loss: 1.3324 - classification_loss: 0.4578\n",
      " 42/500 [=>............................] - ETA: 7:44 - loss: 1.7836 - regression_loss: 1.3278 - classification_loss: 0.4558\n",
      " 43/500 [=>............................] - ETA: 7:44 - loss: 1.8103 - regression_loss: 1.3482 - classification_loss: 0.4621\n",
      " 44/500 [=>............................] - ETA: 7:44 - loss: 1.8024 - regression_loss: 1.3405 - classification_loss: 0.4619\n",
      " 45/500 [=>............................] - ETA: 7:43 - loss: 1.7912 - regression_loss: 1.3327 - classification_loss: 0.4585\n",
      " 46/500 [=>............................] - ETA: 7:43 - loss: 1.7963 - regression_loss: 1.3378 - classification_loss: 0.4585\n",
      " 47/500 [=>............................] - ETA: 7:41 - loss: 1.7973 - regression_loss: 1.3380 - classification_loss: 0.4593\n",
      " 48/500 [=>............................] - ETA: 7:40 - loss: 1.8139 - regression_loss: 1.3486 - classification_loss: 0.4654\n",
      " 49/500 [=>............................] - ETA: 7:39 - loss: 1.8115 - regression_loss: 1.3485 - classification_loss: 0.4630\n",
      " 50/500 [==>...........................] - ETA: 7:37 - loss: 1.8061 - regression_loss: 1.3455 - classification_loss: 0.4606\n",
      " 51/500 [==>...........................] - ETA: 7:37 - loss: 1.8086 - regression_loss: 1.3472 - classification_loss: 0.4613\n",
      " 52/500 [==>...........................] - ETA: 7:36 - loss: 1.8202 - regression_loss: 1.3554 - classification_loss: 0.4648\n",
      " 53/500 [==>...........................] - ETA: 7:37 - loss: 1.8260 - regression_loss: 1.3580 - classification_loss: 0.4680\n",
      " 54/500 [==>...........................] - ETA: 7:35 - loss: 1.8374 - regression_loss: 1.3668 - classification_loss: 0.4706\n",
      " 55/500 [==>...........................] - ETA: 7:35 - loss: 1.8293 - regression_loss: 1.3596 - classification_loss: 0.4697\n",
      " 56/500 [==>...........................] - ETA: 7:33 - loss: 1.8323 - regression_loss: 1.3588 - classification_loss: 0.4735\n",
      " 57/500 [==>...........................] - ETA: 7:32 - loss: 1.8484 - regression_loss: 1.3663 - classification_loss: 0.4820\n",
      " 58/500 [==>...........................] - ETA: 7:32 - loss: 1.8574 - regression_loss: 1.3766 - classification_loss: 0.4808\n",
      " 59/500 [==>...........................] - ETA: 7:31 - loss: 1.8613 - regression_loss: 1.3806 - classification_loss: 0.4808\n",
      " 60/500 [==>...........................] - ETA: 7:29 - loss: 1.8561 - regression_loss: 1.3757 - classification_loss: 0.4804\n",
      " 61/500 [==>...........................] - ETA: 7:28 - loss: 1.8673 - regression_loss: 1.3827 - classification_loss: 0.4846\n",
      " 62/500 [==>...........................] - ETA: 7:27 - loss: 1.8684 - regression_loss: 1.3833 - classification_loss: 0.4852\n",
      " 63/500 [==>...........................] - ETA: 7:26 - loss: 1.8769 - regression_loss: 1.3919 - classification_loss: 0.4850\n",
      " 64/500 [==>...........................] - ETA: 7:25 - loss: 1.8701 - regression_loss: 1.3871 - classification_loss: 0.4831\n",
      " 65/500 [==>...........................] - ETA: 7:24 - loss: 1.8598 - regression_loss: 1.3809 - classification_loss: 0.4788\n",
      " 66/500 [==>...........................] - ETA: 7:22 - loss: 1.8532 - regression_loss: 1.3785 - classification_loss: 0.4747\n",
      " 67/500 [===>..........................] - ETA: 7:21 - loss: 1.8657 - regression_loss: 1.3906 - classification_loss: 0.4751\n",
      " 68/500 [===>..........................] - ETA: 7:21 - loss: 1.8612 - regression_loss: 1.3876 - classification_loss: 0.4736\n",
      " 69/500 [===>..........................] - ETA: 7:20 - loss: 1.8585 - regression_loss: 1.3843 - classification_loss: 0.4742\n",
      " 70/500 [===>..........................] - ETA: 7:19 - loss: 1.8554 - regression_loss: 1.3823 - classification_loss: 0.4731\n",
      " 71/500 [===>..........................] - ETA: 7:18 - loss: 1.8533 - regression_loss: 1.3814 - classification_loss: 0.4719\n",
      " 72/500 [===>..........................] - ETA: 7:16 - loss: 1.8472 - regression_loss: 1.3786 - classification_loss: 0.4686\n",
      " 73/500 [===>..........................] - ETA: 7:15 - loss: 1.8580 - regression_loss: 1.3867 - classification_loss: 0.4713\n",
      " 74/500 [===>..........................] - ETA: 7:14 - loss: 1.8579 - regression_loss: 1.3868 - classification_loss: 0.4711\n",
      " 75/500 [===>..........................] - ETA: 7:14 - loss: 1.8543 - regression_loss: 1.3825 - classification_loss: 0.4718\n",
      " 76/500 [===>..........................] - ETA: 7:12 - loss: 1.8592 - regression_loss: 1.3860 - classification_loss: 0.4732\n",
      " 77/500 [===>..........................] - ETA: 7:12 - loss: 1.8559 - regression_loss: 1.3842 - classification_loss: 0.4717\n",
      " 78/500 [===>..........................] - ETA: 7:10 - loss: 1.8670 - regression_loss: 1.3911 - classification_loss: 0.4759\n",
      " 79/500 [===>..........................] - ETA: 7:09 - loss: 1.8664 - regression_loss: 1.3893 - classification_loss: 0.4771\n",
      " 80/500 [===>..........................] - ETA: 7:07 - loss: 1.8650 - regression_loss: 1.3897 - classification_loss: 0.4753\n",
      " 81/500 [===>..........................] - ETA: 7:05 - loss: 1.8741 - regression_loss: 1.3927 - classification_loss: 0.4814\n",
      " 82/500 [===>..........................] - ETA: 7:04 - loss: 1.8749 - regression_loss: 1.3927 - classification_loss: 0.4822\n",
      " 83/500 [===>..........................] - ETA: 7:03 - loss: 1.8665 - regression_loss: 1.3866 - classification_loss: 0.4799\n",
      " 84/500 [====>.........................] - ETA: 7:03 - loss: 1.8730 - regression_loss: 1.3899 - classification_loss: 0.4831\n",
      " 85/500 [====>.........................] - ETA: 7:02 - loss: 1.8732 - regression_loss: 1.3902 - classification_loss: 0.4830\n",
      " 86/500 [====>.........................] - ETA: 7:01 - loss: 1.8772 - regression_loss: 1.3933 - classification_loss: 0.4840\n",
      " 87/500 [====>.........................] - ETA: 7:00 - loss: 1.8727 - regression_loss: 1.3898 - classification_loss: 0.4828\n",
      " 88/500 [====>.........................] - ETA: 6:59 - loss: 1.8730 - regression_loss: 1.3899 - classification_loss: 0.4831\n",
      " 89/500 [====>.........................] - ETA: 6:58 - loss: 1.8630 - regression_loss: 1.3827 - classification_loss: 0.4804\n",
      " 90/500 [====>.........................] - ETA: 6:57 - loss: 1.8582 - regression_loss: 1.3797 - classification_loss: 0.4785\n",
      " 91/500 [====>.........................] - ETA: 6:56 - loss: 1.8634 - regression_loss: 1.3856 - classification_loss: 0.4778\n",
      " 92/500 [====>.........................] - ETA: 6:55 - loss: 1.8629 - regression_loss: 1.3858 - classification_loss: 0.4771\n",
      " 93/500 [====>.........................] - ETA: 6:54 - loss: 1.8718 - regression_loss: 1.3925 - classification_loss: 0.4793\n",
      " 94/500 [====>.........................] - ETA: 6:53 - loss: 1.8704 - regression_loss: 1.3920 - classification_loss: 0.4784\n",
      " 95/500 [====>.........................] - ETA: 6:53 - loss: 1.8732 - regression_loss: 1.3899 - classification_loss: 0.4833\n",
      " 96/500 [====>.........................] - ETA: 6:51 - loss: 1.8694 - regression_loss: 1.3856 - classification_loss: 0.4838\n",
      " 97/500 [====>.........................] - ETA: 6:50 - loss: 1.8678 - regression_loss: 1.3835 - classification_loss: 0.4843\n",
      " 98/500 [====>.........................] - ETA: 6:49 - loss: 1.8705 - regression_loss: 1.3867 - classification_loss: 0.4838\n",
      " 99/500 [====>.........................] - ETA: 6:49 - loss: 1.8681 - regression_loss: 1.3838 - classification_loss: 0.4843\n",
      "100/500 [=====>........................] - ETA: 6:48 - loss: 1.8684 - regression_loss: 1.3854 - classification_loss: 0.4830\n",
      "101/500 [=====>........................] - ETA: 6:47 - loss: 1.8699 - regression_loss: 1.3871 - classification_loss: 0.4828\n",
      "102/500 [=====>........................] - ETA: 6:46 - loss: 1.8749 - regression_loss: 1.3904 - classification_loss: 0.4845\n",
      "103/500 [=====>........................] - ETA: 6:45 - loss: 1.8716 - regression_loss: 1.3882 - classification_loss: 0.4834\n",
      "104/500 [=====>........................] - ETA: 6:44 - loss: 1.8783 - regression_loss: 1.3948 - classification_loss: 0.4835\n",
      "105/500 [=====>........................] - ETA: 6:43 - loss: 1.8856 - regression_loss: 1.3987 - classification_loss: 0.4870\n",
      "106/500 [=====>........................] - ETA: 6:42 - loss: 1.8822 - regression_loss: 1.3970 - classification_loss: 0.4851\n",
      "107/500 [=====>........................] - ETA: 6:41 - loss: 1.8819 - regression_loss: 1.3977 - classification_loss: 0.4842\n",
      "108/500 [=====>........................] - ETA: 6:40 - loss: 1.8755 - regression_loss: 1.3922 - classification_loss: 0.4833\n",
      "109/500 [=====>........................] - ETA: 6:39 - loss: 1.8720 - regression_loss: 1.3905 - classification_loss: 0.4815\n",
      "110/500 [=====>........................] - ETA: 6:38 - loss: 1.8763 - regression_loss: 1.3931 - classification_loss: 0.4832\n",
      "111/500 [=====>........................] - ETA: 6:37 - loss: 1.8789 - regression_loss: 1.3953 - classification_loss: 0.4837\n",
      "112/500 [=====>........................] - ETA: 6:35 - loss: 1.8797 - regression_loss: 1.3969 - classification_loss: 0.4828\n",
      "113/500 [=====>........................] - ETA: 6:35 - loss: 1.8884 - regression_loss: 1.4011 - classification_loss: 0.4873\n",
      "114/500 [=====>........................] - ETA: 6:33 - loss: 1.8819 - regression_loss: 1.3962 - classification_loss: 0.4856\n",
      "115/500 [=====>........................] - ETA: 6:33 - loss: 1.8773 - regression_loss: 1.3933 - classification_loss: 0.4840\n",
      "116/500 [=====>........................] - ETA: 6:32 - loss: 1.8738 - regression_loss: 1.3911 - classification_loss: 0.4827\n",
      "117/500 [======>.......................] - ETA: 6:31 - loss: 1.8814 - regression_loss: 1.3959 - classification_loss: 0.4855\n",
      "118/500 [======>.......................] - ETA: 6:30 - loss: 1.8871 - regression_loss: 1.4017 - classification_loss: 0.4854\n",
      "119/500 [======>.......................] - ETA: 6:29 - loss: 1.8802 - regression_loss: 1.3964 - classification_loss: 0.4838\n",
      "120/500 [======>.......................] - ETA: 6:29 - loss: 1.8840 - regression_loss: 1.3993 - classification_loss: 0.4847\n",
      "121/500 [======>.......................] - ETA: 6:27 - loss: 1.8817 - regression_loss: 1.3976 - classification_loss: 0.4841\n",
      "122/500 [======>.......................] - ETA: 6:26 - loss: 1.8826 - regression_loss: 1.3982 - classification_loss: 0.4844\n",
      "123/500 [======>.......................] - ETA: 6:25 - loss: 1.8787 - regression_loss: 1.3956 - classification_loss: 0.4832\n",
      "124/500 [======>.......................] - ETA: 6:24 - loss: 1.8760 - regression_loss: 1.3941 - classification_loss: 0.4819\n",
      "125/500 [======>.......................] - ETA: 6:23 - loss: 1.8804 - regression_loss: 1.3964 - classification_loss: 0.4840\n",
      "126/500 [======>.......................] - ETA: 6:22 - loss: 1.8779 - regression_loss: 1.3944 - classification_loss: 0.4835\n",
      "127/500 [======>.......................] - ETA: 6:21 - loss: 1.8769 - regression_loss: 1.3936 - classification_loss: 0.4833\n",
      "128/500 [======>.......................] - ETA: 6:20 - loss: 1.8765 - regression_loss: 1.3946 - classification_loss: 0.4820\n",
      "129/500 [======>.......................] - ETA: 6:19 - loss: 1.8753 - regression_loss: 1.3943 - classification_loss: 0.4809\n",
      "130/500 [======>.......................] - ETA: 6:18 - loss: 1.8732 - regression_loss: 1.3929 - classification_loss: 0.4803\n",
      "131/500 [======>.......................] - ETA: 6:17 - loss: 1.8687 - regression_loss: 1.3898 - classification_loss: 0.4789\n",
      "132/500 [======>.......................] - ETA: 6:16 - loss: 1.8659 - regression_loss: 1.3881 - classification_loss: 0.4777\n",
      "133/500 [======>.......................] - ETA: 6:14 - loss: 1.8661 - regression_loss: 1.3874 - classification_loss: 0.4787\n",
      "134/500 [=======>......................] - ETA: 6:14 - loss: 1.8632 - regression_loss: 1.3857 - classification_loss: 0.4775\n",
      "135/500 [=======>......................] - ETA: 6:13 - loss: 1.8695 - regression_loss: 1.3901 - classification_loss: 0.4794\n",
      "136/500 [=======>......................] - ETA: 6:12 - loss: 1.8713 - regression_loss: 1.3911 - classification_loss: 0.4802\n",
      "137/500 [=======>......................] - ETA: 6:11 - loss: 1.8733 - regression_loss: 1.3930 - classification_loss: 0.4803\n",
      "138/500 [=======>......................] - ETA: 6:10 - loss: 1.8771 - regression_loss: 1.3955 - classification_loss: 0.4816\n",
      "139/500 [=======>......................] - ETA: 6:09 - loss: 1.8756 - regression_loss: 1.3942 - classification_loss: 0.4814\n",
      "140/500 [=======>......................] - ETA: 6:08 - loss: 1.8796 - regression_loss: 1.3976 - classification_loss: 0.4821\n",
      "141/500 [=======>......................] - ETA: 6:07 - loss: 1.8802 - regression_loss: 1.3982 - classification_loss: 0.4820\n",
      "142/500 [=======>......................] - ETA: 6:06 - loss: 1.8876 - regression_loss: 1.4039 - classification_loss: 0.4837\n",
      "143/500 [=======>......................] - ETA: 6:05 - loss: 1.8907 - regression_loss: 1.4069 - classification_loss: 0.4838\n",
      "144/500 [=======>......................] - ETA: 6:04 - loss: 1.8871 - regression_loss: 1.4042 - classification_loss: 0.4828\n",
      "145/500 [=======>......................] - ETA: 6:03 - loss: 1.8830 - regression_loss: 1.4014 - classification_loss: 0.4817\n",
      "146/500 [=======>......................] - ETA: 6:02 - loss: 1.8819 - regression_loss: 1.4011 - classification_loss: 0.4807\n",
      "147/500 [=======>......................] - ETA: 6:01 - loss: 1.8774 - regression_loss: 1.3974 - classification_loss: 0.4800\n",
      "148/500 [=======>......................] - ETA: 5:59 - loss: 1.8786 - regression_loss: 1.3987 - classification_loss: 0.4799\n",
      "149/500 [=======>......................] - ETA: 5:59 - loss: 1.8833 - regression_loss: 1.4005 - classification_loss: 0.4828\n",
      "150/500 [========>.....................] - ETA: 5:58 - loss: 1.8801 - regression_loss: 1.3982 - classification_loss: 0.4819\n",
      "151/500 [========>.....................] - ETA: 5:57 - loss: 1.8807 - regression_loss: 1.3995 - classification_loss: 0.4811\n",
      "152/500 [========>.....................] - ETA: 5:56 - loss: 1.8823 - regression_loss: 1.3998 - classification_loss: 0.4825\n",
      "153/500 [========>.....................] - ETA: 5:55 - loss: 1.8898 - regression_loss: 1.4062 - classification_loss: 0.4836\n",
      "154/500 [========>.....................] - ETA: 5:54 - loss: 1.8860 - regression_loss: 1.4035 - classification_loss: 0.4825\n",
      "155/500 [========>.....................] - ETA: 5:53 - loss: 1.8839 - regression_loss: 1.4016 - classification_loss: 0.4823\n",
      "156/500 [========>.....................] - ETA: 5:52 - loss: 1.8828 - regression_loss: 1.4009 - classification_loss: 0.4818\n",
      "157/500 [========>.....................] - ETA: 5:51 - loss: 1.8783 - regression_loss: 1.3974 - classification_loss: 0.4810\n",
      "158/500 [========>.....................] - ETA: 5:50 - loss: 1.8842 - regression_loss: 1.4009 - classification_loss: 0.4833\n",
      "159/500 [========>.....................] - ETA: 5:49 - loss: 1.8803 - regression_loss: 1.3985 - classification_loss: 0.4818\n",
      "160/500 [========>.....................] - ETA: 5:48 - loss: 1.8814 - regression_loss: 1.3989 - classification_loss: 0.4825\n",
      "161/500 [========>.....................] - ETA: 5:47 - loss: 1.8759 - regression_loss: 1.3951 - classification_loss: 0.4808\n",
      "162/500 [========>.....................] - ETA: 5:45 - loss: 1.8743 - regression_loss: 1.3943 - classification_loss: 0.4800\n",
      "163/500 [========>.....................] - ETA: 5:45 - loss: 1.8725 - regression_loss: 1.3933 - classification_loss: 0.4792\n",
      "164/500 [========>.....................] - ETA: 5:44 - loss: 1.8690 - regression_loss: 1.3904 - classification_loss: 0.4786\n",
      "165/500 [========>.....................] - ETA: 5:43 - loss: 1.8695 - regression_loss: 1.3907 - classification_loss: 0.4787\n",
      "166/500 [========>.....................] - ETA: 5:42 - loss: 1.8711 - regression_loss: 1.3921 - classification_loss: 0.4791\n",
      "167/500 [=========>....................] - ETA: 5:41 - loss: 1.8674 - regression_loss: 1.3892 - classification_loss: 0.4782\n",
      "168/500 [=========>....................] - ETA: 5:40 - loss: 1.8691 - regression_loss: 1.3906 - classification_loss: 0.4785\n",
      "169/500 [=========>....................] - ETA: 5:39 - loss: 1.8666 - regression_loss: 1.3893 - classification_loss: 0.4773\n",
      "170/500 [=========>....................] - ETA: 5:38 - loss: 1.8671 - regression_loss: 1.3900 - classification_loss: 0.4771\n",
      "171/500 [=========>....................] - ETA: 5:37 - loss: 1.8663 - regression_loss: 1.3895 - classification_loss: 0.4769\n",
      "172/500 [=========>....................] - ETA: 5:36 - loss: 1.8680 - regression_loss: 1.3893 - classification_loss: 0.4787\n",
      "173/500 [=========>....................] - ETA: 5:34 - loss: 1.8679 - regression_loss: 1.3887 - classification_loss: 0.4791\n",
      "174/500 [=========>....................] - ETA: 5:34 - loss: 1.8631 - regression_loss: 1.3852 - classification_loss: 0.4779\n",
      "175/500 [=========>....................] - ETA: 5:32 - loss: 1.8640 - regression_loss: 1.3855 - classification_loss: 0.4785\n",
      "176/500 [=========>....................] - ETA: 5:31 - loss: 1.8671 - regression_loss: 1.3877 - classification_loss: 0.4794\n",
      "177/500 [=========>....................] - ETA: 5:30 - loss: 1.8670 - regression_loss: 1.3877 - classification_loss: 0.4793\n",
      "178/500 [=========>....................] - ETA: 5:29 - loss: 1.8629 - regression_loss: 1.3841 - classification_loss: 0.4788\n",
      "179/500 [=========>....................] - ETA: 5:28 - loss: 1.8639 - regression_loss: 1.3855 - classification_loss: 0.4785\n",
      "180/500 [=========>....................] - ETA: 5:27 - loss: 1.8723 - regression_loss: 1.3910 - classification_loss: 0.4812\n",
      "181/500 [=========>....................] - ETA: 5:26 - loss: 1.8677 - regression_loss: 1.3871 - classification_loss: 0.4806\n",
      "182/500 [=========>....................] - ETA: 5:25 - loss: 1.8669 - regression_loss: 1.3863 - classification_loss: 0.4806\n",
      "183/500 [=========>....................] - ETA: 5:28 - loss: 1.8628 - regression_loss: 1.3832 - classification_loss: 0.4796\n",
      "184/500 [==========>...................] - ETA: 5:27 - loss: 1.8602 - regression_loss: 1.3813 - classification_loss: 0.4790\n",
      "185/500 [==========>...................] - ETA: 5:26 - loss: 1.8623 - regression_loss: 1.3819 - classification_loss: 0.4804\n",
      "186/500 [==========>...................] - ETA: 5:25 - loss: 1.8603 - regression_loss: 1.3809 - classification_loss: 0.4794\n",
      "187/500 [==========>...................] - ETA: 5:24 - loss: 1.8640 - regression_loss: 1.3854 - classification_loss: 0.4786\n",
      "188/500 [==========>...................] - ETA: 5:23 - loss: 1.8620 - regression_loss: 1.3840 - classification_loss: 0.4780\n",
      "189/500 [==========>...................] - ETA: 5:22 - loss: 1.8641 - regression_loss: 1.3858 - classification_loss: 0.4783\n",
      "190/500 [==========>...................] - ETA: 5:21 - loss: 1.8633 - regression_loss: 1.3847 - classification_loss: 0.4786\n",
      "191/500 [==========>...................] - ETA: 5:20 - loss: 1.8617 - regression_loss: 1.3835 - classification_loss: 0.4782\n",
      "192/500 [==========>...................] - ETA: 5:19 - loss: 1.8622 - regression_loss: 1.3839 - classification_loss: 0.4783\n",
      "193/500 [==========>...................] - ETA: 5:18 - loss: 1.8663 - regression_loss: 1.3870 - classification_loss: 0.4793\n",
      "194/500 [==========>...................] - ETA: 5:17 - loss: 1.8655 - regression_loss: 1.3868 - classification_loss: 0.4788\n",
      "195/500 [==========>...................] - ETA: 5:16 - loss: 1.8643 - regression_loss: 1.3856 - classification_loss: 0.4787\n",
      "196/500 [==========>...................] - ETA: 5:15 - loss: 1.8642 - regression_loss: 1.3855 - classification_loss: 0.4788\n",
      "197/500 [==========>...................] - ETA: 5:14 - loss: 1.8631 - regression_loss: 1.3846 - classification_loss: 0.4785\n",
      "198/500 [==========>...................] - ETA: 5:12 - loss: 1.8604 - regression_loss: 1.3829 - classification_loss: 0.4775\n",
      "199/500 [==========>...................] - ETA: 5:11 - loss: 1.8572 - regression_loss: 1.3803 - classification_loss: 0.4769\n",
      "200/500 [===========>..................] - ETA: 5:10 - loss: 1.8581 - regression_loss: 1.3816 - classification_loss: 0.4765\n",
      "201/500 [===========>..................] - ETA: 5:09 - loss: 1.8597 - regression_loss: 1.3828 - classification_loss: 0.4769\n",
      "202/500 [===========>..................] - ETA: 5:08 - loss: 1.8582 - regression_loss: 1.3817 - classification_loss: 0.4765\n",
      "203/500 [===========>..................] - ETA: 5:07 - loss: 1.8543 - regression_loss: 1.3789 - classification_loss: 0.4754\n",
      "204/500 [===========>..................] - ETA: 5:06 - loss: 1.8573 - regression_loss: 1.3807 - classification_loss: 0.4766\n",
      "205/500 [===========>..................] - ETA: 5:05 - loss: 1.8553 - regression_loss: 1.3795 - classification_loss: 0.4758\n",
      "206/500 [===========>..................] - ETA: 5:04 - loss: 1.8524 - regression_loss: 1.3771 - classification_loss: 0.4753\n",
      "207/500 [===========>..................] - ETA: 5:03 - loss: 1.8518 - regression_loss: 1.3770 - classification_loss: 0.4748\n",
      "208/500 [===========>..................] - ETA: 5:02 - loss: 1.8507 - regression_loss: 1.3761 - classification_loss: 0.4747\n",
      "209/500 [===========>..................] - ETA: 5:01 - loss: 1.8504 - regression_loss: 1.3757 - classification_loss: 0.4747\n",
      "210/500 [===========>..................] - ETA: 5:00 - loss: 1.8496 - regression_loss: 1.3753 - classification_loss: 0.4742\n",
      "211/500 [===========>..................] - ETA: 4:59 - loss: 1.8469 - regression_loss: 1.3738 - classification_loss: 0.4731\n",
      "212/500 [===========>..................] - ETA: 4:58 - loss: 1.8446 - regression_loss: 1.3722 - classification_loss: 0.4725\n",
      "213/500 [===========>..................] - ETA: 4:56 - loss: 1.8450 - regression_loss: 1.3729 - classification_loss: 0.4721\n",
      "214/500 [===========>..................] - ETA: 4:55 - loss: 1.8465 - regression_loss: 1.3739 - classification_loss: 0.4726\n",
      "215/500 [===========>..................] - ETA: 4:54 - loss: 1.8482 - regression_loss: 1.3760 - classification_loss: 0.4722\n",
      "216/500 [===========>..................] - ETA: 4:53 - loss: 1.8516 - regression_loss: 1.3788 - classification_loss: 0.4729\n",
      "217/500 [============>.................] - ETA: 4:52 - loss: 1.8488 - regression_loss: 1.3770 - classification_loss: 0.4718\n",
      "218/500 [============>.................] - ETA: 4:51 - loss: 1.8495 - regression_loss: 1.3778 - classification_loss: 0.4718\n",
      "219/500 [============>.................] - ETA: 4:50 - loss: 1.8483 - regression_loss: 1.3766 - classification_loss: 0.4717\n",
      "220/500 [============>.................] - ETA: 4:49 - loss: 1.8478 - regression_loss: 1.3764 - classification_loss: 0.4714\n",
      "221/500 [============>.................] - ETA: 4:48 - loss: 1.8444 - regression_loss: 1.3737 - classification_loss: 0.4706\n",
      "222/500 [============>.................] - ETA: 4:47 - loss: 1.8446 - regression_loss: 1.3739 - classification_loss: 0.4707\n",
      "223/500 [============>.................] - ETA: 4:46 - loss: 1.8451 - regression_loss: 1.3749 - classification_loss: 0.4703\n",
      "224/500 [============>.................] - ETA: 4:45 - loss: 1.8415 - regression_loss: 1.3725 - classification_loss: 0.4690\n",
      "225/500 [============>.................] - ETA: 4:44 - loss: 1.8412 - regression_loss: 1.3726 - classification_loss: 0.4686\n",
      "226/500 [============>.................] - ETA: 4:43 - loss: 1.8414 - regression_loss: 1.3730 - classification_loss: 0.4684\n",
      "227/500 [============>.................] - ETA: 4:42 - loss: 1.8411 - regression_loss: 1.3731 - classification_loss: 0.4680\n",
      "228/500 [============>.................] - ETA: 4:41 - loss: 1.8430 - regression_loss: 1.3751 - classification_loss: 0.4679\n",
      "229/500 [============>.................] - ETA: 4:40 - loss: 1.8438 - regression_loss: 1.3760 - classification_loss: 0.4678\n",
      "230/500 [============>.................] - ETA: 4:39 - loss: 1.8439 - regression_loss: 1.3760 - classification_loss: 0.4679\n",
      "231/500 [============>.................] - ETA: 4:38 - loss: 1.8441 - regression_loss: 1.3763 - classification_loss: 0.4678\n",
      "232/500 [============>.................] - ETA: 4:37 - loss: 1.8420 - regression_loss: 1.3752 - classification_loss: 0.4668\n",
      "233/500 [============>.................] - ETA: 4:35 - loss: 1.8407 - regression_loss: 1.3742 - classification_loss: 0.4665\n",
      "234/500 [=============>................] - ETA: 4:34 - loss: 1.8391 - regression_loss: 1.3717 - classification_loss: 0.4674\n",
      "235/500 [=============>................] - ETA: 4:33 - loss: 1.8399 - regression_loss: 1.3720 - classification_loss: 0.4678\n",
      "236/500 [=============>................] - ETA: 4:32 - loss: 1.8408 - regression_loss: 1.3731 - classification_loss: 0.4677\n",
      "237/500 [=============>................] - ETA: 4:31 - loss: 1.8419 - regression_loss: 1.3739 - classification_loss: 0.4679\n",
      "238/500 [=============>................] - ETA: 4:30 - loss: 1.8460 - regression_loss: 1.3771 - classification_loss: 0.4690\n",
      "239/500 [=============>................] - ETA: 4:29 - loss: 1.8473 - regression_loss: 1.3784 - classification_loss: 0.4689\n",
      "240/500 [=============>................] - ETA: 4:28 - loss: 1.8475 - regression_loss: 1.3788 - classification_loss: 0.4687\n",
      "241/500 [=============>................] - ETA: 4:27 - loss: 1.8530 - regression_loss: 1.3827 - classification_loss: 0.4703\n",
      "242/500 [=============>................] - ETA: 4:26 - loss: 1.8525 - regression_loss: 1.3823 - classification_loss: 0.4701\n",
      "243/500 [=============>................] - ETA: 4:25 - loss: 1.8558 - regression_loss: 1.3846 - classification_loss: 0.4712\n",
      "244/500 [=============>................] - ETA: 4:24 - loss: 1.8564 - regression_loss: 1.3854 - classification_loss: 0.4710\n",
      "245/500 [=============>................] - ETA: 4:23 - loss: 1.8611 - regression_loss: 1.3885 - classification_loss: 0.4726\n",
      "246/500 [=============>................] - ETA: 4:22 - loss: 1.8596 - regression_loss: 1.3877 - classification_loss: 0.4719\n",
      "247/500 [=============>................] - ETA: 4:21 - loss: 1.8599 - regression_loss: 1.3877 - classification_loss: 0.4722\n",
      "248/500 [=============>................] - ETA: 4:20 - loss: 1.8594 - regression_loss: 1.3874 - classification_loss: 0.4719\n",
      "249/500 [=============>................] - ETA: 4:19 - loss: 1.8572 - regression_loss: 1.3856 - classification_loss: 0.4717\n",
      "250/500 [==============>...............] - ETA: 4:18 - loss: 1.8594 - regression_loss: 1.3871 - classification_loss: 0.4723\n",
      "251/500 [==============>...............] - ETA: 4:17 - loss: 1.8617 - regression_loss: 1.3890 - classification_loss: 0.4727\n",
      "252/500 [==============>...............] - ETA: 4:16 - loss: 1.8637 - regression_loss: 1.3904 - classification_loss: 0.4733\n",
      "253/500 [==============>...............] - ETA: 4:15 - loss: 1.8619 - regression_loss: 1.3891 - classification_loss: 0.4728\n",
      "254/500 [==============>...............] - ETA: 4:14 - loss: 1.8632 - regression_loss: 1.3902 - classification_loss: 0.4730\n",
      "255/500 [==============>...............] - ETA: 4:13 - loss: 1.8650 - regression_loss: 1.3921 - classification_loss: 0.4729\n",
      "256/500 [==============>...............] - ETA: 4:12 - loss: 1.8674 - regression_loss: 1.3941 - classification_loss: 0.4733\n",
      "257/500 [==============>...............] - ETA: 4:11 - loss: 1.8656 - regression_loss: 1.3930 - classification_loss: 0.4727\n",
      "258/500 [==============>...............] - ETA: 4:10 - loss: 1.8655 - regression_loss: 1.3926 - classification_loss: 0.4729\n",
      "259/500 [==============>...............] - ETA: 4:09 - loss: 1.8715 - regression_loss: 1.3947 - classification_loss: 0.4768\n",
      "260/500 [==============>...............] - ETA: 4:08 - loss: 1.8720 - regression_loss: 1.3941 - classification_loss: 0.4779\n",
      "261/500 [==============>...............] - ETA: 4:07 - loss: 1.8740 - regression_loss: 1.3959 - classification_loss: 0.4781\n",
      "262/500 [==============>...............] - ETA: 4:06 - loss: 1.8755 - regression_loss: 1.3972 - classification_loss: 0.4783\n",
      "263/500 [==============>...............] - ETA: 4:05 - loss: 1.8759 - regression_loss: 1.3976 - classification_loss: 0.4783\n",
      "264/500 [==============>...............] - ETA: 4:04 - loss: 1.8789 - regression_loss: 1.4001 - classification_loss: 0.4788\n",
      "265/500 [==============>...............] - ETA: 4:02 - loss: 1.8787 - regression_loss: 1.3996 - classification_loss: 0.4791\n",
      "266/500 [==============>...............] - ETA: 4:01 - loss: 1.8775 - regression_loss: 1.3983 - classification_loss: 0.4792\n",
      "267/500 [===============>..............] - ETA: 4:00 - loss: 1.8777 - regression_loss: 1.3989 - classification_loss: 0.4788\n",
      "268/500 [===============>..............] - ETA: 3:59 - loss: 1.8796 - regression_loss: 1.4003 - classification_loss: 0.4793\n",
      "269/500 [===============>..............] - ETA: 3:58 - loss: 1.8811 - regression_loss: 1.4013 - classification_loss: 0.4798\n",
      "270/500 [===============>..............] - ETA: 3:57 - loss: 1.8836 - regression_loss: 1.4022 - classification_loss: 0.4815\n",
      "271/500 [===============>..............] - ETA: 3:56 - loss: 1.8833 - regression_loss: 1.4009 - classification_loss: 0.4824\n",
      "272/500 [===============>..............] - ETA: 3:55 - loss: 1.8824 - regression_loss: 1.4003 - classification_loss: 0.4821\n",
      "273/500 [===============>..............] - ETA: 3:54 - loss: 1.8827 - regression_loss: 1.4002 - classification_loss: 0.4825\n",
      "274/500 [===============>..............] - ETA: 3:53 - loss: 1.8825 - regression_loss: 1.3999 - classification_loss: 0.4825\n",
      "275/500 [===============>..............] - ETA: 3:52 - loss: 1.8834 - regression_loss: 1.4002 - classification_loss: 0.4832\n",
      "276/500 [===============>..............] - ETA: 3:51 - loss: 1.8842 - regression_loss: 1.4006 - classification_loss: 0.4836\n",
      "277/500 [===============>..............] - ETA: 3:50 - loss: 1.8844 - regression_loss: 1.4009 - classification_loss: 0.4835\n",
      "278/500 [===============>..............] - ETA: 3:49 - loss: 1.8838 - regression_loss: 1.3998 - classification_loss: 0.4840\n",
      "279/500 [===============>..............] - ETA: 3:48 - loss: 1.8821 - regression_loss: 1.3991 - classification_loss: 0.4830\n",
      "280/500 [===============>..............] - ETA: 3:47 - loss: 1.8832 - regression_loss: 1.4001 - classification_loss: 0.4831\n",
      "281/500 [===============>..............] - ETA: 3:46 - loss: 1.8856 - regression_loss: 1.4020 - classification_loss: 0.4836\n",
      "282/500 [===============>..............] - ETA: 3:45 - loss: 1.8857 - regression_loss: 1.4020 - classification_loss: 0.4837\n",
      "283/500 [===============>..............] - ETA: 3:44 - loss: 1.8843 - regression_loss: 1.4009 - classification_loss: 0.4834\n",
      "284/500 [================>.............] - ETA: 3:43 - loss: 1.8822 - regression_loss: 1.3993 - classification_loss: 0.4828\n",
      "285/500 [================>.............] - ETA: 3:42 - loss: 1.8829 - regression_loss: 1.4002 - classification_loss: 0.4826\n",
      "286/500 [================>.............] - ETA: 3:41 - loss: 1.8795 - regression_loss: 1.3978 - classification_loss: 0.4818\n",
      "287/500 [================>.............] - ETA: 3:40 - loss: 1.8808 - regression_loss: 1.3984 - classification_loss: 0.4823\n",
      "288/500 [================>.............] - ETA: 3:39 - loss: 1.8807 - regression_loss: 1.3985 - classification_loss: 0.4822\n",
      "289/500 [================>.............] - ETA: 3:38 - loss: 1.8802 - regression_loss: 1.3979 - classification_loss: 0.4823\n",
      "290/500 [================>.............] - ETA: 3:37 - loss: 1.8788 - regression_loss: 1.3969 - classification_loss: 0.4819\n",
      "291/500 [================>.............] - ETA: 3:36 - loss: 1.8790 - regression_loss: 1.3971 - classification_loss: 0.4818\n",
      "292/500 [================>.............] - ETA: 3:35 - loss: 1.8790 - regression_loss: 1.3970 - classification_loss: 0.4820\n",
      "293/500 [================>.............] - ETA: 3:34 - loss: 1.8797 - regression_loss: 1.3977 - classification_loss: 0.4820\n",
      "294/500 [================>.............] - ETA: 3:33 - loss: 1.8808 - regression_loss: 1.3989 - classification_loss: 0.4820\n",
      "295/500 [================>.............] - ETA: 3:32 - loss: 1.8797 - regression_loss: 1.3980 - classification_loss: 0.4816\n",
      "296/500 [================>.............] - ETA: 3:31 - loss: 1.8842 - regression_loss: 1.4012 - classification_loss: 0.4830\n",
      "297/500 [================>.............] - ETA: 3:30 - loss: 1.8850 - regression_loss: 1.4020 - classification_loss: 0.4831\n",
      "298/500 [================>.............] - ETA: 3:29 - loss: 1.8866 - regression_loss: 1.4025 - classification_loss: 0.4841\n",
      "299/500 [================>.............] - ETA: 3:28 - loss: 1.8908 - regression_loss: 1.4055 - classification_loss: 0.4853\n",
      "300/500 [=================>............] - ETA: 3:26 - loss: 1.8899 - regression_loss: 1.4051 - classification_loss: 0.4848\n",
      "301/500 [=================>............] - ETA: 3:25 - loss: 1.8914 - regression_loss: 1.4060 - classification_loss: 0.4854\n",
      "302/500 [=================>............] - ETA: 3:24 - loss: 1.8930 - regression_loss: 1.4069 - classification_loss: 0.4861\n",
      "303/500 [=================>............] - ETA: 3:23 - loss: 1.8913 - regression_loss: 1.4057 - classification_loss: 0.4856\n",
      "304/500 [=================>............] - ETA: 3:22 - loss: 1.8917 - regression_loss: 1.4056 - classification_loss: 0.4861\n",
      "305/500 [=================>............] - ETA: 3:21 - loss: 1.8925 - regression_loss: 1.4065 - classification_loss: 0.4860\n",
      "306/500 [=================>............] - ETA: 3:20 - loss: 1.8971 - regression_loss: 1.4093 - classification_loss: 0.4878\n",
      "307/500 [=================>............] - ETA: 3:19 - loss: 1.8987 - regression_loss: 1.4102 - classification_loss: 0.4885\n",
      "308/500 [=================>............] - ETA: 3:18 - loss: 1.8994 - regression_loss: 1.4111 - classification_loss: 0.4883\n",
      "309/500 [=================>............] - ETA: 3:17 - loss: 1.8979 - regression_loss: 1.4098 - classification_loss: 0.4881\n",
      "310/500 [=================>............] - ETA: 3:16 - loss: 1.8981 - regression_loss: 1.4101 - classification_loss: 0.4880\n",
      "311/500 [=================>............] - ETA: 3:15 - loss: 1.8982 - regression_loss: 1.4101 - classification_loss: 0.4880\n",
      "312/500 [=================>............] - ETA: 3:14 - loss: 1.8978 - regression_loss: 1.4096 - classification_loss: 0.4882\n",
      "313/500 [=================>............] - ETA: 3:13 - loss: 1.8981 - regression_loss: 1.4102 - classification_loss: 0.4880\n",
      "314/500 [=================>............] - ETA: 3:12 - loss: 1.8982 - regression_loss: 1.4099 - classification_loss: 0.4883\n",
      "315/500 [=================>............] - ETA: 3:11 - loss: 1.8971 - regression_loss: 1.4090 - classification_loss: 0.4881\n",
      "316/500 [=================>............] - ETA: 3:10 - loss: 1.8991 - regression_loss: 1.4105 - classification_loss: 0.4886\n",
      "317/500 [==================>...........] - ETA: 3:09 - loss: 1.8991 - regression_loss: 1.4107 - classification_loss: 0.4884\n",
      "318/500 [==================>...........] - ETA: 3:08 - loss: 1.8970 - regression_loss: 1.4090 - classification_loss: 0.4881\n",
      "319/500 [==================>...........] - ETA: 3:07 - loss: 1.8983 - regression_loss: 1.4102 - classification_loss: 0.4881\n",
      "320/500 [==================>...........] - ETA: 3:06 - loss: 1.9011 - regression_loss: 1.4123 - classification_loss: 0.4887\n",
      "321/500 [==================>...........] - ETA: 3:05 - loss: 1.9007 - regression_loss: 1.4116 - classification_loss: 0.4890\n",
      "322/500 [==================>...........] - ETA: 3:04 - loss: 1.9000 - regression_loss: 1.4111 - classification_loss: 0.4889\n",
      "323/500 [==================>...........] - ETA: 3:03 - loss: 1.9033 - regression_loss: 1.4130 - classification_loss: 0.4904\n",
      "324/500 [==================>...........] - ETA: 3:02 - loss: 1.9020 - regression_loss: 1.4121 - classification_loss: 0.4898\n",
      "325/500 [==================>...........] - ETA: 3:01 - loss: 1.9006 - regression_loss: 1.4114 - classification_loss: 0.4892\n",
      "326/500 [==================>...........] - ETA: 3:00 - loss: 1.9010 - regression_loss: 1.4112 - classification_loss: 0.4898\n",
      "327/500 [==================>...........] - ETA: 2:59 - loss: 1.9018 - regression_loss: 1.4122 - classification_loss: 0.4896\n",
      "328/500 [==================>...........] - ETA: 2:57 - loss: 1.9026 - regression_loss: 1.4125 - classification_loss: 0.4901\n",
      "329/500 [==================>...........] - ETA: 2:56 - loss: 1.9032 - regression_loss: 1.4127 - classification_loss: 0.4905\n",
      "330/500 [==================>...........] - ETA: 2:55 - loss: 1.9038 - regression_loss: 1.4129 - classification_loss: 0.4909\n",
      "331/500 [==================>...........] - ETA: 2:54 - loss: 1.9065 - regression_loss: 1.4145 - classification_loss: 0.4921\n",
      "332/500 [==================>...........] - ETA: 2:53 - loss: 1.9073 - regression_loss: 1.4148 - classification_loss: 0.4926\n",
      "333/500 [==================>...........] - ETA: 2:52 - loss: 1.9074 - regression_loss: 1.4148 - classification_loss: 0.4926\n",
      "334/500 [===================>..........] - ETA: 2:51 - loss: 1.9080 - regression_loss: 1.4151 - classification_loss: 0.4929\n",
      "335/500 [===================>..........] - ETA: 2:50 - loss: 1.9098 - regression_loss: 1.4169 - classification_loss: 0.4930\n",
      "336/500 [===================>..........] - ETA: 2:49 - loss: 1.9082 - regression_loss: 1.4157 - classification_loss: 0.4925\n",
      "337/500 [===================>..........] - ETA: 2:48 - loss: 1.9076 - regression_loss: 1.4153 - classification_loss: 0.4923\n",
      "338/500 [===================>..........] - ETA: 2:47 - loss: 1.9059 - regression_loss: 1.4141 - classification_loss: 0.4917\n",
      "339/500 [===================>..........] - ETA: 2:46 - loss: 1.9069 - regression_loss: 1.4151 - classification_loss: 0.4918\n",
      "340/500 [===================>..........] - ETA: 2:45 - loss: 1.9061 - regression_loss: 1.4148 - classification_loss: 0.4913\n",
      "341/500 [===================>..........] - ETA: 2:44 - loss: 1.9036 - regression_loss: 1.4131 - classification_loss: 0.4905\n",
      "342/500 [===================>..........] - ETA: 2:43 - loss: 1.9029 - regression_loss: 1.4126 - classification_loss: 0.4903\n",
      "343/500 [===================>..........] - ETA: 2:42 - loss: 1.9013 - regression_loss: 1.4115 - classification_loss: 0.4898\n",
      "344/500 [===================>..........] - ETA: 2:41 - loss: 1.8999 - regression_loss: 1.4105 - classification_loss: 0.4894\n",
      "345/500 [===================>..........] - ETA: 2:40 - loss: 1.8991 - regression_loss: 1.4095 - classification_loss: 0.4896\n",
      "346/500 [===================>..........] - ETA: 2:39 - loss: 1.8971 - regression_loss: 1.4081 - classification_loss: 0.4890\n",
      "347/500 [===================>..........] - ETA: 2:38 - loss: 1.8973 - regression_loss: 1.4081 - classification_loss: 0.4892\n",
      "348/500 [===================>..........] - ETA: 2:37 - loss: 1.8961 - regression_loss: 1.4075 - classification_loss: 0.4887\n",
      "349/500 [===================>..........] - ETA: 2:36 - loss: 1.8945 - regression_loss: 1.4063 - classification_loss: 0.4882\n",
      "350/500 [====================>.........] - ETA: 2:35 - loss: 1.8968 - regression_loss: 1.4087 - classification_loss: 0.4881\n",
      "351/500 [====================>.........] - ETA: 2:34 - loss: 1.8989 - regression_loss: 1.4095 - classification_loss: 0.4894\n",
      "352/500 [====================>.........] - ETA: 2:33 - loss: 1.8995 - regression_loss: 1.4102 - classification_loss: 0.4893\n",
      "353/500 [====================>.........] - ETA: 2:31 - loss: 1.8981 - regression_loss: 1.4093 - classification_loss: 0.4888\n",
      "354/500 [====================>.........] - ETA: 2:30 - loss: 1.9000 - regression_loss: 1.4107 - classification_loss: 0.4894\n",
      "355/500 [====================>.........] - ETA: 2:29 - loss: 1.9010 - regression_loss: 1.4117 - classification_loss: 0.4893\n",
      "356/500 [====================>.........] - ETA: 2:28 - loss: 1.8987 - regression_loss: 1.4101 - classification_loss: 0.4886\n",
      "357/500 [====================>.........] - ETA: 2:27 - loss: 1.8999 - regression_loss: 1.4109 - classification_loss: 0.4890\n",
      "358/500 [====================>.........] - ETA: 2:26 - loss: 1.8982 - regression_loss: 1.4094 - classification_loss: 0.4888\n",
      "359/500 [====================>.........] - ETA: 2:25 - loss: 1.8961 - regression_loss: 1.4078 - classification_loss: 0.4882\n",
      "360/500 [====================>.........] - ETA: 2:24 - loss: 1.8937 - regression_loss: 1.4062 - classification_loss: 0.4875\n",
      "361/500 [====================>.........] - ETA: 2:23 - loss: 1.8920 - regression_loss: 1.4047 - classification_loss: 0.4873\n",
      "362/500 [====================>.........] - ETA: 2:22 - loss: 1.8929 - regression_loss: 1.4056 - classification_loss: 0.4873\n",
      "363/500 [====================>.........] - ETA: 2:21 - loss: 1.8925 - regression_loss: 1.4057 - classification_loss: 0.4868\n",
      "364/500 [====================>.........] - ETA: 2:20 - loss: 1.8936 - regression_loss: 1.4069 - classification_loss: 0.4867\n",
      "365/500 [====================>.........] - ETA: 2:19 - loss: 1.8923 - regression_loss: 1.4059 - classification_loss: 0.4865\n",
      "366/500 [====================>.........] - ETA: 2:18 - loss: 1.8941 - regression_loss: 1.4071 - classification_loss: 0.4869\n",
      "367/500 [=====================>........] - ETA: 2:17 - loss: 1.8935 - regression_loss: 1.4070 - classification_loss: 0.4864\n",
      "368/500 [=====================>........] - ETA: 2:16 - loss: 1.8928 - regression_loss: 1.4064 - classification_loss: 0.4864\n",
      "369/500 [=====================>........] - ETA: 2:15 - loss: 1.8915 - regression_loss: 1.4058 - classification_loss: 0.4858\n",
      "370/500 [=====================>........] - ETA: 2:14 - loss: 1.8906 - regression_loss: 1.4050 - classification_loss: 0.4856\n",
      "371/500 [=====================>........] - ETA: 2:13 - loss: 1.8941 - regression_loss: 1.4072 - classification_loss: 0.4869\n",
      "372/500 [=====================>........] - ETA: 2:12 - loss: 1.8936 - regression_loss: 1.4071 - classification_loss: 0.4865\n",
      "373/500 [=====================>........] - ETA: 2:11 - loss: 1.8935 - regression_loss: 1.4072 - classification_loss: 0.4863\n",
      "374/500 [=====================>........] - ETA: 2:10 - loss: 1.8952 - regression_loss: 1.4084 - classification_loss: 0.4868\n",
      "375/500 [=====================>........] - ETA: 2:09 - loss: 1.8972 - regression_loss: 1.4093 - classification_loss: 0.4879\n",
      "376/500 [=====================>........] - ETA: 2:08 - loss: 1.8962 - regression_loss: 1.4085 - classification_loss: 0.4877\n",
      "377/500 [=====================>........] - ETA: 2:06 - loss: 1.8984 - regression_loss: 1.4100 - classification_loss: 0.4884\n",
      "378/500 [=====================>........] - ETA: 2:05 - loss: 1.8969 - regression_loss: 1.4087 - classification_loss: 0.4881\n",
      "379/500 [=====================>........] - ETA: 2:04 - loss: 1.8953 - regression_loss: 1.4075 - classification_loss: 0.4878\n",
      "380/500 [=====================>........] - ETA: 2:03 - loss: 1.8960 - regression_loss: 1.4081 - classification_loss: 0.4878\n",
      "381/500 [=====================>........] - ETA: 2:02 - loss: 1.8949 - regression_loss: 1.4074 - classification_loss: 0.4875\n",
      "382/500 [=====================>........] - ETA: 2:01 - loss: 1.8947 - regression_loss: 1.4077 - classification_loss: 0.4870\n",
      "383/500 [=====================>........] - ETA: 2:00 - loss: 1.8959 - regression_loss: 1.4081 - classification_loss: 0.4878\n",
      "384/500 [======================>.......] - ETA: 1:59 - loss: 1.8932 - regression_loss: 1.4063 - classification_loss: 0.4869\n",
      "385/500 [======================>.......] - ETA: 1:58 - loss: 1.8918 - regression_loss: 1.4055 - classification_loss: 0.4863\n",
      "386/500 [======================>.......] - ETA: 1:57 - loss: 1.8907 - regression_loss: 1.4048 - classification_loss: 0.4859\n",
      "387/500 [======================>.......] - ETA: 1:56 - loss: 1.8929 - regression_loss: 1.4063 - classification_loss: 0.4867\n",
      "388/500 [======================>.......] - ETA: 1:55 - loss: 1.8943 - regression_loss: 1.4073 - classification_loss: 0.4870\n",
      "389/500 [======================>.......] - ETA: 1:54 - loss: 1.8926 - regression_loss: 1.4062 - classification_loss: 0.4864\n",
      "390/500 [======================>.......] - ETA: 1:53 - loss: 1.8922 - regression_loss: 1.4059 - classification_loss: 0.4863\n",
      "391/500 [======================>.......] - ETA: 1:52 - loss: 1.8915 - regression_loss: 1.4056 - classification_loss: 0.4859\n",
      "392/500 [======================>.......] - ETA: 1:51 - loss: 1.8931 - regression_loss: 1.4071 - classification_loss: 0.4860\n",
      "393/500 [======================>.......] - ETA: 1:50 - loss: 1.8941 - regression_loss: 1.4079 - classification_loss: 0.4862\n",
      "394/500 [======================>.......] - ETA: 1:49 - loss: 1.8953 - regression_loss: 1.4087 - classification_loss: 0.4866\n",
      "395/500 [======================>.......] - ETA: 1:48 - loss: 1.8937 - regression_loss: 1.4075 - classification_loss: 0.4862\n",
      "396/500 [======================>.......] - ETA: 1:47 - loss: 1.8950 - regression_loss: 1.4083 - classification_loss: 0.4867\n",
      "397/500 [======================>.......] - ETA: 1:46 - loss: 1.8960 - regression_loss: 1.4093 - classification_loss: 0.4867\n",
      "398/500 [======================>.......] - ETA: 1:45 - loss: 1.8953 - regression_loss: 1.4090 - classification_loss: 0.4863\n",
      "399/500 [======================>.......] - ETA: 1:44 - loss: 1.8944 - regression_loss: 1.4083 - classification_loss: 0.4860\n",
      "400/500 [=======================>......] - ETA: 1:43 - loss: 1.8952 - regression_loss: 1.4092 - classification_loss: 0.4860\n",
      "401/500 [=======================>......] - ETA: 1:42 - loss: 1.8942 - regression_loss: 1.4084 - classification_loss: 0.4857\n",
      "402/500 [=======================>......] - ETA: 1:41 - loss: 1.8926 - regression_loss: 1.4072 - classification_loss: 0.4853\n",
      "403/500 [=======================>......] - ETA: 1:40 - loss: 1.8950 - regression_loss: 1.4091 - classification_loss: 0.4859\n",
      "404/500 [=======================>......] - ETA: 1:39 - loss: 1.8945 - regression_loss: 1.4087 - classification_loss: 0.4858\n",
      "405/500 [=======================>......] - ETA: 1:38 - loss: 1.8939 - regression_loss: 1.4082 - classification_loss: 0.4857\n",
      "406/500 [=======================>......] - ETA: 1:36 - loss: 1.8956 - regression_loss: 1.4093 - classification_loss: 0.4862\n",
      "407/500 [=======================>......] - ETA: 1:35 - loss: 1.8951 - regression_loss: 1.4093 - classification_loss: 0.4858\n",
      "408/500 [=======================>......] - ETA: 1:34 - loss: 1.8972 - regression_loss: 1.4101 - classification_loss: 0.4871\n",
      "409/500 [=======================>......] - ETA: 1:33 - loss: 1.8964 - regression_loss: 1.4096 - classification_loss: 0.4867\n",
      "410/500 [=======================>......] - ETA: 1:32 - loss: 1.8961 - regression_loss: 1.4092 - classification_loss: 0.4868\n",
      "411/500 [=======================>......] - ETA: 1:31 - loss: 1.8956 - regression_loss: 1.4087 - classification_loss: 0.4868\n",
      "412/500 [=======================>......] - ETA: 1:30 - loss: 1.8956 - regression_loss: 1.4091 - classification_loss: 0.4865\n",
      "413/500 [=======================>......] - ETA: 1:29 - loss: 1.8952 - regression_loss: 1.4088 - classification_loss: 0.4864\n",
      "414/500 [=======================>......] - ETA: 1:28 - loss: 1.8967 - regression_loss: 1.4097 - classification_loss: 0.4870\n",
      "415/500 [=======================>......] - ETA: 1:27 - loss: 1.8951 - regression_loss: 1.4086 - classification_loss: 0.4866\n",
      "416/500 [=======================>......] - ETA: 1:26 - loss: 1.8947 - regression_loss: 1.4084 - classification_loss: 0.4863\n",
      "417/500 [========================>.....] - ETA: 1:25 - loss: 1.8937 - regression_loss: 1.4075 - classification_loss: 0.4862\n",
      "418/500 [========================>.....] - ETA: 1:24 - loss: 1.8931 - regression_loss: 1.4073 - classification_loss: 0.4858\n",
      "419/500 [========================>.....] - ETA: 1:23 - loss: 1.8953 - regression_loss: 1.4091 - classification_loss: 0.4863\n",
      "420/500 [========================>.....] - ETA: 1:22 - loss: 1.8938 - regression_loss: 1.4079 - classification_loss: 0.4859\n",
      "421/500 [========================>.....] - ETA: 1:21 - loss: 1.8937 - regression_loss: 1.4075 - classification_loss: 0.4862\n",
      "422/500 [========================>.....] - ETA: 1:20 - loss: 1.8961 - regression_loss: 1.4095 - classification_loss: 0.4866\n",
      "423/500 [========================>.....] - ETA: 1:19 - loss: 1.8970 - regression_loss: 1.4104 - classification_loss: 0.4866\n",
      "424/500 [========================>.....] - ETA: 1:18 - loss: 1.8974 - regression_loss: 1.4107 - classification_loss: 0.4867\n",
      "425/500 [========================>.....] - ETA: 1:17 - loss: 1.8985 - regression_loss: 1.4115 - classification_loss: 0.4870\n",
      "426/500 [========================>.....] - ETA: 1:16 - loss: 1.8984 - regression_loss: 1.4111 - classification_loss: 0.4873\n",
      "427/500 [========================>.....] - ETA: 1:15 - loss: 1.8987 - regression_loss: 1.4115 - classification_loss: 0.4872\n",
      "428/500 [========================>.....] - ETA: 1:14 - loss: 1.8972 - regression_loss: 1.4103 - classification_loss: 0.4868\n",
      "429/500 [========================>.....] - ETA: 1:13 - loss: 1.8966 - regression_loss: 1.4098 - classification_loss: 0.4867\n",
      "430/500 [========================>.....] - ETA: 1:12 - loss: 1.8956 - regression_loss: 1.4089 - classification_loss: 0.4867\n",
      "431/500 [========================>.....] - ETA: 1:11 - loss: 1.8959 - regression_loss: 1.4091 - classification_loss: 0.4869\n",
      "432/500 [========================>.....] - ETA: 1:10 - loss: 1.8942 - regression_loss: 1.4078 - classification_loss: 0.4863\n",
      "433/500 [========================>.....] - ETA: 1:09 - loss: 1.8945 - regression_loss: 1.4081 - classification_loss: 0.4864\n",
      "434/500 [=========================>....] - ETA: 1:08 - loss: 1.8975 - regression_loss: 1.4101 - classification_loss: 0.4874\n",
      "435/500 [=========================>....] - ETA: 1:07 - loss: 1.8984 - regression_loss: 1.4107 - classification_loss: 0.4876\n",
      "436/500 [=========================>....] - ETA: 1:06 - loss: 1.8998 - regression_loss: 1.4116 - classification_loss: 0.4881\n",
      "437/500 [=========================>....] - ETA: 1:05 - loss: 1.9017 - regression_loss: 1.4130 - classification_loss: 0.4887\n",
      "438/500 [=========================>....] - ETA: 1:04 - loss: 1.9002 - regression_loss: 1.4121 - classification_loss: 0.4881\n",
      "439/500 [=========================>....] - ETA: 1:03 - loss: 1.9011 - regression_loss: 1.4128 - classification_loss: 0.4883\n",
      "440/500 [=========================>....] - ETA: 1:01 - loss: 1.9010 - regression_loss: 1.4124 - classification_loss: 0.4886\n",
      "441/500 [=========================>....] - ETA: 1:00 - loss: 1.9019 - regression_loss: 1.4130 - classification_loss: 0.4889\n",
      "442/500 [=========================>....] - ETA: 59s - loss: 1.9018 - regression_loss: 1.4132 - classification_loss: 0.4886 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443/500 [=========================>....] - ETA: 58s - loss: 1.8997 - regression_loss: 1.4116 - classification_loss: 0.4881\n",
      "444/500 [=========================>....] - ETA: 57s - loss: 1.9024 - regression_loss: 1.4140 - classification_loss: 0.4884\n",
      "445/500 [=========================>....] - ETA: 56s - loss: 1.9047 - regression_loss: 1.4157 - classification_loss: 0.4890\n",
      "446/500 [=========================>....] - ETA: 55s - loss: 1.9051 - regression_loss: 1.4159 - classification_loss: 0.4892\n",
      "447/500 [=========================>....] - ETA: 54s - loss: 1.9037 - regression_loss: 1.4150 - classification_loss: 0.4887\n",
      "448/500 [=========================>....] - ETA: 53s - loss: 1.9064 - regression_loss: 1.4167 - classification_loss: 0.4898\n",
      "449/500 [=========================>....] - ETA: 52s - loss: 1.9069 - regression_loss: 1.4171 - classification_loss: 0.4898\n",
      "450/500 [==========================>...] - ETA: 51s - loss: 1.9066 - regression_loss: 1.4167 - classification_loss: 0.4898\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 1.9072 - regression_loss: 1.4173 - classification_loss: 0.4899\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 1.9096 - regression_loss: 1.4190 - classification_loss: 0.4906\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.9098 - regression_loss: 1.4191 - classification_loss: 0.4907\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.9101 - regression_loss: 1.4194 - classification_loss: 0.4907\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.9110 - regression_loss: 1.4199 - classification_loss: 0.4911\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.9097 - regression_loss: 1.4190 - classification_loss: 0.4907\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.9083 - regression_loss: 1.4180 - classification_loss: 0.4904\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.9083 - regression_loss: 1.4181 - classification_loss: 0.4901\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.9093 - regression_loss: 1.4191 - classification_loss: 0.4902\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.9101 - regression_loss: 1.4197 - classification_loss: 0.4904\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 1.9116 - regression_loss: 1.4208 - classification_loss: 0.4908\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 1.9125 - regression_loss: 1.4214 - classification_loss: 0.4911\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 1.9129 - regression_loss: 1.4217 - classification_loss: 0.4912\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 1.9134 - regression_loss: 1.4218 - classification_loss: 0.4916\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 1.9136 - regression_loss: 1.4223 - classification_loss: 0.4913\n",
      "466/500 [==========================>...] - ETA: 35s - loss: 1.9147 - regression_loss: 1.4236 - classification_loss: 0.4911\n",
      "467/500 [===========================>..] - ETA: 34s - loss: 1.9149 - regression_loss: 1.4239 - classification_loss: 0.4910\n",
      "468/500 [===========================>..] - ETA: 33s - loss: 1.9147 - regression_loss: 1.4238 - classification_loss: 0.4910\n",
      "469/500 [===========================>..] - ETA: 32s - loss: 1.9153 - regression_loss: 1.4241 - classification_loss: 0.4912\n",
      "470/500 [===========================>..] - ETA: 31s - loss: 1.9145 - regression_loss: 1.4237 - classification_loss: 0.4908\n",
      "471/500 [===========================>..] - ETA: 30s - loss: 1.9136 - regression_loss: 1.4231 - classification_loss: 0.4906\n",
      "472/500 [===========================>..] - ETA: 28s - loss: 1.9138 - regression_loss: 1.4229 - classification_loss: 0.4909\n",
      "473/500 [===========================>..] - ETA: 27s - loss: 1.9133 - regression_loss: 1.4226 - classification_loss: 0.4907\n",
      "474/500 [===========================>..] - ETA: 26s - loss: 1.9146 - regression_loss: 1.4234 - classification_loss: 0.4912\n",
      "475/500 [===========================>..] - ETA: 25s - loss: 1.9122 - regression_loss: 1.4215 - classification_loss: 0.4907\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 1.9114 - regression_loss: 1.4210 - classification_loss: 0.4904\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.9097 - regression_loss: 1.4195 - classification_loss: 0.4902\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.9107 - regression_loss: 1.4198 - classification_loss: 0.4908\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.9101 - regression_loss: 1.4192 - classification_loss: 0.4908\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.9111 - regression_loss: 1.4199 - classification_loss: 0.4912\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.9118 - regression_loss: 1.4205 - classification_loss: 0.4913\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.9127 - regression_loss: 1.4215 - classification_loss: 0.4912\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.9124 - regression_loss: 1.4212 - classification_loss: 0.4911\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.9131 - regression_loss: 1.4219 - classification_loss: 0.4912\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.9126 - regression_loss: 1.4216 - classification_loss: 0.4910\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.9137 - regression_loss: 1.4223 - classification_loss: 0.4914\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.9142 - regression_loss: 1.4230 - classification_loss: 0.4912\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.9139 - regression_loss: 1.4227 - classification_loss: 0.4912\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.9151 - regression_loss: 1.4236 - classification_loss: 0.4916\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.9150 - regression_loss: 1.4237 - classification_loss: 0.4913\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.9146 - regression_loss: 1.4236 - classification_loss: 0.4910 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.9146 - regression_loss: 1.4236 - classification_loss: 0.4910\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.9151 - regression_loss: 1.4237 - classification_loss: 0.4915\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.9170 - regression_loss: 1.4246 - classification_loss: 0.4924\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.9188 - regression_loss: 1.4261 - classification_loss: 0.4927\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.9187 - regression_loss: 1.4262 - classification_loss: 0.4925\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.9206 - regression_loss: 1.4274 - classification_loss: 0.4931\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.9205 - regression_loss: 1.4276 - classification_loss: 0.4929\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.9199 - regression_loss: 1.4270 - classification_loss: 0.4929\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9206 - regression_loss: 1.4276 - classification_loss: 0.4930\n",
      "Epoch 00019: saving model to ./snapshots\\resnet50_csv_19.h5\n",
      "\n",
      "500/500 [==============================] - 518s 1s/step - loss: 1.9206 - regression_loss: 1.4276 - classification_loss: 0.4930\n",
      "Epoch 20/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 2.3197 - regression_loss: 1.7380 - classification_loss: 0.5817\n",
      "  2/500 [..............................] - ETA: 4:11 - loss: 1.7455 - regression_loss: 1.3442 - classification_loss: 0.4013\n",
      "  3/500 [..............................] - ETA: 12:05 - loss: 1.7538 - regression_loss: 1.3455 - classification_loss: 0.4083\n",
      "  4/500 [..............................] - ETA: 11:16 - loss: 1.6892 - regression_loss: 1.2598 - classification_loss: 0.4294\n",
      "  5/500 [..............................] - ETA: 10:47 - loss: 1.7181 - regression_loss: 1.2771 - classification_loss: 0.4410\n",
      "  6/500 [..............................] - ETA: 10:27 - loss: 1.7223 - regression_loss: 1.2753 - classification_loss: 0.4471\n",
      "  7/500 [..............................] - ETA: 10:12 - loss: 1.8327 - regression_loss: 1.3710 - classification_loss: 0.4617\n",
      "  8/500 [..............................] - ETA: 10:02 - loss: 1.7854 - regression_loss: 1.3513 - classification_loss: 0.4342\n",
      "  9/500 [..............................] - ETA: 9:48 - loss: 1.8474 - regression_loss: 1.3886 - classification_loss: 0.4588 \n",
      " 10/500 [..............................] - ETA: 9:36 - loss: 1.8392 - regression_loss: 1.3932 - classification_loss: 0.4460\n",
      " 11/500 [..............................] - ETA: 9:30 - loss: 1.8425 - regression_loss: 1.3980 - classification_loss: 0.4446\n",
      " 12/500 [..............................] - ETA: 9:26 - loss: 1.9080 - regression_loss: 1.4551 - classification_loss: 0.4529\n",
      " 13/500 [..............................] - ETA: 9:22 - loss: 1.9096 - regression_loss: 1.4615 - classification_loss: 0.4481\n",
      " 14/500 [..............................] - ETA: 9:15 - loss: 1.8633 - regression_loss: 1.4267 - classification_loss: 0.4367\n",
      " 15/500 [..............................] - ETA: 9:12 - loss: 1.8871 - regression_loss: 1.4456 - classification_loss: 0.4416\n",
      " 16/500 [..............................] - ETA: 9:09 - loss: 1.8607 - regression_loss: 1.4271 - classification_loss: 0.4336\n",
      " 17/500 [>.............................] - ETA: 9:01 - loss: 1.8593 - regression_loss: 1.4230 - classification_loss: 0.4363\n",
      " 18/500 [>.............................] - ETA: 8:58 - loss: 1.8128 - regression_loss: 1.3859 - classification_loss: 0.4269\n",
      " 19/500 [>.............................] - ETA: 8:53 - loss: 1.8297 - regression_loss: 1.4034 - classification_loss: 0.4263\n",
      " 20/500 [>.............................] - ETA: 8:51 - loss: 1.7926 - regression_loss: 1.3759 - classification_loss: 0.4167\n",
      " 21/500 [>.............................] - ETA: 8:50 - loss: 1.7757 - regression_loss: 1.3661 - classification_loss: 0.4096\n",
      " 22/500 [>.............................] - ETA: 8:46 - loss: 1.7513 - regression_loss: 1.3497 - classification_loss: 0.4015\n",
      " 23/500 [>.............................] - ETA: 8:45 - loss: 1.7786 - regression_loss: 1.3616 - classification_loss: 0.4170\n",
      " 24/500 [>.............................] - ETA: 8:42 - loss: 1.7622 - regression_loss: 1.3487 - classification_loss: 0.4134\n",
      " 25/500 [>.............................] - ETA: 8:40 - loss: 1.7502 - regression_loss: 1.3396 - classification_loss: 0.4106\n",
      " 26/500 [>.............................] - ETA: 8:37 - loss: 1.7573 - regression_loss: 1.3390 - classification_loss: 0.4183\n",
      " 27/500 [>.............................] - ETA: 8:36 - loss: 1.7651 - regression_loss: 1.3451 - classification_loss: 0.4200\n",
      " 28/500 [>.............................] - ETA: 8:35 - loss: 1.7540 - regression_loss: 1.3276 - classification_loss: 0.4264\n",
      " 29/500 [>.............................] - ETA: 8:34 - loss: 1.7506 - regression_loss: 1.3217 - classification_loss: 0.4288\n",
      " 30/500 [>.............................] - ETA: 8:31 - loss: 1.7539 - regression_loss: 1.3237 - classification_loss: 0.4302\n",
      " 31/500 [>.............................] - ETA: 8:30 - loss: 1.7396 - regression_loss: 1.3134 - classification_loss: 0.4262\n",
      " 32/500 [>.............................] - ETA: 8:29 - loss: 1.7360 - regression_loss: 1.3101 - classification_loss: 0.4259\n",
      " 33/500 [>.............................] - ETA: 8:28 - loss: 1.7360 - regression_loss: 1.3102 - classification_loss: 0.4257\n",
      " 34/500 [=>............................] - ETA: 8:25 - loss: 1.7266 - regression_loss: 1.3063 - classification_loss: 0.4203\n",
      " 35/500 [=>............................] - ETA: 8:23 - loss: 1.7437 - regression_loss: 1.3183 - classification_loss: 0.4254\n",
      " 36/500 [=>............................] - ETA: 8:22 - loss: 1.7399 - regression_loss: 1.3152 - classification_loss: 0.4247\n",
      " 37/500 [=>............................] - ETA: 8:21 - loss: 1.7316 - regression_loss: 1.3063 - classification_loss: 0.4252\n",
      " 38/500 [=>............................] - ETA: 8:18 - loss: 1.7320 - regression_loss: 1.3057 - classification_loss: 0.4263\n",
      " 39/500 [=>............................] - ETA: 8:15 - loss: 1.7284 - regression_loss: 1.3030 - classification_loss: 0.4255\n",
      " 40/500 [=>............................] - ETA: 8:15 - loss: 1.7779 - regression_loss: 1.3326 - classification_loss: 0.4453\n",
      " 41/500 [=>............................] - ETA: 8:13 - loss: 1.7844 - regression_loss: 1.3346 - classification_loss: 0.4498\n",
      " 42/500 [=>............................] - ETA: 8:12 - loss: 1.7804 - regression_loss: 1.3304 - classification_loss: 0.4500\n",
      " 43/500 [=>............................] - ETA: 8:11 - loss: 1.8133 - regression_loss: 1.3395 - classification_loss: 0.4737\n",
      " 44/500 [=>............................] - ETA: 8:07 - loss: 1.8061 - regression_loss: 1.3354 - classification_loss: 0.4707\n",
      " 45/500 [=>............................] - ETA: 8:07 - loss: 1.8214 - regression_loss: 1.3474 - classification_loss: 0.4740\n",
      " 46/500 [=>............................] - ETA: 8:05 - loss: 1.8008 - regression_loss: 1.3320 - classification_loss: 0.4688\n",
      " 47/500 [=>............................] - ETA: 8:03 - loss: 1.7929 - regression_loss: 1.3284 - classification_loss: 0.4645\n",
      " 48/500 [=>............................] - ETA: 8:02 - loss: 1.8180 - regression_loss: 1.3428 - classification_loss: 0.4753\n",
      " 49/500 [=>............................] - ETA: 8:01 - loss: 1.8476 - regression_loss: 1.3644 - classification_loss: 0.4832\n",
      " 50/500 [==>...........................] - ETA: 7:59 - loss: 1.8609 - regression_loss: 1.3733 - classification_loss: 0.4875\n",
      " 51/500 [==>...........................] - ETA: 7:57 - loss: 1.8750 - regression_loss: 1.3817 - classification_loss: 0.4933\n",
      " 52/500 [==>...........................] - ETA: 7:56 - loss: 1.8862 - regression_loss: 1.3935 - classification_loss: 0.4927\n",
      " 53/500 [==>...........................] - ETA: 7:54 - loss: 1.8893 - regression_loss: 1.3934 - classification_loss: 0.4959\n",
      " 54/500 [==>...........................] - ETA: 7:52 - loss: 1.8912 - regression_loss: 1.3956 - classification_loss: 0.4957\n",
      " 55/500 [==>...........................] - ETA: 7:51 - loss: 1.8915 - regression_loss: 1.3962 - classification_loss: 0.4953\n",
      " 56/500 [==>...........................] - ETA: 7:50 - loss: 1.8976 - regression_loss: 1.4003 - classification_loss: 0.4972\n",
      " 57/500 [==>...........................] - ETA: 7:49 - loss: 1.9037 - regression_loss: 1.4057 - classification_loss: 0.4981\n",
      " 58/500 [==>...........................] - ETA: 7:47 - loss: 1.8992 - regression_loss: 1.4032 - classification_loss: 0.4960\n",
      " 59/500 [==>...........................] - ETA: 7:46 - loss: 1.8911 - regression_loss: 1.3967 - classification_loss: 0.4944\n",
      " 60/500 [==>...........................] - ETA: 7:44 - loss: 1.8826 - regression_loss: 1.3903 - classification_loss: 0.4922\n",
      " 61/500 [==>...........................] - ETA: 7:43 - loss: 1.8924 - regression_loss: 1.3968 - classification_loss: 0.4956\n",
      " 62/500 [==>...........................] - ETA: 7:41 - loss: 1.8867 - regression_loss: 1.3942 - classification_loss: 0.4925\n",
      " 63/500 [==>...........................] - ETA: 7:41 - loss: 1.8916 - regression_loss: 1.3959 - classification_loss: 0.4957\n",
      " 64/500 [==>...........................] - ETA: 7:40 - loss: 1.8929 - regression_loss: 1.3968 - classification_loss: 0.4961\n",
      " 65/500 [==>...........................] - ETA: 7:39 - loss: 1.9019 - regression_loss: 1.4047 - classification_loss: 0.4972\n",
      " 66/500 [==>...........................] - ETA: 7:38 - loss: 1.9019 - regression_loss: 1.4053 - classification_loss: 0.4967\n",
      " 67/500 [===>..........................] - ETA: 7:36 - loss: 1.8964 - regression_loss: 1.3986 - classification_loss: 0.4977\n",
      " 68/500 [===>..........................] - ETA: 7:35 - loss: 1.8894 - regression_loss: 1.3950 - classification_loss: 0.4944\n",
      " 69/500 [===>..........................] - ETA: 7:33 - loss: 1.8917 - regression_loss: 1.3949 - classification_loss: 0.4968\n",
      " 70/500 [===>..........................] - ETA: 7:31 - loss: 1.8943 - regression_loss: 1.3968 - classification_loss: 0.4975\n",
      " 71/500 [===>..........................] - ETA: 7:30 - loss: 1.8913 - regression_loss: 1.3937 - classification_loss: 0.4976\n",
      " 72/500 [===>..........................] - ETA: 7:29 - loss: 1.8787 - regression_loss: 1.3842 - classification_loss: 0.4945\n",
      " 73/500 [===>..........................] - ETA: 7:28 - loss: 1.8806 - regression_loss: 1.3875 - classification_loss: 0.4932\n",
      " 74/500 [===>..........................] - ETA: 7:27 - loss: 1.8731 - regression_loss: 1.3821 - classification_loss: 0.4910\n",
      " 75/500 [===>..........................] - ETA: 7:26 - loss: 1.8756 - regression_loss: 1.3842 - classification_loss: 0.4914\n",
      " 76/500 [===>..........................] - ETA: 7:24 - loss: 1.8834 - regression_loss: 1.3906 - classification_loss: 0.4928\n",
      " 77/500 [===>..........................] - ETA: 7:23 - loss: 1.8902 - regression_loss: 1.3980 - classification_loss: 0.4923\n",
      " 78/500 [===>..........................] - ETA: 7:21 - loss: 1.8934 - regression_loss: 1.3994 - classification_loss: 0.4940\n",
      " 79/500 [===>..........................] - ETA: 7:20 - loss: 1.9034 - regression_loss: 1.4051 - classification_loss: 0.4984\n",
      " 80/500 [===>..........................] - ETA: 7:17 - loss: 1.9024 - regression_loss: 1.4046 - classification_loss: 0.4978\n",
      " 81/500 [===>..........................] - ETA: 7:17 - loss: 1.9016 - regression_loss: 1.4032 - classification_loss: 0.4984\n",
      " 82/500 [===>..........................] - ETA: 7:16 - loss: 1.9124 - regression_loss: 1.4125 - classification_loss: 0.4999\n",
      " 83/500 [===>..........................] - ETA: 7:15 - loss: 1.9115 - regression_loss: 1.4131 - classification_loss: 0.4984\n",
      " 84/500 [====>.........................] - ETA: 7:14 - loss: 1.9125 - regression_loss: 1.4160 - classification_loss: 0.4965\n",
      " 85/500 [====>.........................] - ETA: 7:14 - loss: 1.9191 - regression_loss: 1.4211 - classification_loss: 0.4980\n",
      " 86/500 [====>.........................] - ETA: 7:13 - loss: 1.9232 - regression_loss: 1.4227 - classification_loss: 0.5005\n",
      " 87/500 [====>.........................] - ETA: 7:12 - loss: 1.9182 - regression_loss: 1.4197 - classification_loss: 0.4984\n",
      " 88/500 [====>.........................] - ETA: 7:11 - loss: 1.9328 - regression_loss: 1.4295 - classification_loss: 0.5034\n",
      " 89/500 [====>.........................] - ETA: 7:10 - loss: 1.9300 - regression_loss: 1.4274 - classification_loss: 0.5026\n",
      " 90/500 [====>.........................] - ETA: 7:09 - loss: 1.9364 - regression_loss: 1.4288 - classification_loss: 0.5076\n",
      " 91/500 [====>.........................] - ETA: 7:08 - loss: 1.9391 - regression_loss: 1.4318 - classification_loss: 0.5073\n",
      " 92/500 [====>.........................] - ETA: 7:07 - loss: 1.9403 - regression_loss: 1.4334 - classification_loss: 0.5069\n",
      " 93/500 [====>.........................] - ETA: 7:05 - loss: 1.9367 - regression_loss: 1.4303 - classification_loss: 0.5063\n",
      " 94/500 [====>.........................] - ETA: 7:05 - loss: 1.9303 - regression_loss: 1.4253 - classification_loss: 0.5049\n",
      " 95/500 [====>.........................] - ETA: 7:04 - loss: 1.9387 - regression_loss: 1.4332 - classification_loss: 0.5054\n",
      " 96/500 [====>.........................] - ETA: 7:03 - loss: 1.9420 - regression_loss: 1.4351 - classification_loss: 0.5069\n",
      " 97/500 [====>.........................] - ETA: 7:02 - loss: 1.9523 - regression_loss: 1.4432 - classification_loss: 0.5091\n",
      " 98/500 [====>.........................] - ETA: 7:00 - loss: 1.9485 - regression_loss: 1.4401 - classification_loss: 0.5084\n",
      " 99/500 [====>.........................] - ETA: 6:59 - loss: 1.9504 - regression_loss: 1.4429 - classification_loss: 0.5074\n",
      "100/500 [=====>........................] - ETA: 6:58 - loss: 1.9560 - regression_loss: 1.4480 - classification_loss: 0.5080\n",
      "101/500 [=====>........................] - ETA: 6:57 - loss: 1.9507 - regression_loss: 1.4443 - classification_loss: 0.5064\n",
      "102/500 [=====>........................] - ETA: 6:56 - loss: 1.9599 - regression_loss: 1.4493 - classification_loss: 0.5106\n",
      "103/500 [=====>........................] - ETA: 6:55 - loss: 1.9600 - regression_loss: 1.4488 - classification_loss: 0.5112\n",
      "104/500 [=====>........................] - ETA: 6:54 - loss: 1.9543 - regression_loss: 1.4458 - classification_loss: 0.5084\n",
      "105/500 [=====>........................] - ETA: 6:52 - loss: 1.9513 - regression_loss: 1.4444 - classification_loss: 0.5069\n",
      "106/500 [=====>........................] - ETA: 6:52 - loss: 1.9503 - regression_loss: 1.4442 - classification_loss: 0.5060\n",
      "107/500 [=====>........................] - ETA: 6:50 - loss: 1.9499 - regression_loss: 1.4437 - classification_loss: 0.5062\n",
      "108/500 [=====>........................] - ETA: 6:49 - loss: 1.9471 - regression_loss: 1.4424 - classification_loss: 0.5047\n",
      "109/500 [=====>........................] - ETA: 6:47 - loss: 1.9503 - regression_loss: 1.4449 - classification_loss: 0.5054\n",
      "110/500 [=====>........................] - ETA: 6:47 - loss: 1.9428 - regression_loss: 1.4401 - classification_loss: 0.5027\n",
      "111/500 [=====>........................] - ETA: 6:45 - loss: 1.9536 - regression_loss: 1.4497 - classification_loss: 0.5039\n",
      "112/500 [=====>........................] - ETA: 6:44 - loss: 1.9547 - regression_loss: 1.4511 - classification_loss: 0.5036\n",
      "113/500 [=====>........................] - ETA: 6:43 - loss: 1.9516 - regression_loss: 1.4485 - classification_loss: 0.5032\n",
      "114/500 [=====>........................] - ETA: 6:42 - loss: 1.9450 - regression_loss: 1.4447 - classification_loss: 0.5003\n",
      "115/500 [=====>........................] - ETA: 6:40 - loss: 1.9456 - regression_loss: 1.4452 - classification_loss: 0.5004\n",
      "116/500 [=====>........................] - ETA: 6:39 - loss: 1.9392 - regression_loss: 1.4413 - classification_loss: 0.4979\n",
      "117/500 [======>.......................] - ETA: 6:38 - loss: 1.9409 - regression_loss: 1.4421 - classification_loss: 0.4988\n",
      "118/500 [======>.......................] - ETA: 6:37 - loss: 1.9391 - regression_loss: 1.4415 - classification_loss: 0.4976\n",
      "119/500 [======>.......................] - ETA: 6:36 - loss: 1.9459 - regression_loss: 1.4465 - classification_loss: 0.4994\n",
      "120/500 [======>.......................] - ETA: 6:35 - loss: 1.9492 - regression_loss: 1.4493 - classification_loss: 0.4999\n",
      "121/500 [======>.......................] - ETA: 6:34 - loss: 1.9478 - regression_loss: 1.4490 - classification_loss: 0.4988\n",
      "122/500 [======>.......................] - ETA: 6:33 - loss: 1.9543 - regression_loss: 1.4548 - classification_loss: 0.4994\n",
      "123/500 [======>.......................] - ETA: 6:31 - loss: 1.9529 - regression_loss: 1.4540 - classification_loss: 0.4989\n",
      "124/500 [======>.......................] - ETA: 6:30 - loss: 1.9532 - regression_loss: 1.4547 - classification_loss: 0.4985\n",
      "125/500 [======>.......................] - ETA: 6:29 - loss: 1.9516 - regression_loss: 1.4529 - classification_loss: 0.4987\n",
      "126/500 [======>.......................] - ETA: 6:28 - loss: 1.9532 - regression_loss: 1.4534 - classification_loss: 0.4998\n",
      "127/500 [======>.......................] - ETA: 6:27 - loss: 1.9536 - regression_loss: 1.4545 - classification_loss: 0.4991\n",
      "128/500 [======>.......................] - ETA: 6:26 - loss: 1.9501 - regression_loss: 1.4522 - classification_loss: 0.4979\n",
      "129/500 [======>.......................] - ETA: 6:25 - loss: 1.9461 - regression_loss: 1.4499 - classification_loss: 0.4962\n",
      "130/500 [======>.......................] - ETA: 6:23 - loss: 1.9457 - regression_loss: 1.4506 - classification_loss: 0.4951\n",
      "131/500 [======>.......................] - ETA: 6:23 - loss: 1.9475 - regression_loss: 1.4520 - classification_loss: 0.4955\n",
      "132/500 [======>.......................] - ETA: 6:22 - loss: 1.9468 - regression_loss: 1.4513 - classification_loss: 0.4954\n",
      "133/500 [======>.......................] - ETA: 6:20 - loss: 1.9432 - regression_loss: 1.4485 - classification_loss: 0.4947\n",
      "134/500 [=======>......................] - ETA: 6:19 - loss: 1.9428 - regression_loss: 1.4491 - classification_loss: 0.4937\n",
      "135/500 [=======>......................] - ETA: 6:18 - loss: 1.9414 - regression_loss: 1.4479 - classification_loss: 0.4935\n",
      "136/500 [=======>......................] - ETA: 6:17 - loss: 1.9387 - regression_loss: 1.4454 - classification_loss: 0.4933\n",
      "137/500 [=======>......................] - ETA: 6:16 - loss: 1.9410 - regression_loss: 1.4461 - classification_loss: 0.4949\n",
      "138/500 [=======>......................] - ETA: 6:14 - loss: 1.9379 - regression_loss: 1.4435 - classification_loss: 0.4943\n",
      "139/500 [=======>......................] - ETA: 6:14 - loss: 1.9394 - regression_loss: 1.4430 - classification_loss: 0.4965\n",
      "140/500 [=======>......................] - ETA: 6:13 - loss: 1.9389 - regression_loss: 1.4429 - classification_loss: 0.4960\n",
      "141/500 [=======>......................] - ETA: 6:12 - loss: 1.9355 - regression_loss: 1.4400 - classification_loss: 0.4955\n",
      "142/500 [=======>......................] - ETA: 6:11 - loss: 1.9316 - regression_loss: 1.4378 - classification_loss: 0.4938\n",
      "143/500 [=======>......................] - ETA: 6:10 - loss: 1.9301 - regression_loss: 1.4366 - classification_loss: 0.4935\n",
      "144/500 [=======>......................] - ETA: 6:09 - loss: 1.9380 - regression_loss: 1.4421 - classification_loss: 0.4959\n",
      "145/500 [=======>......................] - ETA: 6:08 - loss: 1.9471 - regression_loss: 1.4481 - classification_loss: 0.4990\n",
      "146/500 [=======>......................] - ETA: 6:07 - loss: 1.9550 - regression_loss: 1.4537 - classification_loss: 0.5013\n",
      "147/500 [=======>......................] - ETA: 6:06 - loss: 1.9525 - regression_loss: 1.4526 - classification_loss: 0.4999\n",
      "148/500 [=======>......................] - ETA: 6:05 - loss: 1.9545 - regression_loss: 1.4539 - classification_loss: 0.5006\n",
      "149/500 [=======>......................] - ETA: 6:03 - loss: 1.9594 - regression_loss: 1.4563 - classification_loss: 0.5031\n",
      "150/500 [========>.....................] - ETA: 6:02 - loss: 1.9596 - regression_loss: 1.4575 - classification_loss: 0.5020\n",
      "151/500 [========>.....................] - ETA: 6:01 - loss: 1.9598 - regression_loss: 1.4582 - classification_loss: 0.5016\n",
      "152/500 [========>.....................] - ETA: 6:00 - loss: 1.9586 - regression_loss: 1.4580 - classification_loss: 0.5006\n",
      "153/500 [========>.....................] - ETA: 5:58 - loss: 1.9587 - regression_loss: 1.4580 - classification_loss: 0.5007\n",
      "154/500 [========>.....................] - ETA: 5:58 - loss: 1.9609 - regression_loss: 1.4599 - classification_loss: 0.5010\n",
      "155/500 [========>.....................] - ETA: 5:56 - loss: 1.9582 - regression_loss: 1.4575 - classification_loss: 0.5007\n",
      "156/500 [========>.....................] - ETA: 5:55 - loss: 1.9583 - regression_loss: 1.4565 - classification_loss: 0.5017\n",
      "157/500 [========>.....................] - ETA: 5:54 - loss: 1.9568 - regression_loss: 1.4558 - classification_loss: 0.5010\n",
      "158/500 [========>.....................] - ETA: 5:53 - loss: 1.9613 - regression_loss: 1.4593 - classification_loss: 0.5020\n",
      "159/500 [========>.....................] - ETA: 5:52 - loss: 1.9658 - regression_loss: 1.4625 - classification_loss: 0.5033\n",
      "160/500 [========>.....................] - ETA: 5:51 - loss: 1.9652 - regression_loss: 1.4623 - classification_loss: 0.5029\n",
      "161/500 [========>.....................] - ETA: 5:50 - loss: 1.9614 - regression_loss: 1.4598 - classification_loss: 0.5015\n",
      "162/500 [========>.....................] - ETA: 5:49 - loss: 1.9615 - regression_loss: 1.4605 - classification_loss: 0.5010\n",
      "163/500 [========>.....................] - ETA: 5:48 - loss: 1.9547 - regression_loss: 1.4553 - classification_loss: 0.4994\n",
      "164/500 [========>.....................] - ETA: 5:47 - loss: 1.9526 - regression_loss: 1.4543 - classification_loss: 0.4983\n",
      "165/500 [========>.....................] - ETA: 5:46 - loss: 1.9562 - regression_loss: 1.4560 - classification_loss: 0.5001\n",
      "166/500 [========>.....................] - ETA: 5:45 - loss: 1.9555 - regression_loss: 1.4549 - classification_loss: 0.5006\n",
      "167/500 [=========>....................] - ETA: 5:44 - loss: 1.9608 - regression_loss: 1.4588 - classification_loss: 0.5021\n",
      "168/500 [=========>....................] - ETA: 5:43 - loss: 1.9632 - regression_loss: 1.4606 - classification_loss: 0.5026\n",
      "169/500 [=========>....................] - ETA: 5:42 - loss: 1.9658 - regression_loss: 1.4630 - classification_loss: 0.5028\n",
      "170/500 [=========>....................] - ETA: 5:41 - loss: 1.9629 - regression_loss: 1.4609 - classification_loss: 0.5020\n",
      "171/500 [=========>....................] - ETA: 5:40 - loss: 1.9593 - regression_loss: 1.4584 - classification_loss: 0.5009\n",
      "172/500 [=========>....................] - ETA: 5:39 - loss: 1.9573 - regression_loss: 1.4570 - classification_loss: 0.5004\n",
      "173/500 [=========>....................] - ETA: 5:38 - loss: 1.9573 - regression_loss: 1.4564 - classification_loss: 0.5009\n",
      "174/500 [=========>....................] - ETA: 5:37 - loss: 1.9611 - regression_loss: 1.4594 - classification_loss: 0.5017\n",
      "175/500 [=========>....................] - ETA: 5:36 - loss: 1.9666 - regression_loss: 1.4630 - classification_loss: 0.5037\n",
      "176/500 [=========>....................] - ETA: 5:35 - loss: 1.9676 - regression_loss: 1.4633 - classification_loss: 0.5044\n",
      "177/500 [=========>....................] - ETA: 5:34 - loss: 1.9645 - regression_loss: 1.4616 - classification_loss: 0.5029\n",
      "178/500 [=========>....................] - ETA: 5:33 - loss: 1.9693 - regression_loss: 1.4655 - classification_loss: 0.5038\n",
      "179/500 [=========>....................] - ETA: 5:32 - loss: 1.9680 - regression_loss: 1.4646 - classification_loss: 0.5034\n",
      "180/500 [=========>....................] - ETA: 5:31 - loss: 1.9691 - regression_loss: 1.4664 - classification_loss: 0.5027\n",
      "181/500 [=========>....................] - ETA: 5:30 - loss: 1.9691 - regression_loss: 1.4661 - classification_loss: 0.5030\n",
      "182/500 [=========>....................] - ETA: 5:29 - loss: 1.9690 - regression_loss: 1.4668 - classification_loss: 0.5022\n",
      "183/500 [=========>....................] - ETA: 5:28 - loss: 1.9702 - regression_loss: 1.4679 - classification_loss: 0.5023\n",
      "184/500 [==========>...................] - ETA: 5:27 - loss: 1.9753 - regression_loss: 1.4719 - classification_loss: 0.5034\n",
      "185/500 [==========>...................] - ETA: 5:26 - loss: 1.9730 - regression_loss: 1.4709 - classification_loss: 0.5021\n",
      "186/500 [==========>...................] - ETA: 5:24 - loss: 1.9703 - regression_loss: 1.4685 - classification_loss: 0.5018\n",
      "187/500 [==========>...................] - ETA: 5:23 - loss: 1.9674 - regression_loss: 1.4664 - classification_loss: 0.5009\n",
      "188/500 [==========>...................] - ETA: 5:22 - loss: 1.9632 - regression_loss: 1.4639 - classification_loss: 0.4994\n",
      "189/500 [==========>...................] - ETA: 5:21 - loss: 1.9582 - regression_loss: 1.4598 - classification_loss: 0.4984\n",
      "190/500 [==========>...................] - ETA: 5:20 - loss: 1.9569 - regression_loss: 1.4585 - classification_loss: 0.4985\n",
      "191/500 [==========>...................] - ETA: 5:19 - loss: 1.9591 - regression_loss: 1.4606 - classification_loss: 0.4984\n",
      "192/500 [==========>...................] - ETA: 5:18 - loss: 1.9556 - regression_loss: 1.4581 - classification_loss: 0.4974\n",
      "193/500 [==========>...................] - ETA: 5:17 - loss: 1.9574 - regression_loss: 1.4592 - classification_loss: 0.4982\n",
      "194/500 [==========>...................] - ETA: 5:16 - loss: 1.9558 - regression_loss: 1.4590 - classification_loss: 0.4968\n",
      "195/500 [==========>...................] - ETA: 5:15 - loss: 1.9586 - regression_loss: 1.4608 - classification_loss: 0.4978\n",
      "196/500 [==========>...................] - ETA: 5:14 - loss: 1.9576 - regression_loss: 1.4604 - classification_loss: 0.4972\n",
      "197/500 [==========>...................] - ETA: 5:13 - loss: 1.9533 - regression_loss: 1.4571 - classification_loss: 0.4962\n",
      "198/500 [==========>...................] - ETA: 5:12 - loss: 1.9542 - regression_loss: 1.4582 - classification_loss: 0.4960\n",
      "199/500 [==========>...................] - ETA: 5:11 - loss: 1.9510 - regression_loss: 1.4562 - classification_loss: 0.4948\n",
      "200/500 [===========>..................] - ETA: 5:09 - loss: 1.9517 - regression_loss: 1.4572 - classification_loss: 0.4945\n",
      "201/500 [===========>..................] - ETA: 5:09 - loss: 1.9513 - regression_loss: 1.4570 - classification_loss: 0.4944\n",
      "202/500 [===========>..................] - ETA: 5:07 - loss: 1.9461 - regression_loss: 1.4530 - classification_loss: 0.4931\n",
      "203/500 [===========>..................] - ETA: 5:06 - loss: 1.9446 - regression_loss: 1.4525 - classification_loss: 0.4921\n",
      "204/500 [===========>..................] - ETA: 5:05 - loss: 1.9437 - regression_loss: 1.4523 - classification_loss: 0.4914\n",
      "205/500 [===========>..................] - ETA: 5:04 - loss: 1.9422 - regression_loss: 1.4515 - classification_loss: 0.4908\n",
      "206/500 [===========>..................] - ETA: 5:03 - loss: 1.9424 - regression_loss: 1.4509 - classification_loss: 0.4915\n",
      "207/500 [===========>..................] - ETA: 5:02 - loss: 1.9405 - regression_loss: 1.4496 - classification_loss: 0.4909\n",
      "208/500 [===========>..................] - ETA: 5:01 - loss: 1.9439 - regression_loss: 1.4516 - classification_loss: 0.4923\n",
      "209/500 [===========>..................] - ETA: 5:00 - loss: 1.9426 - regression_loss: 1.4512 - classification_loss: 0.4914\n",
      "210/500 [===========>..................] - ETA: 4:59 - loss: 1.9418 - regression_loss: 1.4499 - classification_loss: 0.4918\n",
      "211/500 [===========>..................] - ETA: 4:58 - loss: 1.9420 - regression_loss: 1.4504 - classification_loss: 0.4916\n",
      "212/500 [===========>..................] - ETA: 4:57 - loss: 1.9406 - regression_loss: 1.4490 - classification_loss: 0.4916\n",
      "213/500 [===========>..................] - ETA: 4:56 - loss: 1.9403 - regression_loss: 1.4490 - classification_loss: 0.4913\n",
      "214/500 [===========>..................] - ETA: 4:55 - loss: 1.9412 - regression_loss: 1.4507 - classification_loss: 0.4905\n",
      "215/500 [===========>..................] - ETA: 4:53 - loss: 1.9417 - regression_loss: 1.4506 - classification_loss: 0.4911\n",
      "216/500 [===========>..................] - ETA: 4:53 - loss: 1.9375 - regression_loss: 1.4475 - classification_loss: 0.4900\n",
      "217/500 [============>.................] - ETA: 4:52 - loss: 1.9370 - regression_loss: 1.4474 - classification_loss: 0.4897\n",
      "218/500 [============>.................] - ETA: 4:51 - loss: 1.9371 - regression_loss: 1.4479 - classification_loss: 0.4892\n",
      "219/500 [============>.................] - ETA: 4:49 - loss: 1.9342 - regression_loss: 1.4457 - classification_loss: 0.4884\n",
      "220/500 [============>.................] - ETA: 4:48 - loss: 1.9311 - regression_loss: 1.4429 - classification_loss: 0.4881\n",
      "221/500 [============>.................] - ETA: 4:47 - loss: 1.9288 - regression_loss: 1.4411 - classification_loss: 0.4877\n",
      "222/500 [============>.................] - ETA: 4:46 - loss: 1.9270 - regression_loss: 1.4399 - classification_loss: 0.4871\n",
      "223/500 [============>.................] - ETA: 4:45 - loss: 1.9289 - regression_loss: 1.4405 - classification_loss: 0.4884\n",
      "224/500 [============>.................] - ETA: 4:44 - loss: 1.9282 - regression_loss: 1.4404 - classification_loss: 0.4878\n",
      "225/500 [============>.................] - ETA: 4:43 - loss: 1.9270 - regression_loss: 1.4399 - classification_loss: 0.4871\n",
      "226/500 [============>.................] - ETA: 4:42 - loss: 1.9307 - regression_loss: 1.4427 - classification_loss: 0.4880\n",
      "227/500 [============>.................] - ETA: 4:41 - loss: 1.9277 - regression_loss: 1.4406 - classification_loss: 0.4872\n",
      "228/500 [============>.................] - ETA: 4:40 - loss: 1.9292 - regression_loss: 1.4415 - classification_loss: 0.4877\n",
      "229/500 [============>.................] - ETA: 4:39 - loss: 1.9302 - regression_loss: 1.4423 - classification_loss: 0.4879\n",
      "230/500 [============>.................] - ETA: 4:38 - loss: 1.9300 - regression_loss: 1.4420 - classification_loss: 0.4879\n",
      "231/500 [============>.................] - ETA: 4:37 - loss: 1.9314 - regression_loss: 1.4432 - classification_loss: 0.4882\n",
      "232/500 [============>.................] - ETA: 4:36 - loss: 1.9269 - regression_loss: 1.4398 - classification_loss: 0.4871\n",
      "233/500 [============>.................] - ETA: 4:35 - loss: 1.9249 - regression_loss: 1.4386 - classification_loss: 0.4862\n",
      "234/500 [=============>................] - ETA: 4:34 - loss: 1.9236 - regression_loss: 1.4374 - classification_loss: 0.4862\n",
      "235/500 [=============>................] - ETA: 4:33 - loss: 1.9265 - regression_loss: 1.4390 - classification_loss: 0.4875\n",
      "236/500 [=============>................] - ETA: 4:32 - loss: 1.9247 - regression_loss: 1.4374 - classification_loss: 0.4873\n",
      "237/500 [=============>................] - ETA: 4:30 - loss: 1.9248 - regression_loss: 1.4381 - classification_loss: 0.4867\n",
      "238/500 [=============>................] - ETA: 4:30 - loss: 1.9248 - regression_loss: 1.4379 - classification_loss: 0.4869\n",
      "239/500 [=============>................] - ETA: 4:28 - loss: 1.9240 - regression_loss: 1.4370 - classification_loss: 0.4870\n",
      "240/500 [=============>................] - ETA: 4:27 - loss: 1.9250 - regression_loss: 1.4378 - classification_loss: 0.4872\n",
      "241/500 [=============>................] - ETA: 4:26 - loss: 1.9275 - regression_loss: 1.4403 - classification_loss: 0.4872\n",
      "242/500 [=============>................] - ETA: 4:25 - loss: 1.9251 - regression_loss: 1.4385 - classification_loss: 0.4866\n",
      "243/500 [=============>................] - ETA: 4:24 - loss: 1.9282 - regression_loss: 1.4413 - classification_loss: 0.4868\n",
      "244/500 [=============>................] - ETA: 4:23 - loss: 1.9289 - regression_loss: 1.4419 - classification_loss: 0.4870\n",
      "245/500 [=============>................] - ETA: 4:22 - loss: 1.9287 - regression_loss: 1.4422 - classification_loss: 0.4865\n",
      "246/500 [=============>................] - ETA: 4:21 - loss: 1.9276 - regression_loss: 1.4419 - classification_loss: 0.4858\n",
      "247/500 [=============>................] - ETA: 4:20 - loss: 1.9295 - regression_loss: 1.4435 - classification_loss: 0.4861\n",
      "248/500 [=============>................] - ETA: 4:19 - loss: 1.9286 - regression_loss: 1.4422 - classification_loss: 0.4864\n",
      "249/500 [=============>................] - ETA: 4:18 - loss: 1.9320 - regression_loss: 1.4441 - classification_loss: 0.4878\n",
      "250/500 [==============>...............] - ETA: 4:17 - loss: 1.9318 - regression_loss: 1.4444 - classification_loss: 0.4875\n",
      "251/500 [==============>...............] - ETA: 4:18 - loss: 1.9346 - regression_loss: 1.4464 - classification_loss: 0.4882\n",
      "252/500 [==============>...............] - ETA: 4:17 - loss: 1.9378 - regression_loss: 1.4481 - classification_loss: 0.4897\n",
      "253/500 [==============>...............] - ETA: 4:16 - loss: 1.9378 - regression_loss: 1.4480 - classification_loss: 0.4898\n",
      "254/500 [==============>...............] - ETA: 4:15 - loss: 1.9380 - regression_loss: 1.4483 - classification_loss: 0.4897\n",
      "255/500 [==============>...............] - ETA: 4:14 - loss: 1.9374 - regression_loss: 1.4479 - classification_loss: 0.4895\n",
      "256/500 [==============>...............] - ETA: 4:13 - loss: 1.9343 - regression_loss: 1.4455 - classification_loss: 0.4888\n",
      "257/500 [==============>...............] - ETA: 4:12 - loss: 1.9342 - regression_loss: 1.4448 - classification_loss: 0.4894\n",
      "258/500 [==============>...............] - ETA: 4:11 - loss: 1.9319 - regression_loss: 1.4428 - classification_loss: 0.4891\n",
      "259/500 [==============>...............] - ETA: 4:10 - loss: 1.9327 - regression_loss: 1.4432 - classification_loss: 0.4895\n",
      "260/500 [==============>...............] - ETA: 4:09 - loss: 1.9299 - regression_loss: 1.4408 - classification_loss: 0.4892\n",
      "261/500 [==============>...............] - ETA: 4:08 - loss: 1.9314 - regression_loss: 1.4426 - classification_loss: 0.4888\n",
      "262/500 [==============>...............] - ETA: 4:07 - loss: 1.9310 - regression_loss: 1.4427 - classification_loss: 0.4884\n",
      "263/500 [==============>...............] - ETA: 4:06 - loss: 1.9299 - regression_loss: 1.4422 - classification_loss: 0.4877\n",
      "264/500 [==============>...............] - ETA: 4:04 - loss: 1.9287 - regression_loss: 1.4410 - classification_loss: 0.4876\n",
      "265/500 [==============>...............] - ETA: 4:03 - loss: 1.9279 - regression_loss: 1.4395 - classification_loss: 0.4885\n",
      "266/500 [==============>...............] - ETA: 4:02 - loss: 1.9286 - regression_loss: 1.4402 - classification_loss: 0.4884\n",
      "267/500 [===============>..............] - ETA: 4:01 - loss: 1.9268 - regression_loss: 1.4382 - classification_loss: 0.4886\n",
      "268/500 [===============>..............] - ETA: 4:00 - loss: 1.9259 - regression_loss: 1.4371 - classification_loss: 0.4888\n",
      "269/500 [===============>..............] - ETA: 3:59 - loss: 1.9267 - regression_loss: 1.4381 - classification_loss: 0.4886\n",
      "270/500 [===============>..............] - ETA: 3:58 - loss: 1.9252 - regression_loss: 1.4375 - classification_loss: 0.4877\n",
      "271/500 [===============>..............] - ETA: 3:57 - loss: 1.9236 - regression_loss: 1.4366 - classification_loss: 0.4869\n",
      "272/500 [===============>..............] - ETA: 3:56 - loss: 1.9265 - regression_loss: 1.4381 - classification_loss: 0.4884\n",
      "273/500 [===============>..............] - ETA: 3:55 - loss: 1.9252 - regression_loss: 1.4375 - classification_loss: 0.4877\n",
      "274/500 [===============>..............] - ETA: 3:54 - loss: 1.9278 - regression_loss: 1.4389 - classification_loss: 0.4889\n",
      "275/500 [===============>..............] - ETA: 3:53 - loss: 1.9289 - regression_loss: 1.4395 - classification_loss: 0.4894\n",
      "276/500 [===============>..............] - ETA: 3:52 - loss: 1.9272 - regression_loss: 1.4381 - classification_loss: 0.4891\n",
      "277/500 [===============>..............] - ETA: 3:51 - loss: 1.9293 - regression_loss: 1.4387 - classification_loss: 0.4906\n",
      "278/500 [===============>..............] - ETA: 3:50 - loss: 1.9289 - regression_loss: 1.4388 - classification_loss: 0.4902\n",
      "279/500 [===============>..............] - ETA: 3:49 - loss: 1.9277 - regression_loss: 1.4378 - classification_loss: 0.4900\n",
      "280/500 [===============>..............] - ETA: 3:48 - loss: 1.9271 - regression_loss: 1.4376 - classification_loss: 0.4895\n",
      "281/500 [===============>..............] - ETA: 3:47 - loss: 1.9289 - regression_loss: 1.4392 - classification_loss: 0.4897\n",
      "282/500 [===============>..............] - ETA: 3:46 - loss: 1.9291 - regression_loss: 1.4392 - classification_loss: 0.4899\n",
      "283/500 [===============>..............] - ETA: 3:45 - loss: 1.9313 - regression_loss: 1.4415 - classification_loss: 0.4898\n",
      "284/500 [================>.............] - ETA: 3:44 - loss: 1.9309 - regression_loss: 1.4415 - classification_loss: 0.4893\n",
      "285/500 [================>.............] - ETA: 3:42 - loss: 1.9300 - regression_loss: 1.4411 - classification_loss: 0.4888\n",
      "286/500 [================>.............] - ETA: 3:41 - loss: 1.9302 - regression_loss: 1.4412 - classification_loss: 0.4890\n",
      "287/500 [================>.............] - ETA: 3:40 - loss: 1.9299 - regression_loss: 1.4411 - classification_loss: 0.4889\n",
      "288/500 [================>.............] - ETA: 3:39 - loss: 1.9309 - regression_loss: 1.4415 - classification_loss: 0.4894\n",
      "289/500 [================>.............] - ETA: 3:38 - loss: 1.9296 - regression_loss: 1.4405 - classification_loss: 0.4891\n",
      "290/500 [================>.............] - ETA: 3:37 - loss: 1.9285 - regression_loss: 1.4399 - classification_loss: 0.4886\n",
      "291/500 [================>.............] - ETA: 3:36 - loss: 1.9265 - regression_loss: 1.4381 - classification_loss: 0.4884\n",
      "292/500 [================>.............] - ETA: 3:35 - loss: 1.9273 - regression_loss: 1.4389 - classification_loss: 0.4884\n",
      "293/500 [================>.............] - ETA: 3:34 - loss: 1.9276 - regression_loss: 1.4392 - classification_loss: 0.4884\n",
      "294/500 [================>.............] - ETA: 3:33 - loss: 1.9259 - regression_loss: 1.4383 - classification_loss: 0.4876\n",
      "295/500 [================>.............] - ETA: 3:32 - loss: 1.9279 - regression_loss: 1.4399 - classification_loss: 0.4880\n",
      "296/500 [================>.............] - ETA: 3:31 - loss: 1.9251 - regression_loss: 1.4379 - classification_loss: 0.4872\n",
      "297/500 [================>.............] - ETA: 3:30 - loss: 1.9223 - regression_loss: 1.4357 - classification_loss: 0.4867\n",
      "298/500 [================>.............] - ETA: 3:29 - loss: 1.9212 - regression_loss: 1.4351 - classification_loss: 0.4861\n",
      "299/500 [================>.............] - ETA: 3:28 - loss: 1.9214 - regression_loss: 1.4356 - classification_loss: 0.4858\n",
      "300/500 [=================>............] - ETA: 3:27 - loss: 1.9218 - regression_loss: 1.4362 - classification_loss: 0.4856\n",
      "301/500 [=================>............] - ETA: 3:26 - loss: 1.9189 - regression_loss: 1.4339 - classification_loss: 0.4849\n",
      "302/500 [=================>............] - ETA: 3:25 - loss: 1.9198 - regression_loss: 1.4346 - classification_loss: 0.4853\n",
      "303/500 [=================>............] - ETA: 3:24 - loss: 1.9199 - regression_loss: 1.4343 - classification_loss: 0.4857\n",
      "304/500 [=================>............] - ETA: 3:23 - loss: 1.9218 - regression_loss: 1.4356 - classification_loss: 0.4862\n",
      "305/500 [=================>............] - ETA: 3:22 - loss: 1.9219 - regression_loss: 1.4352 - classification_loss: 0.4867\n",
      "306/500 [=================>............] - ETA: 3:21 - loss: 1.9232 - regression_loss: 1.4359 - classification_loss: 0.4873\n",
      "307/500 [=================>............] - ETA: 3:20 - loss: 1.9241 - regression_loss: 1.4365 - classification_loss: 0.4876\n",
      "308/500 [=================>............] - ETA: 3:19 - loss: 1.9251 - regression_loss: 1.4371 - classification_loss: 0.4880\n",
      "309/500 [=================>............] - ETA: 3:18 - loss: 1.9261 - regression_loss: 1.4377 - classification_loss: 0.4884\n",
      "310/500 [=================>............] - ETA: 3:17 - loss: 1.9241 - regression_loss: 1.4363 - classification_loss: 0.4878\n",
      "311/500 [=================>............] - ETA: 3:16 - loss: 1.9224 - regression_loss: 1.4353 - classification_loss: 0.4871\n",
      "312/500 [=================>............] - ETA: 3:15 - loss: 1.9228 - regression_loss: 1.4360 - classification_loss: 0.4868\n",
      "313/500 [=================>............] - ETA: 3:14 - loss: 1.9256 - regression_loss: 1.4380 - classification_loss: 0.4877\n",
      "314/500 [=================>............] - ETA: 3:13 - loss: 1.9280 - regression_loss: 1.4388 - classification_loss: 0.4892\n",
      "315/500 [=================>............] - ETA: 3:11 - loss: 1.9317 - regression_loss: 1.4412 - classification_loss: 0.4905\n",
      "316/500 [=================>............] - ETA: 3:10 - loss: 1.9341 - regression_loss: 1.4429 - classification_loss: 0.4912\n",
      "317/500 [==================>...........] - ETA: 3:09 - loss: 1.9364 - regression_loss: 1.4446 - classification_loss: 0.4918\n",
      "318/500 [==================>...........] - ETA: 3:08 - loss: 1.9333 - regression_loss: 1.4423 - classification_loss: 0.4910\n",
      "319/500 [==================>...........] - ETA: 3:07 - loss: 1.9336 - regression_loss: 1.4418 - classification_loss: 0.4917\n",
      "320/500 [==================>...........] - ETA: 3:06 - loss: 1.9328 - regression_loss: 1.4407 - classification_loss: 0.4921\n",
      "321/500 [==================>...........] - ETA: 3:05 - loss: 1.9321 - regression_loss: 1.4401 - classification_loss: 0.4920\n",
      "322/500 [==================>...........] - ETA: 3:04 - loss: 1.9306 - regression_loss: 1.4390 - classification_loss: 0.4916\n",
      "323/500 [==================>...........] - ETA: 3:03 - loss: 1.9308 - regression_loss: 1.4393 - classification_loss: 0.4915\n",
      "324/500 [==================>...........] - ETA: 3:02 - loss: 1.9309 - regression_loss: 1.4395 - classification_loss: 0.4914\n",
      "325/500 [==================>...........] - ETA: 3:01 - loss: 1.9320 - regression_loss: 1.4403 - classification_loss: 0.4917\n",
      "326/500 [==================>...........] - ETA: 3:00 - loss: 1.9324 - regression_loss: 1.4406 - classification_loss: 0.4918\n",
      "327/500 [==================>...........] - ETA: 2:59 - loss: 1.9342 - regression_loss: 1.4421 - classification_loss: 0.4921\n",
      "328/500 [==================>...........] - ETA: 2:58 - loss: 1.9346 - regression_loss: 1.4423 - classification_loss: 0.4923\n",
      "329/500 [==================>...........] - ETA: 2:57 - loss: 1.9336 - regression_loss: 1.4414 - classification_loss: 0.4922\n",
      "330/500 [==================>...........] - ETA: 2:56 - loss: 1.9341 - regression_loss: 1.4424 - classification_loss: 0.4918\n",
      "331/500 [==================>...........] - ETA: 2:55 - loss: 1.9369 - regression_loss: 1.4445 - classification_loss: 0.4924\n",
      "332/500 [==================>...........] - ETA: 2:54 - loss: 1.9344 - regression_loss: 1.4427 - classification_loss: 0.4917\n",
      "333/500 [==================>...........] - ETA: 2:53 - loss: 1.9328 - regression_loss: 1.4414 - classification_loss: 0.4914\n",
      "334/500 [===================>..........] - ETA: 2:52 - loss: 1.9335 - regression_loss: 1.4417 - classification_loss: 0.4918\n",
      "335/500 [===================>..........] - ETA: 2:51 - loss: 1.9333 - regression_loss: 1.4418 - classification_loss: 0.4916\n",
      "336/500 [===================>..........] - ETA: 2:50 - loss: 1.9334 - regression_loss: 1.4422 - classification_loss: 0.4912\n",
      "337/500 [===================>..........] - ETA: 2:49 - loss: 1.9343 - regression_loss: 1.4429 - classification_loss: 0.4914\n",
      "338/500 [===================>..........] - ETA: 2:48 - loss: 1.9347 - regression_loss: 1.4434 - classification_loss: 0.4912\n",
      "339/500 [===================>..........] - ETA: 2:47 - loss: 1.9335 - regression_loss: 1.4424 - classification_loss: 0.4911\n",
      "340/500 [===================>..........] - ETA: 2:46 - loss: 1.9352 - regression_loss: 1.4438 - classification_loss: 0.4914\n",
      "341/500 [===================>..........] - ETA: 2:45 - loss: 1.9334 - regression_loss: 1.4426 - classification_loss: 0.4909\n",
      "342/500 [===================>..........] - ETA: 2:43 - loss: 1.9322 - regression_loss: 1.4420 - classification_loss: 0.4902\n",
      "343/500 [===================>..........] - ETA: 2:42 - loss: 1.9312 - regression_loss: 1.4415 - classification_loss: 0.4898\n",
      "344/500 [===================>..........] - ETA: 2:41 - loss: 1.9311 - regression_loss: 1.4410 - classification_loss: 0.4902\n",
      "345/500 [===================>..........] - ETA: 2:40 - loss: 1.9334 - regression_loss: 1.4427 - classification_loss: 0.4906\n",
      "346/500 [===================>..........] - ETA: 2:39 - loss: 1.9311 - regression_loss: 1.4410 - classification_loss: 0.4902\n",
      "347/500 [===================>..........] - ETA: 2:38 - loss: 1.9308 - regression_loss: 1.4408 - classification_loss: 0.4900\n",
      "348/500 [===================>..........] - ETA: 2:37 - loss: 1.9323 - regression_loss: 1.4421 - classification_loss: 0.4902\n",
      "349/500 [===================>..........] - ETA: 2:36 - loss: 1.9314 - regression_loss: 1.4417 - classification_loss: 0.4897\n",
      "350/500 [====================>.........] - ETA: 2:35 - loss: 1.9341 - regression_loss: 1.4436 - classification_loss: 0.4905\n",
      "351/500 [====================>.........] - ETA: 2:34 - loss: 1.9362 - regression_loss: 1.4452 - classification_loss: 0.4910\n",
      "352/500 [====================>.........] - ETA: 2:33 - loss: 1.9360 - regression_loss: 1.4448 - classification_loss: 0.4912\n",
      "353/500 [====================>.........] - ETA: 2:32 - loss: 1.9348 - regression_loss: 1.4440 - classification_loss: 0.4907\n",
      "354/500 [====================>.........] - ETA: 2:31 - loss: 1.9324 - regression_loss: 1.4426 - classification_loss: 0.4899\n",
      "355/500 [====================>.........] - ETA: 2:30 - loss: 1.9350 - regression_loss: 1.4435 - classification_loss: 0.4914\n",
      "356/500 [====================>.........] - ETA: 2:29 - loss: 1.9351 - regression_loss: 1.4437 - classification_loss: 0.4914\n",
      "357/500 [====================>.........] - ETA: 2:28 - loss: 1.9367 - regression_loss: 1.4449 - classification_loss: 0.4918\n",
      "358/500 [====================>.........] - ETA: 2:27 - loss: 1.9359 - regression_loss: 1.4442 - classification_loss: 0.4917\n",
      "359/500 [====================>.........] - ETA: 2:26 - loss: 1.9371 - regression_loss: 1.4443 - classification_loss: 0.4928\n",
      "360/500 [====================>.........] - ETA: 2:25 - loss: 1.9364 - regression_loss: 1.4437 - classification_loss: 0.4927\n",
      "361/500 [====================>.........] - ETA: 2:24 - loss: 1.9398 - regression_loss: 1.4460 - classification_loss: 0.4937\n",
      "362/500 [====================>.........] - ETA: 2:23 - loss: 1.9403 - regression_loss: 1.4464 - classification_loss: 0.4939\n",
      "363/500 [====================>.........] - ETA: 2:21 - loss: 1.9395 - regression_loss: 1.4461 - classification_loss: 0.4934\n",
      "364/500 [====================>.........] - ETA: 2:20 - loss: 1.9391 - regression_loss: 1.4462 - classification_loss: 0.4930\n",
      "365/500 [====================>.........] - ETA: 2:19 - loss: 1.9386 - regression_loss: 1.4456 - classification_loss: 0.4930\n",
      "366/500 [====================>.........] - ETA: 2:18 - loss: 1.9385 - regression_loss: 1.4453 - classification_loss: 0.4932\n",
      "367/500 [=====================>........] - ETA: 2:17 - loss: 1.9358 - regression_loss: 1.4432 - classification_loss: 0.4926\n",
      "368/500 [=====================>........] - ETA: 2:16 - loss: 1.9364 - regression_loss: 1.4436 - classification_loss: 0.4928\n",
      "369/500 [=====================>........] - ETA: 2:15 - loss: 1.9335 - regression_loss: 1.4416 - classification_loss: 0.4919\n",
      "370/500 [=====================>........] - ETA: 2:14 - loss: 1.9320 - regression_loss: 1.4408 - classification_loss: 0.4912\n",
      "371/500 [=====================>........] - ETA: 2:13 - loss: 1.9308 - regression_loss: 1.4398 - classification_loss: 0.4910\n",
      "372/500 [=====================>........] - ETA: 2:12 - loss: 1.9323 - regression_loss: 1.4408 - classification_loss: 0.4915\n",
      "373/500 [=====================>........] - ETA: 2:11 - loss: 1.9302 - regression_loss: 1.4392 - classification_loss: 0.4910\n",
      "374/500 [=====================>........] - ETA: 2:10 - loss: 1.9304 - regression_loss: 1.4392 - classification_loss: 0.4912\n",
      "375/500 [=====================>........] - ETA: 2:09 - loss: 1.9295 - regression_loss: 1.4387 - classification_loss: 0.4908\n",
      "376/500 [=====================>........] - ETA: 2:08 - loss: 1.9299 - regression_loss: 1.4384 - classification_loss: 0.4915\n",
      "377/500 [=====================>........] - ETA: 2:07 - loss: 1.9310 - regression_loss: 1.4384 - classification_loss: 0.4926\n",
      "378/500 [=====================>........] - ETA: 2:06 - loss: 1.9306 - regression_loss: 1.4384 - classification_loss: 0.4922\n",
      "379/500 [=====================>........] - ETA: 2:05 - loss: 1.9308 - regression_loss: 1.4384 - classification_loss: 0.4924\n",
      "380/500 [=====================>........] - ETA: 2:04 - loss: 1.9301 - regression_loss: 1.4376 - classification_loss: 0.4925\n",
      "381/500 [=====================>........] - ETA: 2:03 - loss: 1.9283 - regression_loss: 1.4361 - classification_loss: 0.4923\n",
      "382/500 [=====================>........] - ETA: 2:02 - loss: 1.9294 - regression_loss: 1.4370 - classification_loss: 0.4924\n",
      "383/500 [=====================>........] - ETA: 2:01 - loss: 1.9303 - regression_loss: 1.4378 - classification_loss: 0.4924\n",
      "384/500 [======================>.......] - ETA: 2:00 - loss: 1.9287 - regression_loss: 1.4368 - classification_loss: 0.4919\n",
      "385/500 [======================>.......] - ETA: 1:59 - loss: 1.9283 - regression_loss: 1.4365 - classification_loss: 0.4918\n",
      "386/500 [======================>.......] - ETA: 1:58 - loss: 1.9290 - regression_loss: 1.4371 - classification_loss: 0.4919\n",
      "387/500 [======================>.......] - ETA: 1:57 - loss: 1.9305 - regression_loss: 1.4389 - classification_loss: 0.4916\n",
      "388/500 [======================>.......] - ETA: 1:56 - loss: 1.9298 - regression_loss: 1.4385 - classification_loss: 0.4913\n",
      "389/500 [======================>.......] - ETA: 1:55 - loss: 1.9307 - regression_loss: 1.4393 - classification_loss: 0.4914\n",
      "390/500 [======================>.......] - ETA: 1:54 - loss: 1.9284 - regression_loss: 1.4378 - classification_loss: 0.4906\n",
      "391/500 [======================>.......] - ETA: 1:53 - loss: 1.9275 - regression_loss: 1.4369 - classification_loss: 0.4906\n",
      "392/500 [======================>.......] - ETA: 1:52 - loss: 1.9282 - regression_loss: 1.4373 - classification_loss: 0.4908\n",
      "393/500 [======================>.......] - ETA: 1:51 - loss: 1.9288 - regression_loss: 1.4378 - classification_loss: 0.4910\n",
      "394/500 [======================>.......] - ETA: 1:50 - loss: 1.9281 - regression_loss: 1.4373 - classification_loss: 0.4908\n",
      "395/500 [======================>.......] - ETA: 1:49 - loss: 1.9283 - regression_loss: 1.4375 - classification_loss: 0.4908\n",
      "396/500 [======================>.......] - ETA: 1:48 - loss: 1.9263 - regression_loss: 1.4361 - classification_loss: 0.4902\n",
      "397/500 [======================>.......] - ETA: 1:46 - loss: 1.9254 - regression_loss: 1.4356 - classification_loss: 0.4898\n",
      "398/500 [======================>.......] - ETA: 1:45 - loss: 1.9257 - regression_loss: 1.4356 - classification_loss: 0.4902\n",
      "399/500 [======================>.......] - ETA: 1:44 - loss: 1.9243 - regression_loss: 1.4346 - classification_loss: 0.4897\n",
      "400/500 [=======================>......] - ETA: 1:43 - loss: 1.9244 - regression_loss: 1.4350 - classification_loss: 0.4895\n",
      "401/500 [=======================>......] - ETA: 1:42 - loss: 1.9241 - regression_loss: 1.4348 - classification_loss: 0.4893\n",
      "402/500 [=======================>......] - ETA: 1:41 - loss: 1.9246 - regression_loss: 1.4348 - classification_loss: 0.4898\n",
      "403/500 [=======================>......] - ETA: 1:40 - loss: 1.9222 - regression_loss: 1.4331 - classification_loss: 0.4891\n",
      "404/500 [=======================>......] - ETA: 1:39 - loss: 1.9207 - regression_loss: 1.4321 - classification_loss: 0.4886\n",
      "405/500 [=======================>......] - ETA: 1:38 - loss: 1.9189 - regression_loss: 1.4309 - classification_loss: 0.4880\n",
      "406/500 [=======================>......] - ETA: 1:37 - loss: 1.9197 - regression_loss: 1.4316 - classification_loss: 0.4882\n",
      "407/500 [=======================>......] - ETA: 1:36 - loss: 1.9198 - regression_loss: 1.4316 - classification_loss: 0.4883\n",
      "408/500 [=======================>......] - ETA: 1:35 - loss: 1.9213 - regression_loss: 1.4329 - classification_loss: 0.4884\n",
      "409/500 [=======================>......] - ETA: 1:34 - loss: 1.9221 - regression_loss: 1.4334 - classification_loss: 0.4887\n",
      "410/500 [=======================>......] - ETA: 1:33 - loss: 1.9218 - regression_loss: 1.4327 - classification_loss: 0.4891\n",
      "411/500 [=======================>......] - ETA: 1:32 - loss: 1.9225 - regression_loss: 1.4334 - classification_loss: 0.4891\n",
      "412/500 [=======================>......] - ETA: 1:31 - loss: 1.9215 - regression_loss: 1.4328 - classification_loss: 0.4887\n",
      "413/500 [=======================>......] - ETA: 1:30 - loss: 1.9208 - regression_loss: 1.4322 - classification_loss: 0.4887\n",
      "414/500 [=======================>......] - ETA: 1:29 - loss: 1.9179 - regression_loss: 1.4300 - classification_loss: 0.4879\n",
      "415/500 [=======================>......] - ETA: 1:28 - loss: 1.9179 - regression_loss: 1.4296 - classification_loss: 0.4883\n",
      "416/500 [=======================>......] - ETA: 1:27 - loss: 1.9196 - regression_loss: 1.4307 - classification_loss: 0.4888\n",
      "417/500 [========================>.....] - ETA: 1:26 - loss: 1.9182 - regression_loss: 1.4296 - classification_loss: 0.4886\n",
      "418/500 [========================>.....] - ETA: 1:25 - loss: 1.9184 - regression_loss: 1.4301 - classification_loss: 0.4883\n",
      "419/500 [========================>.....] - ETA: 1:24 - loss: 1.9175 - regression_loss: 1.4296 - classification_loss: 0.4879\n",
      "420/500 [========================>.....] - ETA: 1:23 - loss: 1.9188 - regression_loss: 1.4303 - classification_loss: 0.4885\n",
      "421/500 [========================>.....] - ETA: 1:22 - loss: 1.9194 - regression_loss: 1.4309 - classification_loss: 0.4885\n",
      "422/500 [========================>.....] - ETA: 1:21 - loss: 1.9184 - regression_loss: 1.4303 - classification_loss: 0.4881\n",
      "423/500 [========================>.....] - ETA: 1:19 - loss: 1.9184 - regression_loss: 1.4305 - classification_loss: 0.4879\n",
      "424/500 [========================>.....] - ETA: 1:18 - loss: 1.9181 - regression_loss: 1.4300 - classification_loss: 0.4881\n",
      "425/500 [========================>.....] - ETA: 1:17 - loss: 1.9166 - regression_loss: 1.4291 - classification_loss: 0.4875\n",
      "426/500 [========================>.....] - ETA: 1:16 - loss: 1.9183 - regression_loss: 1.4303 - classification_loss: 0.4880\n",
      "427/500 [========================>.....] - ETA: 1:15 - loss: 1.9189 - regression_loss: 1.4308 - classification_loss: 0.4882\n",
      "428/500 [========================>.....] - ETA: 1:14 - loss: 1.9218 - regression_loss: 1.4327 - classification_loss: 0.4891\n",
      "429/500 [========================>.....] - ETA: 1:13 - loss: 1.9207 - regression_loss: 1.4318 - classification_loss: 0.4889\n",
      "430/500 [========================>.....] - ETA: 1:12 - loss: 1.9215 - regression_loss: 1.4326 - classification_loss: 0.4889\n",
      "431/500 [========================>.....] - ETA: 1:11 - loss: 1.9205 - regression_loss: 1.4321 - classification_loss: 0.4884\n",
      "432/500 [========================>.....] - ETA: 1:10 - loss: 1.9200 - regression_loss: 1.4319 - classification_loss: 0.4880\n",
      "433/500 [========================>.....] - ETA: 1:09 - loss: 1.9192 - regression_loss: 1.4315 - classification_loss: 0.4877\n",
      "434/500 [=========================>....] - ETA: 1:08 - loss: 1.9186 - regression_loss: 1.4310 - classification_loss: 0.4875\n",
      "435/500 [=========================>....] - ETA: 1:07 - loss: 1.9171 - regression_loss: 1.4298 - classification_loss: 0.4873\n",
      "436/500 [=========================>....] - ETA: 1:06 - loss: 1.9155 - regression_loss: 1.4286 - classification_loss: 0.4869\n",
      "437/500 [=========================>....] - ETA: 1:05 - loss: 1.9158 - regression_loss: 1.4287 - classification_loss: 0.4871\n",
      "438/500 [=========================>....] - ETA: 1:04 - loss: 1.9144 - regression_loss: 1.4277 - classification_loss: 0.4868\n",
      "439/500 [=========================>....] - ETA: 1:03 - loss: 1.9131 - regression_loss: 1.4268 - classification_loss: 0.4863\n",
      "440/500 [=========================>....] - ETA: 1:02 - loss: 1.9127 - regression_loss: 1.4267 - classification_loss: 0.4860\n",
      "441/500 [=========================>....] - ETA: 1:01 - loss: 1.9124 - regression_loss: 1.4266 - classification_loss: 0.4858\n",
      "442/500 [=========================>....] - ETA: 1:00 - loss: 1.9116 - regression_loss: 1.4259 - classification_loss: 0.4857\n",
      "443/500 [=========================>....] - ETA: 59s - loss: 1.9108 - regression_loss: 1.4254 - classification_loss: 0.4854 \n",
      "444/500 [=========================>....] - ETA: 58s - loss: 1.9104 - regression_loss: 1.4252 - classification_loss: 0.4852\n",
      "445/500 [=========================>....] - ETA: 57s - loss: 1.9102 - regression_loss: 1.4251 - classification_loss: 0.4851\n",
      "446/500 [=========================>....] - ETA: 56s - loss: 1.9126 - regression_loss: 1.4266 - classification_loss: 0.4860\n",
      "447/500 [=========================>....] - ETA: 55s - loss: 1.9134 - regression_loss: 1.4267 - classification_loss: 0.4867\n",
      "448/500 [=========================>....] - ETA: 54s - loss: 1.9129 - regression_loss: 1.4265 - classification_loss: 0.4864\n",
      "449/500 [=========================>....] - ETA: 53s - loss: 1.9113 - regression_loss: 1.4254 - classification_loss: 0.4859\n",
      "450/500 [==========================>...] - ETA: 52s - loss: 1.9118 - regression_loss: 1.4260 - classification_loss: 0.4859\n",
      "451/500 [==========================>...] - ETA: 51s - loss: 1.9128 - regression_loss: 1.4267 - classification_loss: 0.4861\n",
      "452/500 [==========================>...] - ETA: 50s - loss: 1.9121 - regression_loss: 1.4265 - classification_loss: 0.4856\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.9110 - regression_loss: 1.4254 - classification_loss: 0.4857\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.9102 - regression_loss: 1.4250 - classification_loss: 0.4852\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.9083 - regression_loss: 1.4238 - classification_loss: 0.4846\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.9097 - regression_loss: 1.4244 - classification_loss: 0.4852\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.9085 - regression_loss: 1.4237 - classification_loss: 0.4849\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.9104 - regression_loss: 1.4247 - classification_loss: 0.4858\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.9088 - regression_loss: 1.4236 - classification_loss: 0.4852\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.9089 - regression_loss: 1.4238 - classification_loss: 0.4851\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 1.9102 - regression_loss: 1.4247 - classification_loss: 0.4855\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 1.9106 - regression_loss: 1.4249 - classification_loss: 0.4857\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 1.9101 - regression_loss: 1.4244 - classification_loss: 0.4856\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 1.9115 - regression_loss: 1.4259 - classification_loss: 0.4856\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 1.9098 - regression_loss: 1.4246 - classification_loss: 0.4852\n",
      "466/500 [==========================>...] - ETA: 35s - loss: 1.9119 - regression_loss: 1.4259 - classification_loss: 0.4860\n",
      "467/500 [===========================>..] - ETA: 34s - loss: 1.9137 - regression_loss: 1.4268 - classification_loss: 0.4869\n",
      "468/500 [===========================>..] - ETA: 33s - loss: 1.9155 - regression_loss: 1.4280 - classification_loss: 0.4875\n",
      "469/500 [===========================>..] - ETA: 32s - loss: 1.9178 - regression_loss: 1.4293 - classification_loss: 0.4885\n",
      "470/500 [===========================>..] - ETA: 31s - loss: 1.9187 - regression_loss: 1.4300 - classification_loss: 0.4887\n",
      "471/500 [===========================>..] - ETA: 30s - loss: 1.9189 - regression_loss: 1.4302 - classification_loss: 0.4887\n",
      "472/500 [===========================>..] - ETA: 29s - loss: 1.9197 - regression_loss: 1.4306 - classification_loss: 0.4891\n",
      "473/500 [===========================>..] - ETA: 28s - loss: 1.9223 - regression_loss: 1.4324 - classification_loss: 0.4900\n",
      "474/500 [===========================>..] - ETA: 27s - loss: 1.9223 - regression_loss: 1.4325 - classification_loss: 0.4899\n",
      "475/500 [===========================>..] - ETA: 26s - loss: 1.9217 - regression_loss: 1.4322 - classification_loss: 0.4895\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 1.9220 - regression_loss: 1.4322 - classification_loss: 0.4899\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.9213 - regression_loss: 1.4318 - classification_loss: 0.4896\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.9227 - regression_loss: 1.4331 - classification_loss: 0.4896\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.9241 - regression_loss: 1.4342 - classification_loss: 0.4900\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.9231 - regression_loss: 1.4333 - classification_loss: 0.4898\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.9242 - regression_loss: 1.4343 - classification_loss: 0.4899\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.9234 - regression_loss: 1.4338 - classification_loss: 0.4896\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.9240 - regression_loss: 1.4345 - classification_loss: 0.4896\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.9236 - regression_loss: 1.4342 - classification_loss: 0.4894\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.9235 - regression_loss: 1.4342 - classification_loss: 0.4893\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.9249 - regression_loss: 1.4352 - classification_loss: 0.4897\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.9243 - regression_loss: 1.4347 - classification_loss: 0.4896\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.9242 - regression_loss: 1.4348 - classification_loss: 0.4894\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.9239 - regression_loss: 1.4347 - classification_loss: 0.4891\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.9227 - regression_loss: 1.4340 - classification_loss: 0.4887\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.9241 - regression_loss: 1.4351 - classification_loss: 0.4889 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.9247 - regression_loss: 1.4356 - classification_loss: 0.4890\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.9241 - regression_loss: 1.4352 - classification_loss: 0.4889\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.9249 - regression_loss: 1.4359 - classification_loss: 0.4890\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.9255 - regression_loss: 1.4362 - classification_loss: 0.4893\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.9248 - regression_loss: 1.4358 - classification_loss: 0.4890\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.9258 - regression_loss: 1.4366 - classification_loss: 0.4892\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.9251 - regression_loss: 1.4358 - classification_loss: 0.4893\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.9256 - regression_loss: 1.4365 - classification_loss: 0.4891\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9255 - regression_loss: 1.4366 - classification_loss: 0.4890\n",
      "Epoch 00020: saving model to ./snapshots\\resnet50_csv_20.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "\n",
      "500/500 [==============================] - 520s 1s/step - loss: 1.9255 - regression_loss: 1.4366 - classification_loss: 0.4890\n",
      "Epoch 21/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 1.2363 - regression_loss: 0.9769 - classification_loss: 0.2594\n",
      "  2/500 [..............................] - ETA: 5:23 - loss: 2.1285 - regression_loss: 1.6447 - classification_loss: 0.4838\n",
      "  3/500 [..............................] - ETA: 6:32 - loss: 1.8569 - regression_loss: 1.4328 - classification_loss: 0.4241\n",
      "  4/500 [..............................] - ETA: 6:26 - loss: 1.8142 - regression_loss: 1.4259 - classification_loss: 0.3883\n",
      "  5/500 [..............................] - ETA: 7:12 - loss: 1.9291 - regression_loss: 1.5003 - classification_loss: 0.4288\n",
      "  6/500 [..............................] - ETA: 7:28 - loss: 1.9002 - regression_loss: 1.4668 - classification_loss: 0.4334\n",
      "  7/500 [..............................] - ETA: 7:40 - loss: 1.9103 - regression_loss: 1.4597 - classification_loss: 0.4507\n",
      "  8/500 [..............................] - ETA: 7:49 - loss: 2.0674 - regression_loss: 1.5659 - classification_loss: 0.5015\n",
      "  9/500 [..............................] - ETA: 7:55 - loss: 2.0031 - regression_loss: 1.5238 - classification_loss: 0.4793\n",
      " 10/500 [..............................] - ETA: 7:54 - loss: 1.9439 - regression_loss: 1.4783 - classification_loss: 0.4656\n",
      " 11/500 [..............................] - ETA: 7:58 - loss: 1.9143 - regression_loss: 1.4538 - classification_loss: 0.4605\n",
      " 12/500 [..............................] - ETA: 8:02 - loss: 1.9067 - regression_loss: 1.4561 - classification_loss: 0.4506\n",
      " 13/500 [..............................] - ETA: 8:00 - loss: 1.9087 - regression_loss: 1.4552 - classification_loss: 0.4535\n",
      " 14/500 [..............................] - ETA: 8:03 - loss: 1.9278 - regression_loss: 1.4562 - classification_loss: 0.4716\n",
      " 15/500 [..............................] - ETA: 8:05 - loss: 1.9444 - regression_loss: 1.4646 - classification_loss: 0.4799\n",
      " 16/500 [..............................] - ETA: 8:03 - loss: 1.9471 - regression_loss: 1.4647 - classification_loss: 0.4824\n",
      " 17/500 [>.............................] - ETA: 8:02 - loss: 1.9301 - regression_loss: 1.4509 - classification_loss: 0.4793\n",
      " 18/500 [>.............................] - ETA: 7:57 - loss: 1.9525 - regression_loss: 1.4593 - classification_loss: 0.4932\n",
      " 19/500 [>.............................] - ETA: 7:58 - loss: 1.9160 - regression_loss: 1.4301 - classification_loss: 0.4859\n",
      " 20/500 [>.............................] - ETA: 7:57 - loss: 1.9112 - regression_loss: 1.4309 - classification_loss: 0.4804\n",
      " 21/500 [>.............................] - ETA: 7:56 - loss: 1.9417 - regression_loss: 1.4469 - classification_loss: 0.4948\n",
      " 22/500 [>.............................] - ETA: 7:54 - loss: 1.9424 - regression_loss: 1.4380 - classification_loss: 0.5043\n",
      " 23/500 [>.............................] - ETA: 7:53 - loss: 1.9209 - regression_loss: 1.4210 - classification_loss: 0.4998\n",
      " 24/500 [>.............................] - ETA: 7:52 - loss: 1.9020 - regression_loss: 1.4137 - classification_loss: 0.4884\n",
      " 25/500 [>.............................] - ETA: 7:52 - loss: 1.8814 - regression_loss: 1.4015 - classification_loss: 0.4800\n",
      " 26/500 [>.............................] - ETA: 7:53 - loss: 1.8631 - regression_loss: 1.3851 - classification_loss: 0.4780\n",
      " 27/500 [>.............................] - ETA: 7:54 - loss: 1.8831 - regression_loss: 1.3984 - classification_loss: 0.4847\n",
      " 28/500 [>.............................] - ETA: 7:52 - loss: 1.8880 - regression_loss: 1.3961 - classification_loss: 0.4919\n",
      " 29/500 [>.............................] - ETA: 7:51 - loss: 1.8709 - regression_loss: 1.3872 - classification_loss: 0.4837\n",
      " 30/500 [>.............................] - ETA: 7:50 - loss: 1.8735 - regression_loss: 1.3907 - classification_loss: 0.4828\n",
      " 31/500 [>.............................] - ETA: 7:48 - loss: 1.8608 - regression_loss: 1.3826 - classification_loss: 0.4783\n",
      " 32/500 [>.............................] - ETA: 7:46 - loss: 1.8382 - regression_loss: 1.3674 - classification_loss: 0.4708\n",
      " 33/500 [>.............................] - ETA: 7:46 - loss: 1.8225 - regression_loss: 1.3565 - classification_loss: 0.4660\n",
      " 34/500 [=>............................] - ETA: 7:46 - loss: 1.8217 - regression_loss: 1.3600 - classification_loss: 0.4617\n",
      " 35/500 [=>............................] - ETA: 7:46 - loss: 1.8297 - regression_loss: 1.3682 - classification_loss: 0.4615\n",
      " 36/500 [=>............................] - ETA: 7:45 - loss: 1.8229 - regression_loss: 1.3621 - classification_loss: 0.4608\n",
      " 37/500 [=>............................] - ETA: 7:45 - loss: 1.8529 - regression_loss: 1.3833 - classification_loss: 0.4696\n",
      " 38/500 [=>............................] - ETA: 7:45 - loss: 1.8740 - regression_loss: 1.3974 - classification_loss: 0.4766\n",
      " 39/500 [=>............................] - ETA: 7:45 - loss: 1.8759 - regression_loss: 1.3969 - classification_loss: 0.4790\n",
      " 40/500 [=>............................] - ETA: 7:44 - loss: 1.8646 - regression_loss: 1.3911 - classification_loss: 0.4735\n",
      " 41/500 [=>............................] - ETA: 7:43 - loss: 1.8574 - regression_loss: 1.3865 - classification_loss: 0.4709\n",
      " 42/500 [=>............................] - ETA: 7:42 - loss: 1.8841 - regression_loss: 1.4091 - classification_loss: 0.4750\n",
      " 43/500 [=>............................] - ETA: 7:41 - loss: 1.8868 - regression_loss: 1.4129 - classification_loss: 0.4739\n",
      " 44/500 [=>............................] - ETA: 7:41 - loss: 1.8911 - regression_loss: 1.4166 - classification_loss: 0.4745\n",
      " 45/500 [=>............................] - ETA: 7:39 - loss: 1.8859 - regression_loss: 1.4149 - classification_loss: 0.4710\n",
      " 46/500 [=>............................] - ETA: 7:38 - loss: 1.8862 - regression_loss: 1.4186 - classification_loss: 0.4676\n",
      " 47/500 [=>............................] - ETA: 7:38 - loss: 1.8781 - regression_loss: 1.4125 - classification_loss: 0.4656\n",
      " 48/500 [=>............................] - ETA: 7:35 - loss: 1.8771 - regression_loss: 1.4126 - classification_loss: 0.4644\n",
      " 49/500 [=>............................] - ETA: 7:35 - loss: 1.8640 - regression_loss: 1.4022 - classification_loss: 0.4619\n",
      " 50/500 [==>...........................] - ETA: 7:34 - loss: 1.8525 - regression_loss: 1.3939 - classification_loss: 0.4586\n",
      " 51/500 [==>...........................] - ETA: 7:33 - loss: 1.8694 - regression_loss: 1.4040 - classification_loss: 0.4654\n",
      " 52/500 [==>...........................] - ETA: 7:32 - loss: 1.8766 - regression_loss: 1.4100 - classification_loss: 0.4667\n",
      " 53/500 [==>...........................] - ETA: 7:33 - loss: 1.8814 - regression_loss: 1.4145 - classification_loss: 0.4670\n",
      " 54/500 [==>...........................] - ETA: 7:31 - loss: 1.8839 - regression_loss: 1.4156 - classification_loss: 0.4683\n",
      " 55/500 [==>...........................] - ETA: 7:30 - loss: 1.8826 - regression_loss: 1.4144 - classification_loss: 0.4682\n",
      " 56/500 [==>...........................] - ETA: 7:30 - loss: 1.8980 - regression_loss: 1.4295 - classification_loss: 0.4686\n",
      " 57/500 [==>...........................] - ETA: 7:28 - loss: 1.8919 - regression_loss: 1.4241 - classification_loss: 0.4678\n",
      " 58/500 [==>...........................] - ETA: 7:28 - loss: 1.8976 - regression_loss: 1.4291 - classification_loss: 0.4685\n",
      " 59/500 [==>...........................] - ETA: 7:25 - loss: 1.9008 - regression_loss: 1.4339 - classification_loss: 0.4669\n",
      " 60/500 [==>...........................] - ETA: 7:24 - loss: 1.9036 - regression_loss: 1.4360 - classification_loss: 0.4676\n",
      " 61/500 [==>...........................] - ETA: 7:25 - loss: 1.8973 - regression_loss: 1.4317 - classification_loss: 0.4656\n",
      " 62/500 [==>...........................] - ETA: 7:24 - loss: 1.8877 - regression_loss: 1.4253 - classification_loss: 0.4624\n",
      " 63/500 [==>...........................] - ETA: 7:23 - loss: 1.8857 - regression_loss: 1.4201 - classification_loss: 0.4655\n",
      " 64/500 [==>...........................] - ETA: 7:23 - loss: 1.8863 - regression_loss: 1.4220 - classification_loss: 0.4643\n",
      " 65/500 [==>...........................] - ETA: 7:22 - loss: 1.8723 - regression_loss: 1.4114 - classification_loss: 0.4609\n",
      " 66/500 [==>...........................] - ETA: 7:21 - loss: 1.8750 - regression_loss: 1.4159 - classification_loss: 0.4591\n",
      " 67/500 [===>..........................] - ETA: 7:21 - loss: 1.8662 - regression_loss: 1.4080 - classification_loss: 0.4582\n",
      " 68/500 [===>..........................] - ETA: 7:20 - loss: 1.8666 - regression_loss: 1.4102 - classification_loss: 0.4565\n",
      " 69/500 [===>..........................] - ETA: 7:20 - loss: 1.8747 - regression_loss: 1.4156 - classification_loss: 0.4592\n",
      " 70/500 [===>..........................] - ETA: 7:18 - loss: 1.8695 - regression_loss: 1.4115 - classification_loss: 0.4580\n",
      " 71/500 [===>..........................] - ETA: 7:18 - loss: 1.8767 - regression_loss: 1.4172 - classification_loss: 0.4595\n",
      " 72/500 [===>..........................] - ETA: 7:17 - loss: 1.8787 - regression_loss: 1.4202 - classification_loss: 0.4585\n",
      " 73/500 [===>..........................] - ETA: 7:16 - loss: 1.8825 - regression_loss: 1.4216 - classification_loss: 0.4608\n",
      " 74/500 [===>..........................] - ETA: 7:15 - loss: 1.8835 - regression_loss: 1.4211 - classification_loss: 0.4624\n",
      " 75/500 [===>..........................] - ETA: 7:13 - loss: 1.8872 - regression_loss: 1.4219 - classification_loss: 0.4653\n",
      " 76/500 [===>..........................] - ETA: 7:13 - loss: 1.8867 - regression_loss: 1.4210 - classification_loss: 0.4657\n",
      " 77/500 [===>..........................] - ETA: 7:12 - loss: 1.8811 - regression_loss: 1.4167 - classification_loss: 0.4644\n",
      " 78/500 [===>..........................] - ETA: 7:11 - loss: 1.8824 - regression_loss: 1.4172 - classification_loss: 0.4652\n",
      " 79/500 [===>..........................] - ETA: 7:11 - loss: 1.8885 - regression_loss: 1.4184 - classification_loss: 0.4701\n",
      " 80/500 [===>..........................] - ETA: 7:10 - loss: 1.8946 - regression_loss: 1.4249 - classification_loss: 0.4697\n",
      " 81/500 [===>..........................] - ETA: 7:10 - loss: 1.8943 - regression_loss: 1.4232 - classification_loss: 0.4711\n",
      " 82/500 [===>..........................] - ETA: 7:08 - loss: 1.8932 - regression_loss: 1.4240 - classification_loss: 0.4692\n",
      " 83/500 [===>..........................] - ETA: 7:07 - loss: 1.9036 - regression_loss: 1.4304 - classification_loss: 0.4732\n",
      " 84/500 [====>.........................] - ETA: 7:06 - loss: 1.8959 - regression_loss: 1.4241 - classification_loss: 0.4718\n",
      " 85/500 [====>.........................] - ETA: 7:03 - loss: 1.8908 - regression_loss: 1.4201 - classification_loss: 0.4707\n",
      " 86/500 [====>.........................] - ETA: 7:03 - loss: 1.8975 - regression_loss: 1.4248 - classification_loss: 0.4726\n",
      " 87/500 [====>.........................] - ETA: 7:03 - loss: 1.8943 - regression_loss: 1.4232 - classification_loss: 0.4711\n",
      " 88/500 [====>.........................] - ETA: 7:01 - loss: 1.9003 - regression_loss: 1.4269 - classification_loss: 0.4735\n",
      " 89/500 [====>.........................] - ETA: 7:00 - loss: 1.9040 - regression_loss: 1.4309 - classification_loss: 0.4731\n",
      " 90/500 [====>.........................] - ETA: 6:59 - loss: 1.8947 - regression_loss: 1.4244 - classification_loss: 0.4703\n",
      " 91/500 [====>.........................] - ETA: 6:56 - loss: 1.8964 - regression_loss: 1.4241 - classification_loss: 0.4723\n",
      " 92/500 [====>.........................] - ETA: 6:56 - loss: 1.9069 - regression_loss: 1.4323 - classification_loss: 0.4746\n",
      " 93/500 [====>.........................] - ETA: 6:55 - loss: 1.9133 - regression_loss: 1.4384 - classification_loss: 0.4749\n",
      " 94/500 [====>.........................] - ETA: 6:55 - loss: 1.9165 - regression_loss: 1.4408 - classification_loss: 0.4758\n",
      " 95/500 [====>.........................] - ETA: 6:54 - loss: 1.9223 - regression_loss: 1.4452 - classification_loss: 0.4770\n",
      " 96/500 [====>.........................] - ETA: 6:53 - loss: 1.9139 - regression_loss: 1.4393 - classification_loss: 0.4745\n",
      " 97/500 [====>.........................] - ETA: 6:52 - loss: 1.9185 - regression_loss: 1.4423 - classification_loss: 0.4762\n",
      " 98/500 [====>.........................] - ETA: 6:51 - loss: 1.9170 - regression_loss: 1.4420 - classification_loss: 0.4750\n",
      " 99/500 [====>.........................] - ETA: 6:49 - loss: 1.9148 - regression_loss: 1.4409 - classification_loss: 0.4738\n",
      "100/500 [=====>........................] - ETA: 6:48 - loss: 1.9197 - regression_loss: 1.4440 - classification_loss: 0.4757\n",
      "101/500 [=====>........................] - ETA: 6:46 - loss: 1.9219 - regression_loss: 1.4446 - classification_loss: 0.4773\n",
      "102/500 [=====>........................] - ETA: 6:46 - loss: 1.9261 - regression_loss: 1.4477 - classification_loss: 0.4784\n",
      "103/500 [=====>........................] - ETA: 6:44 - loss: 1.9211 - regression_loss: 1.4446 - classification_loss: 0.4766\n",
      "104/500 [=====>........................] - ETA: 6:44 - loss: 1.9307 - regression_loss: 1.4531 - classification_loss: 0.4776\n",
      "105/500 [=====>........................] - ETA: 6:42 - loss: 1.9275 - regression_loss: 1.4494 - classification_loss: 0.4781\n",
      "106/500 [=====>........................] - ETA: 6:41 - loss: 1.9303 - regression_loss: 1.4528 - classification_loss: 0.4774\n",
      "107/500 [=====>........................] - ETA: 6:41 - loss: 1.9347 - regression_loss: 1.4556 - classification_loss: 0.4791\n",
      "108/500 [=====>........................] - ETA: 6:40 - loss: 1.9450 - regression_loss: 1.4623 - classification_loss: 0.4827\n",
      "109/500 [=====>........................] - ETA: 6:39 - loss: 1.9490 - regression_loss: 1.4640 - classification_loss: 0.4850\n",
      "110/500 [=====>........................] - ETA: 6:38 - loss: 1.9443 - regression_loss: 1.4608 - classification_loss: 0.4835\n",
      "111/500 [=====>........................] - ETA: 6:37 - loss: 1.9393 - regression_loss: 1.4572 - classification_loss: 0.4820\n",
      "112/500 [=====>........................] - ETA: 6:36 - loss: 1.9444 - regression_loss: 1.4597 - classification_loss: 0.4846\n",
      "113/500 [=====>........................] - ETA: 6:35 - loss: 1.9451 - regression_loss: 1.4593 - classification_loss: 0.4858\n",
      "114/500 [=====>........................] - ETA: 6:34 - loss: 1.9432 - regression_loss: 1.4586 - classification_loss: 0.4846\n",
      "115/500 [=====>........................] - ETA: 6:33 - loss: 1.9399 - regression_loss: 1.4556 - classification_loss: 0.4843\n",
      "116/500 [=====>........................] - ETA: 6:32 - loss: 1.9426 - regression_loss: 1.4577 - classification_loss: 0.4849\n",
      "117/500 [======>.......................] - ETA: 6:31 - loss: 1.9492 - regression_loss: 1.4628 - classification_loss: 0.4864\n",
      "118/500 [======>.......................] - ETA: 6:29 - loss: 1.9535 - regression_loss: 1.4649 - classification_loss: 0.4886\n",
      "119/500 [======>.......................] - ETA: 6:29 - loss: 1.9534 - regression_loss: 1.4660 - classification_loss: 0.4875\n",
      "120/500 [======>.......................] - ETA: 6:27 - loss: 1.9461 - regression_loss: 1.4606 - classification_loss: 0.4854\n",
      "121/500 [======>.......................] - ETA: 6:27 - loss: 1.9472 - regression_loss: 1.4620 - classification_loss: 0.4853\n",
      "122/500 [======>.......................] - ETA: 6:26 - loss: 1.9542 - regression_loss: 1.4660 - classification_loss: 0.4883\n",
      "123/500 [======>.......................] - ETA: 6:25 - loss: 1.9656 - regression_loss: 1.4734 - classification_loss: 0.4922\n",
      "124/500 [======>.......................] - ETA: 6:24 - loss: 1.9703 - regression_loss: 1.4765 - classification_loss: 0.4938\n",
      "125/500 [======>.......................] - ETA: 6:30 - loss: 1.9651 - regression_loss: 1.4720 - classification_loss: 0.4931\n",
      "126/500 [======>.......................] - ETA: 6:28 - loss: 1.9627 - regression_loss: 1.4695 - classification_loss: 0.4932\n",
      "127/500 [======>.......................] - ETA: 6:28 - loss: 1.9635 - regression_loss: 1.4696 - classification_loss: 0.4939\n",
      "128/500 [======>.......................] - ETA: 6:27 - loss: 1.9646 - regression_loss: 1.4718 - classification_loss: 0.4928\n",
      "129/500 [======>.......................] - ETA: 6:26 - loss: 1.9667 - regression_loss: 1.4728 - classification_loss: 0.4939\n",
      "130/500 [======>.......................] - ETA: 6:24 - loss: 1.9664 - regression_loss: 1.4709 - classification_loss: 0.4955\n",
      "131/500 [======>.......................] - ETA: 6:23 - loss: 1.9703 - regression_loss: 1.4741 - classification_loss: 0.4962\n",
      "132/500 [======>.......................] - ETA: 6:23 - loss: 1.9733 - regression_loss: 1.4772 - classification_loss: 0.4961\n",
      "133/500 [======>.......................] - ETA: 6:21 - loss: 1.9804 - regression_loss: 1.4826 - classification_loss: 0.4978\n",
      "134/500 [=======>......................] - ETA: 6:20 - loss: 1.9744 - regression_loss: 1.4778 - classification_loss: 0.4966\n",
      "135/500 [=======>......................] - ETA: 6:19 - loss: 1.9676 - regression_loss: 1.4728 - classification_loss: 0.4948\n",
      "136/500 [=======>......................] - ETA: 6:18 - loss: 1.9621 - regression_loss: 1.4684 - classification_loss: 0.4937\n",
      "137/500 [=======>......................] - ETA: 6:17 - loss: 1.9544 - regression_loss: 1.4629 - classification_loss: 0.4916\n",
      "138/500 [=======>......................] - ETA: 6:15 - loss: 1.9564 - regression_loss: 1.4644 - classification_loss: 0.4920\n",
      "139/500 [=======>......................] - ETA: 6:14 - loss: 1.9563 - regression_loss: 1.4648 - classification_loss: 0.4915\n",
      "140/500 [=======>......................] - ETA: 6:13 - loss: 1.9588 - regression_loss: 1.4660 - classification_loss: 0.4928\n",
      "141/500 [=======>......................] - ETA: 6:12 - loss: 1.9599 - regression_loss: 1.4661 - classification_loss: 0.4938\n",
      "142/500 [=======>......................] - ETA: 6:11 - loss: 1.9611 - regression_loss: 1.4672 - classification_loss: 0.4939\n",
      "143/500 [=======>......................] - ETA: 6:10 - loss: 1.9543 - regression_loss: 1.4619 - classification_loss: 0.4924\n",
      "144/500 [=======>......................] - ETA: 6:09 - loss: 1.9525 - regression_loss: 1.4610 - classification_loss: 0.4915\n",
      "145/500 [=======>......................] - ETA: 6:08 - loss: 1.9522 - regression_loss: 1.4598 - classification_loss: 0.4924\n",
      "146/500 [=======>......................] - ETA: 6:07 - loss: 1.9502 - regression_loss: 1.4571 - classification_loss: 0.4931\n",
      "147/500 [=======>......................] - ETA: 6:06 - loss: 1.9482 - regression_loss: 1.4559 - classification_loss: 0.4923\n",
      "148/500 [=======>......................] - ETA: 6:05 - loss: 1.9449 - regression_loss: 1.4543 - classification_loss: 0.4905\n",
      "149/500 [=======>......................] - ETA: 6:04 - loss: 1.9434 - regression_loss: 1.4534 - classification_loss: 0.4900\n",
      "150/500 [========>.....................] - ETA: 6:03 - loss: 1.9406 - regression_loss: 1.4508 - classification_loss: 0.4898\n",
      "151/500 [========>.....................] - ETA: 6:01 - loss: 1.9435 - regression_loss: 1.4532 - classification_loss: 0.4904\n",
      "152/500 [========>.....................] - ETA: 6:01 - loss: 1.9404 - regression_loss: 1.4510 - classification_loss: 0.4894\n",
      "153/500 [========>.....................] - ETA: 6:00 - loss: 1.9350 - regression_loss: 1.4468 - classification_loss: 0.4882\n",
      "154/500 [========>.....................] - ETA: 5:59 - loss: 1.9366 - regression_loss: 1.4483 - classification_loss: 0.4883\n",
      "155/500 [========>.....................] - ETA: 5:58 - loss: 1.9394 - regression_loss: 1.4511 - classification_loss: 0.4882\n",
      "156/500 [========>.....................] - ETA: 5:57 - loss: 1.9425 - regression_loss: 1.4537 - classification_loss: 0.4888\n",
      "157/500 [========>.....................] - ETA: 5:55 - loss: 1.9405 - regression_loss: 1.4519 - classification_loss: 0.4886\n",
      "158/500 [========>.....................] - ETA: 5:54 - loss: 1.9404 - regression_loss: 1.4521 - classification_loss: 0.4883\n",
      "159/500 [========>.....................] - ETA: 5:54 - loss: 1.9373 - regression_loss: 1.4492 - classification_loss: 0.4881\n",
      "160/500 [========>.....................] - ETA: 5:52 - loss: 1.9345 - regression_loss: 1.4478 - classification_loss: 0.4868\n",
      "161/500 [========>.....................] - ETA: 5:52 - loss: 1.9294 - regression_loss: 1.4438 - classification_loss: 0.4856\n",
      "162/500 [========>.....................] - ETA: 5:51 - loss: 1.9293 - regression_loss: 1.4434 - classification_loss: 0.4859\n",
      "163/500 [========>.....................] - ETA: 5:50 - loss: 1.9317 - regression_loss: 1.4448 - classification_loss: 0.4868\n",
      "164/500 [========>.....................] - ETA: 5:48 - loss: 1.9290 - regression_loss: 1.4432 - classification_loss: 0.4858\n",
      "165/500 [========>.....................] - ETA: 5:47 - loss: 1.9271 - regression_loss: 1.4421 - classification_loss: 0.4851\n",
      "166/500 [========>.....................] - ETA: 5:46 - loss: 1.9223 - regression_loss: 1.4384 - classification_loss: 0.4839\n",
      "167/500 [=========>....................] - ETA: 5:45 - loss: 1.9243 - regression_loss: 1.4397 - classification_loss: 0.4846\n",
      "168/500 [=========>....................] - ETA: 5:44 - loss: 1.9263 - regression_loss: 1.4410 - classification_loss: 0.4853\n",
      "169/500 [=========>....................] - ETA: 5:43 - loss: 1.9243 - regression_loss: 1.4401 - classification_loss: 0.4842\n",
      "170/500 [=========>....................] - ETA: 5:42 - loss: 1.9276 - regression_loss: 1.4414 - classification_loss: 0.4862\n",
      "171/500 [=========>....................] - ETA: 5:41 - loss: 1.9230 - regression_loss: 1.4384 - classification_loss: 0.4846\n",
      "172/500 [=========>....................] - ETA: 5:40 - loss: 1.9220 - regression_loss: 1.4371 - classification_loss: 0.4849\n",
      "173/500 [=========>....................] - ETA: 5:38 - loss: 1.9178 - regression_loss: 1.4346 - classification_loss: 0.4833\n",
      "174/500 [=========>....................] - ETA: 5:38 - loss: 1.9153 - regression_loss: 1.4327 - classification_loss: 0.4827\n",
      "175/500 [=========>....................] - ETA: 5:37 - loss: 1.9151 - regression_loss: 1.4321 - classification_loss: 0.4830\n",
      "176/500 [=========>....................] - ETA: 5:36 - loss: 1.9149 - regression_loss: 1.4312 - classification_loss: 0.4838\n",
      "177/500 [=========>....................] - ETA: 5:35 - loss: 1.9128 - regression_loss: 1.4297 - classification_loss: 0.4831\n",
      "178/500 [=========>....................] - ETA: 5:34 - loss: 1.9125 - regression_loss: 1.4298 - classification_loss: 0.4827\n",
      "179/500 [=========>....................] - ETA: 5:33 - loss: 1.9096 - regression_loss: 1.4282 - classification_loss: 0.4815\n",
      "180/500 [=========>....................] - ETA: 5:32 - loss: 1.9093 - regression_loss: 1.4281 - classification_loss: 0.4812\n",
      "181/500 [=========>....................] - ETA: 5:30 - loss: 1.9075 - regression_loss: 1.4263 - classification_loss: 0.4812\n",
      "182/500 [=========>....................] - ETA: 5:29 - loss: 1.9119 - regression_loss: 1.4298 - classification_loss: 0.4821\n",
      "183/500 [=========>....................] - ETA: 5:28 - loss: 1.9102 - regression_loss: 1.4283 - classification_loss: 0.4819\n",
      "184/500 [==========>...................] - ETA: 5:27 - loss: 1.9103 - regression_loss: 1.4284 - classification_loss: 0.4819\n",
      "185/500 [==========>...................] - ETA: 5:26 - loss: 1.9116 - regression_loss: 1.4298 - classification_loss: 0.4818\n",
      "186/500 [==========>...................] - ETA: 5:25 - loss: 1.9109 - regression_loss: 1.4294 - classification_loss: 0.4815\n",
      "187/500 [==========>...................] - ETA: 5:24 - loss: 1.9106 - regression_loss: 1.4289 - classification_loss: 0.4817\n",
      "188/500 [==========>...................] - ETA: 5:23 - loss: 1.9065 - regression_loss: 1.4253 - classification_loss: 0.4812\n",
      "189/500 [==========>...................] - ETA: 5:22 - loss: 1.9085 - regression_loss: 1.4253 - classification_loss: 0.4833\n",
      "190/500 [==========>...................] - ETA: 5:21 - loss: 1.9121 - regression_loss: 1.4275 - classification_loss: 0.4846\n",
      "191/500 [==========>...................] - ETA: 5:20 - loss: 1.9114 - regression_loss: 1.4280 - classification_loss: 0.4834\n",
      "192/500 [==========>...................] - ETA: 5:19 - loss: 1.9144 - regression_loss: 1.4300 - classification_loss: 0.4844\n",
      "193/500 [==========>...................] - ETA: 5:18 - loss: 1.9142 - regression_loss: 1.4307 - classification_loss: 0.4835\n",
      "194/500 [==========>...................] - ETA: 5:17 - loss: 1.9136 - regression_loss: 1.4306 - classification_loss: 0.4830\n",
      "195/500 [==========>...................] - ETA: 5:16 - loss: 1.9133 - regression_loss: 1.4301 - classification_loss: 0.4832\n",
      "196/500 [==========>...................] - ETA: 5:15 - loss: 1.9102 - regression_loss: 1.4283 - classification_loss: 0.4819\n",
      "197/500 [==========>...................] - ETA: 5:14 - loss: 1.9118 - regression_loss: 1.4295 - classification_loss: 0.4823\n",
      "198/500 [==========>...................] - ETA: 5:13 - loss: 1.9148 - regression_loss: 1.4314 - classification_loss: 0.4834\n",
      "199/500 [==========>...................] - ETA: 5:12 - loss: 1.9140 - regression_loss: 1.4308 - classification_loss: 0.4833\n",
      "200/500 [===========>..................] - ETA: 5:10 - loss: 1.9150 - regression_loss: 1.4321 - classification_loss: 0.4830\n",
      "201/500 [===========>..................] - ETA: 5:09 - loss: 1.9130 - regression_loss: 1.4312 - classification_loss: 0.4817\n",
      "202/500 [===========>..................] - ETA: 5:08 - loss: 1.9143 - regression_loss: 1.4310 - classification_loss: 0.4832\n",
      "203/500 [===========>..................] - ETA: 5:07 - loss: 1.9203 - regression_loss: 1.4358 - classification_loss: 0.4845\n",
      "204/500 [===========>..................] - ETA: 5:06 - loss: 1.9146 - regression_loss: 1.4316 - classification_loss: 0.4829\n",
      "205/500 [===========>..................] - ETA: 5:05 - loss: 1.9137 - regression_loss: 1.4310 - classification_loss: 0.4827\n",
      "206/500 [===========>..................] - ETA: 5:04 - loss: 1.9141 - regression_loss: 1.4316 - classification_loss: 0.4825\n",
      "207/500 [===========>..................] - ETA: 5:04 - loss: 1.9145 - regression_loss: 1.4321 - classification_loss: 0.4824\n",
      "208/500 [===========>..................] - ETA: 5:02 - loss: 1.9176 - regression_loss: 1.4346 - classification_loss: 0.4831\n",
      "209/500 [===========>..................] - ETA: 5:01 - loss: 1.9126 - regression_loss: 1.4309 - classification_loss: 0.4817\n",
      "210/500 [===========>..................] - ETA: 5:00 - loss: 1.9134 - regression_loss: 1.4308 - classification_loss: 0.4826\n",
      "211/500 [===========>..................] - ETA: 4:59 - loss: 1.9160 - regression_loss: 1.4324 - classification_loss: 0.4835\n",
      "212/500 [===========>..................] - ETA: 4:58 - loss: 1.9135 - regression_loss: 1.4303 - classification_loss: 0.4832\n",
      "213/500 [===========>..................] - ETA: 4:57 - loss: 1.9122 - regression_loss: 1.4293 - classification_loss: 0.4829\n",
      "214/500 [===========>..................] - ETA: 4:56 - loss: 1.9140 - regression_loss: 1.4311 - classification_loss: 0.4829\n",
      "215/500 [===========>..................] - ETA: 4:55 - loss: 1.9187 - regression_loss: 1.4340 - classification_loss: 0.4847\n",
      "216/500 [===========>..................] - ETA: 4:54 - loss: 1.9217 - regression_loss: 1.4361 - classification_loss: 0.4856\n",
      "217/500 [============>.................] - ETA: 4:53 - loss: 1.9208 - regression_loss: 1.4355 - classification_loss: 0.4854\n",
      "218/500 [============>.................] - ETA: 4:52 - loss: 1.9208 - regression_loss: 1.4355 - classification_loss: 0.4853\n",
      "219/500 [============>.................] - ETA: 4:51 - loss: 1.9204 - regression_loss: 1.4357 - classification_loss: 0.4848\n",
      "220/500 [============>.................] - ETA: 4:50 - loss: 1.9206 - regression_loss: 1.4357 - classification_loss: 0.4849\n",
      "221/500 [============>.................] - ETA: 4:49 - loss: 1.9189 - regression_loss: 1.4346 - classification_loss: 0.4843\n",
      "222/500 [============>.................] - ETA: 4:48 - loss: 1.9210 - regression_loss: 1.4362 - classification_loss: 0.4848\n",
      "223/500 [============>.................] - ETA: 4:47 - loss: 1.9229 - regression_loss: 1.4377 - classification_loss: 0.4852\n",
      "224/500 [============>.................] - ETA: 4:46 - loss: 1.9221 - regression_loss: 1.4370 - classification_loss: 0.4850\n",
      "225/500 [============>.................] - ETA: 4:44 - loss: 1.9225 - regression_loss: 1.4368 - classification_loss: 0.4857\n",
      "226/500 [============>.................] - ETA: 4:43 - loss: 1.9255 - regression_loss: 1.4389 - classification_loss: 0.4865\n",
      "227/500 [============>.................] - ETA: 4:42 - loss: 1.9241 - regression_loss: 1.4384 - classification_loss: 0.4857\n",
      "228/500 [============>.................] - ETA: 4:41 - loss: 1.9213 - regression_loss: 1.4361 - classification_loss: 0.4852\n",
      "229/500 [============>.................] - ETA: 4:40 - loss: 1.9248 - regression_loss: 1.4383 - classification_loss: 0.4865\n",
      "230/500 [============>.................] - ETA: 4:39 - loss: 1.9248 - regression_loss: 1.4387 - classification_loss: 0.4861\n",
      "231/500 [============>.................] - ETA: 4:38 - loss: 1.9212 - regression_loss: 1.4361 - classification_loss: 0.4851\n",
      "232/500 [============>.................] - ETA: 4:37 - loss: 1.9228 - regression_loss: 1.4376 - classification_loss: 0.4853\n",
      "233/500 [============>.................] - ETA: 4:36 - loss: 1.9222 - regression_loss: 1.4374 - classification_loss: 0.4848\n",
      "234/500 [=============>................] - ETA: 4:35 - loss: 1.9243 - regression_loss: 1.4390 - classification_loss: 0.4853\n",
      "235/500 [=============>................] - ETA: 4:34 - loss: 1.9217 - regression_loss: 1.4373 - classification_loss: 0.4844\n",
      "236/500 [=============>................] - ETA: 4:33 - loss: 1.9222 - regression_loss: 1.4375 - classification_loss: 0.4847\n",
      "237/500 [=============>................] - ETA: 4:32 - loss: 1.9239 - regression_loss: 1.4389 - classification_loss: 0.4850\n",
      "238/500 [=============>................] - ETA: 4:31 - loss: 1.9264 - regression_loss: 1.4406 - classification_loss: 0.4858\n",
      "239/500 [=============>................] - ETA: 4:30 - loss: 1.9251 - regression_loss: 1.4398 - classification_loss: 0.4854\n",
      "240/500 [=============>................] - ETA: 4:29 - loss: 1.9259 - regression_loss: 1.4401 - classification_loss: 0.4859\n",
      "241/500 [=============>................] - ETA: 4:28 - loss: 1.9277 - regression_loss: 1.4408 - classification_loss: 0.4869\n",
      "242/500 [=============>................] - ETA: 4:27 - loss: 1.9242 - regression_loss: 1.4384 - classification_loss: 0.4858\n",
      "243/500 [=============>................] - ETA: 4:26 - loss: 1.9263 - regression_loss: 1.4406 - classification_loss: 0.4857\n",
      "244/500 [=============>................] - ETA: 4:25 - loss: 1.9256 - regression_loss: 1.4402 - classification_loss: 0.4855\n",
      "245/500 [=============>................] - ETA: 4:24 - loss: 1.9250 - regression_loss: 1.4399 - classification_loss: 0.4851\n",
      "246/500 [=============>................] - ETA: 4:23 - loss: 1.9267 - regression_loss: 1.4416 - classification_loss: 0.4851\n",
      "247/500 [=============>................] - ETA: 4:22 - loss: 1.9252 - regression_loss: 1.4404 - classification_loss: 0.4849\n",
      "248/500 [=============>................] - ETA: 4:21 - loss: 1.9249 - regression_loss: 1.4408 - classification_loss: 0.4841\n",
      "249/500 [=============>................] - ETA: 4:19 - loss: 1.9252 - regression_loss: 1.4410 - classification_loss: 0.4842\n",
      "250/500 [==============>...............] - ETA: 4:18 - loss: 1.9242 - regression_loss: 1.4392 - classification_loss: 0.4850\n",
      "251/500 [==============>...............] - ETA: 4:17 - loss: 1.9267 - regression_loss: 1.4407 - classification_loss: 0.4860\n",
      "252/500 [==============>...............] - ETA: 4:16 - loss: 1.9296 - regression_loss: 1.4423 - classification_loss: 0.4872\n",
      "253/500 [==============>...............] - ETA: 4:15 - loss: 1.9273 - regression_loss: 1.4405 - classification_loss: 0.4868\n",
      "254/500 [==============>...............] - ETA: 4:14 - loss: 1.9262 - regression_loss: 1.4399 - classification_loss: 0.4864\n",
      "255/500 [==============>...............] - ETA: 4:13 - loss: 1.9227 - regression_loss: 1.4375 - classification_loss: 0.4852\n",
      "256/500 [==============>...............] - ETA: 4:12 - loss: 1.9220 - regression_loss: 1.4370 - classification_loss: 0.4850\n",
      "257/500 [==============>...............] - ETA: 4:11 - loss: 1.9199 - regression_loss: 1.4352 - classification_loss: 0.4846\n",
      "258/500 [==============>...............] - ETA: 4:10 - loss: 1.9204 - regression_loss: 1.4356 - classification_loss: 0.4848\n",
      "259/500 [==============>...............] - ETA: 4:09 - loss: 1.9213 - regression_loss: 1.4363 - classification_loss: 0.4851\n",
      "260/500 [==============>...............] - ETA: 4:08 - loss: 1.9222 - regression_loss: 1.4363 - classification_loss: 0.4859\n",
      "261/500 [==============>...............] - ETA: 4:07 - loss: 1.9222 - regression_loss: 1.4357 - classification_loss: 0.4865\n",
      "262/500 [==============>...............] - ETA: 4:06 - loss: 1.9220 - regression_loss: 1.4354 - classification_loss: 0.4866\n",
      "263/500 [==============>...............] - ETA: 4:05 - loss: 1.9265 - regression_loss: 1.4385 - classification_loss: 0.4880\n",
      "264/500 [==============>...............] - ETA: 4:04 - loss: 1.9290 - regression_loss: 1.4398 - classification_loss: 0.4892\n",
      "265/500 [==============>...............] - ETA: 4:03 - loss: 1.9311 - regression_loss: 1.4412 - classification_loss: 0.4899\n",
      "266/500 [==============>...............] - ETA: 4:02 - loss: 1.9328 - regression_loss: 1.4418 - classification_loss: 0.4910\n",
      "267/500 [===============>..............] - ETA: 4:01 - loss: 1.9304 - regression_loss: 1.4401 - classification_loss: 0.4903\n",
      "268/500 [===============>..............] - ETA: 4:00 - loss: 1.9312 - regression_loss: 1.4402 - classification_loss: 0.4911\n",
      "269/500 [===============>..............] - ETA: 3:59 - loss: 1.9297 - regression_loss: 1.4393 - classification_loss: 0.4904\n",
      "270/500 [===============>..............] - ETA: 3:58 - loss: 1.9309 - regression_loss: 1.4400 - classification_loss: 0.4909\n",
      "271/500 [===============>..............] - ETA: 3:56 - loss: 1.9297 - regression_loss: 1.4384 - classification_loss: 0.4913\n",
      "272/500 [===============>..............] - ETA: 3:55 - loss: 1.9315 - regression_loss: 1.4396 - classification_loss: 0.4919\n",
      "273/500 [===============>..............] - ETA: 3:54 - loss: 1.9326 - regression_loss: 1.4407 - classification_loss: 0.4918\n",
      "274/500 [===============>..............] - ETA: 3:53 - loss: 1.9325 - regression_loss: 1.4402 - classification_loss: 0.4923\n",
      "275/500 [===============>..............] - ETA: 3:52 - loss: 1.9298 - regression_loss: 1.4383 - classification_loss: 0.4915\n",
      "276/500 [===============>..............] - ETA: 3:51 - loss: 1.9281 - regression_loss: 1.4368 - classification_loss: 0.4913\n",
      "277/500 [===============>..............] - ETA: 3:50 - loss: 1.9284 - regression_loss: 1.4365 - classification_loss: 0.4918\n",
      "278/500 [===============>..............] - ETA: 3:49 - loss: 1.9300 - regression_loss: 1.4377 - classification_loss: 0.4923\n",
      "279/500 [===============>..............] - ETA: 3:48 - loss: 1.9306 - regression_loss: 1.4384 - classification_loss: 0.4922\n",
      "280/500 [===============>..............] - ETA: 3:47 - loss: 1.9328 - regression_loss: 1.4406 - classification_loss: 0.4922\n",
      "281/500 [===============>..............] - ETA: 3:46 - loss: 1.9328 - regression_loss: 1.4401 - classification_loss: 0.4926\n",
      "282/500 [===============>..............] - ETA: 3:45 - loss: 1.9312 - regression_loss: 1.4389 - classification_loss: 0.4922\n",
      "283/500 [===============>..............] - ETA: 3:44 - loss: 1.9311 - regression_loss: 1.4390 - classification_loss: 0.4921\n",
      "284/500 [================>.............] - ETA: 3:43 - loss: 1.9273 - regression_loss: 1.4361 - classification_loss: 0.4912\n",
      "285/500 [================>.............] - ETA: 3:42 - loss: 1.9289 - regression_loss: 1.4377 - classification_loss: 0.4912\n",
      "286/500 [================>.............] - ETA: 3:41 - loss: 1.9313 - regression_loss: 1.4386 - classification_loss: 0.4927\n",
      "287/500 [================>.............] - ETA: 3:40 - loss: 1.9326 - regression_loss: 1.4393 - classification_loss: 0.4933\n",
      "288/500 [================>.............] - ETA: 3:39 - loss: 1.9329 - regression_loss: 1.4396 - classification_loss: 0.4932\n",
      "289/500 [================>.............] - ETA: 3:38 - loss: 1.9311 - regression_loss: 1.4382 - classification_loss: 0.4928\n",
      "290/500 [================>.............] - ETA: 3:37 - loss: 1.9303 - regression_loss: 1.4380 - classification_loss: 0.4923\n",
      "291/500 [================>.............] - ETA: 3:36 - loss: 1.9308 - regression_loss: 1.4388 - classification_loss: 0.4919\n",
      "292/500 [================>.............] - ETA: 3:35 - loss: 1.9283 - regression_loss: 1.4368 - classification_loss: 0.4914\n",
      "293/500 [================>.............] - ETA: 3:34 - loss: 1.9267 - regression_loss: 1.4358 - classification_loss: 0.4909\n",
      "294/500 [================>.............] - ETA: 3:33 - loss: 1.9291 - regression_loss: 1.4379 - classification_loss: 0.4912\n",
      "295/500 [================>.............] - ETA: 3:31 - loss: 1.9286 - regression_loss: 1.4375 - classification_loss: 0.4911\n",
      "296/500 [================>.............] - ETA: 3:30 - loss: 1.9288 - regression_loss: 1.4374 - classification_loss: 0.4913\n",
      "297/500 [================>.............] - ETA: 3:29 - loss: 1.9276 - regression_loss: 1.4367 - classification_loss: 0.4909\n",
      "298/500 [================>.............] - ETA: 3:28 - loss: 1.9262 - regression_loss: 1.4354 - classification_loss: 0.4908\n",
      "299/500 [================>.............] - ETA: 3:27 - loss: 1.9262 - regression_loss: 1.4355 - classification_loss: 0.4908\n",
      "300/500 [=================>............] - ETA: 3:26 - loss: 1.9237 - regression_loss: 1.4334 - classification_loss: 0.4902\n",
      "301/500 [=================>............] - ETA: 3:25 - loss: 1.9211 - regression_loss: 1.4317 - classification_loss: 0.4893\n",
      "302/500 [=================>............] - ETA: 3:24 - loss: 1.9204 - regression_loss: 1.4312 - classification_loss: 0.4892\n",
      "303/500 [=================>............] - ETA: 3:23 - loss: 1.9197 - regression_loss: 1.4308 - classification_loss: 0.4888\n",
      "304/500 [=================>............] - ETA: 3:22 - loss: 1.9176 - regression_loss: 1.4292 - classification_loss: 0.4883\n",
      "305/500 [=================>............] - ETA: 3:21 - loss: 1.9176 - regression_loss: 1.4292 - classification_loss: 0.4884\n",
      "306/500 [=================>............] - ETA: 3:20 - loss: 1.9141 - regression_loss: 1.4266 - classification_loss: 0.4875\n",
      "307/500 [=================>............] - ETA: 3:19 - loss: 1.9171 - regression_loss: 1.4283 - classification_loss: 0.4888\n",
      "308/500 [=================>............] - ETA: 3:18 - loss: 1.9160 - regression_loss: 1.4276 - classification_loss: 0.4884\n",
      "309/500 [=================>............] - ETA: 3:17 - loss: 1.9139 - regression_loss: 1.4260 - classification_loss: 0.4879\n",
      "310/500 [=================>............] - ETA: 3:16 - loss: 1.9139 - regression_loss: 1.4261 - classification_loss: 0.4878\n",
      "311/500 [=================>............] - ETA: 3:15 - loss: 1.9127 - regression_loss: 1.4254 - classification_loss: 0.4873\n",
      "312/500 [=================>............] - ETA: 3:14 - loss: 1.9138 - regression_loss: 1.4265 - classification_loss: 0.4873\n",
      "313/500 [=================>............] - ETA: 3:13 - loss: 1.9116 - regression_loss: 1.4249 - classification_loss: 0.4867\n",
      "314/500 [=================>............] - ETA: 3:12 - loss: 1.9087 - regression_loss: 1.4226 - classification_loss: 0.4860\n",
      "315/500 [=================>............] - ETA: 3:11 - loss: 1.9084 - regression_loss: 1.4223 - classification_loss: 0.4860\n",
      "316/500 [=================>............] - ETA: 3:10 - loss: 1.9070 - regression_loss: 1.4217 - classification_loss: 0.4853\n",
      "317/500 [==================>...........] - ETA: 3:09 - loss: 1.9070 - regression_loss: 1.4220 - classification_loss: 0.4850\n",
      "318/500 [==================>...........] - ETA: 3:08 - loss: 1.9037 - regression_loss: 1.4194 - classification_loss: 0.4843\n",
      "319/500 [==================>...........] - ETA: 3:07 - loss: 1.9074 - regression_loss: 1.4218 - classification_loss: 0.4856\n",
      "320/500 [==================>...........] - ETA: 3:06 - loss: 1.9092 - regression_loss: 1.4233 - classification_loss: 0.4859\n",
      "321/500 [==================>...........] - ETA: 3:05 - loss: 1.9096 - regression_loss: 1.4237 - classification_loss: 0.4859\n",
      "322/500 [==================>...........] - ETA: 3:04 - loss: 1.9113 - regression_loss: 1.4244 - classification_loss: 0.4868\n",
      "323/500 [==================>...........] - ETA: 3:03 - loss: 1.9092 - regression_loss: 1.4233 - classification_loss: 0.4859\n",
      "324/500 [==================>...........] - ETA: 3:02 - loss: 1.9076 - regression_loss: 1.4218 - classification_loss: 0.4858\n",
      "325/500 [==================>...........] - ETA: 3:01 - loss: 1.9093 - regression_loss: 1.4232 - classification_loss: 0.4861\n",
      "326/500 [==================>...........] - ETA: 3:00 - loss: 1.9125 - regression_loss: 1.4252 - classification_loss: 0.4873\n",
      "327/500 [==================>...........] - ETA: 2:59 - loss: 1.9134 - regression_loss: 1.4261 - classification_loss: 0.4873\n",
      "328/500 [==================>...........] - ETA: 2:57 - loss: 1.9135 - regression_loss: 1.4258 - classification_loss: 0.4877\n",
      "329/500 [==================>...........] - ETA: 2:56 - loss: 1.9124 - regression_loss: 1.4250 - classification_loss: 0.4874\n",
      "330/500 [==================>...........] - ETA: 2:55 - loss: 1.9126 - regression_loss: 1.4252 - classification_loss: 0.4874\n",
      "331/500 [==================>...........] - ETA: 2:54 - loss: 1.9140 - regression_loss: 1.4265 - classification_loss: 0.4875\n",
      "332/500 [==================>...........] - ETA: 2:53 - loss: 1.9145 - regression_loss: 1.4271 - classification_loss: 0.4875\n",
      "333/500 [==================>...........] - ETA: 2:52 - loss: 1.9160 - regression_loss: 1.4286 - classification_loss: 0.4874\n",
      "334/500 [===================>..........] - ETA: 2:51 - loss: 1.9141 - regression_loss: 1.4271 - classification_loss: 0.4870\n",
      "335/500 [===================>..........] - ETA: 2:50 - loss: 1.9132 - regression_loss: 1.4266 - classification_loss: 0.4866\n",
      "336/500 [===================>..........] - ETA: 2:49 - loss: 1.9119 - regression_loss: 1.4254 - classification_loss: 0.4865\n",
      "337/500 [===================>..........] - ETA: 2:48 - loss: 1.9091 - regression_loss: 1.4233 - classification_loss: 0.4858\n",
      "338/500 [===================>..........] - ETA: 2:47 - loss: 1.9090 - regression_loss: 1.4233 - classification_loss: 0.4857\n",
      "339/500 [===================>..........] - ETA: 2:46 - loss: 1.9089 - regression_loss: 1.4233 - classification_loss: 0.4856\n",
      "340/500 [===================>..........] - ETA: 2:45 - loss: 1.9105 - regression_loss: 1.4251 - classification_loss: 0.4853\n",
      "341/500 [===================>..........] - ETA: 2:44 - loss: 1.9114 - regression_loss: 1.4258 - classification_loss: 0.4856\n",
      "342/500 [===================>..........] - ETA: 2:43 - loss: 1.9105 - regression_loss: 1.4249 - classification_loss: 0.4856\n",
      "343/500 [===================>..........] - ETA: 2:42 - loss: 1.9103 - regression_loss: 1.4244 - classification_loss: 0.4859\n",
      "344/500 [===================>..........] - ETA: 2:41 - loss: 1.9137 - regression_loss: 1.4265 - classification_loss: 0.4873\n",
      "345/500 [===================>..........] - ETA: 2:40 - loss: 1.9142 - regression_loss: 1.4264 - classification_loss: 0.4878\n",
      "346/500 [===================>..........] - ETA: 2:39 - loss: 1.9125 - regression_loss: 1.4251 - classification_loss: 0.4875\n",
      "347/500 [===================>..........] - ETA: 2:38 - loss: 1.9116 - regression_loss: 1.4244 - classification_loss: 0.4871\n",
      "348/500 [===================>..........] - ETA: 2:37 - loss: 1.9141 - regression_loss: 1.4262 - classification_loss: 0.4879\n",
      "349/500 [===================>..........] - ETA: 2:36 - loss: 1.9135 - regression_loss: 1.4255 - classification_loss: 0.4880\n",
      "350/500 [====================>.........] - ETA: 2:35 - loss: 1.9126 - regression_loss: 1.4250 - classification_loss: 0.4877\n",
      "351/500 [====================>.........] - ETA: 2:34 - loss: 1.9130 - regression_loss: 1.4255 - classification_loss: 0.4876\n",
      "352/500 [====================>.........] - ETA: 2:33 - loss: 1.9114 - regression_loss: 1.4243 - classification_loss: 0.4871\n",
      "353/500 [====================>.........] - ETA: 2:32 - loss: 1.9101 - regression_loss: 1.4235 - classification_loss: 0.4866\n",
      "354/500 [====================>.........] - ETA: 2:31 - loss: 1.9075 - regression_loss: 1.4215 - classification_loss: 0.4860\n",
      "355/500 [====================>.........] - ETA: 2:30 - loss: 1.9066 - regression_loss: 1.4211 - classification_loss: 0.4855\n",
      "356/500 [====================>.........] - ETA: 2:29 - loss: 1.9071 - regression_loss: 1.4212 - classification_loss: 0.4859\n",
      "357/500 [====================>.........] - ETA: 2:27 - loss: 1.9092 - regression_loss: 1.4227 - classification_loss: 0.4865\n",
      "358/500 [====================>.........] - ETA: 2:26 - loss: 1.9134 - regression_loss: 1.4252 - classification_loss: 0.4882\n",
      "359/500 [====================>.........] - ETA: 2:25 - loss: 1.9144 - regression_loss: 1.4253 - classification_loss: 0.4890\n",
      "360/500 [====================>.........] - ETA: 2:24 - loss: 1.9134 - regression_loss: 1.4248 - classification_loss: 0.4886\n",
      "361/500 [====================>.........] - ETA: 2:23 - loss: 1.9138 - regression_loss: 1.4251 - classification_loss: 0.4887\n",
      "362/500 [====================>.........] - ETA: 2:22 - loss: 1.9121 - regression_loss: 1.4238 - classification_loss: 0.4883\n",
      "363/500 [====================>.........] - ETA: 2:21 - loss: 1.9117 - regression_loss: 1.4234 - classification_loss: 0.4882\n",
      "364/500 [====================>.........] - ETA: 2:20 - loss: 1.9145 - regression_loss: 1.4252 - classification_loss: 0.4893\n",
      "365/500 [====================>.........] - ETA: 2:19 - loss: 1.9147 - regression_loss: 1.4256 - classification_loss: 0.4891\n",
      "366/500 [====================>.........] - ETA: 2:18 - loss: 1.9164 - regression_loss: 1.4268 - classification_loss: 0.4897\n",
      "367/500 [=====================>........] - ETA: 2:17 - loss: 1.9154 - regression_loss: 1.4262 - classification_loss: 0.4891\n",
      "368/500 [=====================>........] - ETA: 2:16 - loss: 1.9177 - regression_loss: 1.4279 - classification_loss: 0.4898\n",
      "369/500 [=====================>........] - ETA: 2:15 - loss: 1.9181 - regression_loss: 1.4284 - classification_loss: 0.4896\n",
      "370/500 [=====================>........] - ETA: 2:14 - loss: 1.9175 - regression_loss: 1.4280 - classification_loss: 0.4896\n",
      "371/500 [=====================>........] - ETA: 2:13 - loss: 1.9177 - regression_loss: 1.4286 - classification_loss: 0.4892\n",
      "372/500 [=====================>........] - ETA: 2:12 - loss: 1.9199 - regression_loss: 1.4299 - classification_loss: 0.4900\n",
      "373/500 [=====================>........] - ETA: 2:11 - loss: 1.9210 - regression_loss: 1.4307 - classification_loss: 0.4903\n",
      "374/500 [=====================>........] - ETA: 2:10 - loss: 1.9201 - regression_loss: 1.4301 - classification_loss: 0.4900\n",
      "375/500 [=====================>........] - ETA: 2:09 - loss: 1.9209 - regression_loss: 1.4310 - classification_loss: 0.4899\n",
      "376/500 [=====================>........] - ETA: 2:08 - loss: 1.9217 - regression_loss: 1.4316 - classification_loss: 0.4901\n",
      "377/500 [=====================>........] - ETA: 2:07 - loss: 1.9211 - regression_loss: 1.4315 - classification_loss: 0.4896\n",
      "378/500 [=====================>........] - ETA: 2:06 - loss: 1.9194 - regression_loss: 1.4300 - classification_loss: 0.4894\n",
      "379/500 [=====================>........] - ETA: 2:05 - loss: 1.9183 - regression_loss: 1.4290 - classification_loss: 0.4893\n",
      "380/500 [=====================>........] - ETA: 2:04 - loss: 1.9170 - regression_loss: 1.4283 - classification_loss: 0.4887\n",
      "381/500 [=====================>........] - ETA: 2:03 - loss: 1.9166 - regression_loss: 1.4279 - classification_loss: 0.4887\n",
      "382/500 [=====================>........] - ETA: 2:02 - loss: 1.9141 - regression_loss: 1.4260 - classification_loss: 0.4881\n",
      "383/500 [=====================>........] - ETA: 2:01 - loss: 1.9155 - regression_loss: 1.4270 - classification_loss: 0.4886\n",
      "384/500 [======================>.......] - ETA: 1:59 - loss: 1.9145 - regression_loss: 1.4265 - classification_loss: 0.4880\n",
      "385/500 [======================>.......] - ETA: 1:58 - loss: 1.9149 - regression_loss: 1.4270 - classification_loss: 0.4879\n",
      "386/500 [======================>.......] - ETA: 1:57 - loss: 1.9168 - regression_loss: 1.4285 - classification_loss: 0.4883\n",
      "387/500 [======================>.......] - ETA: 1:56 - loss: 1.9155 - regression_loss: 1.4273 - classification_loss: 0.4882\n",
      "388/500 [======================>.......] - ETA: 1:55 - loss: 1.9160 - regression_loss: 1.4276 - classification_loss: 0.4884\n",
      "389/500 [======================>.......] - ETA: 1:54 - loss: 1.9154 - regression_loss: 1.4268 - classification_loss: 0.4886\n",
      "390/500 [======================>.......] - ETA: 1:53 - loss: 1.9153 - regression_loss: 1.4268 - classification_loss: 0.4884\n",
      "391/500 [======================>.......] - ETA: 1:52 - loss: 1.9129 - regression_loss: 1.4250 - classification_loss: 0.4878\n",
      "392/500 [======================>.......] - ETA: 1:51 - loss: 1.9129 - regression_loss: 1.4250 - classification_loss: 0.4880\n",
      "393/500 [======================>.......] - ETA: 1:50 - loss: 1.9111 - regression_loss: 1.4237 - classification_loss: 0.4874\n",
      "394/500 [======================>.......] - ETA: 1:49 - loss: 1.9124 - regression_loss: 1.4238 - classification_loss: 0.4885\n",
      "395/500 [======================>.......] - ETA: 1:48 - loss: 1.9106 - regression_loss: 1.4225 - classification_loss: 0.4881\n",
      "396/500 [======================>.......] - ETA: 1:47 - loss: 1.9092 - regression_loss: 1.4218 - classification_loss: 0.4874\n",
      "397/500 [======================>.......] - ETA: 1:46 - loss: 1.9079 - regression_loss: 1.4206 - classification_loss: 0.4873\n",
      "398/500 [======================>.......] - ETA: 1:45 - loss: 1.9087 - regression_loss: 1.4208 - classification_loss: 0.4878\n",
      "399/500 [======================>.......] - ETA: 1:44 - loss: 1.9088 - regression_loss: 1.4202 - classification_loss: 0.4886\n",
      "400/500 [=======================>......] - ETA: 1:43 - loss: 1.9089 - regression_loss: 1.4203 - classification_loss: 0.4886\n",
      "401/500 [=======================>......] - ETA: 1:42 - loss: 1.9093 - regression_loss: 1.4207 - classification_loss: 0.4886\n",
      "402/500 [=======================>......] - ETA: 1:41 - loss: 1.9102 - regression_loss: 1.4214 - classification_loss: 0.4888\n",
      "403/500 [=======================>......] - ETA: 1:40 - loss: 1.9113 - regression_loss: 1.4224 - classification_loss: 0.4888\n",
      "404/500 [=======================>......] - ETA: 1:39 - loss: 1.9118 - regression_loss: 1.4231 - classification_loss: 0.4887\n",
      "405/500 [=======================>......] - ETA: 1:38 - loss: 1.9101 - regression_loss: 1.4217 - classification_loss: 0.4884\n",
      "406/500 [=======================>......] - ETA: 1:37 - loss: 1.9094 - regression_loss: 1.4215 - classification_loss: 0.4880\n",
      "407/500 [=======================>......] - ETA: 1:36 - loss: 1.9101 - regression_loss: 1.4222 - classification_loss: 0.4878\n",
      "408/500 [=======================>......] - ETA: 1:35 - loss: 1.9125 - regression_loss: 1.4232 - classification_loss: 0.4893\n",
      "409/500 [=======================>......] - ETA: 1:34 - loss: 1.9141 - regression_loss: 1.4243 - classification_loss: 0.4897\n",
      "410/500 [=======================>......] - ETA: 1:33 - loss: 1.9138 - regression_loss: 1.4243 - classification_loss: 0.4896\n",
      "411/500 [=======================>......] - ETA: 1:32 - loss: 1.9136 - regression_loss: 1.4244 - classification_loss: 0.4893\n",
      "412/500 [=======================>......] - ETA: 1:31 - loss: 1.9131 - regression_loss: 1.4237 - classification_loss: 0.4894\n",
      "413/500 [=======================>......] - ETA: 1:30 - loss: 1.9102 - regression_loss: 1.4217 - classification_loss: 0.4885\n",
      "414/500 [=======================>......] - ETA: 1:28 - loss: 1.9117 - regression_loss: 1.4224 - classification_loss: 0.4893\n",
      "415/500 [=======================>......] - ETA: 1:27 - loss: 1.9095 - regression_loss: 1.4208 - classification_loss: 0.4887\n",
      "416/500 [=======================>......] - ETA: 1:26 - loss: 1.9092 - regression_loss: 1.4205 - classification_loss: 0.4887\n",
      "417/500 [========================>.....] - ETA: 1:25 - loss: 1.9071 - regression_loss: 1.4187 - classification_loss: 0.4883\n",
      "418/500 [========================>.....] - ETA: 1:24 - loss: 1.9084 - regression_loss: 1.4196 - classification_loss: 0.4888\n",
      "419/500 [========================>.....] - ETA: 1:23 - loss: 1.9084 - regression_loss: 1.4195 - classification_loss: 0.4889\n",
      "420/500 [========================>.....] - ETA: 1:22 - loss: 1.9075 - regression_loss: 1.4189 - classification_loss: 0.4886\n",
      "421/500 [========================>.....] - ETA: 1:21 - loss: 1.9089 - regression_loss: 1.4200 - classification_loss: 0.4889\n",
      "422/500 [========================>.....] - ETA: 1:20 - loss: 1.9087 - regression_loss: 1.4204 - classification_loss: 0.4883\n",
      "423/500 [========================>.....] - ETA: 1:19 - loss: 1.9070 - regression_loss: 1.4190 - classification_loss: 0.4880\n",
      "424/500 [========================>.....] - ETA: 1:18 - loss: 1.9070 - regression_loss: 1.4192 - classification_loss: 0.4878\n",
      "425/500 [========================>.....] - ETA: 1:17 - loss: 1.9073 - regression_loss: 1.4194 - classification_loss: 0.4880\n",
      "426/500 [========================>.....] - ETA: 1:16 - loss: 1.9071 - regression_loss: 1.4196 - classification_loss: 0.4875\n",
      "427/500 [========================>.....] - ETA: 1:15 - loss: 1.9054 - regression_loss: 1.4183 - classification_loss: 0.4872\n",
      "428/500 [========================>.....] - ETA: 1:14 - loss: 1.9077 - regression_loss: 1.4199 - classification_loss: 0.4878\n",
      "429/500 [========================>.....] - ETA: 1:13 - loss: 1.9056 - regression_loss: 1.4185 - classification_loss: 0.4871\n",
      "430/500 [========================>.....] - ETA: 1:12 - loss: 1.9055 - regression_loss: 1.4180 - classification_loss: 0.4875\n",
      "431/500 [========================>.....] - ETA: 1:11 - loss: 1.9068 - regression_loss: 1.4193 - classification_loss: 0.4875\n",
      "432/500 [========================>.....] - ETA: 1:10 - loss: 1.9056 - regression_loss: 1.4183 - classification_loss: 0.4873\n",
      "433/500 [========================>.....] - ETA: 1:09 - loss: 1.9073 - regression_loss: 1.4193 - classification_loss: 0.4880\n",
      "434/500 [=========================>....] - ETA: 1:08 - loss: 1.9070 - regression_loss: 1.4191 - classification_loss: 0.4879\n",
      "435/500 [=========================>....] - ETA: 1:07 - loss: 1.9064 - regression_loss: 1.4184 - classification_loss: 0.4881\n",
      "436/500 [=========================>....] - ETA: 1:06 - loss: 1.9078 - regression_loss: 1.4193 - classification_loss: 0.4884\n",
      "437/500 [=========================>....] - ETA: 1:05 - loss: 1.9075 - regression_loss: 1.4194 - classification_loss: 0.4881\n",
      "438/500 [=========================>....] - ETA: 1:04 - loss: 1.9095 - regression_loss: 1.4211 - classification_loss: 0.4884\n",
      "439/500 [=========================>....] - ETA: 1:03 - loss: 1.9087 - regression_loss: 1.4204 - classification_loss: 0.4882\n",
      "440/500 [=========================>....] - ETA: 1:02 - loss: 1.9081 - regression_loss: 1.4201 - classification_loss: 0.4880\n",
      "441/500 [=========================>....] - ETA: 1:01 - loss: 1.9089 - regression_loss: 1.4206 - classification_loss: 0.4883\n",
      "442/500 [=========================>....] - ETA: 1:00 - loss: 1.9084 - regression_loss: 1.4204 - classification_loss: 0.4880\n",
      "443/500 [=========================>....] - ETA: 59s - loss: 1.9103 - regression_loss: 1.4214 - classification_loss: 0.4888 \n",
      "444/500 [=========================>....] - ETA: 58s - loss: 1.9096 - regression_loss: 1.4210 - classification_loss: 0.4886\n",
      "445/500 [=========================>....] - ETA: 56s - loss: 1.9082 - regression_loss: 1.4200 - classification_loss: 0.4882\n",
      "446/500 [=========================>....] - ETA: 55s - loss: 1.9090 - regression_loss: 1.4202 - classification_loss: 0.4887\n",
      "447/500 [=========================>....] - ETA: 54s - loss: 1.9093 - regression_loss: 1.4206 - classification_loss: 0.4887\n",
      "448/500 [=========================>....] - ETA: 53s - loss: 1.9098 - regression_loss: 1.4211 - classification_loss: 0.4887\n",
      "449/500 [=========================>....] - ETA: 52s - loss: 1.9094 - regression_loss: 1.4210 - classification_loss: 0.4884\n",
      "450/500 [==========================>...] - ETA: 51s - loss: 1.9074 - regression_loss: 1.4197 - classification_loss: 0.4877\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 1.9061 - regression_loss: 1.4187 - classification_loss: 0.4874\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 1.9058 - regression_loss: 1.4187 - classification_loss: 0.4871\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.9056 - regression_loss: 1.4189 - classification_loss: 0.4867\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.9049 - regression_loss: 1.4185 - classification_loss: 0.4864\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.9043 - regression_loss: 1.4180 - classification_loss: 0.4863\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.9048 - regression_loss: 1.4183 - classification_loss: 0.4865\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.9047 - regression_loss: 1.4182 - classification_loss: 0.4865\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.9044 - regression_loss: 1.4181 - classification_loss: 0.4863\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.9032 - regression_loss: 1.4174 - classification_loss: 0.4859\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.9047 - regression_loss: 1.4180 - classification_loss: 0.4867\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 1.9047 - regression_loss: 1.4180 - classification_loss: 0.4867\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 1.9038 - regression_loss: 1.4173 - classification_loss: 0.4865\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 1.9043 - regression_loss: 1.4179 - classification_loss: 0.4864\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 1.9054 - regression_loss: 1.4186 - classification_loss: 0.4868\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 1.9061 - regression_loss: 1.4195 - classification_loss: 0.4866\n",
      "466/500 [==========================>...] - ETA: 35s - loss: 1.9044 - regression_loss: 1.4182 - classification_loss: 0.4862\n",
      "467/500 [===========================>..] - ETA: 34s - loss: 1.9037 - regression_loss: 1.4177 - classification_loss: 0.4860\n",
      "468/500 [===========================>..] - ETA: 33s - loss: 1.9037 - regression_loss: 1.4179 - classification_loss: 0.4858\n",
      "469/500 [===========================>..] - ETA: 32s - loss: 1.9020 - regression_loss: 1.4165 - classification_loss: 0.4855\n",
      "470/500 [===========================>..] - ETA: 31s - loss: 1.9010 - regression_loss: 1.4158 - classification_loss: 0.4852\n",
      "471/500 [===========================>..] - ETA: 30s - loss: 1.9012 - regression_loss: 1.4160 - classification_loss: 0.4852\n",
      "472/500 [===========================>..] - ETA: 29s - loss: 1.9012 - regression_loss: 1.4158 - classification_loss: 0.4854\n",
      "473/500 [===========================>..] - ETA: 27s - loss: 1.9016 - regression_loss: 1.4161 - classification_loss: 0.4855\n",
      "474/500 [===========================>..] - ETA: 26s - loss: 1.9003 - regression_loss: 1.4152 - classification_loss: 0.4850\n",
      "475/500 [===========================>..] - ETA: 25s - loss: 1.9002 - regression_loss: 1.4153 - classification_loss: 0.4849\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 1.9005 - regression_loss: 1.4157 - classification_loss: 0.4847\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.9018 - regression_loss: 1.4171 - classification_loss: 0.4847\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.9021 - regression_loss: 1.4174 - classification_loss: 0.4847\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.9017 - regression_loss: 1.4170 - classification_loss: 0.4847\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.9033 - regression_loss: 1.4179 - classification_loss: 0.4854\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.9023 - regression_loss: 1.4171 - classification_loss: 0.4852\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.9019 - regression_loss: 1.4170 - classification_loss: 0.4849\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.9026 - regression_loss: 1.4173 - classification_loss: 0.4853\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.9023 - regression_loss: 1.4171 - classification_loss: 0.4851\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.9040 - regression_loss: 1.4180 - classification_loss: 0.4860\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.9046 - regression_loss: 1.4187 - classification_loss: 0.4858\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.9046 - regression_loss: 1.4190 - classification_loss: 0.4856\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.9030 - regression_loss: 1.4179 - classification_loss: 0.4852\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.9031 - regression_loss: 1.4177 - classification_loss: 0.4854\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.9019 - regression_loss: 1.4167 - classification_loss: 0.4851\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.9003 - regression_loss: 1.4156 - classification_loss: 0.4847 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.9009 - regression_loss: 1.4159 - classification_loss: 0.4849\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.9014 - regression_loss: 1.4165 - classification_loss: 0.4848\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.9001 - regression_loss: 1.4156 - classification_loss: 0.4845\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.9019 - regression_loss: 1.4163 - classification_loss: 0.4856\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.9026 - regression_loss: 1.4165 - classification_loss: 0.4861\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.9023 - regression_loss: 1.4162 - classification_loss: 0.4861\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.9016 - regression_loss: 1.4158 - classification_loss: 0.4859\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.9005 - regression_loss: 1.4149 - classification_loss: 0.4856\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9002 - regression_loss: 1.4144 - classification_loss: 0.4858\n",
      "Epoch 00021: saving model to ./snapshots\\resnet50_csv_21.h5\n",
      "\n",
      "500/500 [==============================] - 521s 1s/step - loss: 1.9002 - regression_loss: 1.4144 - classification_loss: 0.4858\n",
      "Epoch 22/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 2.5627 - regression_loss: 2.1743 - classification_loss: 0.3884\n",
      "  2/500 [..............................] - ETA: 4:05 - loss: 1.9377 - regression_loss: 1.5704 - classification_loss: 0.3672\n",
      "  3/500 [..............................] - ETA: 5:26 - loss: 1.9150 - regression_loss: 1.5554 - classification_loss: 0.3596\n",
      "  4/500 [..............................] - ETA: 6:06 - loss: 1.7884 - regression_loss: 1.4513 - classification_loss: 0.3371\n",
      "  5/500 [..............................] - ETA: 6:51 - loss: 1.8805 - regression_loss: 1.4830 - classification_loss: 0.3974\n",
      "  6/500 [..............................] - ETA: 7:03 - loss: 2.0629 - regression_loss: 1.5730 - classification_loss: 0.4899\n",
      "  7/500 [..............................] - ETA: 7:12 - loss: 2.0047 - regression_loss: 1.4938 - classification_loss: 0.5109\n",
      "  8/500 [..............................] - ETA: 7:18 - loss: 1.9165 - regression_loss: 1.4367 - classification_loss: 0.4798\n",
      "  9/500 [..............................] - ETA: 7:23 - loss: 1.9995 - regression_loss: 1.4808 - classification_loss: 0.5186\n",
      " 10/500 [..............................] - ETA: 7:26 - loss: 1.9862 - regression_loss: 1.4705 - classification_loss: 0.5157\n",
      " 11/500 [..............................] - ETA: 7:33 - loss: 1.9129 - regression_loss: 1.4182 - classification_loss: 0.4947\n",
      " 12/500 [..............................] - ETA: 7:35 - loss: 1.9173 - regression_loss: 1.4303 - classification_loss: 0.4870\n",
      " 13/500 [..............................] - ETA: 7:37 - loss: 1.8769 - regression_loss: 1.4018 - classification_loss: 0.4750\n",
      " 14/500 [..............................] - ETA: 7:41 - loss: 1.8992 - regression_loss: 1.4264 - classification_loss: 0.4728\n",
      " 15/500 [..............................] - ETA: 7:42 - loss: 1.8886 - regression_loss: 1.4188 - classification_loss: 0.4699\n",
      " 16/500 [..............................] - ETA: 7:42 - loss: 1.8327 - regression_loss: 1.3783 - classification_loss: 0.4544\n",
      " 17/500 [>.............................] - ETA: 7:42 - loss: 1.8304 - regression_loss: 1.3868 - classification_loss: 0.4436\n",
      " 18/500 [>.............................] - ETA: 7:45 - loss: 1.7964 - regression_loss: 1.3601 - classification_loss: 0.4363\n",
      " 19/500 [>.............................] - ETA: 7:44 - loss: 1.7783 - regression_loss: 1.3375 - classification_loss: 0.4408\n",
      " 20/500 [>.............................] - ETA: 7:46 - loss: 1.7689 - regression_loss: 1.3318 - classification_loss: 0.4371\n",
      " 21/500 [>.............................] - ETA: 7:48 - loss: 1.7514 - regression_loss: 1.3190 - classification_loss: 0.4324\n",
      " 22/500 [>.............................] - ETA: 7:44 - loss: 1.7576 - regression_loss: 1.3259 - classification_loss: 0.4317\n",
      " 23/500 [>.............................] - ETA: 7:45 - loss: 1.7509 - regression_loss: 1.3202 - classification_loss: 0.4307\n",
      " 24/500 [>.............................] - ETA: 7:45 - loss: 1.7987 - regression_loss: 1.3499 - classification_loss: 0.4488\n",
      " 25/500 [>.............................] - ETA: 7:44 - loss: 1.7867 - regression_loss: 1.3386 - classification_loss: 0.4481\n",
      " 26/500 [>.............................] - ETA: 7:46 - loss: 1.8053 - regression_loss: 1.3509 - classification_loss: 0.4544\n",
      " 27/500 [>.............................] - ETA: 7:45 - loss: 1.7744 - regression_loss: 1.3283 - classification_loss: 0.4461\n",
      " 28/500 [>.............................] - ETA: 7:44 - loss: 1.7461 - regression_loss: 1.3063 - classification_loss: 0.4398\n",
      " 29/500 [>.............................] - ETA: 7:43 - loss: 1.7445 - regression_loss: 1.3037 - classification_loss: 0.4408\n",
      " 30/500 [>.............................] - ETA: 7:42 - loss: 1.7589 - regression_loss: 1.3128 - classification_loss: 0.4460\n",
      " 31/500 [>.............................] - ETA: 7:42 - loss: 1.7367 - regression_loss: 1.2953 - classification_loss: 0.4413\n",
      " 32/500 [>.............................] - ETA: 7:41 - loss: 1.7408 - regression_loss: 1.2984 - classification_loss: 0.4424\n",
      " 33/500 [>.............................] - ETA: 7:40 - loss: 1.7685 - regression_loss: 1.3187 - classification_loss: 0.4498\n",
      " 34/500 [=>............................] - ETA: 7:41 - loss: 1.7867 - regression_loss: 1.3317 - classification_loss: 0.4550\n",
      " 35/500 [=>............................] - ETA: 7:41 - loss: 1.7984 - regression_loss: 1.3410 - classification_loss: 0.4574\n",
      " 36/500 [=>............................] - ETA: 7:41 - loss: 1.8138 - regression_loss: 1.3522 - classification_loss: 0.4616\n",
      " 37/500 [=>............................] - ETA: 7:41 - loss: 1.8004 - regression_loss: 1.3407 - classification_loss: 0.4597\n",
      " 38/500 [=>............................] - ETA: 7:40 - loss: 1.7890 - regression_loss: 1.3321 - classification_loss: 0.4569\n",
      " 39/500 [=>............................] - ETA: 7:39 - loss: 1.8007 - regression_loss: 1.3417 - classification_loss: 0.4590\n",
      " 40/500 [=>............................] - ETA: 7:38 - loss: 1.8097 - regression_loss: 1.3472 - classification_loss: 0.4625\n",
      " 41/500 [=>............................] - ETA: 7:38 - loss: 1.8105 - regression_loss: 1.3440 - classification_loss: 0.4665\n",
      " 42/500 [=>............................] - ETA: 7:37 - loss: 1.7994 - regression_loss: 1.3368 - classification_loss: 0.4626\n",
      " 43/500 [=>............................] - ETA: 7:36 - loss: 1.7987 - regression_loss: 1.3342 - classification_loss: 0.4644\n",
      " 44/500 [=>............................] - ETA: 7:36 - loss: 1.8071 - regression_loss: 1.3413 - classification_loss: 0.4658\n",
      " 45/500 [=>............................] - ETA: 7:35 - loss: 1.8349 - regression_loss: 1.3590 - classification_loss: 0.4759\n",
      " 46/500 [=>............................] - ETA: 7:34 - loss: 1.8270 - regression_loss: 1.3555 - classification_loss: 0.4714\n",
      " 47/500 [=>............................] - ETA: 7:34 - loss: 1.8245 - regression_loss: 1.3523 - classification_loss: 0.4722\n",
      " 48/500 [=>............................] - ETA: 7:34 - loss: 1.8412 - regression_loss: 1.3635 - classification_loss: 0.4778\n",
      " 49/500 [=>............................] - ETA: 7:30 - loss: 1.8371 - regression_loss: 1.3597 - classification_loss: 0.4774\n",
      " 50/500 [==>...........................] - ETA: 7:31 - loss: 1.8346 - regression_loss: 1.3580 - classification_loss: 0.4766\n",
      " 51/500 [==>...........................] - ETA: 7:29 - loss: 1.8384 - regression_loss: 1.3607 - classification_loss: 0.4777\n",
      " 52/500 [==>...........................] - ETA: 7:29 - loss: 1.8449 - regression_loss: 1.3671 - classification_loss: 0.4778\n",
      " 53/500 [==>...........................] - ETA: 7:28 - loss: 1.8458 - regression_loss: 1.3687 - classification_loss: 0.4772\n",
      " 54/500 [==>...........................] - ETA: 7:27 - loss: 1.8477 - regression_loss: 1.3710 - classification_loss: 0.4766\n",
      " 55/500 [==>...........................] - ETA: 7:25 - loss: 1.8483 - regression_loss: 1.3705 - classification_loss: 0.4777\n",
      " 56/500 [==>...........................] - ETA: 7:26 - loss: 1.8529 - regression_loss: 1.3698 - classification_loss: 0.4831\n",
      " 57/500 [==>...........................] - ETA: 7:25 - loss: 1.8489 - regression_loss: 1.3677 - classification_loss: 0.4812\n",
      " 58/500 [==>...........................] - ETA: 7:24 - loss: 1.8668 - regression_loss: 1.3798 - classification_loss: 0.4870\n",
      " 59/500 [==>...........................] - ETA: 7:24 - loss: 1.8866 - regression_loss: 1.3915 - classification_loss: 0.4951\n",
      " 60/500 [==>...........................] - ETA: 7:23 - loss: 1.8914 - regression_loss: 1.3954 - classification_loss: 0.4960\n",
      " 61/500 [==>...........................] - ETA: 7:22 - loss: 1.8896 - regression_loss: 1.3935 - classification_loss: 0.4961\n",
      " 62/500 [==>...........................] - ETA: 7:22 - loss: 1.8948 - regression_loss: 1.3950 - classification_loss: 0.4998\n",
      " 63/500 [==>...........................] - ETA: 7:22 - loss: 1.8976 - regression_loss: 1.3974 - classification_loss: 0.5002\n",
      " 64/500 [==>...........................] - ETA: 7:20 - loss: 1.8894 - regression_loss: 1.3927 - classification_loss: 0.4968\n",
      " 65/500 [==>...........................] - ETA: 7:20 - loss: 1.8859 - regression_loss: 1.3923 - classification_loss: 0.4936\n",
      " 66/500 [==>...........................] - ETA: 7:18 - loss: 1.8908 - regression_loss: 1.3973 - classification_loss: 0.4935\n",
      " 67/500 [===>..........................] - ETA: 7:18 - loss: 1.8919 - regression_loss: 1.3985 - classification_loss: 0.4934\n",
      " 68/500 [===>..........................] - ETA: 7:17 - loss: 1.9027 - regression_loss: 1.4037 - classification_loss: 0.4990\n",
      " 69/500 [===>..........................] - ETA: 7:16 - loss: 1.8943 - regression_loss: 1.3982 - classification_loss: 0.4961\n",
      " 70/500 [===>..........................] - ETA: 7:15 - loss: 1.8954 - regression_loss: 1.4006 - classification_loss: 0.4948\n",
      " 71/500 [===>..........................] - ETA: 7:13 - loss: 1.9064 - regression_loss: 1.4073 - classification_loss: 0.4991\n",
      " 72/500 [===>..........................] - ETA: 7:13 - loss: 1.9137 - regression_loss: 1.4127 - classification_loss: 0.5010\n",
      " 73/500 [===>..........................] - ETA: 7:12 - loss: 1.9217 - regression_loss: 1.4202 - classification_loss: 0.5014\n",
      " 74/500 [===>..........................] - ETA: 7:11 - loss: 1.9233 - regression_loss: 1.4223 - classification_loss: 0.5010\n",
      " 75/500 [===>..........................] - ETA: 7:10 - loss: 1.9218 - regression_loss: 1.4217 - classification_loss: 0.5002\n",
      " 76/500 [===>..........................] - ETA: 7:09 - loss: 1.9186 - regression_loss: 1.4184 - classification_loss: 0.5002\n",
      " 77/500 [===>..........................] - ETA: 7:06 - loss: 1.9174 - regression_loss: 1.4153 - classification_loss: 0.5020\n",
      " 78/500 [===>..........................] - ETA: 7:06 - loss: 1.9138 - regression_loss: 1.4132 - classification_loss: 0.5006\n",
      " 79/500 [===>..........................] - ETA: 7:06 - loss: 1.9150 - regression_loss: 1.4146 - classification_loss: 0.5004\n",
      " 80/500 [===>..........................] - ETA: 7:05 - loss: 1.9163 - regression_loss: 1.4156 - classification_loss: 0.5007\n",
      " 81/500 [===>..........................] - ETA: 7:04 - loss: 1.9093 - regression_loss: 1.4114 - classification_loss: 0.4978\n",
      " 82/500 [===>..........................] - ETA: 7:03 - loss: 1.8962 - regression_loss: 1.4025 - classification_loss: 0.4937\n",
      " 83/500 [===>..........................] - ETA: 7:03 - loss: 1.9012 - regression_loss: 1.4078 - classification_loss: 0.4934\n",
      " 84/500 [====>.........................] - ETA: 7:02 - loss: 1.9057 - regression_loss: 1.4105 - classification_loss: 0.4951\n",
      " 85/500 [====>.........................] - ETA: 7:01 - loss: 1.9018 - regression_loss: 1.4079 - classification_loss: 0.4940\n",
      " 86/500 [====>.........................] - ETA: 6:59 - loss: 1.9020 - regression_loss: 1.4092 - classification_loss: 0.4927\n",
      " 87/500 [====>.........................] - ETA: 6:59 - loss: 1.9044 - regression_loss: 1.4136 - classification_loss: 0.4908\n",
      " 88/500 [====>.........................] - ETA: 6:58 - loss: 1.9029 - regression_loss: 1.4119 - classification_loss: 0.4910\n",
      " 89/500 [====>.........................] - ETA: 6:57 - loss: 1.9013 - regression_loss: 1.4106 - classification_loss: 0.4908\n",
      " 90/500 [====>.........................] - ETA: 6:57 - loss: 1.9112 - regression_loss: 1.4197 - classification_loss: 0.4915\n",
      " 91/500 [====>.........................] - ETA: 6:55 - loss: 1.9093 - regression_loss: 1.4195 - classification_loss: 0.4898\n",
      " 92/500 [====>.........................] - ETA: 6:54 - loss: 1.8996 - regression_loss: 1.4125 - classification_loss: 0.4871\n",
      " 93/500 [====>.........................] - ETA: 6:53 - loss: 1.9006 - regression_loss: 1.4127 - classification_loss: 0.4878\n",
      " 94/500 [====>.........................] - ETA: 6:52 - loss: 1.8979 - regression_loss: 1.4119 - classification_loss: 0.4860\n",
      " 95/500 [====>.........................] - ETA: 6:51 - loss: 1.8895 - regression_loss: 1.4059 - classification_loss: 0.4837\n",
      " 96/500 [====>.........................] - ETA: 6:49 - loss: 1.8991 - regression_loss: 1.4100 - classification_loss: 0.4891\n",
      " 97/500 [====>.........................] - ETA: 6:49 - loss: 1.8988 - regression_loss: 1.4105 - classification_loss: 0.4883\n",
      " 98/500 [====>.........................] - ETA: 6:47 - loss: 1.8968 - regression_loss: 1.4091 - classification_loss: 0.4876\n",
      " 99/500 [====>.........................] - ETA: 6:46 - loss: 1.8999 - regression_loss: 1.4104 - classification_loss: 0.4895\n",
      "100/500 [=====>........................] - ETA: 6:44 - loss: 1.8957 - regression_loss: 1.4082 - classification_loss: 0.4876\n",
      "101/500 [=====>........................] - ETA: 6:44 - loss: 1.8943 - regression_loss: 1.4082 - classification_loss: 0.4861\n",
      "102/500 [=====>........................] - ETA: 6:44 - loss: 1.8928 - regression_loss: 1.4082 - classification_loss: 0.4846\n",
      "103/500 [=====>........................] - ETA: 6:43 - loss: 1.8976 - regression_loss: 1.4101 - classification_loss: 0.4875\n",
      "104/500 [=====>........................] - ETA: 6:42 - loss: 1.8977 - regression_loss: 1.4088 - classification_loss: 0.4889\n",
      "105/500 [=====>........................] - ETA: 6:41 - loss: 1.8926 - regression_loss: 1.4055 - classification_loss: 0.4872\n",
      "106/500 [=====>........................] - ETA: 6:40 - loss: 1.8919 - regression_loss: 1.4058 - classification_loss: 0.4861\n",
      "107/500 [=====>........................] - ETA: 6:39 - loss: 1.8809 - regression_loss: 1.3973 - classification_loss: 0.4835\n",
      "108/500 [=====>........................] - ETA: 6:38 - loss: 1.8808 - regression_loss: 1.3973 - classification_loss: 0.4835\n",
      "109/500 [=====>........................] - ETA: 6:37 - loss: 1.8757 - regression_loss: 1.3935 - classification_loss: 0.4822\n",
      "110/500 [=====>........................] - ETA: 6:45 - loss: 1.8753 - regression_loss: 1.3942 - classification_loss: 0.4811\n",
      "111/500 [=====>........................] - ETA: 6:43 - loss: 1.8804 - regression_loss: 1.3977 - classification_loss: 0.4828\n",
      "112/500 [=====>........................] - ETA: 6:43 - loss: 1.8752 - regression_loss: 1.3939 - classification_loss: 0.4813\n",
      "113/500 [=====>........................] - ETA: 6:42 - loss: 1.8782 - regression_loss: 1.3963 - classification_loss: 0.4819\n",
      "114/500 [=====>........................] - ETA: 6:41 - loss: 1.8780 - regression_loss: 1.3965 - classification_loss: 0.4815\n",
      "115/500 [=====>........................] - ETA: 6:40 - loss: 1.8827 - regression_loss: 1.4025 - classification_loss: 0.4802\n",
      "116/500 [=====>........................] - ETA: 6:39 - loss: 1.8885 - regression_loss: 1.4068 - classification_loss: 0.4817\n",
      "117/500 [======>.......................] - ETA: 6:38 - loss: 1.8949 - regression_loss: 1.4093 - classification_loss: 0.4856\n",
      "118/500 [======>.......................] - ETA: 6:37 - loss: 1.8959 - regression_loss: 1.4093 - classification_loss: 0.4866\n",
      "119/500 [======>.......................] - ETA: 6:36 - loss: 1.8893 - regression_loss: 1.4042 - classification_loss: 0.4851\n",
      "120/500 [======>.......................] - ETA: 6:42 - loss: 1.8838 - regression_loss: 1.4005 - classification_loss: 0.4833\n",
      "121/500 [======>.......................] - ETA: 6:41 - loss: 1.8922 - regression_loss: 1.4045 - classification_loss: 0.4877\n",
      "122/500 [======>.......................] - ETA: 6:41 - loss: 1.8953 - regression_loss: 1.4078 - classification_loss: 0.4875\n",
      "123/500 [======>.......................] - ETA: 6:39 - loss: 1.9013 - regression_loss: 1.4105 - classification_loss: 0.4908\n",
      "124/500 [======>.......................] - ETA: 6:38 - loss: 1.8948 - regression_loss: 1.4051 - classification_loss: 0.4897\n",
      "125/500 [======>.......................] - ETA: 6:36 - loss: 1.8945 - regression_loss: 1.4046 - classification_loss: 0.4899\n",
      "126/500 [======>.......................] - ETA: 6:36 - loss: 1.8975 - regression_loss: 1.4062 - classification_loss: 0.4913\n",
      "127/500 [======>.......................] - ETA: 6:34 - loss: 1.9000 - regression_loss: 1.4091 - classification_loss: 0.4909\n",
      "128/500 [======>.......................] - ETA: 6:33 - loss: 1.9037 - regression_loss: 1.4114 - classification_loss: 0.4923\n",
      "129/500 [======>.......................] - ETA: 6:32 - loss: 1.9097 - regression_loss: 1.4167 - classification_loss: 0.4930\n",
      "130/500 [======>.......................] - ETA: 6:30 - loss: 1.9089 - regression_loss: 1.4156 - classification_loss: 0.4933\n",
      "131/500 [======>.......................] - ETA: 6:30 - loss: 1.9073 - regression_loss: 1.4143 - classification_loss: 0.4930\n",
      "132/500 [======>.......................] - ETA: 6:29 - loss: 1.9063 - regression_loss: 1.4131 - classification_loss: 0.4932\n",
      "133/500 [======>.......................] - ETA: 6:28 - loss: 1.9038 - regression_loss: 1.4118 - classification_loss: 0.4920\n",
      "134/500 [=======>......................] - ETA: 6:27 - loss: 1.9085 - regression_loss: 1.4137 - classification_loss: 0.4947\n",
      "135/500 [=======>......................] - ETA: 6:26 - loss: 1.9095 - regression_loss: 1.4151 - classification_loss: 0.4944\n",
      "136/500 [=======>......................] - ETA: 6:25 - loss: 1.9103 - regression_loss: 1.4169 - classification_loss: 0.4934\n",
      "137/500 [=======>......................] - ETA: 6:24 - loss: 1.9038 - regression_loss: 1.4124 - classification_loss: 0.4914\n",
      "138/500 [=======>......................] - ETA: 6:23 - loss: 1.9063 - regression_loss: 1.4147 - classification_loss: 0.4916\n",
      "139/500 [=======>......................] - ETA: 6:21 - loss: 1.9066 - regression_loss: 1.4146 - classification_loss: 0.4920\n",
      "140/500 [=======>......................] - ETA: 6:20 - loss: 1.9121 - regression_loss: 1.4187 - classification_loss: 0.4934\n",
      "141/500 [=======>......................] - ETA: 6:19 - loss: 1.9172 - regression_loss: 1.4228 - classification_loss: 0.4944\n",
      "142/500 [=======>......................] - ETA: 6:18 - loss: 1.9152 - regression_loss: 1.4218 - classification_loss: 0.4933\n",
      "143/500 [=======>......................] - ETA: 6:17 - loss: 1.9186 - regression_loss: 1.4241 - classification_loss: 0.4944\n",
      "144/500 [=======>......................] - ETA: 6:16 - loss: 1.9257 - regression_loss: 1.4292 - classification_loss: 0.4965\n",
      "145/500 [=======>......................] - ETA: 6:15 - loss: 1.9230 - regression_loss: 1.4280 - classification_loss: 0.4950\n",
      "146/500 [=======>......................] - ETA: 6:14 - loss: 1.9184 - regression_loss: 1.4246 - classification_loss: 0.4938\n",
      "147/500 [=======>......................] - ETA: 6:12 - loss: 1.9209 - regression_loss: 1.4272 - classification_loss: 0.4937\n",
      "148/500 [=======>......................] - ETA: 6:11 - loss: 1.9229 - regression_loss: 1.4283 - classification_loss: 0.4947\n",
      "149/500 [=======>......................] - ETA: 6:10 - loss: 1.9250 - regression_loss: 1.4304 - classification_loss: 0.4946\n",
      "150/500 [========>.....................] - ETA: 6:09 - loss: 1.9264 - regression_loss: 1.4314 - classification_loss: 0.4950\n",
      "151/500 [========>.....................] - ETA: 6:08 - loss: 1.9265 - regression_loss: 1.4319 - classification_loss: 0.4946\n",
      "152/500 [========>.....................] - ETA: 6:07 - loss: 1.9310 - regression_loss: 1.4352 - classification_loss: 0.4958\n",
      "153/500 [========>.....................] - ETA: 6:05 - loss: 1.9286 - regression_loss: 1.4338 - classification_loss: 0.4948\n",
      "154/500 [========>.....................] - ETA: 6:04 - loss: 1.9337 - regression_loss: 1.4380 - classification_loss: 0.4957\n",
      "155/500 [========>.....................] - ETA: 6:03 - loss: 1.9364 - regression_loss: 1.4403 - classification_loss: 0.4961\n",
      "156/500 [========>.....................] - ETA: 6:02 - loss: 1.9309 - regression_loss: 1.4365 - classification_loss: 0.4944\n",
      "157/500 [========>.....................] - ETA: 6:01 - loss: 1.9290 - regression_loss: 1.4343 - classification_loss: 0.4947\n",
      "158/500 [========>.....................] - ETA: 6:00 - loss: 1.9266 - regression_loss: 1.4321 - classification_loss: 0.4945\n",
      "159/500 [========>.....................] - ETA: 5:59 - loss: 1.9275 - regression_loss: 1.4322 - classification_loss: 0.4952\n",
      "160/500 [========>.....................] - ETA: 5:58 - loss: 1.9308 - regression_loss: 1.4342 - classification_loss: 0.4966\n",
      "161/500 [========>.....................] - ETA: 5:57 - loss: 1.9288 - regression_loss: 1.4329 - classification_loss: 0.4959\n",
      "162/500 [========>.....................] - ETA: 5:56 - loss: 1.9302 - regression_loss: 1.4345 - classification_loss: 0.4957\n",
      "163/500 [========>.....................] - ETA: 5:55 - loss: 1.9291 - regression_loss: 1.4340 - classification_loss: 0.4950\n",
      "164/500 [========>.....................] - ETA: 5:54 - loss: 1.9296 - regression_loss: 1.4349 - classification_loss: 0.4947\n",
      "165/500 [========>.....................] - ETA: 5:52 - loss: 1.9267 - regression_loss: 1.4332 - classification_loss: 0.4935\n",
      "166/500 [========>.....................] - ETA: 5:52 - loss: 1.9271 - regression_loss: 1.4335 - classification_loss: 0.4937\n",
      "167/500 [=========>....................] - ETA: 5:50 - loss: 1.9291 - regression_loss: 1.4340 - classification_loss: 0.4952\n",
      "168/500 [=========>....................] - ETA: 5:49 - loss: 1.9270 - regression_loss: 1.4321 - classification_loss: 0.4949\n",
      "169/500 [=========>....................] - ETA: 5:48 - loss: 1.9280 - regression_loss: 1.4337 - classification_loss: 0.4943\n",
      "170/500 [=========>....................] - ETA: 5:47 - loss: 1.9278 - regression_loss: 1.4337 - classification_loss: 0.4940\n",
      "171/500 [=========>....................] - ETA: 5:46 - loss: 1.9299 - regression_loss: 1.4344 - classification_loss: 0.4955\n",
      "172/500 [=========>....................] - ETA: 5:45 - loss: 1.9333 - regression_loss: 1.4361 - classification_loss: 0.4972\n",
      "173/500 [=========>....................] - ETA: 5:44 - loss: 1.9259 - regression_loss: 1.4307 - classification_loss: 0.4952\n",
      "174/500 [=========>....................] - ETA: 5:43 - loss: 1.9236 - regression_loss: 1.4291 - classification_loss: 0.4945\n",
      "175/500 [=========>....................] - ETA: 5:41 - loss: 1.9213 - regression_loss: 1.4279 - classification_loss: 0.4934\n",
      "176/500 [=========>....................] - ETA: 5:41 - loss: 1.9265 - regression_loss: 1.4317 - classification_loss: 0.4949\n",
      "177/500 [=========>....................] - ETA: 5:40 - loss: 1.9255 - regression_loss: 1.4308 - classification_loss: 0.4946\n",
      "178/500 [=========>....................] - ETA: 5:39 - loss: 1.9228 - regression_loss: 1.4291 - classification_loss: 0.4937\n",
      "179/500 [=========>....................] - ETA: 5:37 - loss: 1.9214 - regression_loss: 1.4282 - classification_loss: 0.4932\n",
      "180/500 [=========>....................] - ETA: 5:36 - loss: 1.9200 - regression_loss: 1.4274 - classification_loss: 0.4927\n",
      "181/500 [=========>....................] - ETA: 5:35 - loss: 1.9202 - regression_loss: 1.4286 - classification_loss: 0.4916\n",
      "182/500 [=========>....................] - ETA: 5:34 - loss: 1.9279 - regression_loss: 1.4311 - classification_loss: 0.4969\n",
      "183/500 [=========>....................] - ETA: 5:33 - loss: 1.9265 - regression_loss: 1.4292 - classification_loss: 0.4973\n",
      "184/500 [==========>...................] - ETA: 5:32 - loss: 1.9240 - regression_loss: 1.4270 - classification_loss: 0.4971\n",
      "185/500 [==========>...................] - ETA: 5:31 - loss: 1.9222 - regression_loss: 1.4256 - classification_loss: 0.4966\n",
      "186/500 [==========>...................] - ETA: 5:30 - loss: 1.9200 - regression_loss: 1.4242 - classification_loss: 0.4958\n",
      "187/500 [==========>...................] - ETA: 5:29 - loss: 1.9186 - regression_loss: 1.4230 - classification_loss: 0.4956\n",
      "188/500 [==========>...................] - ETA: 5:28 - loss: 1.9153 - regression_loss: 1.4209 - classification_loss: 0.4945\n",
      "189/500 [==========>...................] - ETA: 5:27 - loss: 1.9186 - regression_loss: 1.4231 - classification_loss: 0.4954\n",
      "190/500 [==========>...................] - ETA: 5:26 - loss: 1.9165 - regression_loss: 1.4221 - classification_loss: 0.4944\n",
      "191/500 [==========>...................] - ETA: 5:25 - loss: 1.9138 - regression_loss: 1.4196 - classification_loss: 0.4942\n",
      "192/500 [==========>...................] - ETA: 5:24 - loss: 1.9164 - regression_loss: 1.4208 - classification_loss: 0.4956\n",
      "193/500 [==========>...................] - ETA: 5:23 - loss: 1.9108 - regression_loss: 1.4163 - classification_loss: 0.4945\n",
      "194/500 [==========>...................] - ETA: 5:21 - loss: 1.9073 - regression_loss: 1.4140 - classification_loss: 0.4932\n",
      "195/500 [==========>...................] - ETA: 5:20 - loss: 1.9068 - regression_loss: 1.4136 - classification_loss: 0.4932\n",
      "196/500 [==========>...................] - ETA: 5:19 - loss: 1.9042 - regression_loss: 1.4122 - classification_loss: 0.4920\n",
      "197/500 [==========>...................] - ETA: 5:18 - loss: 1.9010 - regression_loss: 1.4097 - classification_loss: 0.4913\n",
      "198/500 [==========>...................] - ETA: 5:17 - loss: 1.8969 - regression_loss: 1.4066 - classification_loss: 0.4903\n",
      "199/500 [==========>...................] - ETA: 5:16 - loss: 1.8965 - regression_loss: 1.4067 - classification_loss: 0.4898\n",
      "200/500 [===========>..................] - ETA: 5:15 - loss: 1.8951 - regression_loss: 1.4048 - classification_loss: 0.4903\n",
      "201/500 [===========>..................] - ETA: 5:14 - loss: 1.8929 - regression_loss: 1.4039 - classification_loss: 0.4890\n",
      "202/500 [===========>..................] - ETA: 5:13 - loss: 1.8934 - regression_loss: 1.4045 - classification_loss: 0.4889\n",
      "203/500 [===========>..................] - ETA: 5:12 - loss: 1.8971 - regression_loss: 1.4066 - classification_loss: 0.4905\n",
      "204/500 [===========>..................] - ETA: 5:11 - loss: 1.9006 - regression_loss: 1.4089 - classification_loss: 0.4917\n",
      "205/500 [===========>..................] - ETA: 5:10 - loss: 1.9042 - regression_loss: 1.4112 - classification_loss: 0.4930\n",
      "206/500 [===========>..................] - ETA: 5:08 - loss: 1.9101 - regression_loss: 1.4150 - classification_loss: 0.4950\n",
      "207/500 [===========>..................] - ETA: 5:07 - loss: 1.9123 - regression_loss: 1.4167 - classification_loss: 0.4956\n",
      "208/500 [===========>..................] - ETA: 5:06 - loss: 1.9134 - regression_loss: 1.4178 - classification_loss: 0.4957\n",
      "209/500 [===========>..................] - ETA: 5:05 - loss: 1.9126 - regression_loss: 1.4166 - classification_loss: 0.4960\n",
      "210/500 [===========>..................] - ETA: 5:07 - loss: 1.9102 - regression_loss: 1.4147 - classification_loss: 0.4955\n",
      "211/500 [===========>..................] - ETA: 5:05 - loss: 1.9109 - regression_loss: 1.4149 - classification_loss: 0.4959\n",
      "212/500 [===========>..................] - ETA: 5:04 - loss: 1.9080 - regression_loss: 1.4130 - classification_loss: 0.4950\n",
      "213/500 [===========>..................] - ETA: 5:03 - loss: 1.9042 - regression_loss: 1.4104 - classification_loss: 0.4937\n",
      "214/500 [===========>..................] - ETA: 5:02 - loss: 1.9013 - regression_loss: 1.4086 - classification_loss: 0.4927\n",
      "215/500 [===========>..................] - ETA: 5:01 - loss: 1.8999 - regression_loss: 1.4073 - classification_loss: 0.4926\n",
      "216/500 [===========>..................] - ETA: 5:00 - loss: 1.8992 - regression_loss: 1.4072 - classification_loss: 0.4920\n",
      "217/500 [============>.................] - ETA: 4:59 - loss: 1.8969 - regression_loss: 1.4052 - classification_loss: 0.4918\n",
      "218/500 [============>.................] - ETA: 4:58 - loss: 1.8952 - regression_loss: 1.4044 - classification_loss: 0.4908\n",
      "219/500 [============>.................] - ETA: 4:57 - loss: 1.8994 - regression_loss: 1.4058 - classification_loss: 0.4935\n",
      "220/500 [============>.................] - ETA: 4:56 - loss: 1.9041 - regression_loss: 1.4091 - classification_loss: 0.4951\n",
      "221/500 [============>.................] - ETA: 4:55 - loss: 1.9084 - regression_loss: 1.4113 - classification_loss: 0.4971\n",
      "222/500 [============>.................] - ETA: 4:54 - loss: 1.9066 - regression_loss: 1.4101 - classification_loss: 0.4965\n",
      "223/500 [============>.................] - ETA: 4:53 - loss: 1.9026 - regression_loss: 1.4075 - classification_loss: 0.4951\n",
      "224/500 [============>.................] - ETA: 4:52 - loss: 1.8991 - regression_loss: 1.4048 - classification_loss: 0.4943\n",
      "225/500 [============>.................] - ETA: 4:51 - loss: 1.8998 - regression_loss: 1.4060 - classification_loss: 0.4937\n",
      "226/500 [============>.................] - ETA: 4:49 - loss: 1.8995 - regression_loss: 1.4061 - classification_loss: 0.4934\n",
      "227/500 [============>.................] - ETA: 4:48 - loss: 1.9001 - regression_loss: 1.4065 - classification_loss: 0.4936\n",
      "228/500 [============>.................] - ETA: 4:47 - loss: 1.8995 - regression_loss: 1.4060 - classification_loss: 0.4935\n",
      "229/500 [============>.................] - ETA: 4:46 - loss: 1.9013 - regression_loss: 1.4069 - classification_loss: 0.4943\n",
      "230/500 [============>.................] - ETA: 4:45 - loss: 1.9001 - regression_loss: 1.4059 - classification_loss: 0.4942\n",
      "231/500 [============>.................] - ETA: 4:44 - loss: 1.9006 - regression_loss: 1.4058 - classification_loss: 0.4948\n",
      "232/500 [============>.................] - ETA: 4:43 - loss: 1.8997 - regression_loss: 1.4050 - classification_loss: 0.4947\n",
      "233/500 [============>.................] - ETA: 4:42 - loss: 1.8972 - regression_loss: 1.4030 - classification_loss: 0.4941\n",
      "234/500 [=============>................] - ETA: 4:41 - loss: 1.8948 - regression_loss: 1.4012 - classification_loss: 0.4935\n",
      "235/500 [=============>................] - ETA: 4:39 - loss: 1.8944 - regression_loss: 1.4009 - classification_loss: 0.4934\n",
      "236/500 [=============>................] - ETA: 4:38 - loss: 1.8981 - regression_loss: 1.4029 - classification_loss: 0.4952\n",
      "237/500 [=============>................] - ETA: 4:37 - loss: 1.8943 - regression_loss: 1.3998 - classification_loss: 0.4945\n",
      "238/500 [=============>................] - ETA: 4:36 - loss: 1.8938 - regression_loss: 1.3998 - classification_loss: 0.4940\n",
      "239/500 [=============>................] - ETA: 4:35 - loss: 1.8925 - regression_loss: 1.3988 - classification_loss: 0.4937\n",
      "240/500 [=============>................] - ETA: 4:34 - loss: 1.8940 - regression_loss: 1.4000 - classification_loss: 0.4939\n",
      "241/500 [=============>................] - ETA: 4:33 - loss: 1.8907 - regression_loss: 1.3975 - classification_loss: 0.4932\n",
      "242/500 [=============>................] - ETA: 4:32 - loss: 1.8904 - regression_loss: 1.3967 - classification_loss: 0.4936\n",
      "243/500 [=============>................] - ETA: 4:31 - loss: 1.8874 - regression_loss: 1.3948 - classification_loss: 0.4925\n",
      "244/500 [=============>................] - ETA: 4:30 - loss: 1.8869 - regression_loss: 1.3944 - classification_loss: 0.4925\n",
      "245/500 [=============>................] - ETA: 4:29 - loss: 1.8867 - regression_loss: 1.3943 - classification_loss: 0.4924\n",
      "246/500 [=============>................] - ETA: 4:28 - loss: 1.8870 - regression_loss: 1.3943 - classification_loss: 0.4927\n",
      "247/500 [=============>................] - ETA: 4:26 - loss: 1.8870 - regression_loss: 1.3938 - classification_loss: 0.4932\n",
      "248/500 [=============>................] - ETA: 4:25 - loss: 1.8888 - regression_loss: 1.3954 - classification_loss: 0.4933\n",
      "249/500 [=============>................] - ETA: 4:24 - loss: 1.8919 - regression_loss: 1.3972 - classification_loss: 0.4947\n",
      "250/500 [==============>...............] - ETA: 4:23 - loss: 1.8922 - regression_loss: 1.3969 - classification_loss: 0.4952\n",
      "251/500 [==============>...............] - ETA: 4:22 - loss: 1.8925 - regression_loss: 1.3971 - classification_loss: 0.4954\n",
      "252/500 [==============>...............] - ETA: 4:21 - loss: 1.8897 - regression_loss: 1.3949 - classification_loss: 0.4948\n",
      "253/500 [==============>...............] - ETA: 4:20 - loss: 1.8905 - regression_loss: 1.3957 - classification_loss: 0.4948\n",
      "254/500 [==============>...............] - ETA: 4:19 - loss: 1.8891 - regression_loss: 1.3943 - classification_loss: 0.4947\n",
      "255/500 [==============>...............] - ETA: 4:18 - loss: 1.8893 - regression_loss: 1.3951 - classification_loss: 0.4942\n",
      "256/500 [==============>...............] - ETA: 4:16 - loss: 1.8856 - regression_loss: 1.3924 - classification_loss: 0.4933\n",
      "257/500 [==============>...............] - ETA: 4:16 - loss: 1.8859 - regression_loss: 1.3926 - classification_loss: 0.4932\n",
      "258/500 [==============>...............] - ETA: 4:15 - loss: 1.8868 - regression_loss: 1.3930 - classification_loss: 0.4937\n",
      "259/500 [==============>...............] - ETA: 4:13 - loss: 1.8891 - regression_loss: 1.3948 - classification_loss: 0.4943\n",
      "260/500 [==============>...............] - ETA: 4:12 - loss: 1.8924 - regression_loss: 1.3974 - classification_loss: 0.4950\n",
      "261/500 [==============>...............] - ETA: 4:11 - loss: 1.8913 - regression_loss: 1.3965 - classification_loss: 0.4948\n",
      "262/500 [==============>...............] - ETA: 4:10 - loss: 1.8939 - regression_loss: 1.3994 - classification_loss: 0.4945\n",
      "263/500 [==============>...............] - ETA: 4:09 - loss: 1.8943 - regression_loss: 1.4002 - classification_loss: 0.4941\n",
      "264/500 [==============>...............] - ETA: 4:08 - loss: 1.8942 - regression_loss: 1.3997 - classification_loss: 0.4946\n",
      "265/500 [==============>...............] - ETA: 4:07 - loss: 1.8961 - regression_loss: 1.4013 - classification_loss: 0.4948\n",
      "266/500 [==============>...............] - ETA: 4:06 - loss: 1.8990 - regression_loss: 1.4025 - classification_loss: 0.4965\n",
      "267/500 [===============>..............] - ETA: 4:05 - loss: 1.8985 - regression_loss: 1.4013 - classification_loss: 0.4972\n",
      "268/500 [===============>..............] - ETA: 4:04 - loss: 1.9006 - regression_loss: 1.4023 - classification_loss: 0.4983\n",
      "269/500 [===============>..............] - ETA: 4:03 - loss: 1.9006 - regression_loss: 1.4029 - classification_loss: 0.4978\n",
      "270/500 [===============>..............] - ETA: 4:01 - loss: 1.9013 - regression_loss: 1.4035 - classification_loss: 0.4978\n",
      "271/500 [===============>..............] - ETA: 4:00 - loss: 1.9009 - regression_loss: 1.4025 - classification_loss: 0.4984\n",
      "272/500 [===============>..............] - ETA: 3:59 - loss: 1.8993 - regression_loss: 1.4013 - classification_loss: 0.4981\n",
      "273/500 [===============>..............] - ETA: 3:58 - loss: 1.9012 - regression_loss: 1.4031 - classification_loss: 0.4981\n",
      "274/500 [===============>..............] - ETA: 3:57 - loss: 1.9007 - regression_loss: 1.4028 - classification_loss: 0.4979\n",
      "275/500 [===============>..............] - ETA: 3:56 - loss: 1.9000 - regression_loss: 1.4032 - classification_loss: 0.4969\n",
      "276/500 [===============>..............] - ETA: 3:55 - loss: 1.9027 - regression_loss: 1.4055 - classification_loss: 0.4973\n",
      "277/500 [===============>..............] - ETA: 3:54 - loss: 1.9028 - regression_loss: 1.4057 - classification_loss: 0.4972\n",
      "278/500 [===============>..............] - ETA: 3:53 - loss: 1.9021 - regression_loss: 1.4050 - classification_loss: 0.4971\n",
      "279/500 [===============>..............] - ETA: 3:52 - loss: 1.9018 - regression_loss: 1.4051 - classification_loss: 0.4967\n",
      "280/500 [===============>..............] - ETA: 3:51 - loss: 1.9035 - regression_loss: 1.4070 - classification_loss: 0.4966\n",
      "281/500 [===============>..............] - ETA: 3:50 - loss: 1.9023 - regression_loss: 1.4062 - classification_loss: 0.4961\n",
      "282/500 [===============>..............] - ETA: 3:49 - loss: 1.9028 - regression_loss: 1.4059 - classification_loss: 0.4969\n",
      "283/500 [===============>..............] - ETA: 3:48 - loss: 1.8999 - regression_loss: 1.4038 - classification_loss: 0.4961\n",
      "284/500 [================>.............] - ETA: 3:46 - loss: 1.8984 - regression_loss: 1.4032 - classification_loss: 0.4952\n",
      "285/500 [================>.............] - ETA: 3:45 - loss: 1.8999 - regression_loss: 1.4049 - classification_loss: 0.4949\n",
      "286/500 [================>.............] - ETA: 3:44 - loss: 1.8980 - regression_loss: 1.4038 - classification_loss: 0.4942\n",
      "287/500 [================>.............] - ETA: 3:43 - loss: 1.9001 - regression_loss: 1.4057 - classification_loss: 0.4944\n",
      "288/500 [================>.............] - ETA: 3:42 - loss: 1.9062 - regression_loss: 1.4095 - classification_loss: 0.4967\n",
      "289/500 [================>.............] - ETA: 3:41 - loss: 1.9040 - regression_loss: 1.4082 - classification_loss: 0.4958\n",
      "290/500 [================>.............] - ETA: 3:40 - loss: 1.9015 - regression_loss: 1.4061 - classification_loss: 0.4954\n",
      "291/500 [================>.............] - ETA: 3:39 - loss: 1.9022 - regression_loss: 1.4068 - classification_loss: 0.4954\n",
      "292/500 [================>.............] - ETA: 3:38 - loss: 1.9067 - regression_loss: 1.4098 - classification_loss: 0.4969\n",
      "293/500 [================>.............] - ETA: 3:37 - loss: 1.9066 - regression_loss: 1.4095 - classification_loss: 0.4971\n",
      "294/500 [================>.............] - ETA: 3:36 - loss: 1.9062 - regression_loss: 1.4094 - classification_loss: 0.4967\n",
      "295/500 [================>.............] - ETA: 3:35 - loss: 1.9048 - regression_loss: 1.4087 - classification_loss: 0.4961\n",
      "296/500 [================>.............] - ETA: 3:34 - loss: 1.9025 - regression_loss: 1.4068 - classification_loss: 0.4958\n",
      "297/500 [================>.............] - ETA: 3:33 - loss: 1.9021 - regression_loss: 1.4062 - classification_loss: 0.4959\n",
      "298/500 [================>.............] - ETA: 3:32 - loss: 1.9034 - regression_loss: 1.4073 - classification_loss: 0.4961\n",
      "299/500 [================>.............] - ETA: 3:30 - loss: 1.9049 - regression_loss: 1.4080 - classification_loss: 0.4968\n",
      "300/500 [=================>............] - ETA: 3:29 - loss: 1.9035 - regression_loss: 1.4072 - classification_loss: 0.4962\n",
      "301/500 [=================>............] - ETA: 3:28 - loss: 1.9039 - regression_loss: 1.4078 - classification_loss: 0.4961\n",
      "302/500 [=================>............] - ETA: 3:27 - loss: 1.9007 - regression_loss: 1.4055 - classification_loss: 0.4952\n",
      "303/500 [=================>............] - ETA: 3:26 - loss: 1.9013 - regression_loss: 1.4061 - classification_loss: 0.4952\n",
      "304/500 [=================>............] - ETA: 3:25 - loss: 1.9030 - regression_loss: 1.4073 - classification_loss: 0.4957\n",
      "305/500 [=================>............] - ETA: 3:24 - loss: 1.9051 - regression_loss: 1.4087 - classification_loss: 0.4965\n",
      "306/500 [=================>............] - ETA: 3:23 - loss: 1.9058 - regression_loss: 1.4089 - classification_loss: 0.4969\n",
      "307/500 [=================>............] - ETA: 3:22 - loss: 1.9044 - regression_loss: 1.4077 - classification_loss: 0.4967\n",
      "308/500 [=================>............] - ETA: 3:21 - loss: 1.9050 - regression_loss: 1.4078 - classification_loss: 0.4972\n",
      "309/500 [=================>............] - ETA: 3:20 - loss: 1.9067 - regression_loss: 1.4093 - classification_loss: 0.4974\n",
      "310/500 [=================>............] - ETA: 3:19 - loss: 1.9046 - regression_loss: 1.4077 - classification_loss: 0.4969\n",
      "311/500 [=================>............] - ETA: 3:18 - loss: 1.9032 - regression_loss: 1.4069 - classification_loss: 0.4963\n",
      "312/500 [=================>............] - ETA: 3:16 - loss: 1.9028 - regression_loss: 1.4063 - classification_loss: 0.4965\n",
      "313/500 [=================>............] - ETA: 3:15 - loss: 1.9055 - regression_loss: 1.4084 - classification_loss: 0.4971\n",
      "314/500 [=================>............] - ETA: 3:14 - loss: 1.9046 - regression_loss: 1.4077 - classification_loss: 0.4969\n",
      "315/500 [=================>............] - ETA: 3:13 - loss: 1.9084 - regression_loss: 1.4099 - classification_loss: 0.4985\n",
      "316/500 [=================>............] - ETA: 3:12 - loss: 1.9065 - regression_loss: 1.4088 - classification_loss: 0.4977\n",
      "317/500 [==================>...........] - ETA: 3:11 - loss: 1.9064 - regression_loss: 1.4089 - classification_loss: 0.4975\n",
      "318/500 [==================>...........] - ETA: 3:10 - loss: 1.9066 - regression_loss: 1.4088 - classification_loss: 0.4978\n",
      "319/500 [==================>...........] - ETA: 3:09 - loss: 1.9036 - regression_loss: 1.4066 - classification_loss: 0.4970\n",
      "320/500 [==================>...........] - ETA: 3:08 - loss: 1.9052 - regression_loss: 1.4078 - classification_loss: 0.4974\n",
      "321/500 [==================>...........] - ETA: 3:07 - loss: 1.9040 - regression_loss: 1.4067 - classification_loss: 0.4974\n",
      "322/500 [==================>...........] - ETA: 3:06 - loss: 1.9049 - regression_loss: 1.4078 - classification_loss: 0.4970\n",
      "323/500 [==================>...........] - ETA: 3:05 - loss: 1.9042 - regression_loss: 1.4076 - classification_loss: 0.4966\n",
      "324/500 [==================>...........] - ETA: 3:04 - loss: 1.9042 - regression_loss: 1.4079 - classification_loss: 0.4963\n",
      "325/500 [==================>...........] - ETA: 3:03 - loss: 1.9049 - regression_loss: 1.4080 - classification_loss: 0.4969\n",
      "326/500 [==================>...........] - ETA: 3:04 - loss: 1.9070 - regression_loss: 1.4091 - classification_loss: 0.4980\n",
      "327/500 [==================>...........] - ETA: 3:03 - loss: 1.9073 - regression_loss: 1.4093 - classification_loss: 0.4981\n",
      "328/500 [==================>...........] - ETA: 3:02 - loss: 1.9062 - regression_loss: 1.4085 - classification_loss: 0.4977\n",
      "329/500 [==================>...........] - ETA: 3:01 - loss: 1.9084 - regression_loss: 1.4107 - classification_loss: 0.4977\n",
      "330/500 [==================>...........] - ETA: 3:00 - loss: 1.9083 - regression_loss: 1.4111 - classification_loss: 0.4973\n",
      "331/500 [==================>...........] - ETA: 2:58 - loss: 1.9081 - regression_loss: 1.4110 - classification_loss: 0.4970\n",
      "332/500 [==================>...........] - ETA: 2:57 - loss: 1.9057 - regression_loss: 1.4094 - classification_loss: 0.4963\n",
      "333/500 [==================>...........] - ETA: 2:56 - loss: 1.9071 - regression_loss: 1.4103 - classification_loss: 0.4969\n",
      "334/500 [===================>..........] - ETA: 2:55 - loss: 1.9052 - regression_loss: 1.4091 - classification_loss: 0.4961\n",
      "335/500 [===================>..........] - ETA: 2:54 - loss: 1.9033 - regression_loss: 1.4075 - classification_loss: 0.4958\n",
      "336/500 [===================>..........] - ETA: 2:53 - loss: 1.9047 - regression_loss: 1.4086 - classification_loss: 0.4961\n",
      "337/500 [===================>..........] - ETA: 2:52 - loss: 1.9049 - regression_loss: 1.4092 - classification_loss: 0.4958\n",
      "338/500 [===================>..........] - ETA: 2:51 - loss: 1.9046 - regression_loss: 1.4092 - classification_loss: 0.4954\n",
      "339/500 [===================>..........] - ETA: 2:50 - loss: 1.9048 - regression_loss: 1.4099 - classification_loss: 0.4949\n",
      "340/500 [===================>..........] - ETA: 2:49 - loss: 1.9055 - regression_loss: 1.4109 - classification_loss: 0.4946\n",
      "341/500 [===================>..........] - ETA: 2:48 - loss: 1.9055 - regression_loss: 1.4111 - classification_loss: 0.4943\n",
      "342/500 [===================>..........] - ETA: 2:47 - loss: 1.9050 - regression_loss: 1.4108 - classification_loss: 0.4942\n",
      "343/500 [===================>..........] - ETA: 2:45 - loss: 1.9079 - regression_loss: 1.4133 - classification_loss: 0.4946\n",
      "344/500 [===================>..........] - ETA: 2:44 - loss: 1.9094 - regression_loss: 1.4142 - classification_loss: 0.4951\n",
      "345/500 [===================>..........] - ETA: 2:43 - loss: 1.9066 - regression_loss: 1.4124 - classification_loss: 0.4942\n",
      "346/500 [===================>..........] - ETA: 2:42 - loss: 1.9074 - regression_loss: 1.4133 - classification_loss: 0.4941\n",
      "347/500 [===================>..........] - ETA: 2:41 - loss: 1.9067 - regression_loss: 1.4130 - classification_loss: 0.4937\n",
      "348/500 [===================>..........] - ETA: 2:40 - loss: 1.9098 - regression_loss: 1.4156 - classification_loss: 0.4942\n",
      "349/500 [===================>..........] - ETA: 2:39 - loss: 1.9094 - regression_loss: 1.4152 - classification_loss: 0.4942\n",
      "350/500 [====================>.........] - ETA: 2:38 - loss: 1.9093 - regression_loss: 1.4152 - classification_loss: 0.4941\n",
      "351/500 [====================>.........] - ETA: 2:37 - loss: 1.9091 - regression_loss: 1.4153 - classification_loss: 0.4938\n",
      "352/500 [====================>.........] - ETA: 2:36 - loss: 1.9098 - regression_loss: 1.4159 - classification_loss: 0.4939\n",
      "353/500 [====================>.........] - ETA: 2:35 - loss: 1.9099 - regression_loss: 1.4154 - classification_loss: 0.4945\n",
      "354/500 [====================>.........] - ETA: 2:34 - loss: 1.9112 - regression_loss: 1.4163 - classification_loss: 0.4949\n",
      "355/500 [====================>.........] - ETA: 2:33 - loss: 1.9110 - regression_loss: 1.4163 - classification_loss: 0.4947\n",
      "356/500 [====================>.........] - ETA: 2:32 - loss: 1.9108 - regression_loss: 1.4160 - classification_loss: 0.4948\n",
      "357/500 [====================>.........] - ETA: 2:30 - loss: 1.9093 - regression_loss: 1.4150 - classification_loss: 0.4944\n",
      "358/500 [====================>.........] - ETA: 2:29 - loss: 1.9121 - regression_loss: 1.4171 - classification_loss: 0.4950\n",
      "359/500 [====================>.........] - ETA: 2:28 - loss: 1.9108 - regression_loss: 1.4163 - classification_loss: 0.4945\n",
      "360/500 [====================>.........] - ETA: 2:27 - loss: 1.9093 - regression_loss: 1.4152 - classification_loss: 0.4941\n",
      "361/500 [====================>.........] - ETA: 2:26 - loss: 1.9084 - regression_loss: 1.4146 - classification_loss: 0.4938\n",
      "362/500 [====================>.........] - ETA: 2:25 - loss: 1.9080 - regression_loss: 1.4142 - classification_loss: 0.4938\n",
      "363/500 [====================>.........] - ETA: 2:24 - loss: 1.9089 - regression_loss: 1.4151 - classification_loss: 0.4938\n",
      "364/500 [====================>.........] - ETA: 2:23 - loss: 1.9102 - regression_loss: 1.4163 - classification_loss: 0.4939\n",
      "365/500 [====================>.........] - ETA: 2:22 - loss: 1.9086 - regression_loss: 1.4155 - classification_loss: 0.4931\n",
      "366/500 [====================>.........] - ETA: 2:21 - loss: 1.9064 - regression_loss: 1.4138 - classification_loss: 0.4925\n",
      "367/500 [=====================>........] - ETA: 2:20 - loss: 1.9059 - regression_loss: 1.4134 - classification_loss: 0.4925\n",
      "368/500 [=====================>........] - ETA: 2:19 - loss: 1.9061 - regression_loss: 1.4131 - classification_loss: 0.4930\n",
      "369/500 [=====================>........] - ETA: 2:18 - loss: 1.9057 - regression_loss: 1.4131 - classification_loss: 0.4926\n",
      "370/500 [=====================>........] - ETA: 2:17 - loss: 1.9057 - regression_loss: 1.4133 - classification_loss: 0.4923\n",
      "371/500 [=====================>........] - ETA: 2:16 - loss: 1.9038 - regression_loss: 1.4121 - classification_loss: 0.4917\n",
      "372/500 [=====================>........] - ETA: 2:15 - loss: 1.9027 - regression_loss: 1.4110 - classification_loss: 0.4917\n",
      "373/500 [=====================>........] - ETA: 2:13 - loss: 1.9020 - regression_loss: 1.4108 - classification_loss: 0.4911\n",
      "374/500 [=====================>........] - ETA: 2:12 - loss: 1.8997 - regression_loss: 1.4092 - classification_loss: 0.4905\n",
      "375/500 [=====================>........] - ETA: 2:11 - loss: 1.8981 - regression_loss: 1.4077 - classification_loss: 0.4903\n",
      "376/500 [=====================>........] - ETA: 2:10 - loss: 1.8961 - regression_loss: 1.4064 - classification_loss: 0.4897\n",
      "377/500 [=====================>........] - ETA: 2:09 - loss: 1.8972 - regression_loss: 1.4075 - classification_loss: 0.4897\n",
      "378/500 [=====================>........] - ETA: 2:08 - loss: 1.8984 - regression_loss: 1.4086 - classification_loss: 0.4897\n",
      "379/500 [=====================>........] - ETA: 2:07 - loss: 1.9002 - regression_loss: 1.4102 - classification_loss: 0.4900\n",
      "380/500 [=====================>........] - ETA: 2:06 - loss: 1.8994 - regression_loss: 1.4098 - classification_loss: 0.4896\n",
      "381/500 [=====================>........] - ETA: 2:05 - loss: 1.9021 - regression_loss: 1.4118 - classification_loss: 0.4903\n",
      "382/500 [=====================>........] - ETA: 2:04 - loss: 1.9019 - regression_loss: 1.4113 - classification_loss: 0.4906\n",
      "383/500 [=====================>........] - ETA: 2:03 - loss: 1.9010 - regression_loss: 1.4106 - classification_loss: 0.4904\n",
      "384/500 [======================>.......] - ETA: 2:02 - loss: 1.9003 - regression_loss: 1.4103 - classification_loss: 0.4900\n",
      "385/500 [======================>.......] - ETA: 2:01 - loss: 1.8998 - regression_loss: 1.4100 - classification_loss: 0.4898\n",
      "386/500 [======================>.......] - ETA: 2:00 - loss: 1.9002 - regression_loss: 1.4103 - classification_loss: 0.4899\n",
      "387/500 [======================>.......] - ETA: 1:59 - loss: 1.9007 - regression_loss: 1.4105 - classification_loss: 0.4902\n",
      "388/500 [======================>.......] - ETA: 1:58 - loss: 1.9038 - regression_loss: 1.4124 - classification_loss: 0.4914\n",
      "389/500 [======================>.......] - ETA: 1:57 - loss: 1.9063 - regression_loss: 1.4144 - classification_loss: 0.4920\n",
      "390/500 [======================>.......] - ETA: 1:56 - loss: 1.9075 - regression_loss: 1.4153 - classification_loss: 0.4921\n",
      "391/500 [======================>.......] - ETA: 1:55 - loss: 1.9082 - regression_loss: 1.4158 - classification_loss: 0.4924\n",
      "392/500 [======================>.......] - ETA: 1:53 - loss: 1.9071 - regression_loss: 1.4151 - classification_loss: 0.4920\n",
      "393/500 [======================>.......] - ETA: 1:52 - loss: 1.9087 - regression_loss: 1.4161 - classification_loss: 0.4926\n",
      "394/500 [======================>.......] - ETA: 1:51 - loss: 1.9088 - regression_loss: 1.4165 - classification_loss: 0.4923\n",
      "395/500 [======================>.......] - ETA: 1:50 - loss: 1.9087 - regression_loss: 1.4163 - classification_loss: 0.4924\n",
      "396/500 [======================>.......] - ETA: 1:49 - loss: 1.9072 - regression_loss: 1.4156 - classification_loss: 0.4916\n",
      "397/500 [======================>.......] - ETA: 1:48 - loss: 1.9067 - regression_loss: 1.4155 - classification_loss: 0.4911\n",
      "398/500 [======================>.......] - ETA: 1:47 - loss: 1.9099 - regression_loss: 1.4175 - classification_loss: 0.4924\n",
      "399/500 [======================>.......] - ETA: 1:46 - loss: 1.9110 - regression_loss: 1.4186 - classification_loss: 0.4923\n",
      "400/500 [=======================>......] - ETA: 1:45 - loss: 1.9092 - regression_loss: 1.4176 - classification_loss: 0.4916\n",
      "401/500 [=======================>......] - ETA: 1:44 - loss: 1.9094 - regression_loss: 1.4182 - classification_loss: 0.4912\n",
      "402/500 [=======================>......] - ETA: 1:43 - loss: 1.9082 - regression_loss: 1.4176 - classification_loss: 0.4906\n",
      "403/500 [=======================>......] - ETA: 1:42 - loss: 1.9067 - regression_loss: 1.4167 - classification_loss: 0.4900\n",
      "404/500 [=======================>......] - ETA: 1:41 - loss: 1.9063 - regression_loss: 1.4162 - classification_loss: 0.4901\n",
      "405/500 [=======================>......] - ETA: 1:40 - loss: 1.9060 - regression_loss: 1.4160 - classification_loss: 0.4900\n",
      "406/500 [=======================>......] - ETA: 1:39 - loss: 1.9046 - regression_loss: 1.4152 - classification_loss: 0.4894\n",
      "407/500 [=======================>......] - ETA: 1:38 - loss: 1.9062 - regression_loss: 1.4167 - classification_loss: 0.4895\n",
      "408/500 [=======================>......] - ETA: 1:36 - loss: 1.9077 - regression_loss: 1.4177 - classification_loss: 0.4900\n",
      "409/500 [=======================>......] - ETA: 1:35 - loss: 1.9090 - regression_loss: 1.4184 - classification_loss: 0.4905\n",
      "410/500 [=======================>......] - ETA: 1:34 - loss: 1.9111 - regression_loss: 1.4201 - classification_loss: 0.4911\n",
      "411/500 [=======================>......] - ETA: 1:33 - loss: 1.9112 - regression_loss: 1.4203 - classification_loss: 0.4909\n",
      "412/500 [=======================>......] - ETA: 1:32 - loss: 1.9102 - regression_loss: 1.4197 - classification_loss: 0.4905\n",
      "413/500 [=======================>......] - ETA: 1:31 - loss: 1.9122 - regression_loss: 1.4207 - classification_loss: 0.4915\n",
      "414/500 [=======================>......] - ETA: 1:30 - loss: 1.9109 - regression_loss: 1.4197 - classification_loss: 0.4912\n",
      "415/500 [=======================>......] - ETA: 1:29 - loss: 1.9116 - regression_loss: 1.4205 - classification_loss: 0.4912\n",
      "416/500 [=======================>......] - ETA: 1:28 - loss: 1.9128 - regression_loss: 1.4213 - classification_loss: 0.4915\n",
      "417/500 [========================>.....] - ETA: 1:27 - loss: 1.9132 - regression_loss: 1.4219 - classification_loss: 0.4913\n",
      "418/500 [========================>.....] - ETA: 1:26 - loss: 1.9138 - regression_loss: 1.4223 - classification_loss: 0.4914\n",
      "419/500 [========================>.....] - ETA: 1:25 - loss: 1.9129 - regression_loss: 1.4217 - classification_loss: 0.4912\n",
      "420/500 [========================>.....] - ETA: 1:24 - loss: 1.9133 - regression_loss: 1.4220 - classification_loss: 0.4913\n",
      "421/500 [========================>.....] - ETA: 1:23 - loss: 1.9129 - regression_loss: 1.4217 - classification_loss: 0.4913\n",
      "422/500 [========================>.....] - ETA: 1:22 - loss: 1.9131 - regression_loss: 1.4217 - classification_loss: 0.4914\n",
      "423/500 [========================>.....] - ETA: 1:21 - loss: 1.9121 - regression_loss: 1.4209 - classification_loss: 0.4912\n",
      "424/500 [========================>.....] - ETA: 1:20 - loss: 1.9116 - regression_loss: 1.4205 - classification_loss: 0.4910\n",
      "425/500 [========================>.....] - ETA: 1:19 - loss: 1.9107 - regression_loss: 1.4200 - classification_loss: 0.4907\n",
      "426/500 [========================>.....] - ETA: 1:17 - loss: 1.9112 - regression_loss: 1.4206 - classification_loss: 0.4905\n",
      "427/500 [========================>.....] - ETA: 1:16 - loss: 1.9118 - regression_loss: 1.4202 - classification_loss: 0.4916\n",
      "428/500 [========================>.....] - ETA: 1:15 - loss: 1.9111 - regression_loss: 1.4197 - classification_loss: 0.4914\n",
      "429/500 [========================>.....] - ETA: 1:14 - loss: 1.9106 - regression_loss: 1.4194 - classification_loss: 0.4911\n",
      "430/500 [========================>.....] - ETA: 1:13 - loss: 1.9123 - regression_loss: 1.4205 - classification_loss: 0.4918\n",
      "431/500 [========================>.....] - ETA: 1:12 - loss: 1.9137 - regression_loss: 1.4217 - classification_loss: 0.4920\n",
      "432/500 [========================>.....] - ETA: 1:11 - loss: 1.9138 - regression_loss: 1.4221 - classification_loss: 0.4918\n",
      "433/500 [========================>.....] - ETA: 1:10 - loss: 1.9150 - regression_loss: 1.4229 - classification_loss: 0.4921\n",
      "434/500 [=========================>....] - ETA: 1:09 - loss: 1.9157 - regression_loss: 1.4235 - classification_loss: 0.4922\n",
      "435/500 [=========================>....] - ETA: 1:08 - loss: 1.9158 - regression_loss: 1.4239 - classification_loss: 0.4919\n",
      "436/500 [=========================>....] - ETA: 1:07 - loss: 1.9158 - regression_loss: 1.4241 - classification_loss: 0.4916\n",
      "437/500 [=========================>....] - ETA: 1:06 - loss: 1.9143 - regression_loss: 1.4232 - classification_loss: 0.4911\n",
      "438/500 [=========================>....] - ETA: 1:05 - loss: 1.9167 - regression_loss: 1.4244 - classification_loss: 0.4923\n",
      "439/500 [=========================>....] - ETA: 1:04 - loss: 1.9176 - regression_loss: 1.4250 - classification_loss: 0.4925\n",
      "440/500 [=========================>....] - ETA: 1:03 - loss: 1.9169 - regression_loss: 1.4243 - classification_loss: 0.4926\n",
      "441/500 [=========================>....] - ETA: 1:02 - loss: 1.9180 - regression_loss: 1.4250 - classification_loss: 0.4930\n",
      "442/500 [=========================>....] - ETA: 1:01 - loss: 1.9165 - regression_loss: 1.4239 - classification_loss: 0.4926\n",
      "443/500 [=========================>....] - ETA: 1:00 - loss: 1.9152 - regression_loss: 1.4232 - classification_loss: 0.4920\n",
      "444/500 [=========================>....] - ETA: 58s - loss: 1.9172 - regression_loss: 1.4250 - classification_loss: 0.4922 \n",
      "445/500 [=========================>....] - ETA: 57s - loss: 1.9171 - regression_loss: 1.4251 - classification_loss: 0.4919\n",
      "446/500 [=========================>....] - ETA: 56s - loss: 1.9186 - regression_loss: 1.4264 - classification_loss: 0.4923\n",
      "447/500 [=========================>....] - ETA: 55s - loss: 1.9182 - regression_loss: 1.4262 - classification_loss: 0.4920\n",
      "448/500 [=========================>....] - ETA: 54s - loss: 1.9199 - regression_loss: 1.4276 - classification_loss: 0.4923\n",
      "449/500 [=========================>....] - ETA: 53s - loss: 1.9187 - regression_loss: 1.4269 - classification_loss: 0.4918\n",
      "450/500 [==========================>...] - ETA: 52s - loss: 1.9182 - regression_loss: 1.4269 - classification_loss: 0.4913\n",
      "451/500 [==========================>...] - ETA: 51s - loss: 1.9184 - regression_loss: 1.4273 - classification_loss: 0.4911\n",
      "452/500 [==========================>...] - ETA: 50s - loss: 1.9175 - regression_loss: 1.4268 - classification_loss: 0.4908\n",
      "453/500 [==========================>...] - ETA: 49s - loss: 1.9199 - regression_loss: 1.4286 - classification_loss: 0.4914\n",
      "454/500 [==========================>...] - ETA: 48s - loss: 1.9225 - regression_loss: 1.4301 - classification_loss: 0.4924\n",
      "455/500 [==========================>...] - ETA: 47s - loss: 1.9212 - regression_loss: 1.4293 - classification_loss: 0.4920\n",
      "456/500 [==========================>...] - ETA: 46s - loss: 1.9201 - regression_loss: 1.4287 - classification_loss: 0.4915\n",
      "457/500 [==========================>...] - ETA: 45s - loss: 1.9190 - regression_loss: 1.4282 - classification_loss: 0.4908\n",
      "458/500 [==========================>...] - ETA: 44s - loss: 1.9180 - regression_loss: 1.4276 - classification_loss: 0.4903\n",
      "459/500 [==========================>...] - ETA: 43s - loss: 1.9207 - regression_loss: 1.4294 - classification_loss: 0.4913\n",
      "460/500 [==========================>...] - ETA: 42s - loss: 1.9209 - regression_loss: 1.4298 - classification_loss: 0.4911\n",
      "461/500 [==========================>...] - ETA: 41s - loss: 1.9208 - regression_loss: 1.4294 - classification_loss: 0.4914\n",
      "462/500 [==========================>...] - ETA: 40s - loss: 1.9207 - regression_loss: 1.4292 - classification_loss: 0.4914\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 1.9191 - regression_loss: 1.4280 - classification_loss: 0.4910\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 1.9181 - regression_loss: 1.4273 - classification_loss: 0.4909\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 1.9181 - regression_loss: 1.4273 - classification_loss: 0.4907\n",
      "466/500 [==========================>...] - ETA: 35s - loss: 1.9170 - regression_loss: 1.4267 - classification_loss: 0.4904\n",
      "467/500 [===========================>..] - ETA: 34s - loss: 1.9193 - regression_loss: 1.4285 - classification_loss: 0.4908\n",
      "468/500 [===========================>..] - ETA: 33s - loss: 1.9210 - regression_loss: 1.4297 - classification_loss: 0.4913\n",
      "469/500 [===========================>..] - ETA: 32s - loss: 1.9222 - regression_loss: 1.4308 - classification_loss: 0.4914\n",
      "470/500 [===========================>..] - ETA: 31s - loss: 1.9236 - regression_loss: 1.4322 - classification_loss: 0.4914\n",
      "471/500 [===========================>..] - ETA: 30s - loss: 1.9241 - regression_loss: 1.4328 - classification_loss: 0.4914\n",
      "472/500 [===========================>..] - ETA: 29s - loss: 1.9231 - regression_loss: 1.4321 - classification_loss: 0.4910\n",
      "473/500 [===========================>..] - ETA: 28s - loss: 1.9212 - regression_loss: 1.4307 - classification_loss: 0.4905\n",
      "474/500 [===========================>..] - ETA: 27s - loss: 1.9207 - regression_loss: 1.4306 - classification_loss: 0.4901\n",
      "475/500 [===========================>..] - ETA: 26s - loss: 1.9188 - regression_loss: 1.4292 - classification_loss: 0.4896\n",
      "476/500 [===========================>..] - ETA: 25s - loss: 1.9179 - regression_loss: 1.4288 - classification_loss: 0.4891\n",
      "477/500 [===========================>..] - ETA: 24s - loss: 1.9175 - regression_loss: 1.4284 - classification_loss: 0.4891\n",
      "478/500 [===========================>..] - ETA: 23s - loss: 1.9161 - regression_loss: 1.4273 - classification_loss: 0.4888\n",
      "479/500 [===========================>..] - ETA: 22s - loss: 1.9158 - regression_loss: 1.4273 - classification_loss: 0.4885\n",
      "480/500 [===========================>..] - ETA: 21s - loss: 1.9151 - regression_loss: 1.4269 - classification_loss: 0.4882\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.9150 - regression_loss: 1.4268 - classification_loss: 0.4882\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.9138 - regression_loss: 1.4260 - classification_loss: 0.4878\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.9124 - regression_loss: 1.4248 - classification_loss: 0.4876\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.9123 - regression_loss: 1.4246 - classification_loss: 0.4877\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.9116 - regression_loss: 1.4241 - classification_loss: 0.4875\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.9108 - regression_loss: 1.4235 - classification_loss: 0.4873\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.9127 - regression_loss: 1.4251 - classification_loss: 0.4876\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.9125 - regression_loss: 1.4251 - classification_loss: 0.4875\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.9123 - regression_loss: 1.4251 - classification_loss: 0.4872\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.9127 - regression_loss: 1.4253 - classification_loss: 0.4874\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.9130 - regression_loss: 1.4254 - classification_loss: 0.4876 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.9124 - regression_loss: 1.4247 - classification_loss: 0.4876\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.9134 - regression_loss: 1.4256 - classification_loss: 0.4878\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.9116 - regression_loss: 1.4244 - classification_loss: 0.4872\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.9124 - regression_loss: 1.4251 - classification_loss: 0.4873\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.9115 - regression_loss: 1.4244 - classification_loss: 0.4871\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.9100 - regression_loss: 1.4233 - classification_loss: 0.4867\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.9091 - regression_loss: 1.4224 - classification_loss: 0.4867\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.9098 - regression_loss: 1.4229 - classification_loss: 0.4869\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9085 - regression_loss: 1.4220 - classification_loss: 0.4864\n",
      "Epoch 00022: saving model to ./snapshots\\resnet50_csv_22.h5\n",
      "\n",
      "500/500 [==============================] - 526s 1s/step - loss: 1.9085 - regression_loss: 1.4220 - classification_loss: 0.4864\n",
      "Epoch 23/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 1.2063 - regression_loss: 0.9169 - classification_loss: 0.2894\n",
      "  2/500 [..............................] - ETA: 4:07 - loss: 1.6812 - regression_loss: 1.2711 - classification_loss: 0.4101\n",
      "  3/500 [..............................] - ETA: 5:46 - loss: 1.6402 - regression_loss: 1.2575 - classification_loss: 0.3827\n",
      "  4/500 [..............................] - ETA: 6:34 - loss: 1.6468 - regression_loss: 1.2363 - classification_loss: 0.4105\n",
      "  5/500 [..............................] - ETA: 6:51 - loss: 1.6244 - regression_loss: 1.2122 - classification_loss: 0.4123\n",
      "  6/500 [..............................] - ETA: 7:04 - loss: 1.6509 - regression_loss: 1.2500 - classification_loss: 0.4009\n",
      "  7/500 [..............................] - ETA: 7:18 - loss: 1.7128 - regression_loss: 1.2834 - classification_loss: 0.4294\n",
      "  8/500 [..............................] - ETA: 7:23 - loss: 1.7283 - regression_loss: 1.3024 - classification_loss: 0.4259\n",
      "  9/500 [..............................] - ETA: 7:27 - loss: 1.7169 - regression_loss: 1.3030 - classification_loss: 0.4139\n",
      " 10/500 [..............................] - ETA: 7:34 - loss: 1.6648 - regression_loss: 1.2578 - classification_loss: 0.4070\n",
      " 11/500 [..............................] - ETA: 7:36 - loss: 1.6827 - regression_loss: 1.2727 - classification_loss: 0.4099\n",
      " 12/500 [..............................] - ETA: 7:39 - loss: 1.7652 - regression_loss: 1.3169 - classification_loss: 0.4483\n",
      " 13/500 [..............................] - ETA: 7:44 - loss: 1.7821 - regression_loss: 1.3298 - classification_loss: 0.4523\n",
      " 14/500 [..............................] - ETA: 7:48 - loss: 1.8006 - regression_loss: 1.3513 - classification_loss: 0.4492\n",
      " 15/500 [..............................] - ETA: 7:48 - loss: 1.8438 - regression_loss: 1.3765 - classification_loss: 0.4672\n",
      " 16/500 [..............................] - ETA: 7:51 - loss: 1.8901 - regression_loss: 1.4140 - classification_loss: 0.4761\n",
      " 17/500 [>.............................] - ETA: 7:54 - loss: 1.8764 - regression_loss: 1.4068 - classification_loss: 0.4696\n",
      " 18/500 [>.............................] - ETA: 7:56 - loss: 1.9229 - regression_loss: 1.4343 - classification_loss: 0.4886\n",
      " 19/500 [>.............................] - ETA: 7:55 - loss: 1.9319 - regression_loss: 1.4436 - classification_loss: 0.4884\n",
      " 20/500 [>.............................] - ETA: 7:57 - loss: 1.9467 - regression_loss: 1.4554 - classification_loss: 0.4912\n",
      " 21/500 [>.............................] - ETA: 7:56 - loss: 1.9460 - regression_loss: 1.4552 - classification_loss: 0.4908\n",
      " 22/500 [>.............................] - ETA: 7:54 - loss: 1.9647 - regression_loss: 1.4682 - classification_loss: 0.4965\n",
      " 23/500 [>.............................] - ETA: 7:58 - loss: 1.9889 - regression_loss: 1.4738 - classification_loss: 0.5150\n",
      " 24/500 [>.............................] - ETA: 7:59 - loss: 2.0223 - regression_loss: 1.5015 - classification_loss: 0.5208\n",
      " 25/500 [>.............................] - ETA: 7:53 - loss: 1.9969 - regression_loss: 1.4873 - classification_loss: 0.5096\n",
      " 26/500 [>.............................] - ETA: 7:55 - loss: 2.0176 - regression_loss: 1.5081 - classification_loss: 0.5095\n",
      " 27/500 [>.............................] - ETA: 7:54 - loss: 1.9905 - regression_loss: 1.4894 - classification_loss: 0.5012\n",
      " 28/500 [>.............................] - ETA: 7:54 - loss: 1.9867 - regression_loss: 1.4850 - classification_loss: 0.5017\n",
      " 29/500 [>.............................] - ETA: 7:53 - loss: 1.9802 - regression_loss: 1.4781 - classification_loss: 0.5022\n",
      " 30/500 [>.............................] - ETA: 7:52 - loss: 1.9687 - regression_loss: 1.4688 - classification_loss: 0.5000\n",
      " 31/500 [>.............................] - ETA: 7:50 - loss: 1.9580 - regression_loss: 1.4645 - classification_loss: 0.4934\n",
      " 32/500 [>.............................] - ETA: 7:49 - loss: 1.9541 - regression_loss: 1.4618 - classification_loss: 0.4923\n",
      " 33/500 [>.............................] - ETA: 7:46 - loss: 1.9550 - regression_loss: 1.4616 - classification_loss: 0.4934\n",
      " 34/500 [=>............................] - ETA: 7:48 - loss: 1.9384 - regression_loss: 1.4515 - classification_loss: 0.4869\n",
      " 35/500 [=>............................] - ETA: 7:47 - loss: 1.9300 - regression_loss: 1.4463 - classification_loss: 0.4837\n",
      " 36/500 [=>............................] - ETA: 7:47 - loss: 1.9538 - regression_loss: 1.4670 - classification_loss: 0.4867\n",
      " 37/500 [=>............................] - ETA: 7:46 - loss: 1.9314 - regression_loss: 1.4523 - classification_loss: 0.4792\n",
      " 38/500 [=>............................] - ETA: 7:45 - loss: 1.9353 - regression_loss: 1.4552 - classification_loss: 0.4801\n",
      " 39/500 [=>............................] - ETA: 7:45 - loss: 1.9419 - regression_loss: 1.4632 - classification_loss: 0.4787\n",
      " 40/500 [=>............................] - ETA: 7:43 - loss: 1.9367 - regression_loss: 1.4598 - classification_loss: 0.4769\n",
      " 41/500 [=>............................] - ETA: 7:43 - loss: 1.9227 - regression_loss: 1.4490 - classification_loss: 0.4737\n",
      " 42/500 [=>............................] - ETA: 7:40 - loss: 1.9209 - regression_loss: 1.4473 - classification_loss: 0.4736\n",
      " 43/500 [=>............................] - ETA: 7:41 - loss: 1.9114 - regression_loss: 1.4436 - classification_loss: 0.4678\n",
      " 44/500 [=>............................] - ETA: 7:39 - loss: 1.8940 - regression_loss: 1.4305 - classification_loss: 0.4636\n",
      " 45/500 [=>............................] - ETA: 7:39 - loss: 1.9023 - regression_loss: 1.4369 - classification_loss: 0.4654\n",
      " 46/500 [=>............................] - ETA: 7:38 - loss: 1.9206 - regression_loss: 1.4535 - classification_loss: 0.4672\n",
      " 47/500 [=>............................] - ETA: 7:38 - loss: 1.9124 - regression_loss: 1.4490 - classification_loss: 0.4634\n",
      " 48/500 [=>............................] - ETA: 7:34 - loss: 1.9088 - regression_loss: 1.4465 - classification_loss: 0.4623\n",
      " 49/500 [=>............................] - ETA: 7:35 - loss: 1.9063 - regression_loss: 1.4474 - classification_loss: 0.4589\n",
      " 50/500 [==>...........................] - ETA: 7:35 - loss: 1.8993 - regression_loss: 1.4438 - classification_loss: 0.4554\n",
      " 51/500 [==>...........................] - ETA: 7:35 - loss: 1.9148 - regression_loss: 1.4567 - classification_loss: 0.4581\n",
      " 52/500 [==>...........................] - ETA: 7:34 - loss: 1.9117 - regression_loss: 1.4548 - classification_loss: 0.4568\n",
      " 53/500 [==>...........................] - ETA: 7:33 - loss: 1.8971 - regression_loss: 1.4445 - classification_loss: 0.4527\n",
      " 54/500 [==>...........................] - ETA: 7:33 - loss: 1.9121 - regression_loss: 1.4538 - classification_loss: 0.4583\n",
      " 55/500 [==>...........................] - ETA: 7:32 - loss: 1.9065 - regression_loss: 1.4504 - classification_loss: 0.4561\n",
      " 56/500 [==>...........................] - ETA: 7:31 - loss: 1.9096 - regression_loss: 1.4539 - classification_loss: 0.4557\n",
      " 57/500 [==>...........................] - ETA: 7:30 - loss: 1.9081 - regression_loss: 1.4524 - classification_loss: 0.4557\n",
      " 58/500 [==>...........................] - ETA: 7:29 - loss: 1.9238 - regression_loss: 1.4622 - classification_loss: 0.4617\n",
      " 59/500 [==>...........................] - ETA: 7:29 - loss: 1.9342 - regression_loss: 1.4687 - classification_loss: 0.4655\n",
      " 60/500 [==>...........................] - ETA: 7:28 - loss: 1.9204 - regression_loss: 1.4584 - classification_loss: 0.4620\n",
      " 61/500 [==>...........................] - ETA: 7:27 - loss: 1.9211 - regression_loss: 1.4589 - classification_loss: 0.4622\n",
      " 62/500 [==>...........................] - ETA: 7:26 - loss: 1.9275 - regression_loss: 1.4639 - classification_loss: 0.4636\n",
      " 63/500 [==>...........................] - ETA: 7:25 - loss: 1.9186 - regression_loss: 1.4566 - classification_loss: 0.4620\n",
      " 64/500 [==>...........................] - ETA: 7:23 - loss: 1.9264 - regression_loss: 1.4623 - classification_loss: 0.4641\n",
      " 65/500 [==>...........................] - ETA: 7:23 - loss: 1.9153 - regression_loss: 1.4531 - classification_loss: 0.4623\n",
      " 66/500 [==>...........................] - ETA: 7:23 - loss: 1.9245 - regression_loss: 1.4548 - classification_loss: 0.4697\n",
      " 67/500 [===>..........................] - ETA: 7:22 - loss: 1.9183 - regression_loss: 1.4518 - classification_loss: 0.4665\n",
      " 68/500 [===>..........................] - ETA: 7:21 - loss: 1.9074 - regression_loss: 1.4428 - classification_loss: 0.4646\n",
      " 69/500 [===>..........................] - ETA: 7:20 - loss: 1.8964 - regression_loss: 1.4348 - classification_loss: 0.4615\n",
      " 70/500 [===>..........................] - ETA: 7:18 - loss: 1.8944 - regression_loss: 1.4327 - classification_loss: 0.4617\n",
      " 71/500 [===>..........................] - ETA: 7:17 - loss: 1.9080 - regression_loss: 1.4424 - classification_loss: 0.4655\n",
      " 72/500 [===>..........................] - ETA: 7:16 - loss: 1.9056 - regression_loss: 1.4399 - classification_loss: 0.4657\n",
      " 73/500 [===>..........................] - ETA: 7:15 - loss: 1.9157 - regression_loss: 1.4465 - classification_loss: 0.4691\n",
      " 74/500 [===>..........................] - ETA: 7:15 - loss: 1.9079 - regression_loss: 1.4394 - classification_loss: 0.4685\n",
      " 75/500 [===>..........................] - ETA: 7:13 - loss: 1.9115 - regression_loss: 1.4419 - classification_loss: 0.4697\n",
      " 76/500 [===>..........................] - ETA: 7:12 - loss: 1.9043 - regression_loss: 1.4362 - classification_loss: 0.4681\n",
      " 77/500 [===>..........................] - ETA: 7:11 - loss: 1.9029 - regression_loss: 1.4360 - classification_loss: 0.4668\n",
      " 78/500 [===>..........................] - ETA: 7:11 - loss: 1.9035 - regression_loss: 1.4367 - classification_loss: 0.4667\n",
      " 79/500 [===>..........................] - ETA: 7:10 - loss: 1.9225 - regression_loss: 1.4482 - classification_loss: 0.4742\n",
      " 80/500 [===>..........................] - ETA: 7:09 - loss: 1.9199 - regression_loss: 1.4472 - classification_loss: 0.4728\n",
      " 81/500 [===>..........................] - ETA: 7:08 - loss: 1.9213 - regression_loss: 1.4498 - classification_loss: 0.4715\n",
      " 82/500 [===>..........................] - ETA: 7:08 - loss: 1.9309 - regression_loss: 1.4554 - classification_loss: 0.4755\n",
      " 83/500 [===>..........................] - ETA: 7:07 - loss: 1.9385 - regression_loss: 1.4599 - classification_loss: 0.4786\n",
      " 84/500 [====>.........................] - ETA: 7:06 - loss: 1.9497 - regression_loss: 1.4645 - classification_loss: 0.4851\n",
      " 85/500 [====>.........................] - ETA: 7:05 - loss: 1.9467 - regression_loss: 1.4635 - classification_loss: 0.4832\n",
      " 86/500 [====>.........................] - ETA: 7:04 - loss: 1.9345 - regression_loss: 1.4549 - classification_loss: 0.4795\n",
      " 87/500 [====>.........................] - ETA: 7:03 - loss: 1.9286 - regression_loss: 1.4508 - classification_loss: 0.4778\n",
      " 88/500 [====>.........................] - ETA: 7:01 - loss: 1.9223 - regression_loss: 1.4467 - classification_loss: 0.4756\n",
      " 89/500 [====>.........................] - ETA: 7:01 - loss: 1.9125 - regression_loss: 1.4392 - classification_loss: 0.4733\n",
      " 90/500 [====>.........................] - ETA: 6:59 - loss: 1.9087 - regression_loss: 1.4358 - classification_loss: 0.4728\n",
      " 91/500 [====>.........................] - ETA: 6:58 - loss: 1.9238 - regression_loss: 1.4462 - classification_loss: 0.4777\n",
      " 92/500 [====>.........................] - ETA: 6:57 - loss: 1.9255 - regression_loss: 1.4467 - classification_loss: 0.4788\n",
      " 93/500 [====>.........................] - ETA: 6:57 - loss: 1.9290 - regression_loss: 1.4491 - classification_loss: 0.4799\n",
      " 94/500 [====>.........................] - ETA: 6:56 - loss: 1.9328 - regression_loss: 1.4530 - classification_loss: 0.4799\n",
      " 95/500 [====>.........................] - ETA: 6:55 - loss: 1.9249 - regression_loss: 1.4466 - classification_loss: 0.4783\n",
      " 96/500 [====>.........................] - ETA: 6:53 - loss: 1.9236 - regression_loss: 1.4465 - classification_loss: 0.4771\n",
      " 97/500 [====>.........................] - ETA: 6:52 - loss: 1.9257 - regression_loss: 1.4470 - classification_loss: 0.4786\n",
      " 98/500 [====>.........................] - ETA: 6:51 - loss: 1.9195 - regression_loss: 1.4426 - classification_loss: 0.4769\n",
      " 99/500 [====>.........................] - ETA: 6:50 - loss: 1.9246 - regression_loss: 1.4451 - classification_loss: 0.4795\n",
      "100/500 [=====>........................] - ETA: 6:49 - loss: 1.9186 - regression_loss: 1.4416 - classification_loss: 0.4770\n",
      "101/500 [=====>........................] - ETA: 6:48 - loss: 1.9209 - regression_loss: 1.4441 - classification_loss: 0.4767\n",
      "102/500 [=====>........................] - ETA: 6:47 - loss: 1.9262 - regression_loss: 1.4508 - classification_loss: 0.4754\n",
      "103/500 [=====>........................] - ETA: 6:45 - loss: 1.9299 - regression_loss: 1.4525 - classification_loss: 0.4774\n",
      "104/500 [=====>........................] - ETA: 6:45 - loss: 1.9263 - regression_loss: 1.4508 - classification_loss: 0.4755\n",
      "105/500 [=====>........................] - ETA: 6:44 - loss: 1.9360 - regression_loss: 1.4558 - classification_loss: 0.4802\n",
      "106/500 [=====>........................] - ETA: 6:44 - loss: 1.9352 - regression_loss: 1.4563 - classification_loss: 0.4789\n",
      "107/500 [=====>........................] - ETA: 6:42 - loss: 1.9350 - regression_loss: 1.4551 - classification_loss: 0.4800\n",
      "108/500 [=====>........................] - ETA: 6:41 - loss: 1.9337 - regression_loss: 1.4532 - classification_loss: 0.4805\n",
      "109/500 [=====>........................] - ETA: 6:40 - loss: 1.9336 - regression_loss: 1.4527 - classification_loss: 0.4809\n",
      "110/500 [=====>........................] - ETA: 6:40 - loss: 1.9293 - regression_loss: 1.4498 - classification_loss: 0.4795\n",
      "111/500 [=====>........................] - ETA: 6:38 - loss: 1.9230 - regression_loss: 1.4442 - classification_loss: 0.4787\n",
      "112/500 [=====>........................] - ETA: 6:37 - loss: 1.9213 - regression_loss: 1.4425 - classification_loss: 0.4788\n",
      "113/500 [=====>........................] - ETA: 6:37 - loss: 1.9210 - regression_loss: 1.4405 - classification_loss: 0.4805\n",
      "114/500 [=====>........................] - ETA: 6:35 - loss: 1.9279 - regression_loss: 1.4447 - classification_loss: 0.4832\n",
      "115/500 [=====>........................] - ETA: 6:35 - loss: 1.9255 - regression_loss: 1.4437 - classification_loss: 0.4817\n",
      "116/500 [=====>........................] - ETA: 6:34 - loss: 1.9200 - regression_loss: 1.4398 - classification_loss: 0.4802\n",
      "117/500 [======>.......................] - ETA: 6:33 - loss: 1.9171 - regression_loss: 1.4372 - classification_loss: 0.4799\n",
      "118/500 [======>.......................] - ETA: 6:31 - loss: 1.9156 - regression_loss: 1.4358 - classification_loss: 0.4798\n",
      "119/500 [======>.......................] - ETA: 6:31 - loss: 1.9182 - regression_loss: 1.4375 - classification_loss: 0.4807\n",
      "120/500 [======>.......................] - ETA: 6:29 - loss: 1.9260 - regression_loss: 1.4425 - classification_loss: 0.4835\n",
      "121/500 [======>.......................] - ETA: 6:29 - loss: 1.9262 - regression_loss: 1.4433 - classification_loss: 0.4828\n",
      "122/500 [======>.......................] - ETA: 6:28 - loss: 1.9283 - regression_loss: 1.4447 - classification_loss: 0.4836\n",
      "123/500 [======>.......................] - ETA: 6:26 - loss: 1.9287 - regression_loss: 1.4453 - classification_loss: 0.4834\n",
      "124/500 [======>.......................] - ETA: 6:25 - loss: 1.9245 - regression_loss: 1.4416 - classification_loss: 0.4829\n",
      "125/500 [======>.......................] - ETA: 6:24 - loss: 1.9258 - regression_loss: 1.4422 - classification_loss: 0.4836\n",
      "126/500 [======>.......................] - ETA: 6:23 - loss: 1.9251 - regression_loss: 1.4429 - classification_loss: 0.4822\n",
      "127/500 [======>.......................] - ETA: 6:22 - loss: 1.9208 - regression_loss: 1.4394 - classification_loss: 0.4814\n",
      "128/500 [======>.......................] - ETA: 6:21 - loss: 1.9186 - regression_loss: 1.4384 - classification_loss: 0.4802\n",
      "129/500 [======>.......................] - ETA: 6:20 - loss: 1.9146 - regression_loss: 1.4356 - classification_loss: 0.4790\n",
      "130/500 [======>.......................] - ETA: 6:19 - loss: 1.9163 - regression_loss: 1.4384 - classification_loss: 0.4778\n",
      "131/500 [======>.......................] - ETA: 6:17 - loss: 1.9142 - regression_loss: 1.4370 - classification_loss: 0.4772\n",
      "132/500 [======>.......................] - ETA: 6:16 - loss: 1.9225 - regression_loss: 1.4424 - classification_loss: 0.4801\n",
      "133/500 [======>.......................] - ETA: 6:15 - loss: 1.9213 - regression_loss: 1.4417 - classification_loss: 0.4796\n",
      "134/500 [=======>......................] - ETA: 6:14 - loss: 1.9199 - regression_loss: 1.4394 - classification_loss: 0.4805\n",
      "135/500 [=======>......................] - ETA: 6:13 - loss: 1.9165 - regression_loss: 1.4373 - classification_loss: 0.4792\n",
      "136/500 [=======>......................] - ETA: 6:12 - loss: 1.9179 - regression_loss: 1.4391 - classification_loss: 0.4788\n",
      "137/500 [=======>......................] - ETA: 6:11 - loss: 1.9175 - regression_loss: 1.4389 - classification_loss: 0.4786\n",
      "138/500 [=======>......................] - ETA: 6:10 - loss: 1.9193 - regression_loss: 1.4395 - classification_loss: 0.4798\n",
      "139/500 [=======>......................] - ETA: 6:09 - loss: 1.9146 - regression_loss: 1.4363 - classification_loss: 0.4783\n",
      "140/500 [=======>......................] - ETA: 6:08 - loss: 1.9190 - regression_loss: 1.4405 - classification_loss: 0.4785\n",
      "141/500 [=======>......................] - ETA: 6:07 - loss: 1.9193 - regression_loss: 1.4400 - classification_loss: 0.4793\n",
      "142/500 [=======>......................] - ETA: 6:05 - loss: 1.9183 - regression_loss: 1.4380 - classification_loss: 0.4803\n",
      "143/500 [=======>......................] - ETA: 6:05 - loss: 1.9167 - regression_loss: 1.4370 - classification_loss: 0.4798\n",
      "144/500 [=======>......................] - ETA: 6:04 - loss: 1.9164 - regression_loss: 1.4371 - classification_loss: 0.4794\n",
      "145/500 [=======>......................] - ETA: 6:03 - loss: 1.9123 - regression_loss: 1.4340 - classification_loss: 0.4783\n",
      "146/500 [=======>......................] - ETA: 6:02 - loss: 1.9192 - regression_loss: 1.4388 - classification_loss: 0.4805\n",
      "147/500 [=======>......................] - ETA: 6:01 - loss: 1.9245 - regression_loss: 1.4422 - classification_loss: 0.4823\n",
      "148/500 [=======>......................] - ETA: 6:00 - loss: 1.9297 - regression_loss: 1.4460 - classification_loss: 0.4836\n",
      "149/500 [=======>......................] - ETA: 5:59 - loss: 1.9234 - regression_loss: 1.4414 - classification_loss: 0.4820\n",
      "150/500 [========>.....................] - ETA: 5:57 - loss: 1.9253 - regression_loss: 1.4412 - classification_loss: 0.4841\n",
      "151/500 [========>.....................] - ETA: 5:56 - loss: 1.9284 - regression_loss: 1.4429 - classification_loss: 0.4855\n",
      "152/500 [========>.....................] - ETA: 5:55 - loss: 1.9325 - regression_loss: 1.4443 - classification_loss: 0.4882\n",
      "153/500 [========>.....................] - ETA: 5:54 - loss: 1.9359 - regression_loss: 1.4454 - classification_loss: 0.4906\n",
      "154/500 [========>.....................] - ETA: 5:52 - loss: 1.9371 - regression_loss: 1.4467 - classification_loss: 0.4904\n",
      "155/500 [========>.....................] - ETA: 5:52 - loss: 1.9379 - regression_loss: 1.4468 - classification_loss: 0.4911\n",
      "156/500 [========>.....................] - ETA: 5:51 - loss: 1.9418 - regression_loss: 1.4500 - classification_loss: 0.4917\n",
      "157/500 [========>.....................] - ETA: 5:50 - loss: 1.9434 - regression_loss: 1.4510 - classification_loss: 0.4924\n",
      "158/500 [========>.....................] - ETA: 5:49 - loss: 1.9376 - regression_loss: 1.4463 - classification_loss: 0.4913\n",
      "159/500 [========>.....................] - ETA: 5:48 - loss: 1.9401 - regression_loss: 1.4481 - classification_loss: 0.4920\n",
      "160/500 [========>.....................] - ETA: 5:47 - loss: 1.9359 - regression_loss: 1.4448 - classification_loss: 0.4911\n",
      "161/500 [========>.....................] - ETA: 5:46 - loss: 1.9340 - regression_loss: 1.4432 - classification_loss: 0.4907\n",
      "162/500 [========>.....................] - ETA: 5:45 - loss: 1.9302 - regression_loss: 1.4408 - classification_loss: 0.4895\n",
      "163/500 [========>.....................] - ETA: 5:44 - loss: 1.9284 - regression_loss: 1.4393 - classification_loss: 0.4891\n",
      "164/500 [========>.....................] - ETA: 5:43 - loss: 1.9247 - regression_loss: 1.4363 - classification_loss: 0.4884\n",
      "165/500 [========>.....................] - ETA: 5:43 - loss: 1.9275 - regression_loss: 1.4386 - classification_loss: 0.4890\n",
      "166/500 [========>.....................] - ETA: 5:42 - loss: 1.9268 - regression_loss: 1.4382 - classification_loss: 0.4886\n",
      "167/500 [=========>....................] - ETA: 5:41 - loss: 1.9298 - regression_loss: 1.4412 - classification_loss: 0.4887\n",
      "168/500 [=========>....................] - ETA: 5:40 - loss: 1.9327 - regression_loss: 1.4426 - classification_loss: 0.4902\n",
      "169/500 [=========>....................] - ETA: 5:39 - loss: 1.9276 - regression_loss: 1.4387 - classification_loss: 0.4890\n",
      "170/500 [=========>....................] - ETA: 5:38 - loss: 1.9297 - regression_loss: 1.4406 - classification_loss: 0.4890\n",
      "171/500 [=========>....................] - ETA: 5:37 - loss: 1.9262 - regression_loss: 1.4384 - classification_loss: 0.4878\n",
      "172/500 [=========>....................] - ETA: 5:36 - loss: 1.9242 - regression_loss: 1.4364 - classification_loss: 0.4878\n",
      "173/500 [=========>....................] - ETA: 5:35 - loss: 1.9236 - regression_loss: 1.4362 - classification_loss: 0.4874\n",
      "174/500 [=========>....................] - ETA: 5:34 - loss: 1.9257 - regression_loss: 1.4378 - classification_loss: 0.4879\n",
      "175/500 [=========>....................] - ETA: 5:33 - loss: 1.9194 - regression_loss: 1.4332 - classification_loss: 0.4863\n",
      "176/500 [=========>....................] - ETA: 5:32 - loss: 1.9219 - regression_loss: 1.4355 - classification_loss: 0.4864\n",
      "177/500 [=========>....................] - ETA: 5:31 - loss: 1.9227 - regression_loss: 1.4347 - classification_loss: 0.4880\n",
      "178/500 [=========>....................] - ETA: 5:30 - loss: 1.9205 - regression_loss: 1.4330 - classification_loss: 0.4875\n",
      "179/500 [=========>....................] - ETA: 5:29 - loss: 1.9253 - regression_loss: 1.4355 - classification_loss: 0.4898\n",
      "180/500 [=========>....................] - ETA: 5:28 - loss: 1.9217 - regression_loss: 1.4332 - classification_loss: 0.4885\n",
      "181/500 [=========>....................] - ETA: 5:27 - loss: 1.9233 - regression_loss: 1.4347 - classification_loss: 0.4886\n",
      "182/500 [=========>....................] - ETA: 5:26 - loss: 1.9247 - regression_loss: 1.4349 - classification_loss: 0.4899\n",
      "183/500 [=========>....................] - ETA: 5:25 - loss: 1.9255 - regression_loss: 1.4356 - classification_loss: 0.4900\n",
      "184/500 [==========>...................] - ETA: 5:24 - loss: 1.9256 - regression_loss: 1.4355 - classification_loss: 0.4900\n",
      "185/500 [==========>...................] - ETA: 5:23 - loss: 1.9236 - regression_loss: 1.4346 - classification_loss: 0.4889\n",
      "186/500 [==========>...................] - ETA: 5:22 - loss: 1.9244 - regression_loss: 1.4353 - classification_loss: 0.4891\n",
      "187/500 [==========>...................] - ETA: 5:21 - loss: 1.9222 - regression_loss: 1.4330 - classification_loss: 0.4892\n",
      "188/500 [==========>...................] - ETA: 5:20 - loss: 1.9225 - regression_loss: 1.4325 - classification_loss: 0.4900\n",
      "189/500 [==========>...................] - ETA: 5:18 - loss: 1.9244 - regression_loss: 1.4342 - classification_loss: 0.4902\n",
      "190/500 [==========>...................] - ETA: 5:17 - loss: 1.9220 - regression_loss: 1.4323 - classification_loss: 0.4896\n",
      "191/500 [==========>...................] - ETA: 5:16 - loss: 1.9223 - regression_loss: 1.4333 - classification_loss: 0.4890\n",
      "192/500 [==========>...................] - ETA: 5:15 - loss: 1.9204 - regression_loss: 1.4320 - classification_loss: 0.4884\n",
      "193/500 [==========>...................] - ETA: 5:14 - loss: 1.9202 - regression_loss: 1.4317 - classification_loss: 0.4885\n",
      "194/500 [==========>...................] - ETA: 5:13 - loss: 1.9179 - regression_loss: 1.4301 - classification_loss: 0.4878\n",
      "195/500 [==========>...................] - ETA: 5:12 - loss: 1.9197 - regression_loss: 1.4318 - classification_loss: 0.4880\n",
      "196/500 [==========>...................] - ETA: 5:11 - loss: 1.9181 - regression_loss: 1.4305 - classification_loss: 0.4876\n",
      "197/500 [==========>...................] - ETA: 5:10 - loss: 1.9179 - regression_loss: 1.4300 - classification_loss: 0.4878\n",
      "198/500 [==========>...................] - ETA: 5:09 - loss: 1.9193 - regression_loss: 1.4307 - classification_loss: 0.4887\n",
      "199/500 [==========>...................] - ETA: 5:08 - loss: 1.9194 - regression_loss: 1.4312 - classification_loss: 0.4882\n",
      "200/500 [===========>..................] - ETA: 5:07 - loss: 1.9212 - regression_loss: 1.4327 - classification_loss: 0.4885\n",
      "201/500 [===========>..................] - ETA: 5:06 - loss: 1.9198 - regression_loss: 1.4318 - classification_loss: 0.4880\n",
      "202/500 [===========>..................] - ETA: 5:05 - loss: 1.9225 - regression_loss: 1.4335 - classification_loss: 0.4891\n",
      "203/500 [===========>..................] - ETA: 5:04 - loss: 1.9244 - regression_loss: 1.4350 - classification_loss: 0.4894\n",
      "204/500 [===========>..................] - ETA: 5:03 - loss: 1.9266 - regression_loss: 1.4366 - classification_loss: 0.4900\n",
      "205/500 [===========>..................] - ETA: 5:02 - loss: 1.9264 - regression_loss: 1.4365 - classification_loss: 0.4899\n",
      "206/500 [===========>..................] - ETA: 5:01 - loss: 1.9295 - regression_loss: 1.4392 - classification_loss: 0.4903\n",
      "207/500 [===========>..................] - ETA: 5:00 - loss: 1.9264 - regression_loss: 1.4371 - classification_loss: 0.4893\n",
      "208/500 [===========>..................] - ETA: 4:59 - loss: 1.9269 - regression_loss: 1.4376 - classification_loss: 0.4893\n",
      "209/500 [===========>..................] - ETA: 4:58 - loss: 1.9234 - regression_loss: 1.4354 - classification_loss: 0.4880\n",
      "210/500 [===========>..................] - ETA: 4:57 - loss: 1.9241 - regression_loss: 1.4350 - classification_loss: 0.4891\n",
      "211/500 [===========>..................] - ETA: 4:56 - loss: 1.9247 - regression_loss: 1.4361 - classification_loss: 0.4886\n",
      "212/500 [===========>..................] - ETA: 4:55 - loss: 1.9222 - regression_loss: 1.4341 - classification_loss: 0.4881\n",
      "213/500 [===========>..................] - ETA: 4:54 - loss: 1.9221 - regression_loss: 1.4345 - classification_loss: 0.4876\n",
      "214/500 [===========>..................] - ETA: 4:53 - loss: 1.9276 - regression_loss: 1.4374 - classification_loss: 0.4902\n",
      "215/500 [===========>..................] - ETA: 4:52 - loss: 1.9283 - regression_loss: 1.4381 - classification_loss: 0.4902\n",
      "216/500 [===========>..................] - ETA: 4:51 - loss: 1.9293 - regression_loss: 1.4398 - classification_loss: 0.4895\n",
      "217/500 [============>.................] - ETA: 4:50 - loss: 1.9268 - regression_loss: 1.4384 - classification_loss: 0.4885\n",
      "218/500 [============>.................] - ETA: 4:49 - loss: 1.9257 - regression_loss: 1.4378 - classification_loss: 0.4879\n",
      "219/500 [============>.................] - ETA: 4:48 - loss: 1.9245 - regression_loss: 1.4374 - classification_loss: 0.4871\n",
      "220/500 [============>.................] - ETA: 4:47 - loss: 1.9231 - regression_loss: 1.4367 - classification_loss: 0.4864\n",
      "221/500 [============>.................] - ETA: 4:46 - loss: 1.9251 - regression_loss: 1.4379 - classification_loss: 0.4872\n",
      "222/500 [============>.................] - ETA: 4:45 - loss: 1.9286 - regression_loss: 1.4409 - classification_loss: 0.4877\n",
      "223/500 [============>.................] - ETA: 4:44 - loss: 1.9249 - regression_loss: 1.4381 - classification_loss: 0.4868\n",
      "224/500 [============>.................] - ETA: 4:43 - loss: 1.9244 - regression_loss: 1.4380 - classification_loss: 0.4864\n",
      "225/500 [============>.................] - ETA: 4:42 - loss: 1.9201 - regression_loss: 1.4349 - classification_loss: 0.4853\n",
      "226/500 [============>.................] - ETA: 4:41 - loss: 1.9228 - regression_loss: 1.4368 - classification_loss: 0.4860\n",
      "227/500 [============>.................] - ETA: 4:40 - loss: 1.9247 - regression_loss: 1.4385 - classification_loss: 0.4862\n",
      "228/500 [============>.................] - ETA: 4:39 - loss: 1.9243 - regression_loss: 1.4384 - classification_loss: 0.4860\n",
      "229/500 [============>.................] - ETA: 4:37 - loss: 1.9213 - regression_loss: 1.4359 - classification_loss: 0.4854\n",
      "230/500 [============>.................] - ETA: 4:36 - loss: 1.9233 - regression_loss: 1.4371 - classification_loss: 0.4862\n",
      "231/500 [============>.................] - ETA: 4:36 - loss: 1.9247 - regression_loss: 1.4385 - classification_loss: 0.4862\n",
      "232/500 [============>.................] - ETA: 4:34 - loss: 1.9207 - regression_loss: 1.4358 - classification_loss: 0.4849\n",
      "233/500 [============>.................] - ETA: 4:33 - loss: 1.9218 - regression_loss: 1.4367 - classification_loss: 0.4852\n",
      "234/500 [=============>................] - ETA: 4:33 - loss: 1.9259 - regression_loss: 1.4392 - classification_loss: 0.4867\n",
      "235/500 [=============>................] - ETA: 4:32 - loss: 1.9253 - regression_loss: 1.4388 - classification_loss: 0.4866\n",
      "236/500 [=============>................] - ETA: 4:30 - loss: 1.9281 - regression_loss: 1.4406 - classification_loss: 0.4874\n",
      "237/500 [=============>................] - ETA: 4:29 - loss: 1.9248 - regression_loss: 1.4388 - classification_loss: 0.4859\n",
      "238/500 [=============>................] - ETA: 4:28 - loss: 1.9215 - regression_loss: 1.4361 - classification_loss: 0.4853\n",
      "239/500 [=============>................] - ETA: 4:27 - loss: 1.9249 - regression_loss: 1.4386 - classification_loss: 0.4863\n",
      "240/500 [=============>................] - ETA: 4:26 - loss: 1.9226 - regression_loss: 1.4370 - classification_loss: 0.4856\n",
      "241/500 [=============>................] - ETA: 4:25 - loss: 1.9209 - regression_loss: 1.4354 - classification_loss: 0.4855\n",
      "242/500 [=============>................] - ETA: 4:24 - loss: 1.9214 - regression_loss: 1.4354 - classification_loss: 0.4860\n",
      "243/500 [=============>................] - ETA: 4:23 - loss: 1.9219 - regression_loss: 1.4358 - classification_loss: 0.4860\n",
      "244/500 [=============>................] - ETA: 4:22 - loss: 1.9199 - regression_loss: 1.4339 - classification_loss: 0.4861\n",
      "245/500 [=============>................] - ETA: 4:21 - loss: 1.9201 - regression_loss: 1.4345 - classification_loss: 0.4856\n",
      "246/500 [=============>................] - ETA: 4:20 - loss: 1.9183 - regression_loss: 1.4331 - classification_loss: 0.4852\n",
      "247/500 [=============>................] - ETA: 4:19 - loss: 1.9216 - regression_loss: 1.4355 - classification_loss: 0.4861\n",
      "248/500 [=============>................] - ETA: 4:18 - loss: 1.9202 - regression_loss: 1.4346 - classification_loss: 0.4856\n",
      "249/500 [=============>................] - ETA: 4:17 - loss: 1.9246 - regression_loss: 1.4379 - classification_loss: 0.4867\n",
      "250/500 [==============>...............] - ETA: 4:16 - loss: 1.9252 - regression_loss: 1.4380 - classification_loss: 0.4872\n",
      "251/500 [==============>...............] - ETA: 4:15 - loss: 1.9240 - regression_loss: 1.4372 - classification_loss: 0.4869\n",
      "252/500 [==============>...............] - ETA: 4:14 - loss: 1.9245 - regression_loss: 1.4377 - classification_loss: 0.4868\n",
      "253/500 [==============>...............] - ETA: 4:13 - loss: 1.9256 - regression_loss: 1.4384 - classification_loss: 0.4872\n",
      "254/500 [==============>...............] - ETA: 4:12 - loss: 1.9216 - regression_loss: 1.4355 - classification_loss: 0.4860\n",
      "255/500 [==============>...............] - ETA: 4:11 - loss: 1.9211 - regression_loss: 1.4356 - classification_loss: 0.4854\n",
      "256/500 [==============>...............] - ETA: 4:10 - loss: 1.9229 - regression_loss: 1.4370 - classification_loss: 0.4859\n",
      "257/500 [==============>...............] - ETA: 4:09 - loss: 1.9255 - regression_loss: 1.4386 - classification_loss: 0.4869\n",
      "258/500 [==============>...............] - ETA: 4:08 - loss: 1.9250 - regression_loss: 1.4379 - classification_loss: 0.4871\n",
      "259/500 [==============>...............] - ETA: 4:07 - loss: 1.9250 - regression_loss: 1.4379 - classification_loss: 0.4871\n",
      "260/500 [==============>...............] - ETA: 4:06 - loss: 1.9247 - regression_loss: 1.4369 - classification_loss: 0.4879\n",
      "261/500 [==============>...............] - ETA: 4:05 - loss: 1.9283 - regression_loss: 1.4387 - classification_loss: 0.4896\n",
      "262/500 [==============>...............] - ETA: 4:03 - loss: 1.9274 - regression_loss: 1.4382 - classification_loss: 0.4892\n",
      "263/500 [==============>...............] - ETA: 4:02 - loss: 1.9271 - regression_loss: 1.4380 - classification_loss: 0.4891\n",
      "264/500 [==============>...............] - ETA: 4:01 - loss: 1.9286 - regression_loss: 1.4388 - classification_loss: 0.4898\n",
      "265/500 [==============>...............] - ETA: 4:00 - loss: 1.9316 - regression_loss: 1.4412 - classification_loss: 0.4904\n",
      "266/500 [==============>...............] - ETA: 3:59 - loss: 1.9309 - regression_loss: 1.4409 - classification_loss: 0.4900\n",
      "267/500 [===============>..............] - ETA: 3:58 - loss: 1.9341 - regression_loss: 1.4441 - classification_loss: 0.4900\n",
      "268/500 [===============>..............] - ETA: 3:57 - loss: 1.9366 - regression_loss: 1.4452 - classification_loss: 0.4915\n",
      "269/500 [===============>..............] - ETA: 3:56 - loss: 1.9352 - regression_loss: 1.4442 - classification_loss: 0.4910\n",
      "270/500 [===============>..............] - ETA: 3:55 - loss: 1.9341 - regression_loss: 1.4437 - classification_loss: 0.4903\n",
      "271/500 [===============>..............] - ETA: 3:54 - loss: 1.9335 - regression_loss: 1.4433 - classification_loss: 0.4902\n",
      "272/500 [===============>..............] - ETA: 3:53 - loss: 1.9344 - regression_loss: 1.4445 - classification_loss: 0.4899\n",
      "273/500 [===============>..............] - ETA: 3:52 - loss: 1.9365 - regression_loss: 1.4460 - classification_loss: 0.4905\n",
      "274/500 [===============>..............] - ETA: 3:51 - loss: 1.9341 - regression_loss: 1.4444 - classification_loss: 0.4897\n",
      "275/500 [===============>..............] - ETA: 3:50 - loss: 1.9325 - regression_loss: 1.4432 - classification_loss: 0.4893\n",
      "276/500 [===============>..............] - ETA: 3:49 - loss: 1.9291 - regression_loss: 1.4402 - classification_loss: 0.4888\n",
      "277/500 [===============>..............] - ETA: 3:48 - loss: 1.9302 - regression_loss: 1.4407 - classification_loss: 0.4894\n",
      "278/500 [===============>..............] - ETA: 3:47 - loss: 1.9328 - regression_loss: 1.4427 - classification_loss: 0.4901\n",
      "279/500 [===============>..............] - ETA: 3:46 - loss: 1.9330 - regression_loss: 1.4429 - classification_loss: 0.4901\n",
      "280/500 [===============>..............] - ETA: 3:45 - loss: 1.9334 - regression_loss: 1.4431 - classification_loss: 0.4903\n",
      "281/500 [===============>..............] - ETA: 3:44 - loss: 1.9368 - regression_loss: 1.4452 - classification_loss: 0.4916\n",
      "282/500 [===============>..............] - ETA: 3:46 - loss: 1.9396 - regression_loss: 1.4463 - classification_loss: 0.4933\n",
      "283/500 [===============>..............] - ETA: 3:45 - loss: 1.9392 - regression_loss: 1.4455 - classification_loss: 0.4936\n",
      "284/500 [================>.............] - ETA: 3:44 - loss: 1.9397 - regression_loss: 1.4461 - classification_loss: 0.4936\n",
      "285/500 [================>.............] - ETA: 3:43 - loss: 1.9400 - regression_loss: 1.4469 - classification_loss: 0.4932\n",
      "286/500 [================>.............] - ETA: 3:42 - loss: 1.9409 - regression_loss: 1.4470 - classification_loss: 0.4939\n",
      "287/500 [================>.............] - ETA: 3:41 - loss: 1.9397 - regression_loss: 1.4463 - classification_loss: 0.4935\n",
      "288/500 [================>.............] - ETA: 3:40 - loss: 1.9401 - regression_loss: 1.4461 - classification_loss: 0.4940\n",
      "289/500 [================>.............] - ETA: 3:39 - loss: 1.9417 - regression_loss: 1.4473 - classification_loss: 0.4944\n",
      "290/500 [================>.............] - ETA: 3:38 - loss: 1.9426 - regression_loss: 1.4483 - classification_loss: 0.4943\n",
      "291/500 [================>.............] - ETA: 3:37 - loss: 1.9441 - regression_loss: 1.4493 - classification_loss: 0.4948\n",
      "292/500 [================>.............] - ETA: 3:36 - loss: 1.9430 - regression_loss: 1.4485 - classification_loss: 0.4945\n",
      "293/500 [================>.............] - ETA: 3:35 - loss: 1.9436 - regression_loss: 1.4490 - classification_loss: 0.4946\n",
      "294/500 [================>.............] - ETA: 3:34 - loss: 1.9427 - regression_loss: 1.4487 - classification_loss: 0.4939\n",
      "295/500 [================>.............] - ETA: 3:33 - loss: 1.9438 - regression_loss: 1.4498 - classification_loss: 0.4939\n",
      "296/500 [================>.............] - ETA: 3:32 - loss: 1.9435 - regression_loss: 1.4496 - classification_loss: 0.4939\n",
      "297/500 [================>.............] - ETA: 3:31 - loss: 1.9409 - regression_loss: 1.4476 - classification_loss: 0.4933\n",
      "298/500 [================>.............] - ETA: 3:29 - loss: 1.9398 - regression_loss: 1.4469 - classification_loss: 0.4929\n",
      "299/500 [================>.............] - ETA: 3:28 - loss: 1.9419 - regression_loss: 1.4489 - classification_loss: 0.4930\n",
      "300/500 [=================>............] - ETA: 3:27 - loss: 1.9447 - regression_loss: 1.4512 - classification_loss: 0.4936\n",
      "301/500 [=================>............] - ETA: 3:26 - loss: 1.9452 - regression_loss: 1.4519 - classification_loss: 0.4933\n",
      "302/500 [=================>............] - ETA: 3:25 - loss: 1.9434 - regression_loss: 1.4503 - classification_loss: 0.4931\n",
      "303/500 [=================>............] - ETA: 3:24 - loss: 1.9430 - regression_loss: 1.4500 - classification_loss: 0.4930\n",
      "304/500 [=================>............] - ETA: 3:23 - loss: 1.9407 - regression_loss: 1.4485 - classification_loss: 0.4922\n",
      "305/500 [=================>............] - ETA: 3:22 - loss: 1.9401 - regression_loss: 1.4474 - classification_loss: 0.4927\n",
      "306/500 [=================>............] - ETA: 3:21 - loss: 1.9382 - regression_loss: 1.4460 - classification_loss: 0.4922\n",
      "307/500 [=================>............] - ETA: 3:20 - loss: 1.9410 - regression_loss: 1.4483 - classification_loss: 0.4927\n",
      "308/500 [=================>............] - ETA: 3:19 - loss: 1.9397 - regression_loss: 1.4475 - classification_loss: 0.4922\n",
      "309/500 [=================>............] - ETA: 3:18 - loss: 1.9387 - regression_loss: 1.4467 - classification_loss: 0.4919\n",
      "310/500 [=================>............] - ETA: 3:17 - loss: 1.9406 - regression_loss: 1.4483 - classification_loss: 0.4923\n",
      "311/500 [=================>............] - ETA: 3:16 - loss: 1.9420 - regression_loss: 1.4482 - classification_loss: 0.4938\n",
      "312/500 [=================>............] - ETA: 3:15 - loss: 1.9432 - regression_loss: 1.4486 - classification_loss: 0.4946\n",
      "313/500 [=================>............] - ETA: 3:14 - loss: 1.9432 - regression_loss: 1.4489 - classification_loss: 0.4942\n",
      "314/500 [=================>............] - ETA: 3:13 - loss: 1.9439 - regression_loss: 1.4488 - classification_loss: 0.4951\n",
      "315/500 [=================>............] - ETA: 3:12 - loss: 1.9418 - regression_loss: 1.4473 - classification_loss: 0.4944\n",
      "316/500 [=================>............] - ETA: 3:11 - loss: 1.9427 - regression_loss: 1.4482 - classification_loss: 0.4946\n",
      "317/500 [==================>...........] - ETA: 3:10 - loss: 1.9451 - regression_loss: 1.4494 - classification_loss: 0.4957\n",
      "318/500 [==================>...........] - ETA: 3:09 - loss: 1.9482 - regression_loss: 1.4521 - classification_loss: 0.4961\n",
      "319/500 [==================>...........] - ETA: 3:08 - loss: 1.9477 - regression_loss: 1.4513 - classification_loss: 0.4964\n",
      "320/500 [==================>...........] - ETA: 3:06 - loss: 1.9483 - regression_loss: 1.4518 - classification_loss: 0.4965\n",
      "321/500 [==================>...........] - ETA: 3:06 - loss: 1.9464 - regression_loss: 1.4505 - classification_loss: 0.4959\n",
      "322/500 [==================>...........] - ETA: 3:04 - loss: 1.9465 - regression_loss: 1.4506 - classification_loss: 0.4959\n",
      "323/500 [==================>...........] - ETA: 3:03 - loss: 1.9470 - regression_loss: 1.4514 - classification_loss: 0.4957\n",
      "324/500 [==================>...........] - ETA: 3:02 - loss: 1.9489 - regression_loss: 1.4526 - classification_loss: 0.4963\n",
      "325/500 [==================>...........] - ETA: 3:01 - loss: 1.9478 - regression_loss: 1.4522 - classification_loss: 0.4956\n",
      "326/500 [==================>...........] - ETA: 3:00 - loss: 1.9502 - regression_loss: 1.4534 - classification_loss: 0.4968\n",
      "327/500 [==================>...........] - ETA: 2:59 - loss: 1.9516 - regression_loss: 1.4548 - classification_loss: 0.4968\n",
      "328/500 [==================>...........] - ETA: 2:58 - loss: 1.9518 - regression_loss: 1.4551 - classification_loss: 0.4966\n",
      "329/500 [==================>...........] - ETA: 2:57 - loss: 1.9549 - regression_loss: 1.4570 - classification_loss: 0.4978\n",
      "330/500 [==================>...........] - ETA: 2:56 - loss: 1.9536 - regression_loss: 1.4563 - classification_loss: 0.4973\n",
      "331/500 [==================>...........] - ETA: 2:55 - loss: 1.9528 - regression_loss: 1.4558 - classification_loss: 0.4969\n",
      "332/500 [==================>...........] - ETA: 2:54 - loss: 1.9540 - regression_loss: 1.4566 - classification_loss: 0.4974\n",
      "333/500 [==================>...........] - ETA: 2:53 - loss: 1.9538 - regression_loss: 1.4565 - classification_loss: 0.4973\n",
      "334/500 [===================>..........] - ETA: 2:52 - loss: 1.9548 - regression_loss: 1.4574 - classification_loss: 0.4974\n",
      "335/500 [===================>..........] - ETA: 2:51 - loss: 1.9560 - regression_loss: 1.4579 - classification_loss: 0.4981\n",
      "336/500 [===================>..........] - ETA: 2:50 - loss: 1.9560 - regression_loss: 1.4580 - classification_loss: 0.4980\n",
      "337/500 [===================>..........] - ETA: 2:49 - loss: 1.9596 - regression_loss: 1.4609 - classification_loss: 0.4988\n",
      "338/500 [===================>..........] - ETA: 2:48 - loss: 1.9585 - regression_loss: 1.4597 - classification_loss: 0.4988\n",
      "339/500 [===================>..........] - ETA: 2:47 - loss: 1.9585 - regression_loss: 1.4599 - classification_loss: 0.4986\n",
      "340/500 [===================>..........] - ETA: 2:46 - loss: 1.9560 - regression_loss: 1.4577 - classification_loss: 0.4983\n",
      "341/500 [===================>..........] - ETA: 2:45 - loss: 1.9577 - regression_loss: 1.4585 - classification_loss: 0.4992\n",
      "342/500 [===================>..........] - ETA: 2:44 - loss: 1.9584 - regression_loss: 1.4593 - classification_loss: 0.4991\n",
      "343/500 [===================>..........] - ETA: 2:43 - loss: 1.9591 - regression_loss: 1.4599 - classification_loss: 0.4993\n",
      "344/500 [===================>..........] - ETA: 2:42 - loss: 1.9623 - regression_loss: 1.4624 - classification_loss: 0.4999\n",
      "345/500 [===================>..........] - ETA: 2:41 - loss: 1.9606 - regression_loss: 1.4611 - classification_loss: 0.4995\n",
      "346/500 [===================>..........] - ETA: 2:40 - loss: 1.9589 - regression_loss: 1.4597 - classification_loss: 0.4991\n",
      "347/500 [===================>..........] - ETA: 2:39 - loss: 1.9612 - regression_loss: 1.4616 - classification_loss: 0.4997\n",
      "348/500 [===================>..........] - ETA: 2:37 - loss: 1.9605 - regression_loss: 1.4613 - classification_loss: 0.4991\n",
      "349/500 [===================>..........] - ETA: 2:36 - loss: 1.9594 - regression_loss: 1.4608 - classification_loss: 0.4985\n",
      "350/500 [====================>.........] - ETA: 2:35 - loss: 1.9588 - regression_loss: 1.4606 - classification_loss: 0.4982\n",
      "351/500 [====================>.........] - ETA: 2:34 - loss: 1.9597 - regression_loss: 1.4615 - classification_loss: 0.4982\n",
      "352/500 [====================>.........] - ETA: 2:33 - loss: 1.9621 - regression_loss: 1.4629 - classification_loss: 0.4992\n",
      "353/500 [====================>.........] - ETA: 2:32 - loss: 1.9611 - regression_loss: 1.4623 - classification_loss: 0.4988\n",
      "354/500 [====================>.........] - ETA: 2:31 - loss: 1.9623 - regression_loss: 1.4632 - classification_loss: 0.4991\n",
      "355/500 [====================>.........] - ETA: 2:30 - loss: 1.9621 - regression_loss: 1.4632 - classification_loss: 0.4989\n",
      "356/500 [====================>.........] - ETA: 2:29 - loss: 1.9612 - regression_loss: 1.4626 - classification_loss: 0.4985\n",
      "357/500 [====================>.........] - ETA: 2:28 - loss: 1.9612 - regression_loss: 1.4632 - classification_loss: 0.4980\n",
      "358/500 [====================>.........] - ETA: 2:27 - loss: 1.9614 - regression_loss: 1.4633 - classification_loss: 0.4981\n",
      "359/500 [====================>.........] - ETA: 2:26 - loss: 1.9605 - regression_loss: 1.4623 - classification_loss: 0.4981\n",
      "360/500 [====================>.........] - ETA: 2:25 - loss: 1.9584 - regression_loss: 1.4605 - classification_loss: 0.4979\n",
      "361/500 [====================>.........] - ETA: 2:24 - loss: 1.9595 - regression_loss: 1.4613 - classification_loss: 0.4982\n",
      "362/500 [====================>.........] - ETA: 2:23 - loss: 1.9595 - regression_loss: 1.4611 - classification_loss: 0.4984\n",
      "363/500 [====================>.........] - ETA: 2:22 - loss: 1.9590 - regression_loss: 1.4608 - classification_loss: 0.4982\n",
      "364/500 [====================>.........] - ETA: 2:21 - loss: 1.9594 - regression_loss: 1.4609 - classification_loss: 0.4985\n",
      "365/500 [====================>.........] - ETA: 2:20 - loss: 1.9581 - regression_loss: 1.4598 - classification_loss: 0.4983\n",
      "366/500 [====================>.........] - ETA: 2:19 - loss: 1.9569 - regression_loss: 1.4589 - classification_loss: 0.4980\n",
      "367/500 [=====================>........] - ETA: 2:18 - loss: 1.9584 - regression_loss: 1.4596 - classification_loss: 0.4988\n",
      "368/500 [=====================>........] - ETA: 2:17 - loss: 1.9581 - regression_loss: 1.4597 - classification_loss: 0.4984\n",
      "369/500 [=====================>........] - ETA: 2:16 - loss: 1.9591 - regression_loss: 1.4601 - classification_loss: 0.4990\n",
      "370/500 [=====================>........] - ETA: 2:15 - loss: 1.9583 - regression_loss: 1.4596 - classification_loss: 0.4987\n",
      "371/500 [=====================>........] - ETA: 2:13 - loss: 1.9585 - regression_loss: 1.4597 - classification_loss: 0.4988\n",
      "372/500 [=====================>........] - ETA: 2:12 - loss: 1.9612 - regression_loss: 1.4619 - classification_loss: 0.4993\n",
      "373/500 [=====================>........] - ETA: 2:11 - loss: 1.9615 - regression_loss: 1.4621 - classification_loss: 0.4994\n",
      "374/500 [=====================>........] - ETA: 2:10 - loss: 1.9623 - regression_loss: 1.4623 - classification_loss: 0.5000\n",
      "375/500 [=====================>........] - ETA: 2:09 - loss: 1.9635 - regression_loss: 1.4628 - classification_loss: 0.5007\n",
      "376/500 [=====================>........] - ETA: 2:08 - loss: 1.9653 - regression_loss: 1.4645 - classification_loss: 0.5008\n",
      "377/500 [=====================>........] - ETA: 2:07 - loss: 1.9664 - regression_loss: 1.4649 - classification_loss: 0.5014\n",
      "378/500 [=====================>........] - ETA: 2:06 - loss: 1.9656 - regression_loss: 1.4644 - classification_loss: 0.5011\n",
      "379/500 [=====================>........] - ETA: 2:05 - loss: 1.9652 - regression_loss: 1.4644 - classification_loss: 0.5007\n",
      "380/500 [=====================>........] - ETA: 2:04 - loss: 1.9633 - regression_loss: 1.4629 - classification_loss: 0.5004\n",
      "381/500 [=====================>........] - ETA: 2:03 - loss: 1.9656 - regression_loss: 1.4644 - classification_loss: 0.5011\n",
      "382/500 [=====================>........] - ETA: 2:02 - loss: 1.9665 - regression_loss: 1.4651 - classification_loss: 0.5013\n",
      "383/500 [=====================>........] - ETA: 2:01 - loss: 1.9664 - regression_loss: 1.4652 - classification_loss: 0.5013\n",
      "384/500 [======================>.......] - ETA: 2:00 - loss: 1.9661 - regression_loss: 1.4649 - classification_loss: 0.5012\n",
      "385/500 [======================>.......] - ETA: 1:59 - loss: 1.9648 - regression_loss: 1.4632 - classification_loss: 0.5015\n",
      "386/500 [======================>.......] - ETA: 1:58 - loss: 1.9644 - regression_loss: 1.4631 - classification_loss: 0.5013\n",
      "387/500 [======================>.......] - ETA: 1:57 - loss: 1.9671 - regression_loss: 1.4648 - classification_loss: 0.5024\n",
      "388/500 [======================>.......] - ETA: 1:56 - loss: 1.9684 - regression_loss: 1.4657 - classification_loss: 0.5027\n",
      "389/500 [======================>.......] - ETA: 1:55 - loss: 1.9692 - regression_loss: 1.4664 - classification_loss: 0.5029\n",
      "390/500 [======================>.......] - ETA: 1:54 - loss: 1.9687 - regression_loss: 1.4660 - classification_loss: 0.5027\n",
      "391/500 [======================>.......] - ETA: 1:53 - loss: 1.9701 - regression_loss: 1.4667 - classification_loss: 0.5033\n",
      "392/500 [======================>.......] - ETA: 1:51 - loss: 1.9680 - regression_loss: 1.4653 - classification_loss: 0.5027\n",
      "393/500 [======================>.......] - ETA: 1:50 - loss: 1.9693 - regression_loss: 1.4663 - classification_loss: 0.5030\n",
      "394/500 [======================>.......] - ETA: 1:49 - loss: 1.9699 - regression_loss: 1.4671 - classification_loss: 0.5027\n",
      "395/500 [======================>.......] - ETA: 1:48 - loss: 1.9701 - regression_loss: 1.4672 - classification_loss: 0.5028\n",
      "396/500 [======================>.......] - ETA: 1:47 - loss: 1.9676 - regression_loss: 1.4653 - classification_loss: 0.5024\n",
      "397/500 [======================>.......] - ETA: 1:46 - loss: 1.9659 - regression_loss: 1.4643 - classification_loss: 0.5017\n",
      "398/500 [======================>.......] - ETA: 1:45 - loss: 1.9633 - regression_loss: 1.4622 - classification_loss: 0.5011\n",
      "399/500 [======================>.......] - ETA: 1:44 - loss: 1.9615 - regression_loss: 1.4608 - classification_loss: 0.5007\n",
      "400/500 [=======================>......] - ETA: 1:43 - loss: 1.9605 - regression_loss: 1.4599 - classification_loss: 0.5005\n",
      "401/500 [=======================>......] - ETA: 1:42 - loss: 1.9599 - regression_loss: 1.4598 - classification_loss: 0.5001\n",
      "402/500 [=======================>......] - ETA: 1:41 - loss: 1.9596 - regression_loss: 1.4595 - classification_loss: 0.5001\n",
      "403/500 [=======================>......] - ETA: 1:40 - loss: 1.9602 - regression_loss: 1.4597 - classification_loss: 0.5005\n",
      "404/500 [=======================>......] - ETA: 1:39 - loss: 1.9592 - regression_loss: 1.4589 - classification_loss: 0.5003\n",
      "405/500 [=======================>......] - ETA: 1:38 - loss: 1.9580 - regression_loss: 1.4582 - classification_loss: 0.4998\n",
      "406/500 [=======================>......] - ETA: 1:37 - loss: 1.9582 - regression_loss: 1.4588 - classification_loss: 0.4995\n",
      "407/500 [=======================>......] - ETA: 1:36 - loss: 1.9582 - regression_loss: 1.4586 - classification_loss: 0.4996\n",
      "408/500 [=======================>......] - ETA: 1:35 - loss: 1.9584 - regression_loss: 1.4591 - classification_loss: 0.4993\n",
      "409/500 [=======================>......] - ETA: 1:34 - loss: 1.9575 - regression_loss: 1.4587 - classification_loss: 0.4988\n",
      "410/500 [=======================>......] - ETA: 1:33 - loss: 1.9568 - regression_loss: 1.4580 - classification_loss: 0.4988\n",
      "411/500 [=======================>......] - ETA: 1:32 - loss: 1.9567 - regression_loss: 1.4577 - classification_loss: 0.4990\n",
      "412/500 [=======================>......] - ETA: 1:31 - loss: 1.9559 - regression_loss: 1.4572 - classification_loss: 0.4987\n",
      "413/500 [=======================>......] - ETA: 1:30 - loss: 1.9584 - regression_loss: 1.4589 - classification_loss: 0.4995\n",
      "414/500 [=======================>......] - ETA: 1:29 - loss: 1.9575 - regression_loss: 1.4581 - classification_loss: 0.4994\n",
      "415/500 [=======================>......] - ETA: 1:28 - loss: 1.9566 - regression_loss: 1.4575 - classification_loss: 0.4991\n",
      "416/500 [=======================>......] - ETA: 1:27 - loss: 1.9556 - regression_loss: 1.4568 - classification_loss: 0.4988\n",
      "417/500 [========================>.....] - ETA: 1:26 - loss: 1.9560 - regression_loss: 1.4572 - classification_loss: 0.4988\n",
      "418/500 [========================>.....] - ETA: 1:24 - loss: 1.9578 - regression_loss: 1.4590 - classification_loss: 0.4988\n",
      "419/500 [========================>.....] - ETA: 1:23 - loss: 1.9597 - regression_loss: 1.4604 - classification_loss: 0.4993\n",
      "420/500 [========================>.....] - ETA: 1:22 - loss: 1.9594 - regression_loss: 1.4604 - classification_loss: 0.4990\n",
      "421/500 [========================>.....] - ETA: 1:21 - loss: 1.9590 - regression_loss: 1.4600 - classification_loss: 0.4990\n",
      "422/500 [========================>.....] - ETA: 1:20 - loss: 1.9597 - regression_loss: 1.4610 - classification_loss: 0.4988\n",
      "423/500 [========================>.....] - ETA: 1:19 - loss: 1.9589 - regression_loss: 1.4603 - classification_loss: 0.4986\n",
      "424/500 [========================>.....] - ETA: 1:19 - loss: 1.9577 - regression_loss: 1.4594 - classification_loss: 0.4983\n",
      "425/500 [========================>.....] - ETA: 1:18 - loss: 1.9581 - regression_loss: 1.4595 - classification_loss: 0.4987\n",
      "426/500 [========================>.....] - ETA: 1:17 - loss: 1.9574 - regression_loss: 1.4590 - classification_loss: 0.4984\n",
      "427/500 [========================>.....] - ETA: 1:16 - loss: 1.9560 - regression_loss: 1.4582 - classification_loss: 0.4978\n",
      "428/500 [========================>.....] - ETA: 1:14 - loss: 1.9546 - regression_loss: 1.4572 - classification_loss: 0.4973\n",
      "429/500 [========================>.....] - ETA: 1:13 - loss: 1.9545 - regression_loss: 1.4573 - classification_loss: 0.4971\n",
      "430/500 [========================>.....] - ETA: 1:12 - loss: 1.9564 - regression_loss: 1.4587 - classification_loss: 0.4977\n",
      "431/500 [========================>.....] - ETA: 1:11 - loss: 1.9555 - regression_loss: 1.4576 - classification_loss: 0.4979\n",
      "432/500 [========================>.....] - ETA: 1:10 - loss: 1.9575 - regression_loss: 1.4589 - classification_loss: 0.4986\n",
      "433/500 [========================>.....] - ETA: 1:09 - loss: 1.9565 - regression_loss: 1.4582 - classification_loss: 0.4982\n",
      "434/500 [=========================>....] - ETA: 1:08 - loss: 1.9560 - regression_loss: 1.4579 - classification_loss: 0.4981\n",
      "435/500 [=========================>....] - ETA: 1:07 - loss: 1.9557 - regression_loss: 1.4579 - classification_loss: 0.4979\n",
      "436/500 [=========================>....] - ETA: 1:06 - loss: 1.9573 - regression_loss: 1.4586 - classification_loss: 0.4987\n",
      "437/500 [=========================>....] - ETA: 1:05 - loss: 1.9569 - regression_loss: 1.4585 - classification_loss: 0.4984\n",
      "438/500 [=========================>....] - ETA: 1:04 - loss: 1.9575 - regression_loss: 1.4592 - classification_loss: 0.4983\n",
      "439/500 [=========================>....] - ETA: 1:03 - loss: 1.9574 - regression_loss: 1.4593 - classification_loss: 0.4981\n",
      "440/500 [=========================>....] - ETA: 1:02 - loss: 1.9573 - regression_loss: 1.4592 - classification_loss: 0.4981\n",
      "441/500 [=========================>....] - ETA: 1:01 - loss: 1.9594 - regression_loss: 1.4603 - classification_loss: 0.4991\n",
      "442/500 [=========================>....] - ETA: 1:00 - loss: 1.9591 - regression_loss: 1.4603 - classification_loss: 0.4988\n",
      "443/500 [=========================>....] - ETA: 59s - loss: 1.9596 - regression_loss: 1.4608 - classification_loss: 0.4988 \n",
      "444/500 [=========================>....] - ETA: 58s - loss: 1.9576 - regression_loss: 1.4593 - classification_loss: 0.4983\n",
      "445/500 [=========================>....] - ETA: 57s - loss: 1.9571 - regression_loss: 1.4588 - classification_loss: 0.4983\n",
      "446/500 [=========================>....] - ETA: 56s - loss: 1.9556 - regression_loss: 1.4579 - classification_loss: 0.4977\n",
      "447/500 [=========================>....] - ETA: 55s - loss: 1.9574 - regression_loss: 1.4597 - classification_loss: 0.4977\n",
      "448/500 [=========================>....] - ETA: 54s - loss: 1.9570 - regression_loss: 1.4592 - classification_loss: 0.4977\n",
      "449/500 [=========================>....] - ETA: 53s - loss: 1.9560 - regression_loss: 1.4583 - classification_loss: 0.4976\n",
      "450/500 [==========================>...] - ETA: 52s - loss: 1.9567 - regression_loss: 1.4589 - classification_loss: 0.4977\n",
      "451/500 [==========================>...] - ETA: 51s - loss: 1.9555 - regression_loss: 1.4583 - classification_loss: 0.4973\n",
      "452/500 [==========================>...] - ETA: 50s - loss: 1.9546 - regression_loss: 1.4579 - classification_loss: 0.4967\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.9558 - regression_loss: 1.4591 - classification_loss: 0.4967\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.9556 - regression_loss: 1.4592 - classification_loss: 0.4964\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.9537 - regression_loss: 1.4578 - classification_loss: 0.4959\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.9527 - regression_loss: 1.4573 - classification_loss: 0.4954\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.9530 - regression_loss: 1.4577 - classification_loss: 0.4953\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.9525 - regression_loss: 1.4576 - classification_loss: 0.4950\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.9530 - regression_loss: 1.4580 - classification_loss: 0.4950\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.9530 - regression_loss: 1.4580 - classification_loss: 0.4949\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 1.9519 - regression_loss: 1.4573 - classification_loss: 0.4946\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 1.9523 - regression_loss: 1.4578 - classification_loss: 0.4945\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 1.9515 - regression_loss: 1.4572 - classification_loss: 0.4944\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 1.9503 - regression_loss: 1.4562 - classification_loss: 0.4941\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 1.9504 - regression_loss: 1.4561 - classification_loss: 0.4943\n",
      "466/500 [==========================>...] - ETA: 35s - loss: 1.9491 - regression_loss: 1.4549 - classification_loss: 0.4942\n",
      "467/500 [===========================>..] - ETA: 34s - loss: 1.9510 - regression_loss: 1.4562 - classification_loss: 0.4948\n",
      "468/500 [===========================>..] - ETA: 33s - loss: 1.9492 - regression_loss: 1.4549 - classification_loss: 0.4942\n",
      "469/500 [===========================>..] - ETA: 32s - loss: 1.9488 - regression_loss: 1.4546 - classification_loss: 0.4942\n",
      "470/500 [===========================>..] - ETA: 31s - loss: 1.9484 - regression_loss: 1.4543 - classification_loss: 0.4941\n",
      "471/500 [===========================>..] - ETA: 30s - loss: 1.9487 - regression_loss: 1.4543 - classification_loss: 0.4944\n",
      "472/500 [===========================>..] - ETA: 29s - loss: 1.9478 - regression_loss: 1.4538 - classification_loss: 0.4941\n",
      "473/500 [===========================>..] - ETA: 28s - loss: 1.9485 - regression_loss: 1.4545 - classification_loss: 0.4940\n",
      "474/500 [===========================>..] - ETA: 27s - loss: 1.9493 - regression_loss: 1.4552 - classification_loss: 0.4941\n",
      "475/500 [===========================>..] - ETA: 26s - loss: 1.9487 - regression_loss: 1.4549 - classification_loss: 0.4939\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 1.9481 - regression_loss: 1.4546 - classification_loss: 0.4934\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.9477 - regression_loss: 1.4545 - classification_loss: 0.4933\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.9504 - regression_loss: 1.4562 - classification_loss: 0.4942\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.9493 - regression_loss: 1.4555 - classification_loss: 0.4938\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.9482 - regression_loss: 1.4545 - classification_loss: 0.4937\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.9492 - regression_loss: 1.4552 - classification_loss: 0.4941\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.9483 - regression_loss: 1.4545 - classification_loss: 0.4938\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.9484 - regression_loss: 1.4547 - classification_loss: 0.4937\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.9487 - regression_loss: 1.4545 - classification_loss: 0.4942\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.9488 - regression_loss: 1.4547 - classification_loss: 0.4940\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.9498 - regression_loss: 1.4552 - classification_loss: 0.4946\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.9509 - regression_loss: 1.4562 - classification_loss: 0.4947\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.9521 - regression_loss: 1.4568 - classification_loss: 0.4952\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.9514 - regression_loss: 1.4564 - classification_loss: 0.4950\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.9508 - regression_loss: 1.4560 - classification_loss: 0.4948\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.9495 - regression_loss: 1.4552 - classification_loss: 0.4943 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.9500 - regression_loss: 1.4554 - classification_loss: 0.4945\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.9483 - regression_loss: 1.4542 - classification_loss: 0.4941\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.9480 - regression_loss: 1.4542 - classification_loss: 0.4938\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.9463 - regression_loss: 1.4529 - classification_loss: 0.4934\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.9466 - regression_loss: 1.4531 - classification_loss: 0.4935\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.9459 - regression_loss: 1.4525 - classification_loss: 0.4934\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.9454 - regression_loss: 1.4523 - classification_loss: 0.4931\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.9451 - regression_loss: 1.4520 - classification_loss: 0.4930\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9456 - regression_loss: 1.4525 - classification_loss: 0.4932\n",
      "Epoch 00023: saving model to ./snapshots\\resnet50_csv_23.h5\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
      "\n",
      "500/500 [==============================] - 520s 1s/step - loss: 1.9456 - regression_loss: 1.4525 - classification_loss: 0.4932\n",
      "Epoch 24/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 1.8868 - regression_loss: 1.4208 - classification_loss: 0.4661\n",
      "  2/500 [..............................] - ETA: 4:06 - loss: 2.2429 - regression_loss: 1.6103 - classification_loss: 0.6326\n",
      "  3/500 [..............................] - ETA: 5:15 - loss: 2.2001 - regression_loss: 1.5740 - classification_loss: 0.6261\n",
      "  4/500 [..............................] - ETA: 6:04 - loss: 2.1962 - regression_loss: 1.5684 - classification_loss: 0.6279\n",
      "  5/500 [..............................] - ETA: 6:38 - loss: 2.1813 - regression_loss: 1.5720 - classification_loss: 0.6094\n",
      "  6/500 [..............................] - ETA: 6:44 - loss: 2.1321 - regression_loss: 1.5679 - classification_loss: 0.5642\n",
      "  7/500 [..............................] - ETA: 6:58 - loss: 1.9865 - regression_loss: 1.4647 - classification_loss: 0.5218\n",
      "  8/500 [..............................] - ETA: 7:05 - loss: 1.8776 - regression_loss: 1.3905 - classification_loss: 0.4870\n",
      "  9/500 [..............................] - ETA: 7:17 - loss: 1.8634 - regression_loss: 1.3738 - classification_loss: 0.4896\n",
      " 10/500 [..............................] - ETA: 7:27 - loss: 1.8398 - regression_loss: 1.3378 - classification_loss: 0.5020\n",
      " 11/500 [..............................] - ETA: 7:29 - loss: 1.8574 - regression_loss: 1.3604 - classification_loss: 0.4970\n",
      " 12/500 [..............................] - ETA: 7:23 - loss: 1.9028 - regression_loss: 1.3948 - classification_loss: 0.5080\n",
      " 13/500 [..............................] - ETA: 7:33 - loss: 1.9054 - regression_loss: 1.4067 - classification_loss: 0.4986\n",
      " 14/500 [..............................] - ETA: 7:37 - loss: 1.9602 - regression_loss: 1.4416 - classification_loss: 0.5186\n",
      " 15/500 [..............................] - ETA: 7:41 - loss: 1.9751 - regression_loss: 1.4522 - classification_loss: 0.5229\n",
      " 16/500 [..............................] - ETA: 7:34 - loss: 1.9413 - regression_loss: 1.4322 - classification_loss: 0.5091\n",
      " 17/500 [>.............................] - ETA: 7:41 - loss: 1.9116 - regression_loss: 1.4113 - classification_loss: 0.5002\n",
      " 18/500 [>.............................] - ETA: 7:40 - loss: 1.8996 - regression_loss: 1.4097 - classification_loss: 0.4899\n",
      " 19/500 [>.............................] - ETA: 7:41 - loss: 1.8713 - regression_loss: 1.3890 - classification_loss: 0.4824\n",
      " 20/500 [>.............................] - ETA: 7:43 - loss: 1.8835 - regression_loss: 1.4047 - classification_loss: 0.4788\n",
      " 21/500 [>.............................] - ETA: 7:44 - loss: 1.8690 - regression_loss: 1.3956 - classification_loss: 0.4734\n",
      " 22/500 [>.............................] - ETA: 7:44 - loss: 1.8682 - regression_loss: 1.3970 - classification_loss: 0.4711\n",
      " 23/500 [>.............................] - ETA: 7:40 - loss: 1.8961 - regression_loss: 1.4219 - classification_loss: 0.4742\n",
      " 24/500 [>.............................] - ETA: 7:44 - loss: 1.8962 - regression_loss: 1.4232 - classification_loss: 0.4730\n",
      " 25/500 [>.............................] - ETA: 7:43 - loss: 1.9289 - regression_loss: 1.4469 - classification_loss: 0.4820\n",
      " 26/500 [>.............................] - ETA: 7:42 - loss: 1.9280 - regression_loss: 1.4447 - classification_loss: 0.4833\n",
      " 27/500 [>.............................] - ETA: 7:43 - loss: 1.9060 - regression_loss: 1.4274 - classification_loss: 0.4787\n",
      " 28/500 [>.............................] - ETA: 7:42 - loss: 1.9009 - regression_loss: 1.4184 - classification_loss: 0.4825\n",
      " 29/500 [>.............................] - ETA: 7:42 - loss: 1.8915 - regression_loss: 1.4111 - classification_loss: 0.4804\n",
      " 30/500 [>.............................] - ETA: 7:43 - loss: 1.9139 - regression_loss: 1.4240 - classification_loss: 0.4899\n",
      " 31/500 [>.............................] - ETA: 7:43 - loss: 1.9117 - regression_loss: 1.4233 - classification_loss: 0.4884\n",
      " 32/500 [>.............................] - ETA: 7:46 - loss: 1.9375 - regression_loss: 1.4370 - classification_loss: 0.5005\n",
      " 33/500 [>.............................] - ETA: 7:47 - loss: 1.9437 - regression_loss: 1.4392 - classification_loss: 0.5044\n",
      " 34/500 [=>............................] - ETA: 7:46 - loss: 1.9246 - regression_loss: 1.4254 - classification_loss: 0.4992\n",
      " 35/500 [=>............................] - ETA: 7:44 - loss: 1.9339 - regression_loss: 1.4288 - classification_loss: 0.5051\n",
      " 36/500 [=>............................] - ETA: 7:43 - loss: 1.9567 - regression_loss: 1.4461 - classification_loss: 0.5105\n",
      " 37/500 [=>............................] - ETA: 7:43 - loss: 1.9589 - regression_loss: 1.4523 - classification_loss: 0.5067\n",
      " 38/500 [=>............................] - ETA: 7:42 - loss: 1.9579 - regression_loss: 1.4512 - classification_loss: 0.5067\n",
      " 39/500 [=>............................] - ETA: 7:42 - loss: 1.9383 - regression_loss: 1.4361 - classification_loss: 0.5023\n",
      " 40/500 [=>............................] - ETA: 7:41 - loss: 1.9421 - regression_loss: 1.4378 - classification_loss: 0.5043\n",
      " 41/500 [=>............................] - ETA: 7:40 - loss: 1.9343 - regression_loss: 1.4330 - classification_loss: 0.5013\n",
      " 42/500 [=>............................] - ETA: 7:36 - loss: 1.9276 - regression_loss: 1.4232 - classification_loss: 0.5044\n",
      " 43/500 [=>............................] - ETA: 7:38 - loss: 1.9223 - regression_loss: 1.4222 - classification_loss: 0.5001\n",
      " 44/500 [=>............................] - ETA: 7:38 - loss: 1.9151 - regression_loss: 1.4187 - classification_loss: 0.4965\n",
      " 45/500 [=>............................] - ETA: 7:37 - loss: 1.9200 - regression_loss: 1.4192 - classification_loss: 0.5009\n",
      " 46/500 [=>............................] - ETA: 7:35 - loss: 1.9092 - regression_loss: 1.4117 - classification_loss: 0.4975\n",
      " 47/500 [=>............................] - ETA: 7:34 - loss: 1.9135 - regression_loss: 1.4155 - classification_loss: 0.4980\n",
      " 48/500 [=>............................] - ETA: 7:34 - loss: 1.9114 - regression_loss: 1.4124 - classification_loss: 0.4991\n",
      " 49/500 [=>............................] - ETA: 7:35 - loss: 1.9154 - regression_loss: 1.4171 - classification_loss: 0.4983\n",
      " 50/500 [==>...........................] - ETA: 7:34 - loss: 1.9089 - regression_loss: 1.4134 - classification_loss: 0.4955\n",
      " 51/500 [==>...........................] - ETA: 7:31 - loss: 1.9216 - regression_loss: 1.4176 - classification_loss: 0.5040\n",
      " 52/500 [==>...........................] - ETA: 7:31 - loss: 1.9294 - regression_loss: 1.4211 - classification_loss: 0.5083\n",
      " 53/500 [==>...........................] - ETA: 7:31 - loss: 1.9245 - regression_loss: 1.4207 - classification_loss: 0.5038\n",
      " 54/500 [==>...........................] - ETA: 7:31 - loss: 1.9246 - regression_loss: 1.4244 - classification_loss: 0.5002\n",
      " 55/500 [==>...........................] - ETA: 7:30 - loss: 1.9160 - regression_loss: 1.4196 - classification_loss: 0.4964\n",
      " 56/500 [==>...........................] - ETA: 7:30 - loss: 1.9087 - regression_loss: 1.4161 - classification_loss: 0.4926\n",
      " 57/500 [==>...........................] - ETA: 7:29 - loss: 1.9055 - regression_loss: 1.4127 - classification_loss: 0.4928\n",
      " 58/500 [==>...........................] - ETA: 7:26 - loss: 1.9010 - regression_loss: 1.4102 - classification_loss: 0.4908\n",
      " 59/500 [==>...........................] - ETA: 7:26 - loss: 1.9123 - regression_loss: 1.4159 - classification_loss: 0.4963\n",
      " 60/500 [==>...........................] - ETA: 7:25 - loss: 1.9264 - regression_loss: 1.4260 - classification_loss: 0.5004\n",
      " 61/500 [==>...........................] - ETA: 7:23 - loss: 1.9190 - regression_loss: 1.4222 - classification_loss: 0.4968\n",
      " 62/500 [==>...........................] - ETA: 7:23 - loss: 1.9111 - regression_loss: 1.4163 - classification_loss: 0.4948\n",
      " 63/500 [==>...........................] - ETA: 7:22 - loss: 1.9127 - regression_loss: 1.4174 - classification_loss: 0.4953\n",
      " 64/500 [==>...........................] - ETA: 7:22 - loss: 1.9065 - regression_loss: 1.4137 - classification_loss: 0.4927\n",
      " 65/500 [==>...........................] - ETA: 7:21 - loss: 1.9079 - regression_loss: 1.4125 - classification_loss: 0.4953\n",
      " 66/500 [==>...........................] - ETA: 7:21 - loss: 1.9160 - regression_loss: 1.4195 - classification_loss: 0.4965\n",
      " 67/500 [===>..........................] - ETA: 7:19 - loss: 1.9094 - regression_loss: 1.4158 - classification_loss: 0.4936\n",
      " 68/500 [===>..........................] - ETA: 7:17 - loss: 1.9055 - regression_loss: 1.4128 - classification_loss: 0.4927\n",
      " 69/500 [===>..........................] - ETA: 7:17 - loss: 1.9144 - regression_loss: 1.4165 - classification_loss: 0.4980\n",
      " 70/500 [===>..........................] - ETA: 7:16 - loss: 1.9079 - regression_loss: 1.4128 - classification_loss: 0.4950\n",
      " 71/500 [===>..........................] - ETA: 7:15 - loss: 1.9080 - regression_loss: 1.4124 - classification_loss: 0.4956\n",
      " 72/500 [===>..........................] - ETA: 7:13 - loss: 1.9066 - regression_loss: 1.4119 - classification_loss: 0.4948\n",
      " 73/500 [===>..........................] - ETA: 7:13 - loss: 1.9045 - regression_loss: 1.4103 - classification_loss: 0.4942\n",
      " 74/500 [===>..........................] - ETA: 7:12 - loss: 1.9142 - regression_loss: 1.4182 - classification_loss: 0.4960\n",
      " 75/500 [===>..........................] - ETA: 7:12 - loss: 1.9192 - regression_loss: 1.4177 - classification_loss: 0.5015\n",
      " 76/500 [===>..........................] - ETA: 7:11 - loss: 1.9215 - regression_loss: 1.4192 - classification_loss: 0.5023\n",
      " 77/500 [===>..........................] - ETA: 7:10 - loss: 1.9339 - regression_loss: 1.4250 - classification_loss: 0.5088\n",
      " 78/500 [===>..........................] - ETA: 7:08 - loss: 1.9369 - regression_loss: 1.4275 - classification_loss: 0.5094\n",
      " 79/500 [===>..........................] - ETA: 7:07 - loss: 1.9307 - regression_loss: 1.4214 - classification_loss: 0.5093\n",
      " 80/500 [===>..........................] - ETA: 7:07 - loss: 1.9331 - regression_loss: 1.4231 - classification_loss: 0.5100\n",
      " 81/500 [===>..........................] - ETA: 7:06 - loss: 1.9292 - regression_loss: 1.4205 - classification_loss: 0.5087\n",
      " 82/500 [===>..........................] - ETA: 7:05 - loss: 1.9224 - regression_loss: 1.4152 - classification_loss: 0.5072\n",
      " 83/500 [===>..........................] - ETA: 7:03 - loss: 1.9201 - regression_loss: 1.4145 - classification_loss: 0.5057\n",
      " 84/500 [====>.........................] - ETA: 7:03 - loss: 1.9238 - regression_loss: 1.4166 - classification_loss: 0.5071\n",
      " 85/500 [====>.........................] - ETA: 7:02 - loss: 1.9178 - regression_loss: 1.4107 - classification_loss: 0.5071\n",
      " 86/500 [====>.........................] - ETA: 7:01 - loss: 1.9184 - regression_loss: 1.4078 - classification_loss: 0.5106\n",
      " 87/500 [====>.........................] - ETA: 7:00 - loss: 1.9237 - regression_loss: 1.4095 - classification_loss: 0.5142\n",
      " 88/500 [====>.........................] - ETA: 7:00 - loss: 1.9314 - regression_loss: 1.4128 - classification_loss: 0.5186\n",
      " 89/500 [====>.........................] - ETA: 6:59 - loss: 1.9249 - regression_loss: 1.4079 - classification_loss: 0.5170\n",
      " 90/500 [====>.........................] - ETA: 6:58 - loss: 1.9209 - regression_loss: 1.4053 - classification_loss: 0.5156\n",
      " 91/500 [====>.........................] - ETA: 6:57 - loss: 1.9195 - regression_loss: 1.4047 - classification_loss: 0.5148\n",
      " 92/500 [====>.........................] - ETA: 6:56 - loss: 1.9201 - regression_loss: 1.4057 - classification_loss: 0.5144\n",
      " 93/500 [====>.........................] - ETA: 6:55 - loss: 1.9252 - regression_loss: 1.4079 - classification_loss: 0.5173\n",
      " 94/500 [====>.........................] - ETA: 6:54 - loss: 1.9247 - regression_loss: 1.4097 - classification_loss: 0.5150\n",
      " 95/500 [====>.........................] - ETA: 6:53 - loss: 1.9248 - regression_loss: 1.4088 - classification_loss: 0.5160\n",
      " 96/500 [====>.........................] - ETA: 6:53 - loss: 1.9327 - regression_loss: 1.4137 - classification_loss: 0.5190\n",
      " 97/500 [====>.........................] - ETA: 6:52 - loss: 1.9354 - regression_loss: 1.4162 - classification_loss: 0.5192\n",
      " 98/500 [====>.........................] - ETA: 6:51 - loss: 1.9272 - regression_loss: 1.4106 - classification_loss: 0.5166\n",
      " 99/500 [====>.........................] - ETA: 6:50 - loss: 1.9269 - regression_loss: 1.4115 - classification_loss: 0.5153\n",
      "100/500 [=====>........................] - ETA: 6:49 - loss: 1.9277 - regression_loss: 1.4123 - classification_loss: 0.5155\n",
      "101/500 [=====>........................] - ETA: 6:48 - loss: 1.9286 - regression_loss: 1.4121 - classification_loss: 0.5165\n",
      "102/500 [=====>........................] - ETA: 6:47 - loss: 1.9405 - regression_loss: 1.4202 - classification_loss: 0.5204\n",
      "103/500 [=====>........................] - ETA: 6:47 - loss: 1.9463 - regression_loss: 1.4236 - classification_loss: 0.5227\n",
      "104/500 [=====>........................] - ETA: 6:45 - loss: 1.9476 - regression_loss: 1.4253 - classification_loss: 0.5223\n",
      "105/500 [=====>........................] - ETA: 6:45 - loss: 1.9446 - regression_loss: 1.4243 - classification_loss: 0.5203\n",
      "106/500 [=====>........................] - ETA: 6:43 - loss: 1.9384 - regression_loss: 1.4206 - classification_loss: 0.5179\n",
      "107/500 [=====>........................] - ETA: 6:43 - loss: 1.9432 - regression_loss: 1.4246 - classification_loss: 0.5186\n",
      "108/500 [=====>........................] - ETA: 6:42 - loss: 1.9412 - regression_loss: 1.4226 - classification_loss: 0.5185\n",
      "109/500 [=====>........................] - ETA: 6:41 - loss: 1.9404 - regression_loss: 1.4221 - classification_loss: 0.5183\n",
      "110/500 [=====>........................] - ETA: 6:40 - loss: 1.9318 - regression_loss: 1.4160 - classification_loss: 0.5158\n",
      "111/500 [=====>........................] - ETA: 6:39 - loss: 1.9335 - regression_loss: 1.4176 - classification_loss: 0.5158\n",
      "112/500 [=====>........................] - ETA: 6:38 - loss: 1.9289 - regression_loss: 1.4143 - classification_loss: 0.5146\n",
      "113/500 [=====>........................] - ETA: 6:37 - loss: 1.9278 - regression_loss: 1.4150 - classification_loss: 0.5128\n",
      "114/500 [=====>........................] - ETA: 6:36 - loss: 1.9263 - regression_loss: 1.4143 - classification_loss: 0.5120\n",
      "115/500 [=====>........................] - ETA: 6:34 - loss: 1.9199 - regression_loss: 1.4093 - classification_loss: 0.5106\n",
      "116/500 [=====>........................] - ETA: 6:34 - loss: 1.9192 - regression_loss: 1.4099 - classification_loss: 0.5093\n",
      "117/500 [======>.......................] - ETA: 6:33 - loss: 1.9228 - regression_loss: 1.4124 - classification_loss: 0.5104\n",
      "118/500 [======>.......................] - ETA: 6:32 - loss: 1.9227 - regression_loss: 1.4125 - classification_loss: 0.5102\n",
      "119/500 [======>.......................] - ETA: 6:31 - loss: 1.9204 - regression_loss: 1.4107 - classification_loss: 0.5098\n",
      "120/500 [======>.......................] - ETA: 6:30 - loss: 1.9165 - regression_loss: 1.4089 - classification_loss: 0.5076\n",
      "121/500 [======>.......................] - ETA: 6:29 - loss: 1.9184 - regression_loss: 1.4096 - classification_loss: 0.5088\n",
      "122/500 [======>.......................] - ETA: 6:28 - loss: 1.9278 - regression_loss: 1.4154 - classification_loss: 0.5124\n",
      "123/500 [======>.......................] - ETA: 6:27 - loss: 1.9213 - regression_loss: 1.4109 - classification_loss: 0.5104\n",
      "124/500 [======>.......................] - ETA: 6:26 - loss: 1.9199 - regression_loss: 1.4104 - classification_loss: 0.5096\n",
      "125/500 [======>.......................] - ETA: 6:25 - loss: 1.9130 - regression_loss: 1.4055 - classification_loss: 0.5075\n",
      "126/500 [======>.......................] - ETA: 6:24 - loss: 1.9149 - regression_loss: 1.4075 - classification_loss: 0.5074\n",
      "127/500 [======>.......................] - ETA: 6:23 - loss: 1.9131 - regression_loss: 1.4059 - classification_loss: 0.5072\n",
      "128/500 [======>.......................] - ETA: 6:22 - loss: 1.9174 - regression_loss: 1.4101 - classification_loss: 0.5073\n",
      "129/500 [======>.......................] - ETA: 6:21 - loss: 1.9143 - regression_loss: 1.4086 - classification_loss: 0.5057\n",
      "130/500 [======>.......................] - ETA: 6:20 - loss: 1.9145 - regression_loss: 1.4088 - classification_loss: 0.5057\n",
      "131/500 [======>.......................] - ETA: 6:19 - loss: 1.9123 - regression_loss: 1.4074 - classification_loss: 0.5050\n",
      "132/500 [======>.......................] - ETA: 6:18 - loss: 1.9144 - regression_loss: 1.4105 - classification_loss: 0.5039\n",
      "133/500 [======>.......................] - ETA: 6:17 - loss: 1.9134 - regression_loss: 1.4107 - classification_loss: 0.5027\n",
      "134/500 [=======>......................] - ETA: 6:15 - loss: 1.9092 - regression_loss: 1.4075 - classification_loss: 0.5017\n",
      "135/500 [=======>......................] - ETA: 6:14 - loss: 1.9076 - regression_loss: 1.4063 - classification_loss: 0.5013\n",
      "136/500 [=======>......................] - ETA: 6:14 - loss: 1.9108 - regression_loss: 1.4079 - classification_loss: 0.5029\n",
      "137/500 [=======>......................] - ETA: 6:13 - loss: 1.9190 - regression_loss: 1.4148 - classification_loss: 0.5042\n",
      "138/500 [=======>......................] - ETA: 6:12 - loss: 1.9196 - regression_loss: 1.4152 - classification_loss: 0.5044\n",
      "139/500 [=======>......................] - ETA: 6:11 - loss: 1.9192 - regression_loss: 1.4147 - classification_loss: 0.5045\n",
      "140/500 [=======>......................] - ETA: 6:10 - loss: 1.9154 - regression_loss: 1.4124 - classification_loss: 0.5030\n",
      "141/500 [=======>......................] - ETA: 6:09 - loss: 1.9171 - regression_loss: 1.4142 - classification_loss: 0.5030\n",
      "142/500 [=======>......................] - ETA: 6:08 - loss: 1.9230 - regression_loss: 1.4188 - classification_loss: 0.5043\n",
      "143/500 [=======>......................] - ETA: 6:06 - loss: 1.9242 - regression_loss: 1.4209 - classification_loss: 0.5033\n",
      "144/500 [=======>......................] - ETA: 6:05 - loss: 1.9222 - regression_loss: 1.4202 - classification_loss: 0.5020\n",
      "145/500 [=======>......................] - ETA: 6:04 - loss: 1.9255 - regression_loss: 1.4218 - classification_loss: 0.5037\n",
      "146/500 [=======>......................] - ETA: 6:03 - loss: 1.9270 - regression_loss: 1.4237 - classification_loss: 0.5033\n",
      "147/500 [=======>......................] - ETA: 6:02 - loss: 1.9229 - regression_loss: 1.4211 - classification_loss: 0.5018\n",
      "148/500 [=======>......................] - ETA: 6:01 - loss: 1.9231 - regression_loss: 1.4215 - classification_loss: 0.5017\n",
      "149/500 [=======>......................] - ETA: 6:00 - loss: 1.9218 - regression_loss: 1.4201 - classification_loss: 0.5017\n",
      "150/500 [========>.....................] - ETA: 5:59 - loss: 1.9182 - regression_loss: 1.4182 - classification_loss: 0.5000\n",
      "151/500 [========>.....................] - ETA: 5:58 - loss: 1.9155 - regression_loss: 1.4160 - classification_loss: 0.4994\n",
      "152/500 [========>.....................] - ETA: 5:57 - loss: 1.9167 - regression_loss: 1.4186 - classification_loss: 0.4981\n",
      "153/500 [========>.....................] - ETA: 5:56 - loss: 1.9198 - regression_loss: 1.4203 - classification_loss: 0.4995\n",
      "154/500 [========>.....................] - ETA: 5:55 - loss: 1.9224 - regression_loss: 1.4206 - classification_loss: 0.5018\n",
      "155/500 [========>.....................] - ETA: 5:55 - loss: 1.9212 - regression_loss: 1.4202 - classification_loss: 0.5010\n",
      "156/500 [========>.....................] - ETA: 5:53 - loss: 1.9169 - regression_loss: 1.4173 - classification_loss: 0.4996\n",
      "157/500 [========>.....................] - ETA: 5:53 - loss: 1.9181 - regression_loss: 1.4176 - classification_loss: 0.5005\n",
      "158/500 [========>.....................] - ETA: 5:51 - loss: 1.9181 - regression_loss: 1.4184 - classification_loss: 0.4996\n",
      "159/500 [========>.....................] - ETA: 5:50 - loss: 1.9154 - regression_loss: 1.4171 - classification_loss: 0.4983\n",
      "160/500 [========>.....................] - ETA: 5:49 - loss: 1.9171 - regression_loss: 1.4190 - classification_loss: 0.4981\n",
      "161/500 [========>.....................] - ETA: 5:48 - loss: 1.9211 - regression_loss: 1.4232 - classification_loss: 0.4979\n",
      "162/500 [========>.....................] - ETA: 5:47 - loss: 1.9255 - regression_loss: 1.4259 - classification_loss: 0.4996\n",
      "163/500 [========>.....................] - ETA: 5:46 - loss: 1.9239 - regression_loss: 1.4253 - classification_loss: 0.4986\n",
      "164/500 [========>.....................] - ETA: 5:45 - loss: 1.9185 - regression_loss: 1.4216 - classification_loss: 0.4969\n",
      "165/500 [========>.....................] - ETA: 5:44 - loss: 1.9158 - regression_loss: 1.4198 - classification_loss: 0.4959\n",
      "166/500 [========>.....................] - ETA: 5:43 - loss: 1.9208 - regression_loss: 1.4232 - classification_loss: 0.4976\n",
      "167/500 [=========>....................] - ETA: 5:42 - loss: 1.9183 - regression_loss: 1.4216 - classification_loss: 0.4967\n",
      "168/500 [=========>....................] - ETA: 5:40 - loss: 1.9157 - regression_loss: 1.4193 - classification_loss: 0.4964\n",
      "169/500 [=========>....................] - ETA: 5:40 - loss: 1.9119 - regression_loss: 1.4167 - classification_loss: 0.4952\n",
      "170/500 [=========>....................] - ETA: 5:39 - loss: 1.9136 - regression_loss: 1.4188 - classification_loss: 0.4948\n",
      "171/500 [=========>....................] - ETA: 5:38 - loss: 1.9204 - regression_loss: 1.4241 - classification_loss: 0.4963\n",
      "172/500 [=========>....................] - ETA: 5:37 - loss: 1.9212 - regression_loss: 1.4253 - classification_loss: 0.4959\n",
      "173/500 [=========>....................] - ETA: 5:36 - loss: 1.9190 - regression_loss: 1.4226 - classification_loss: 0.4964\n",
      "174/500 [=========>....................] - ETA: 5:34 - loss: 1.9227 - regression_loss: 1.4253 - classification_loss: 0.4974\n",
      "175/500 [=========>....................] - ETA: 5:33 - loss: 1.9216 - regression_loss: 1.4245 - classification_loss: 0.4971\n",
      "176/500 [=========>....................] - ETA: 5:32 - loss: 1.9181 - regression_loss: 1.4219 - classification_loss: 0.4962\n",
      "177/500 [=========>....................] - ETA: 5:31 - loss: 1.9153 - regression_loss: 1.4192 - classification_loss: 0.4961\n",
      "178/500 [=========>....................] - ETA: 5:30 - loss: 1.9132 - regression_loss: 1.4172 - classification_loss: 0.4960\n",
      "179/500 [=========>....................] - ETA: 5:29 - loss: 1.9141 - regression_loss: 1.4178 - classification_loss: 0.4963\n",
      "180/500 [=========>....................] - ETA: 5:29 - loss: 1.9097 - regression_loss: 1.4145 - classification_loss: 0.4952\n",
      "181/500 [=========>....................] - ETA: 5:27 - loss: 1.9051 - regression_loss: 1.4109 - classification_loss: 0.4942\n",
      "182/500 [=========>....................] - ETA: 5:26 - loss: 1.9028 - regression_loss: 1.4094 - classification_loss: 0.4934\n",
      "183/500 [=========>....................] - ETA: 5:25 - loss: 1.9055 - regression_loss: 1.4115 - classification_loss: 0.4940\n",
      "184/500 [==========>...................] - ETA: 5:24 - loss: 1.9052 - regression_loss: 1.4118 - classification_loss: 0.4933\n",
      "185/500 [==========>...................] - ETA: 5:23 - loss: 1.9072 - regression_loss: 1.4129 - classification_loss: 0.4943\n",
      "186/500 [==========>...................] - ETA: 5:22 - loss: 1.9085 - regression_loss: 1.4143 - classification_loss: 0.4942\n",
      "187/500 [==========>...................] - ETA: 5:21 - loss: 1.9117 - regression_loss: 1.4179 - classification_loss: 0.4938\n",
      "188/500 [==========>...................] - ETA: 5:20 - loss: 1.9117 - regression_loss: 1.4172 - classification_loss: 0.4945\n",
      "189/500 [==========>...................] - ETA: 5:19 - loss: 1.9118 - regression_loss: 1.4172 - classification_loss: 0.4946\n",
      "190/500 [==========>...................] - ETA: 5:18 - loss: 1.9086 - regression_loss: 1.4150 - classification_loss: 0.4936\n",
      "191/500 [==========>...................] - ETA: 5:17 - loss: 1.9055 - regression_loss: 1.4126 - classification_loss: 0.4930\n",
      "192/500 [==========>...................] - ETA: 5:16 - loss: 1.9023 - regression_loss: 1.4099 - classification_loss: 0.4924\n",
      "193/500 [==========>...................] - ETA: 5:15 - loss: 1.8988 - regression_loss: 1.4075 - classification_loss: 0.4913\n",
      "194/500 [==========>...................] - ETA: 5:14 - loss: 1.8947 - regression_loss: 1.4046 - classification_loss: 0.4902\n",
      "195/500 [==========>...................] - ETA: 5:13 - loss: 1.8911 - regression_loss: 1.4021 - classification_loss: 0.4890\n",
      "196/500 [==========>...................] - ETA: 5:11 - loss: 1.8899 - regression_loss: 1.4014 - classification_loss: 0.4885\n",
      "197/500 [==========>...................] - ETA: 5:10 - loss: 1.8878 - regression_loss: 1.4002 - classification_loss: 0.4876\n",
      "198/500 [==========>...................] - ETA: 5:09 - loss: 1.8863 - regression_loss: 1.3983 - classification_loss: 0.4880\n",
      "199/500 [==========>...................] - ETA: 5:08 - loss: 1.8863 - regression_loss: 1.3986 - classification_loss: 0.4878\n",
      "200/500 [===========>..................] - ETA: 5:07 - loss: 1.8847 - regression_loss: 1.3971 - classification_loss: 0.4876\n",
      "201/500 [===========>..................] - ETA: 5:06 - loss: 1.8844 - regression_loss: 1.3967 - classification_loss: 0.4877\n",
      "202/500 [===========>..................] - ETA: 5:05 - loss: 1.8822 - regression_loss: 1.3949 - classification_loss: 0.4873\n",
      "203/500 [===========>..................] - ETA: 5:04 - loss: 1.8803 - regression_loss: 1.3933 - classification_loss: 0.4870\n",
      "204/500 [===========>..................] - ETA: 5:03 - loss: 1.8820 - regression_loss: 1.3946 - classification_loss: 0.4874\n",
      "205/500 [===========>..................] - ETA: 5:02 - loss: 1.8826 - regression_loss: 1.3952 - classification_loss: 0.4875\n",
      "206/500 [===========>..................] - ETA: 5:01 - loss: 1.8801 - regression_loss: 1.3938 - classification_loss: 0.4863\n",
      "207/500 [===========>..................] - ETA: 5:00 - loss: 1.8810 - regression_loss: 1.3947 - classification_loss: 0.4863\n",
      "208/500 [===========>..................] - ETA: 4:59 - loss: 1.8824 - regression_loss: 1.3953 - classification_loss: 0.4871\n",
      "209/500 [===========>..................] - ETA: 4:58 - loss: 1.8806 - regression_loss: 1.3938 - classification_loss: 0.4868\n",
      "210/500 [===========>..................] - ETA: 4:57 - loss: 1.8817 - regression_loss: 1.3949 - classification_loss: 0.4868\n",
      "211/500 [===========>..................] - ETA: 4:56 - loss: 1.8798 - regression_loss: 1.3934 - classification_loss: 0.4864\n",
      "212/500 [===========>..................] - ETA: 4:55 - loss: 1.8842 - regression_loss: 1.3960 - classification_loss: 0.4882\n",
      "213/500 [===========>..................] - ETA: 4:54 - loss: 1.8857 - regression_loss: 1.3974 - classification_loss: 0.4884\n",
      "214/500 [===========>..................] - ETA: 4:53 - loss: 1.8848 - regression_loss: 1.3968 - classification_loss: 0.4880\n",
      "215/500 [===========>..................] - ETA: 4:52 - loss: 1.8820 - regression_loss: 1.3949 - classification_loss: 0.4871\n",
      "216/500 [===========>..................] - ETA: 4:51 - loss: 1.8807 - regression_loss: 1.3942 - classification_loss: 0.4865\n",
      "217/500 [============>.................] - ETA: 4:50 - loss: 1.8834 - regression_loss: 1.3961 - classification_loss: 0.4872\n",
      "218/500 [============>.................] - ETA: 4:49 - loss: 1.8838 - regression_loss: 1.3962 - classification_loss: 0.4875\n",
      "219/500 [============>.................] - ETA: 4:47 - loss: 1.8817 - regression_loss: 1.3949 - classification_loss: 0.4868\n",
      "220/500 [============>.................] - ETA: 4:46 - loss: 1.8812 - regression_loss: 1.3949 - classification_loss: 0.4863\n",
      "221/500 [============>.................] - ETA: 4:45 - loss: 1.8793 - regression_loss: 1.3935 - classification_loss: 0.4858\n",
      "222/500 [============>.................] - ETA: 4:44 - loss: 1.8806 - regression_loss: 1.3950 - classification_loss: 0.4857\n",
      "223/500 [============>.................] - ETA: 4:44 - loss: 1.8839 - regression_loss: 1.3967 - classification_loss: 0.4872\n",
      "224/500 [============>.................] - ETA: 4:42 - loss: 1.8853 - regression_loss: 1.3981 - classification_loss: 0.4872\n",
      "225/500 [============>.................] - ETA: 4:41 - loss: 1.8868 - regression_loss: 1.3981 - classification_loss: 0.4886\n",
      "226/500 [============>.................] - ETA: 4:40 - loss: 1.8847 - regression_loss: 1.3967 - classification_loss: 0.4880\n",
      "227/500 [============>.................] - ETA: 4:39 - loss: 1.8847 - regression_loss: 1.3969 - classification_loss: 0.4878\n",
      "228/500 [============>.................] - ETA: 4:38 - loss: 1.8856 - regression_loss: 1.3972 - classification_loss: 0.4885\n",
      "229/500 [============>.................] - ETA: 4:37 - loss: 1.8885 - regression_loss: 1.3981 - classification_loss: 0.4904\n",
      "230/500 [============>.................] - ETA: 4:36 - loss: 1.8914 - regression_loss: 1.4004 - classification_loss: 0.4910\n",
      "231/500 [============>.................] - ETA: 4:35 - loss: 1.8929 - regression_loss: 1.4026 - classification_loss: 0.4903\n",
      "232/500 [============>.................] - ETA: 4:34 - loss: 1.8915 - regression_loss: 1.4016 - classification_loss: 0.4899\n",
      "233/500 [============>.................] - ETA: 4:34 - loss: 1.8913 - regression_loss: 1.4022 - classification_loss: 0.4891\n",
      "234/500 [=============>................] - ETA: 4:32 - loss: 1.8929 - regression_loss: 1.4029 - classification_loss: 0.4900\n",
      "235/500 [=============>................] - ETA: 4:31 - loss: 1.8890 - regression_loss: 1.3998 - classification_loss: 0.4892\n",
      "236/500 [=============>................] - ETA: 4:30 - loss: 1.8883 - regression_loss: 1.3990 - classification_loss: 0.4893\n",
      "237/500 [=============>................] - ETA: 4:29 - loss: 1.8910 - regression_loss: 1.4011 - classification_loss: 0.4898\n",
      "238/500 [=============>................] - ETA: 4:28 - loss: 1.8907 - regression_loss: 1.4011 - classification_loss: 0.4896\n",
      "239/500 [=============>................] - ETA: 4:27 - loss: 1.8936 - regression_loss: 1.4032 - classification_loss: 0.4904\n",
      "240/500 [=============>................] - ETA: 4:26 - loss: 1.8967 - regression_loss: 1.4054 - classification_loss: 0.4913\n",
      "241/500 [=============>................] - ETA: 4:25 - loss: 1.8981 - regression_loss: 1.4068 - classification_loss: 0.4913\n",
      "242/500 [=============>................] - ETA: 4:24 - loss: 1.8959 - regression_loss: 1.4054 - classification_loss: 0.4906\n",
      "243/500 [=============>................] - ETA: 4:23 - loss: 1.8963 - regression_loss: 1.4057 - classification_loss: 0.4906\n",
      "244/500 [=============>................] - ETA: 4:22 - loss: 1.8992 - regression_loss: 1.4076 - classification_loss: 0.4916\n",
      "245/500 [=============>................] - ETA: 4:21 - loss: 1.9020 - regression_loss: 1.4095 - classification_loss: 0.4925\n",
      "246/500 [=============>................] - ETA: 4:20 - loss: 1.9010 - regression_loss: 1.4091 - classification_loss: 0.4919\n",
      "247/500 [=============>................] - ETA: 4:19 - loss: 1.9040 - regression_loss: 1.4117 - classification_loss: 0.4924\n",
      "248/500 [=============>................] - ETA: 4:18 - loss: 1.9031 - regression_loss: 1.4112 - classification_loss: 0.4919\n",
      "249/500 [=============>................] - ETA: 4:17 - loss: 1.9016 - regression_loss: 1.4095 - classification_loss: 0.4921\n",
      "250/500 [==============>...............] - ETA: 4:16 - loss: 1.9007 - regression_loss: 1.4094 - classification_loss: 0.4913\n",
      "251/500 [==============>...............] - ETA: 4:15 - loss: 1.9007 - regression_loss: 1.4094 - classification_loss: 0.4913\n",
      "252/500 [==============>...............] - ETA: 4:14 - loss: 1.8995 - regression_loss: 1.4091 - classification_loss: 0.4904\n",
      "253/500 [==============>...............] - ETA: 4:13 - loss: 1.8997 - regression_loss: 1.4091 - classification_loss: 0.4907\n",
      "254/500 [==============>...............] - ETA: 4:12 - loss: 1.9001 - regression_loss: 1.4095 - classification_loss: 0.4905\n",
      "255/500 [==============>...............] - ETA: 4:11 - loss: 1.9001 - regression_loss: 1.4100 - classification_loss: 0.4901\n",
      "256/500 [==============>...............] - ETA: 4:10 - loss: 1.9001 - regression_loss: 1.4098 - classification_loss: 0.4903\n",
      "257/500 [==============>...............] - ETA: 4:08 - loss: 1.8979 - regression_loss: 1.4082 - classification_loss: 0.4897\n",
      "258/500 [==============>...............] - ETA: 4:08 - loss: 1.8990 - regression_loss: 1.4094 - classification_loss: 0.4896\n",
      "259/500 [==============>...............] - ETA: 4:07 - loss: 1.8968 - regression_loss: 1.4076 - classification_loss: 0.4892\n",
      "260/500 [==============>...............] - ETA: 4:06 - loss: 1.8985 - regression_loss: 1.4092 - classification_loss: 0.4893\n",
      "261/500 [==============>...............] - ETA: 4:05 - loss: 1.8981 - regression_loss: 1.4085 - classification_loss: 0.4896\n",
      "262/500 [==============>...............] - ETA: 4:03 - loss: 1.8949 - regression_loss: 1.4059 - classification_loss: 0.4889\n",
      "263/500 [==============>...............] - ETA: 4:02 - loss: 1.8935 - regression_loss: 1.4051 - classification_loss: 0.4884\n",
      "264/500 [==============>...............] - ETA: 4:01 - loss: 1.8938 - regression_loss: 1.4056 - classification_loss: 0.4882\n",
      "265/500 [==============>...............] - ETA: 4:00 - loss: 1.8905 - regression_loss: 1.4032 - classification_loss: 0.4873\n",
      "266/500 [==============>...............] - ETA: 3:59 - loss: 1.8919 - regression_loss: 1.4043 - classification_loss: 0.4876\n",
      "267/500 [===============>..............] - ETA: 3:58 - loss: 1.8941 - regression_loss: 1.4053 - classification_loss: 0.4888\n",
      "268/500 [===============>..............] - ETA: 3:57 - loss: 1.8950 - regression_loss: 1.4065 - classification_loss: 0.4884\n",
      "269/500 [===============>..............] - ETA: 3:56 - loss: 1.8953 - regression_loss: 1.4070 - classification_loss: 0.4883\n",
      "270/500 [===============>..............] - ETA: 3:55 - loss: 1.8950 - regression_loss: 1.4069 - classification_loss: 0.4881\n",
      "271/500 [===============>..............] - ETA: 3:54 - loss: 1.8974 - regression_loss: 1.4075 - classification_loss: 0.4899\n",
      "272/500 [===============>..............] - ETA: 3:53 - loss: 1.8973 - regression_loss: 1.4076 - classification_loss: 0.4897\n",
      "273/500 [===============>..............] - ETA: 3:52 - loss: 1.8941 - regression_loss: 1.4053 - classification_loss: 0.4888\n",
      "274/500 [===============>..............] - ETA: 3:51 - loss: 1.8927 - regression_loss: 1.4046 - classification_loss: 0.4880\n",
      "275/500 [===============>..............] - ETA: 3:50 - loss: 1.8907 - regression_loss: 1.4032 - classification_loss: 0.4875\n",
      "276/500 [===============>..............] - ETA: 3:49 - loss: 1.8910 - regression_loss: 1.4032 - classification_loss: 0.4878\n",
      "277/500 [===============>..............] - ETA: 3:48 - loss: 1.8897 - regression_loss: 1.4023 - classification_loss: 0.4873\n",
      "278/500 [===============>..............] - ETA: 3:47 - loss: 1.8900 - regression_loss: 1.4028 - classification_loss: 0.4871\n",
      "279/500 [===============>..............] - ETA: 3:46 - loss: 1.8892 - regression_loss: 1.4020 - classification_loss: 0.4871\n",
      "280/500 [===============>..............] - ETA: 3:45 - loss: 1.8913 - regression_loss: 1.4041 - classification_loss: 0.4872\n",
      "281/500 [===============>..............] - ETA: 3:44 - loss: 1.8906 - regression_loss: 1.4030 - classification_loss: 0.4876\n",
      "282/500 [===============>..............] - ETA: 3:43 - loss: 1.8893 - regression_loss: 1.4025 - classification_loss: 0.4868\n",
      "283/500 [===============>..............] - ETA: 3:42 - loss: 1.8901 - regression_loss: 1.4032 - classification_loss: 0.4869\n",
      "284/500 [================>.............] - ETA: 3:41 - loss: 1.8906 - regression_loss: 1.4031 - classification_loss: 0.4875\n",
      "285/500 [================>.............] - ETA: 3:40 - loss: 1.8885 - regression_loss: 1.4020 - classification_loss: 0.4865\n",
      "286/500 [================>.............] - ETA: 3:39 - loss: 1.8881 - regression_loss: 1.4018 - classification_loss: 0.4863\n",
      "287/500 [================>.............] - ETA: 3:38 - loss: 1.8882 - regression_loss: 1.4017 - classification_loss: 0.4866\n",
      "288/500 [================>.............] - ETA: 3:37 - loss: 1.8894 - regression_loss: 1.4024 - classification_loss: 0.4870\n",
      "289/500 [================>.............] - ETA: 3:36 - loss: 1.8883 - regression_loss: 1.4017 - classification_loss: 0.4866\n",
      "290/500 [================>.............] - ETA: 3:35 - loss: 1.8925 - regression_loss: 1.4050 - classification_loss: 0.4875\n",
      "291/500 [================>.............] - ETA: 3:34 - loss: 1.8930 - regression_loss: 1.4054 - classification_loss: 0.4876\n",
      "292/500 [================>.............] - ETA: 3:33 - loss: 1.8950 - regression_loss: 1.4062 - classification_loss: 0.4888\n",
      "293/500 [================>.............] - ETA: 3:32 - loss: 1.8958 - regression_loss: 1.4071 - classification_loss: 0.4887\n",
      "294/500 [================>.............] - ETA: 3:31 - loss: 1.8946 - regression_loss: 1.4067 - classification_loss: 0.4879\n",
      "295/500 [================>.............] - ETA: 3:30 - loss: 1.8956 - regression_loss: 1.4073 - classification_loss: 0.4883\n",
      "296/500 [================>.............] - ETA: 3:29 - loss: 1.8953 - regression_loss: 1.4074 - classification_loss: 0.4879\n",
      "297/500 [================>.............] - ETA: 3:28 - loss: 1.8967 - regression_loss: 1.4087 - classification_loss: 0.4880\n",
      "298/500 [================>.............] - ETA: 3:27 - loss: 1.8968 - regression_loss: 1.4085 - classification_loss: 0.4883\n",
      "299/500 [================>.............] - ETA: 3:26 - loss: 1.8983 - regression_loss: 1.4097 - classification_loss: 0.4886\n",
      "300/500 [=================>............] - ETA: 3:25 - loss: 1.8983 - regression_loss: 1.4102 - classification_loss: 0.4881\n",
      "301/500 [=================>............] - ETA: 3:24 - loss: 1.8977 - regression_loss: 1.4097 - classification_loss: 0.4880\n",
      "302/500 [=================>............] - ETA: 3:23 - loss: 1.8959 - regression_loss: 1.4083 - classification_loss: 0.4876\n",
      "303/500 [=================>............] - ETA: 3:22 - loss: 1.8968 - regression_loss: 1.4094 - classification_loss: 0.4874\n",
      "304/500 [=================>............] - ETA: 3:21 - loss: 1.8996 - regression_loss: 1.4114 - classification_loss: 0.4881\n",
      "305/500 [=================>............] - ETA: 3:20 - loss: 1.9015 - regression_loss: 1.4135 - classification_loss: 0.4880\n",
      "306/500 [=================>............] - ETA: 3:19 - loss: 1.9012 - regression_loss: 1.4133 - classification_loss: 0.4879\n",
      "307/500 [=================>............] - ETA: 3:18 - loss: 1.9003 - regression_loss: 1.4129 - classification_loss: 0.4874\n",
      "308/500 [=================>............] - ETA: 3:17 - loss: 1.9011 - regression_loss: 1.4139 - classification_loss: 0.4872\n",
      "309/500 [=================>............] - ETA: 3:16 - loss: 1.9009 - regression_loss: 1.4136 - classification_loss: 0.4873\n",
      "310/500 [=================>............] - ETA: 3:15 - loss: 1.9012 - regression_loss: 1.4139 - classification_loss: 0.4872\n",
      "311/500 [=================>............] - ETA: 3:14 - loss: 1.9010 - regression_loss: 1.4140 - classification_loss: 0.4870\n",
      "312/500 [=================>............] - ETA: 3:13 - loss: 1.9007 - regression_loss: 1.4139 - classification_loss: 0.4868\n",
      "313/500 [=================>............] - ETA: 3:12 - loss: 1.9026 - regression_loss: 1.4156 - classification_loss: 0.4870\n",
      "314/500 [=================>............] - ETA: 3:11 - loss: 1.9043 - regression_loss: 1.4169 - classification_loss: 0.4874\n",
      "315/500 [=================>............] - ETA: 3:10 - loss: 1.9018 - regression_loss: 1.4152 - classification_loss: 0.4866\n",
      "316/500 [=================>............] - ETA: 3:09 - loss: 1.9028 - regression_loss: 1.4159 - classification_loss: 0.4870\n",
      "317/500 [==================>...........] - ETA: 3:08 - loss: 1.9022 - regression_loss: 1.4156 - classification_loss: 0.4866\n",
      "318/500 [==================>...........] - ETA: 3:07 - loss: 1.9030 - regression_loss: 1.4163 - classification_loss: 0.4867\n",
      "319/500 [==================>...........] - ETA: 3:06 - loss: 1.9022 - regression_loss: 1.4155 - classification_loss: 0.4866\n",
      "320/500 [==================>...........] - ETA: 3:05 - loss: 1.9024 - regression_loss: 1.4151 - classification_loss: 0.4872\n",
      "321/500 [==================>...........] - ETA: 3:04 - loss: 1.9024 - regression_loss: 1.4151 - classification_loss: 0.4872\n",
      "322/500 [==================>...........] - ETA: 3:03 - loss: 1.9020 - regression_loss: 1.4148 - classification_loss: 0.4873\n",
      "323/500 [==================>...........] - ETA: 3:02 - loss: 1.9021 - regression_loss: 1.4146 - classification_loss: 0.4875\n",
      "324/500 [==================>...........] - ETA: 3:01 - loss: 1.8994 - regression_loss: 1.4128 - classification_loss: 0.4866\n",
      "325/500 [==================>...........] - ETA: 3:00 - loss: 1.9007 - regression_loss: 1.4128 - classification_loss: 0.4878\n",
      "326/500 [==================>...........] - ETA: 2:59 - loss: 1.9007 - regression_loss: 1.4128 - classification_loss: 0.4879\n",
      "327/500 [==================>...........] - ETA: 2:58 - loss: 1.9032 - regression_loss: 1.4142 - classification_loss: 0.4891\n",
      "328/500 [==================>...........] - ETA: 2:57 - loss: 1.9013 - regression_loss: 1.4127 - classification_loss: 0.4886\n",
      "329/500 [==================>...........] - ETA: 2:56 - loss: 1.9050 - regression_loss: 1.4152 - classification_loss: 0.4898\n",
      "330/500 [==================>...........] - ETA: 2:55 - loss: 1.9047 - regression_loss: 1.4145 - classification_loss: 0.4902\n",
      "331/500 [==================>...........] - ETA: 2:54 - loss: 1.9056 - regression_loss: 1.4150 - classification_loss: 0.4906\n",
      "332/500 [==================>...........] - ETA: 2:53 - loss: 1.9055 - regression_loss: 1.4147 - classification_loss: 0.4908\n",
      "333/500 [==================>...........] - ETA: 2:51 - loss: 1.9063 - regression_loss: 1.4154 - classification_loss: 0.4908\n",
      "334/500 [===================>..........] - ETA: 2:50 - loss: 1.9060 - regression_loss: 1.4155 - classification_loss: 0.4905\n",
      "335/500 [===================>..........] - ETA: 2:50 - loss: 1.9051 - regression_loss: 1.4143 - classification_loss: 0.4908\n",
      "336/500 [===================>..........] - ETA: 2:48 - loss: 1.9061 - regression_loss: 1.4153 - classification_loss: 0.4909\n",
      "337/500 [===================>..........] - ETA: 2:47 - loss: 1.9066 - regression_loss: 1.4153 - classification_loss: 0.4914\n",
      "338/500 [===================>..........] - ETA: 2:46 - loss: 1.9047 - regression_loss: 1.4136 - classification_loss: 0.4911\n",
      "339/500 [===================>..........] - ETA: 2:45 - loss: 1.9041 - regression_loss: 1.4133 - classification_loss: 0.4908\n",
      "340/500 [===================>..........] - ETA: 2:44 - loss: 1.9057 - regression_loss: 1.4144 - classification_loss: 0.4913\n",
      "341/500 [===================>..........] - ETA: 2:43 - loss: 1.9041 - regression_loss: 1.4132 - classification_loss: 0.4909\n",
      "342/500 [===================>..........] - ETA: 2:42 - loss: 1.9036 - regression_loss: 1.4129 - classification_loss: 0.4907\n",
      "343/500 [===================>..........] - ETA: 2:41 - loss: 1.9036 - regression_loss: 1.4128 - classification_loss: 0.4908\n",
      "344/500 [===================>..........] - ETA: 2:40 - loss: 1.9032 - regression_loss: 1.4124 - classification_loss: 0.4908\n",
      "345/500 [===================>..........] - ETA: 2:39 - loss: 1.9051 - regression_loss: 1.4141 - classification_loss: 0.4911\n",
      "346/500 [===================>..........] - ETA: 2:38 - loss: 1.9022 - regression_loss: 1.4119 - classification_loss: 0.4903\n",
      "347/500 [===================>..........] - ETA: 2:37 - loss: 1.9047 - regression_loss: 1.4137 - classification_loss: 0.4910\n",
      "348/500 [===================>..........] - ETA: 2:36 - loss: 1.9034 - regression_loss: 1.4131 - classification_loss: 0.4903\n",
      "349/500 [===================>..........] - ETA: 2:35 - loss: 1.9030 - regression_loss: 1.4131 - classification_loss: 0.4899\n",
      "350/500 [====================>.........] - ETA: 2:34 - loss: 1.9024 - regression_loss: 1.4127 - classification_loss: 0.4897\n",
      "351/500 [====================>.........] - ETA: 2:33 - loss: 1.9019 - regression_loss: 1.4122 - classification_loss: 0.4898\n",
      "352/500 [====================>.........] - ETA: 2:32 - loss: 1.9018 - regression_loss: 1.4120 - classification_loss: 0.4898\n",
      "353/500 [====================>.........] - ETA: 2:31 - loss: 1.8996 - regression_loss: 1.4103 - classification_loss: 0.4893\n",
      "354/500 [====================>.........] - ETA: 2:30 - loss: 1.8986 - regression_loss: 1.4095 - classification_loss: 0.4890\n",
      "355/500 [====================>.........] - ETA: 2:29 - loss: 1.8983 - regression_loss: 1.4094 - classification_loss: 0.4889\n",
      "356/500 [====================>.........] - ETA: 2:28 - loss: 1.8979 - regression_loss: 1.4090 - classification_loss: 0.4889\n",
      "357/500 [====================>.........] - ETA: 2:27 - loss: 1.8996 - regression_loss: 1.4103 - classification_loss: 0.4893\n",
      "358/500 [====================>.........] - ETA: 2:26 - loss: 1.8998 - regression_loss: 1.4106 - classification_loss: 0.4893\n",
      "359/500 [====================>.........] - ETA: 2:25 - loss: 1.8974 - regression_loss: 1.4090 - classification_loss: 0.4885\n",
      "360/500 [====================>.........] - ETA: 2:24 - loss: 1.8963 - regression_loss: 1.4080 - classification_loss: 0.4883\n",
      "361/500 [====================>.........] - ETA: 2:23 - loss: 1.8954 - regression_loss: 1.4074 - classification_loss: 0.4880\n",
      "362/500 [====================>.........] - ETA: 2:22 - loss: 1.8973 - regression_loss: 1.4083 - classification_loss: 0.4889\n",
      "363/500 [====================>.........] - ETA: 2:21 - loss: 1.8987 - regression_loss: 1.4099 - classification_loss: 0.4888\n",
      "364/500 [====================>.........] - ETA: 2:20 - loss: 1.8984 - regression_loss: 1.4098 - classification_loss: 0.4886\n",
      "365/500 [====================>.........] - ETA: 2:19 - loss: 1.8973 - regression_loss: 1.4092 - classification_loss: 0.4881\n",
      "366/500 [====================>.........] - ETA: 2:18 - loss: 1.8956 - regression_loss: 1.4080 - classification_loss: 0.4876\n",
      "367/500 [=====================>........] - ETA: 2:16 - loss: 1.8952 - regression_loss: 1.4075 - classification_loss: 0.4878\n",
      "368/500 [=====================>........] - ETA: 2:15 - loss: 1.8960 - regression_loss: 1.4080 - classification_loss: 0.4881\n",
      "369/500 [=====================>........] - ETA: 2:14 - loss: 1.8946 - regression_loss: 1.4073 - classification_loss: 0.4873\n",
      "370/500 [=====================>........] - ETA: 2:13 - loss: 1.8946 - regression_loss: 1.4072 - classification_loss: 0.4873\n",
      "371/500 [=====================>........] - ETA: 2:12 - loss: 1.8969 - regression_loss: 1.4088 - classification_loss: 0.4880\n",
      "372/500 [=====================>........] - ETA: 2:11 - loss: 1.8958 - regression_loss: 1.4079 - classification_loss: 0.4879\n",
      "373/500 [=====================>........] - ETA: 2:10 - loss: 1.8940 - regression_loss: 1.4066 - classification_loss: 0.4874\n",
      "374/500 [=====================>........] - ETA: 2:09 - loss: 1.8926 - regression_loss: 1.4056 - classification_loss: 0.4870\n",
      "375/500 [=====================>........] - ETA: 2:08 - loss: 1.8913 - regression_loss: 1.4047 - classification_loss: 0.4866\n",
      "376/500 [=====================>........] - ETA: 2:07 - loss: 1.8913 - regression_loss: 1.4049 - classification_loss: 0.4865\n",
      "377/500 [=====================>........] - ETA: 2:06 - loss: 1.8919 - regression_loss: 1.4058 - classification_loss: 0.4861\n",
      "378/500 [=====================>........] - ETA: 2:05 - loss: 1.8916 - regression_loss: 1.4053 - classification_loss: 0.4863\n",
      "379/500 [=====================>........] - ETA: 2:04 - loss: 1.8933 - regression_loss: 1.4063 - classification_loss: 0.4870\n",
      "380/500 [=====================>........] - ETA: 2:03 - loss: 1.8912 - regression_loss: 1.4049 - classification_loss: 0.4863\n",
      "381/500 [=====================>........] - ETA: 2:02 - loss: 1.8890 - regression_loss: 1.4034 - classification_loss: 0.4857\n",
      "382/500 [=====================>........] - ETA: 2:01 - loss: 1.8869 - regression_loss: 1.4017 - classification_loss: 0.4853\n",
      "383/500 [=====================>........] - ETA: 2:00 - loss: 1.8898 - regression_loss: 1.4036 - classification_loss: 0.4862\n",
      "384/500 [======================>.......] - ETA: 1:59 - loss: 1.8897 - regression_loss: 1.4029 - classification_loss: 0.4868\n",
      "385/500 [======================>.......] - ETA: 1:58 - loss: 1.8898 - regression_loss: 1.4036 - classification_loss: 0.4862\n",
      "386/500 [======================>.......] - ETA: 1:57 - loss: 1.8894 - regression_loss: 1.4035 - classification_loss: 0.4859\n",
      "387/500 [======================>.......] - ETA: 1:56 - loss: 1.8897 - regression_loss: 1.4035 - classification_loss: 0.4861\n",
      "388/500 [======================>.......] - ETA: 1:55 - loss: 1.8886 - regression_loss: 1.4030 - classification_loss: 0.4856\n",
      "389/500 [======================>.......] - ETA: 1:54 - loss: 1.8873 - regression_loss: 1.4022 - classification_loss: 0.4852\n",
      "390/500 [======================>.......] - ETA: 1:53 - loss: 1.8886 - regression_loss: 1.4034 - classification_loss: 0.4852\n",
      "391/500 [======================>.......] - ETA: 1:52 - loss: 1.8874 - regression_loss: 1.4026 - classification_loss: 0.4848\n",
      "392/500 [======================>.......] - ETA: 1:51 - loss: 1.8873 - regression_loss: 1.4024 - classification_loss: 0.4849\n",
      "393/500 [======================>.......] - ETA: 1:50 - loss: 1.8892 - regression_loss: 1.4036 - classification_loss: 0.4855\n",
      "394/500 [======================>.......] - ETA: 1:49 - loss: 1.8907 - regression_loss: 1.4053 - classification_loss: 0.4854\n",
      "395/500 [======================>.......] - ETA: 1:48 - loss: 1.8908 - regression_loss: 1.4053 - classification_loss: 0.4855\n",
      "396/500 [======================>.......] - ETA: 1:47 - loss: 1.8909 - regression_loss: 1.4050 - classification_loss: 0.4859\n",
      "397/500 [======================>.......] - ETA: 1:46 - loss: 1.8911 - regression_loss: 1.4052 - classification_loss: 0.4859\n",
      "398/500 [======================>.......] - ETA: 1:45 - loss: 1.8934 - regression_loss: 1.4067 - classification_loss: 0.4867\n",
      "399/500 [======================>.......] - ETA: 1:43 - loss: 1.8924 - regression_loss: 1.4058 - classification_loss: 0.4866\n",
      "400/500 [=======================>......] - ETA: 1:42 - loss: 1.8939 - regression_loss: 1.4066 - classification_loss: 0.4873\n",
      "401/500 [=======================>......] - ETA: 1:41 - loss: 1.8944 - regression_loss: 1.4071 - classification_loss: 0.4873\n",
      "402/500 [=======================>......] - ETA: 1:40 - loss: 1.8929 - regression_loss: 1.4060 - classification_loss: 0.4869\n",
      "403/500 [=======================>......] - ETA: 1:39 - loss: 1.8940 - regression_loss: 1.4073 - classification_loss: 0.4867\n",
      "404/500 [=======================>......] - ETA: 1:38 - loss: 1.8940 - regression_loss: 1.4077 - classification_loss: 0.4863\n",
      "405/500 [=======================>......] - ETA: 1:37 - loss: 1.8936 - regression_loss: 1.4075 - classification_loss: 0.4861\n",
      "406/500 [=======================>......] - ETA: 1:36 - loss: 1.8935 - regression_loss: 1.4074 - classification_loss: 0.4861\n",
      "407/500 [=======================>......] - ETA: 1:35 - loss: 1.8938 - regression_loss: 1.4074 - classification_loss: 0.4863\n",
      "408/500 [=======================>......] - ETA: 1:34 - loss: 1.8939 - regression_loss: 1.4077 - classification_loss: 0.4862\n",
      "409/500 [=======================>......] - ETA: 1:33 - loss: 1.8929 - regression_loss: 1.4070 - classification_loss: 0.4859\n",
      "410/500 [=======================>......] - ETA: 1:32 - loss: 1.8914 - regression_loss: 1.4062 - classification_loss: 0.4853\n",
      "411/500 [=======================>......] - ETA: 1:31 - loss: 1.8917 - regression_loss: 1.4061 - classification_loss: 0.4856\n",
      "412/500 [=======================>......] - ETA: 1:30 - loss: 1.8936 - regression_loss: 1.4074 - classification_loss: 0.4862\n",
      "413/500 [=======================>......] - ETA: 1:29 - loss: 1.8942 - regression_loss: 1.4080 - classification_loss: 0.4862\n",
      "414/500 [=======================>......] - ETA: 1:28 - loss: 1.8953 - regression_loss: 1.4086 - classification_loss: 0.4867\n",
      "415/500 [=======================>......] - ETA: 1:27 - loss: 1.8942 - regression_loss: 1.4076 - classification_loss: 0.4866\n",
      "416/500 [=======================>......] - ETA: 1:26 - loss: 1.8925 - regression_loss: 1.4065 - classification_loss: 0.4860\n",
      "417/500 [========================>.....] - ETA: 1:25 - loss: 1.8930 - regression_loss: 1.4069 - classification_loss: 0.4862\n",
      "418/500 [========================>.....] - ETA: 1:24 - loss: 1.8918 - regression_loss: 1.4058 - classification_loss: 0.4860\n",
      "419/500 [========================>.....] - ETA: 1:23 - loss: 1.8947 - regression_loss: 1.4080 - classification_loss: 0.4867\n",
      "420/500 [========================>.....] - ETA: 1:22 - loss: 1.8932 - regression_loss: 1.4070 - classification_loss: 0.4862\n",
      "421/500 [========================>.....] - ETA: 1:21 - loss: 1.8917 - regression_loss: 1.4058 - classification_loss: 0.4859\n",
      "422/500 [========================>.....] - ETA: 1:20 - loss: 1.8928 - regression_loss: 1.4064 - classification_loss: 0.4863\n",
      "423/500 [========================>.....] - ETA: 1:19 - loss: 1.8938 - regression_loss: 1.4069 - classification_loss: 0.4869\n",
      "424/500 [========================>.....] - ETA: 1:18 - loss: 1.8938 - regression_loss: 1.4071 - classification_loss: 0.4867\n",
      "425/500 [========================>.....] - ETA: 1:17 - loss: 1.8934 - regression_loss: 1.4065 - classification_loss: 0.4869\n",
      "426/500 [========================>.....] - ETA: 1:16 - loss: 1.8937 - regression_loss: 1.4071 - classification_loss: 0.4866\n",
      "427/500 [========================>.....] - ETA: 1:15 - loss: 1.8935 - regression_loss: 1.4068 - classification_loss: 0.4866\n",
      "428/500 [========================>.....] - ETA: 1:14 - loss: 1.8950 - regression_loss: 1.4081 - classification_loss: 0.4869\n",
      "429/500 [========================>.....] - ETA: 1:13 - loss: 1.8939 - regression_loss: 1.4071 - classification_loss: 0.4867\n",
      "430/500 [========================>.....] - ETA: 1:12 - loss: 1.8940 - regression_loss: 1.4070 - classification_loss: 0.4870\n",
      "431/500 [========================>.....] - ETA: 1:11 - loss: 1.8950 - regression_loss: 1.4076 - classification_loss: 0.4873\n",
      "432/500 [========================>.....] - ETA: 1:10 - loss: 1.8951 - regression_loss: 1.4078 - classification_loss: 0.4873\n",
      "433/500 [========================>.....] - ETA: 1:09 - loss: 1.8942 - regression_loss: 1.4072 - classification_loss: 0.4870\n",
      "434/500 [=========================>....] - ETA: 1:08 - loss: 1.8917 - regression_loss: 1.4055 - classification_loss: 0.4863\n",
      "435/500 [=========================>....] - ETA: 1:07 - loss: 1.8919 - regression_loss: 1.4056 - classification_loss: 0.4863\n",
      "436/500 [=========================>....] - ETA: 1:06 - loss: 1.8937 - regression_loss: 1.4067 - classification_loss: 0.4870\n",
      "437/500 [=========================>....] - ETA: 1:05 - loss: 1.8927 - regression_loss: 1.4060 - classification_loss: 0.4868\n",
      "438/500 [=========================>....] - ETA: 1:04 - loss: 1.8914 - regression_loss: 1.4048 - classification_loss: 0.4866\n",
      "439/500 [=========================>....] - ETA: 1:03 - loss: 1.8912 - regression_loss: 1.4045 - classification_loss: 0.4867\n",
      "440/500 [=========================>....] - ETA: 1:01 - loss: 1.8911 - regression_loss: 1.4048 - classification_loss: 0.4863\n",
      "441/500 [=========================>....] - ETA: 1:00 - loss: 1.8903 - regression_loss: 1.4036 - classification_loss: 0.4866\n",
      "442/500 [=========================>....] - ETA: 59s - loss: 1.8897 - regression_loss: 1.4033 - classification_loss: 0.4864 \n",
      "443/500 [=========================>....] - ETA: 58s - loss: 1.8895 - regression_loss: 1.4030 - classification_loss: 0.4866\n",
      "444/500 [=========================>....] - ETA: 57s - loss: 1.8911 - regression_loss: 1.4039 - classification_loss: 0.4872\n",
      "445/500 [=========================>....] - ETA: 56s - loss: 1.8902 - regression_loss: 1.4032 - classification_loss: 0.4871\n",
      "446/500 [=========================>....] - ETA: 55s - loss: 1.8907 - regression_loss: 1.4032 - classification_loss: 0.4875\n",
      "447/500 [=========================>....] - ETA: 54s - loss: 1.8897 - regression_loss: 1.4026 - classification_loss: 0.4870\n",
      "448/500 [=========================>....] - ETA: 53s - loss: 1.8885 - regression_loss: 1.4016 - classification_loss: 0.4868\n",
      "449/500 [=========================>....] - ETA: 52s - loss: 1.8906 - regression_loss: 1.4036 - classification_loss: 0.4869\n",
      "450/500 [==========================>...] - ETA: 51s - loss: 1.8930 - regression_loss: 1.4051 - classification_loss: 0.4879\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 1.8937 - regression_loss: 1.4055 - classification_loss: 0.4882\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 1.8933 - regression_loss: 1.4055 - classification_loss: 0.4879\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.8920 - regression_loss: 1.4043 - classification_loss: 0.4877\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.8919 - regression_loss: 1.4044 - classification_loss: 0.4875\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.8905 - regression_loss: 1.4035 - classification_loss: 0.4870\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.8889 - regression_loss: 1.4022 - classification_loss: 0.4867\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.8914 - regression_loss: 1.4039 - classification_loss: 0.4876\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.8931 - regression_loss: 1.4052 - classification_loss: 0.4879\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.8943 - regression_loss: 1.4058 - classification_loss: 0.4884\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.8961 - regression_loss: 1.4072 - classification_loss: 0.4889\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 1.8955 - regression_loss: 1.4068 - classification_loss: 0.4888\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 1.8951 - regression_loss: 1.4066 - classification_loss: 0.4886\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 1.8951 - regression_loss: 1.4065 - classification_loss: 0.4886\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 1.8952 - regression_loss: 1.4066 - classification_loss: 0.4886\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 1.8948 - regression_loss: 1.4065 - classification_loss: 0.4883\n",
      "466/500 [==========================>...] - ETA: 35s - loss: 1.8964 - regression_loss: 1.4077 - classification_loss: 0.4887\n",
      "467/500 [===========================>..] - ETA: 34s - loss: 1.8953 - regression_loss: 1.4071 - classification_loss: 0.4883\n",
      "468/500 [===========================>..] - ETA: 33s - loss: 1.8964 - regression_loss: 1.4078 - classification_loss: 0.4886\n",
      "469/500 [===========================>..] - ETA: 32s - loss: 1.8971 - regression_loss: 1.4085 - classification_loss: 0.4886\n",
      "470/500 [===========================>..] - ETA: 30s - loss: 1.8968 - regression_loss: 1.4081 - classification_loss: 0.4887\n",
      "471/500 [===========================>..] - ETA: 29s - loss: 1.8958 - regression_loss: 1.4076 - classification_loss: 0.4882\n",
      "472/500 [===========================>..] - ETA: 28s - loss: 1.8980 - regression_loss: 1.4088 - classification_loss: 0.4892\n",
      "473/500 [===========================>..] - ETA: 27s - loss: 1.8971 - regression_loss: 1.4083 - classification_loss: 0.4888\n",
      "474/500 [===========================>..] - ETA: 26s - loss: 1.8961 - regression_loss: 1.4079 - classification_loss: 0.4882\n",
      "475/500 [===========================>..] - ETA: 25s - loss: 1.8972 - regression_loss: 1.4086 - classification_loss: 0.4886\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 1.8983 - regression_loss: 1.4099 - classification_loss: 0.4884\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.8991 - regression_loss: 1.4102 - classification_loss: 0.4889\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.8984 - regression_loss: 1.4099 - classification_loss: 0.4886\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.8968 - regression_loss: 1.4087 - classification_loss: 0.4881\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.8969 - regression_loss: 1.4089 - classification_loss: 0.4880\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.8960 - regression_loss: 1.4085 - classification_loss: 0.4875\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.8952 - regression_loss: 1.4081 - classification_loss: 0.4871\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.8949 - regression_loss: 1.4080 - classification_loss: 0.4869\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.8955 - regression_loss: 1.4080 - classification_loss: 0.4875\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.8951 - regression_loss: 1.4077 - classification_loss: 0.4874\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.8946 - regression_loss: 1.4074 - classification_loss: 0.4872\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.8951 - regression_loss: 1.4079 - classification_loss: 0.4871\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.8965 - regression_loss: 1.4092 - classification_loss: 0.4873\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.8992 - regression_loss: 1.4110 - classification_loss: 0.4881\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.8983 - regression_loss: 1.4105 - classification_loss: 0.4878\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.8993 - regression_loss: 1.4109 - classification_loss: 0.4885 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.8991 - regression_loss: 1.4106 - classification_loss: 0.4885\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.8995 - regression_loss: 1.4107 - classification_loss: 0.4887\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.8983 - regression_loss: 1.4099 - classification_loss: 0.4884\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.9000 - regression_loss: 1.4109 - classification_loss: 0.4891\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.9023 - regression_loss: 1.4127 - classification_loss: 0.4897\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.9038 - regression_loss: 1.4135 - classification_loss: 0.4903\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.9035 - regression_loss: 1.4135 - classification_loss: 0.4900\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.9040 - regression_loss: 1.4139 - classification_loss: 0.4901\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9041 - regression_loss: 1.4137 - classification_loss: 0.4904\n",
      "Epoch 00024: saving model to ./snapshots\\resnet50_csv_24.h5\n",
      "\n",
      "500/500 [==============================] - 518s 1s/step - loss: 1.9041 - regression_loss: 1.4137 - classification_loss: 0.4904\n",
      "Epoch 25/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 2.1784 - regression_loss: 1.6629 - classification_loss: 0.5155\n",
      "  2/500 [..............................] - ETA: 4:27 - loss: 2.0799 - regression_loss: 1.6145 - classification_loss: 0.4654\n",
      "  3/500 [..............................] - ETA: 5:41 - loss: 2.1051 - regression_loss: 1.6844 - classification_loss: 0.4208\n",
      "  4/500 [..............................] - ETA: 6:28 - loss: 2.0912 - regression_loss: 1.6542 - classification_loss: 0.4370\n",
      "  5/500 [..............................] - ETA: 6:37 - loss: 2.0137 - regression_loss: 1.5813 - classification_loss: 0.4325\n",
      "  6/500 [..............................] - ETA: 6:59 - loss: 1.9097 - regression_loss: 1.4892 - classification_loss: 0.4204\n",
      "  7/500 [..............................] - ETA: 7:08 - loss: 1.8936 - regression_loss: 1.4560 - classification_loss: 0.4376\n",
      "  8/500 [..............................] - ETA: 7:13 - loss: 1.8183 - regression_loss: 1.4016 - classification_loss: 0.4167\n",
      "  9/500 [..............................] - ETA: 7:23 - loss: 1.8139 - regression_loss: 1.3975 - classification_loss: 0.4164\n",
      " 10/500 [..............................] - ETA: 7:32 - loss: 1.7600 - regression_loss: 1.3550 - classification_loss: 0.4050\n",
      " 11/500 [..............................] - ETA: 7:38 - loss: 1.7986 - regression_loss: 1.3870 - classification_loss: 0.4116\n",
      " 12/500 [..............................] - ETA: 7:39 - loss: 1.7516 - regression_loss: 1.3411 - classification_loss: 0.4105\n",
      " 13/500 [..............................] - ETA: 7:48 - loss: 1.7638 - regression_loss: 1.3431 - classification_loss: 0.4206\n",
      " 14/500 [..............................] - ETA: 7:48 - loss: 1.7591 - regression_loss: 1.3408 - classification_loss: 0.4183\n",
      " 15/500 [..............................] - ETA: 7:51 - loss: 1.7682 - regression_loss: 1.3395 - classification_loss: 0.4288\n",
      " 16/500 [..............................] - ETA: 7:50 - loss: 1.7403 - regression_loss: 1.3229 - classification_loss: 0.4174\n",
      " 17/500 [>.............................] - ETA: 7:52 - loss: 1.7023 - regression_loss: 1.2962 - classification_loss: 0.4061\n",
      " 18/500 [>.............................] - ETA: 7:48 - loss: 1.7060 - regression_loss: 1.2998 - classification_loss: 0.4062\n",
      " 19/500 [>.............................] - ETA: 7:50 - loss: 1.6717 - regression_loss: 1.2716 - classification_loss: 0.4001\n",
      " 20/500 [>.............................] - ETA: 7:52 - loss: 1.6625 - regression_loss: 1.2657 - classification_loss: 0.3968\n",
      " 21/500 [>.............................] - ETA: 7:51 - loss: 1.7033 - regression_loss: 1.2973 - classification_loss: 0.4059\n",
      " 22/500 [>.............................] - ETA: 7:50 - loss: 1.7062 - regression_loss: 1.3028 - classification_loss: 0.4034\n",
      " 23/500 [>.............................] - ETA: 7:49 - loss: 1.7034 - regression_loss: 1.2985 - classification_loss: 0.4048\n",
      " 24/500 [>.............................] - ETA: 7:50 - loss: 1.7094 - regression_loss: 1.3000 - classification_loss: 0.4095\n",
      " 25/500 [>.............................] - ETA: 7:51 - loss: 1.7219 - regression_loss: 1.3025 - classification_loss: 0.4194\n",
      " 26/500 [>.............................] - ETA: 7:50 - loss: 1.7042 - regression_loss: 1.2884 - classification_loss: 0.4158\n",
      " 27/500 [>.............................] - ETA: 7:47 - loss: 1.7019 - regression_loss: 1.2841 - classification_loss: 0.4178\n",
      " 28/500 [>.............................] - ETA: 7:47 - loss: 1.7175 - regression_loss: 1.2921 - classification_loss: 0.4255\n",
      " 29/500 [>.............................] - ETA: 7:46 - loss: 1.7386 - regression_loss: 1.3087 - classification_loss: 0.4300\n",
      " 30/500 [>.............................] - ETA: 7:44 - loss: 1.7384 - regression_loss: 1.3029 - classification_loss: 0.4355\n",
      " 31/500 [>.............................] - ETA: 7:44 - loss: 1.7565 - regression_loss: 1.3106 - classification_loss: 0.4459\n",
      " 32/500 [>.............................] - ETA: 7:44 - loss: 1.7913 - regression_loss: 1.3372 - classification_loss: 0.4542\n",
      " 33/500 [>.............................] - ETA: 7:43 - loss: 1.7870 - regression_loss: 1.3353 - classification_loss: 0.4516\n",
      " 34/500 [=>............................] - ETA: 7:43 - loss: 1.7702 - regression_loss: 1.3210 - classification_loss: 0.4492\n",
      " 35/500 [=>............................] - ETA: 7:44 - loss: 1.7760 - regression_loss: 1.3267 - classification_loss: 0.4493\n",
      " 36/500 [=>............................] - ETA: 7:44 - loss: 1.7839 - regression_loss: 1.3345 - classification_loss: 0.4494\n",
      " 37/500 [=>............................] - ETA: 7:41 - loss: 1.7883 - regression_loss: 1.3390 - classification_loss: 0.4492\n",
      " 38/500 [=>............................] - ETA: 7:42 - loss: 1.7902 - regression_loss: 1.3414 - classification_loss: 0.4489\n",
      " 39/500 [=>............................] - ETA: 7:42 - loss: 1.7865 - regression_loss: 1.3407 - classification_loss: 0.4458\n",
      " 40/500 [=>............................] - ETA: 7:40 - loss: 1.7998 - regression_loss: 1.3508 - classification_loss: 0.4490\n",
      " 41/500 [=>............................] - ETA: 7:40 - loss: 1.7918 - regression_loss: 1.3472 - classification_loss: 0.4446\n",
      " 42/500 [=>............................] - ETA: 7:39 - loss: 1.7891 - regression_loss: 1.3463 - classification_loss: 0.4428\n",
      " 43/500 [=>............................] - ETA: 7:36 - loss: 1.7902 - regression_loss: 1.3472 - classification_loss: 0.4430\n",
      " 44/500 [=>............................] - ETA: 7:37 - loss: 1.7892 - regression_loss: 1.3473 - classification_loss: 0.4419\n",
      " 45/500 [=>............................] - ETA: 7:36 - loss: 1.7986 - regression_loss: 1.3534 - classification_loss: 0.4452\n",
      " 46/500 [=>............................] - ETA: 7:35 - loss: 1.7945 - regression_loss: 1.3515 - classification_loss: 0.4429\n",
      " 47/500 [=>............................] - ETA: 7:34 - loss: 1.7845 - regression_loss: 1.3423 - classification_loss: 0.4422\n",
      " 48/500 [=>............................] - ETA: 7:33 - loss: 1.7720 - regression_loss: 1.3334 - classification_loss: 0.4386\n",
      " 49/500 [=>............................] - ETA: 7:32 - loss: 1.7705 - regression_loss: 1.3317 - classification_loss: 0.4388\n",
      " 50/500 [==>...........................] - ETA: 7:32 - loss: 1.7751 - regression_loss: 1.3363 - classification_loss: 0.4388\n",
      " 51/500 [==>...........................] - ETA: 7:31 - loss: 1.7698 - regression_loss: 1.3320 - classification_loss: 0.4379\n",
      " 52/500 [==>...........................] - ETA: 7:31 - loss: 1.7753 - regression_loss: 1.3369 - classification_loss: 0.4384\n",
      " 53/500 [==>...........................] - ETA: 7:31 - loss: 1.7778 - regression_loss: 1.3383 - classification_loss: 0.4395\n",
      " 54/500 [==>...........................] - ETA: 7:29 - loss: 1.7822 - regression_loss: 1.3371 - classification_loss: 0.4452\n",
      " 55/500 [==>...........................] - ETA: 7:29 - loss: 1.7854 - regression_loss: 1.3415 - classification_loss: 0.4439\n",
      " 56/500 [==>...........................] - ETA: 7:28 - loss: 1.7887 - regression_loss: 1.3440 - classification_loss: 0.4447\n",
      " 57/500 [==>...........................] - ETA: 7:27 - loss: 1.7803 - regression_loss: 1.3381 - classification_loss: 0.4422\n",
      " 58/500 [==>...........................] - ETA: 7:26 - loss: 1.7880 - regression_loss: 1.3457 - classification_loss: 0.4423\n",
      " 59/500 [==>...........................] - ETA: 7:25 - loss: 1.7869 - regression_loss: 1.3459 - classification_loss: 0.4410\n",
      " 60/500 [==>...........................] - ETA: 7:24 - loss: 1.7922 - regression_loss: 1.3525 - classification_loss: 0.4396\n",
      " 61/500 [==>...........................] - ETA: 7:23 - loss: 1.7825 - regression_loss: 1.3433 - classification_loss: 0.4393\n",
      " 62/500 [==>...........................] - ETA: 7:23 - loss: 1.7816 - regression_loss: 1.3408 - classification_loss: 0.4409\n",
      " 63/500 [==>...........................] - ETA: 7:22 - loss: 1.7898 - regression_loss: 1.3444 - classification_loss: 0.4454\n",
      " 64/500 [==>...........................] - ETA: 7:21 - loss: 1.7911 - regression_loss: 1.3461 - classification_loss: 0.4450\n",
      " 65/500 [==>...........................] - ETA: 7:21 - loss: 1.7872 - regression_loss: 1.3428 - classification_loss: 0.4444\n",
      " 66/500 [==>...........................] - ETA: 7:20 - loss: 1.7752 - regression_loss: 1.3335 - classification_loss: 0.4417\n",
      " 67/500 [===>..........................] - ETA: 7:19 - loss: 1.7806 - regression_loss: 1.3383 - classification_loss: 0.4423\n",
      " 68/500 [===>..........................] - ETA: 7:18 - loss: 1.7738 - regression_loss: 1.3321 - classification_loss: 0.4417\n",
      " 69/500 [===>..........................] - ETA: 7:17 - loss: 1.7692 - regression_loss: 1.3284 - classification_loss: 0.4408\n",
      " 70/500 [===>..........................] - ETA: 7:16 - loss: 1.7700 - regression_loss: 1.3305 - classification_loss: 0.4395\n",
      " 71/500 [===>..........................] - ETA: 7:15 - loss: 1.7682 - regression_loss: 1.3295 - classification_loss: 0.4387\n",
      " 72/500 [===>..........................] - ETA: 7:14 - loss: 1.7706 - regression_loss: 1.3320 - classification_loss: 0.4386\n",
      " 73/500 [===>..........................] - ETA: 7:13 - loss: 1.7748 - regression_loss: 1.3347 - classification_loss: 0.4401\n",
      " 74/500 [===>..........................] - ETA: 7:12 - loss: 1.7921 - regression_loss: 1.3474 - classification_loss: 0.4447\n",
      " 75/500 [===>..........................] - ETA: 7:11 - loss: 1.7956 - regression_loss: 1.3482 - classification_loss: 0.4475\n",
      " 76/500 [===>..........................] - ETA: 7:09 - loss: 1.7983 - regression_loss: 1.3476 - classification_loss: 0.4507\n",
      " 77/500 [===>..........................] - ETA: 7:09 - loss: 1.7947 - regression_loss: 1.3422 - classification_loss: 0.4525\n",
      " 78/500 [===>..........................] - ETA: 7:08 - loss: 1.7989 - regression_loss: 1.3466 - classification_loss: 0.4523\n",
      " 79/500 [===>..........................] - ETA: 7:07 - loss: 1.8008 - regression_loss: 1.3473 - classification_loss: 0.4535\n",
      " 80/500 [===>..........................] - ETA: 7:06 - loss: 1.7993 - regression_loss: 1.3462 - classification_loss: 0.4531\n",
      " 81/500 [===>..........................] - ETA: 7:05 - loss: 1.7875 - regression_loss: 1.3382 - classification_loss: 0.4494\n",
      " 82/500 [===>..........................] - ETA: 7:04 - loss: 1.7951 - regression_loss: 1.3447 - classification_loss: 0.4504\n",
      " 83/500 [===>..........................] - ETA: 7:03 - loss: 1.7995 - regression_loss: 1.3481 - classification_loss: 0.4513\n",
      " 84/500 [====>.........................] - ETA: 7:01 - loss: 1.7951 - regression_loss: 1.3436 - classification_loss: 0.4516\n",
      " 85/500 [====>.........................] - ETA: 7:01 - loss: 1.7963 - regression_loss: 1.3454 - classification_loss: 0.4509\n",
      " 86/500 [====>.........................] - ETA: 7:00 - loss: 1.7967 - regression_loss: 1.3462 - classification_loss: 0.4505\n",
      " 87/500 [====>.........................] - ETA: 6:59 - loss: 1.7937 - regression_loss: 1.3449 - classification_loss: 0.4488\n",
      " 88/500 [====>.........................] - ETA: 6:59 - loss: 1.7965 - regression_loss: 1.3463 - classification_loss: 0.4502\n",
      " 89/500 [====>.........................] - ETA: 6:58 - loss: 1.7933 - regression_loss: 1.3439 - classification_loss: 0.4493\n",
      " 90/500 [====>.........................] - ETA: 6:57 - loss: 1.7917 - regression_loss: 1.3428 - classification_loss: 0.4489\n",
      " 91/500 [====>.........................] - ETA: 6:56 - loss: 1.7872 - regression_loss: 1.3392 - classification_loss: 0.4480\n",
      " 92/500 [====>.........................] - ETA: 6:55 - loss: 1.7833 - regression_loss: 1.3355 - classification_loss: 0.4477\n",
      " 93/500 [====>.........................] - ETA: 6:53 - loss: 1.7821 - regression_loss: 1.3341 - classification_loss: 0.4480\n",
      " 94/500 [====>.........................] - ETA: 6:53 - loss: 1.7833 - regression_loss: 1.3347 - classification_loss: 0.4487\n",
      " 95/500 [====>.........................] - ETA: 6:52 - loss: 1.7953 - regression_loss: 1.3434 - classification_loss: 0.4519\n",
      " 96/500 [====>.........................] - ETA: 6:51 - loss: 1.7907 - regression_loss: 1.3397 - classification_loss: 0.4510\n",
      " 97/500 [====>.........................] - ETA: 6:50 - loss: 1.7879 - regression_loss: 1.3357 - classification_loss: 0.4522\n",
      " 98/500 [====>.........................] - ETA: 6:49 - loss: 1.7875 - regression_loss: 1.3352 - classification_loss: 0.4523\n",
      " 99/500 [====>.........................] - ETA: 6:48 - loss: 1.8034 - regression_loss: 1.3450 - classification_loss: 0.4584\n",
      "100/500 [=====>........................] - ETA: 6:47 - loss: 1.8078 - regression_loss: 1.3484 - classification_loss: 0.4594\n",
      "101/500 [=====>........................] - ETA: 6:45 - loss: 1.8169 - regression_loss: 1.3544 - classification_loss: 0.4624\n",
      "102/500 [=====>........................] - ETA: 6:45 - loss: 1.8140 - regression_loss: 1.3522 - classification_loss: 0.4618\n",
      "103/500 [=====>........................] - ETA: 6:44 - loss: 1.8109 - regression_loss: 1.3488 - classification_loss: 0.4621\n",
      "104/500 [=====>........................] - ETA: 6:43 - loss: 1.8162 - regression_loss: 1.3527 - classification_loss: 0.4635\n",
      "105/500 [=====>........................] - ETA: 6:42 - loss: 1.8236 - regression_loss: 1.3568 - classification_loss: 0.4668\n",
      "106/500 [=====>........................] - ETA: 6:41 - loss: 1.8215 - regression_loss: 1.3543 - classification_loss: 0.4672\n",
      "107/500 [=====>........................] - ETA: 6:39 - loss: 1.8166 - regression_loss: 1.3512 - classification_loss: 0.4654\n",
      "108/500 [=====>........................] - ETA: 6:39 - loss: 1.8108 - regression_loss: 1.3474 - classification_loss: 0.4634\n",
      "109/500 [=====>........................] - ETA: 6:38 - loss: 1.8074 - regression_loss: 1.3448 - classification_loss: 0.4626\n",
      "110/500 [=====>........................] - ETA: 6:37 - loss: 1.8064 - regression_loss: 1.3444 - classification_loss: 0.4621\n",
      "111/500 [=====>........................] - ETA: 6:36 - loss: 1.8076 - regression_loss: 1.3466 - classification_loss: 0.4610\n",
      "112/500 [=====>........................] - ETA: 6:35 - loss: 1.8101 - regression_loss: 1.3492 - classification_loss: 0.4609\n",
      "113/500 [=====>........................] - ETA: 6:34 - loss: 1.8129 - regression_loss: 1.3523 - classification_loss: 0.4606\n",
      "114/500 [=====>........................] - ETA: 6:33 - loss: 1.8094 - regression_loss: 1.3481 - classification_loss: 0.4613\n",
      "115/500 [=====>........................] - ETA: 6:32 - loss: 1.8139 - regression_loss: 1.3517 - classification_loss: 0.4621\n",
      "116/500 [=====>........................] - ETA: 6:32 - loss: 1.8133 - regression_loss: 1.3519 - classification_loss: 0.4613\n",
      "117/500 [======>.......................] - ETA: 6:31 - loss: 1.8123 - regression_loss: 1.3516 - classification_loss: 0.4607\n",
      "118/500 [======>.......................] - ETA: 6:30 - loss: 1.8244 - regression_loss: 1.3594 - classification_loss: 0.4650\n",
      "119/500 [======>.......................] - ETA: 6:29 - loss: 1.8334 - regression_loss: 1.3657 - classification_loss: 0.4676\n",
      "120/500 [======>.......................] - ETA: 6:28 - loss: 1.8348 - regression_loss: 1.3676 - classification_loss: 0.4672\n",
      "121/500 [======>.......................] - ETA: 6:27 - loss: 1.8419 - regression_loss: 1.3715 - classification_loss: 0.4704\n",
      "122/500 [======>.......................] - ETA: 6:26 - loss: 1.8436 - regression_loss: 1.3731 - classification_loss: 0.4705\n",
      "123/500 [======>.......................] - ETA: 6:25 - loss: 1.8495 - regression_loss: 1.3771 - classification_loss: 0.4724\n",
      "124/500 [======>.......................] - ETA: 6:24 - loss: 1.8484 - regression_loss: 1.3770 - classification_loss: 0.4714\n",
      "125/500 [======>.......................] - ETA: 6:23 - loss: 1.8505 - regression_loss: 1.3774 - classification_loss: 0.4731\n",
      "126/500 [======>.......................] - ETA: 6:21 - loss: 1.8525 - regression_loss: 1.3793 - classification_loss: 0.4732\n",
      "127/500 [======>.......................] - ETA: 6:20 - loss: 1.8582 - regression_loss: 1.3839 - classification_loss: 0.4743\n",
      "128/500 [======>.......................] - ETA: 6:19 - loss: 1.8612 - regression_loss: 1.3854 - classification_loss: 0.4758\n",
      "129/500 [======>.......................] - ETA: 6:18 - loss: 1.8617 - regression_loss: 1.3866 - classification_loss: 0.4751\n",
      "130/500 [======>.......................] - ETA: 6:17 - loss: 1.8707 - regression_loss: 1.3931 - classification_loss: 0.4776\n",
      "131/500 [======>.......................] - ETA: 6:16 - loss: 1.8694 - regression_loss: 1.3926 - classification_loss: 0.4768\n",
      "132/500 [======>.......................] - ETA: 6:15 - loss: 1.8699 - regression_loss: 1.3932 - classification_loss: 0.4767\n",
      "133/500 [======>.......................] - ETA: 6:14 - loss: 1.8720 - regression_loss: 1.3944 - classification_loss: 0.4777\n",
      "134/500 [=======>......................] - ETA: 6:14 - loss: 1.8766 - regression_loss: 1.3978 - classification_loss: 0.4788\n",
      "135/500 [=======>......................] - ETA: 6:13 - loss: 1.8760 - regression_loss: 1.3982 - classification_loss: 0.4777\n",
      "136/500 [=======>......................] - ETA: 6:12 - loss: 1.8712 - regression_loss: 1.3947 - classification_loss: 0.4764\n",
      "137/500 [=======>......................] - ETA: 6:11 - loss: 1.8716 - regression_loss: 1.3954 - classification_loss: 0.4763\n",
      "138/500 [=======>......................] - ETA: 6:10 - loss: 1.8708 - regression_loss: 1.3955 - classification_loss: 0.4752\n",
      "139/500 [=======>......................] - ETA: 6:09 - loss: 1.8648 - regression_loss: 1.3908 - classification_loss: 0.4740\n",
      "140/500 [=======>......................] - ETA: 6:08 - loss: 1.8641 - regression_loss: 1.3904 - classification_loss: 0.4737\n",
      "141/500 [=======>......................] - ETA: 6:06 - loss: 1.8638 - regression_loss: 1.3891 - classification_loss: 0.4746\n",
      "142/500 [=======>......................] - ETA: 6:05 - loss: 1.8640 - regression_loss: 1.3896 - classification_loss: 0.4745\n",
      "143/500 [=======>......................] - ETA: 6:04 - loss: 1.8640 - regression_loss: 1.3891 - classification_loss: 0.4749\n",
      "144/500 [=======>......................] - ETA: 6:04 - loss: 1.8671 - regression_loss: 1.3913 - classification_loss: 0.4759\n",
      "145/500 [=======>......................] - ETA: 6:03 - loss: 1.8617 - regression_loss: 1.3879 - classification_loss: 0.4738\n",
      "146/500 [=======>......................] - ETA: 6:02 - loss: 1.8602 - regression_loss: 1.3866 - classification_loss: 0.4736\n",
      "147/500 [=======>......................] - ETA: 6:01 - loss: 1.8598 - regression_loss: 1.3863 - classification_loss: 0.4735\n",
      "148/500 [=======>......................] - ETA: 6:00 - loss: 1.8583 - regression_loss: 1.3848 - classification_loss: 0.4735\n",
      "149/500 [=======>......................] - ETA: 5:59 - loss: 1.8630 - regression_loss: 1.3889 - classification_loss: 0.4741\n",
      "150/500 [========>.....................] - ETA: 5:58 - loss: 1.8609 - regression_loss: 1.3874 - classification_loss: 0.4734\n",
      "151/500 [========>.....................] - ETA: 5:56 - loss: 1.8550 - regression_loss: 1.3825 - classification_loss: 0.4725\n",
      "152/500 [========>.....................] - ETA: 5:56 - loss: 1.8553 - regression_loss: 1.3830 - classification_loss: 0.4723\n",
      "153/500 [========>.....................] - ETA: 5:55 - loss: 1.8573 - regression_loss: 1.3849 - classification_loss: 0.4724\n",
      "154/500 [========>.....................] - ETA: 5:54 - loss: 1.8554 - regression_loss: 1.3842 - classification_loss: 0.4712\n",
      "155/500 [========>.....................] - ETA: 5:53 - loss: 1.8569 - regression_loss: 1.3855 - classification_loss: 0.4714\n",
      "156/500 [========>.....................] - ETA: 5:52 - loss: 1.8617 - regression_loss: 1.3888 - classification_loss: 0.4729\n",
      "157/500 [========>.....................] - ETA: 5:51 - loss: 1.8588 - regression_loss: 1.3867 - classification_loss: 0.4720\n",
      "158/500 [========>.....................] - ETA: 5:50 - loss: 1.8662 - regression_loss: 1.3913 - classification_loss: 0.4749\n",
      "159/500 [========>.....................] - ETA: 5:49 - loss: 1.8679 - regression_loss: 1.3932 - classification_loss: 0.4748\n",
      "160/500 [========>.....................] - ETA: 5:48 - loss: 1.8698 - regression_loss: 1.3938 - classification_loss: 0.4760\n",
      "161/500 [========>.....................] - ETA: 5:47 - loss: 1.8664 - regression_loss: 1.3916 - classification_loss: 0.4749\n",
      "162/500 [========>.....................] - ETA: 5:46 - loss: 1.8647 - regression_loss: 1.3909 - classification_loss: 0.4738\n",
      "163/500 [========>.....................] - ETA: 5:45 - loss: 1.8692 - regression_loss: 1.3936 - classification_loss: 0.4756\n",
      "164/500 [========>.....................] - ETA: 5:44 - loss: 1.8672 - regression_loss: 1.3928 - classification_loss: 0.4744\n",
      "165/500 [========>.....................] - ETA: 5:43 - loss: 1.8647 - regression_loss: 1.3901 - classification_loss: 0.4746\n",
      "166/500 [========>.....................] - ETA: 5:41 - loss: 1.8636 - regression_loss: 1.3891 - classification_loss: 0.4745\n",
      "167/500 [=========>....................] - ETA: 5:41 - loss: 1.8616 - regression_loss: 1.3869 - classification_loss: 0.4748\n",
      "168/500 [=========>....................] - ETA: 5:40 - loss: 1.8609 - regression_loss: 1.3867 - classification_loss: 0.4741\n",
      "169/500 [=========>....................] - ETA: 5:39 - loss: 1.8579 - regression_loss: 1.3847 - classification_loss: 0.4732\n",
      "170/500 [=========>....................] - ETA: 5:38 - loss: 1.8577 - regression_loss: 1.3855 - classification_loss: 0.4722\n",
      "171/500 [=========>....................] - ETA: 5:37 - loss: 1.8583 - regression_loss: 1.3866 - classification_loss: 0.4718\n",
      "172/500 [=========>....................] - ETA: 5:35 - loss: 1.8549 - regression_loss: 1.3843 - classification_loss: 0.4706\n",
      "173/500 [=========>....................] - ETA: 5:35 - loss: 1.8537 - regression_loss: 1.3838 - classification_loss: 0.4699\n",
      "174/500 [=========>....................] - ETA: 5:34 - loss: 1.8521 - regression_loss: 1.3822 - classification_loss: 0.4699\n",
      "175/500 [=========>....................] - ETA: 5:33 - loss: 1.8558 - regression_loss: 1.3849 - classification_loss: 0.4709\n",
      "176/500 [=========>....................] - ETA: 5:32 - loss: 1.8521 - regression_loss: 1.3823 - classification_loss: 0.4698\n",
      "177/500 [=========>....................] - ETA: 5:31 - loss: 1.8495 - regression_loss: 1.3804 - classification_loss: 0.4691\n",
      "178/500 [=========>....................] - ETA: 5:30 - loss: 1.8544 - regression_loss: 1.3838 - classification_loss: 0.4707\n",
      "179/500 [=========>....................] - ETA: 5:29 - loss: 1.8523 - regression_loss: 1.3821 - classification_loss: 0.4701\n",
      "180/500 [=========>....................] - ETA: 5:28 - loss: 1.8565 - regression_loss: 1.3849 - classification_loss: 0.4716\n",
      "181/500 [=========>....................] - ETA: 5:27 - loss: 1.8576 - regression_loss: 1.3859 - classification_loss: 0.4717\n",
      "182/500 [=========>....................] - ETA: 5:26 - loss: 1.8599 - regression_loss: 1.3880 - classification_loss: 0.4719\n",
      "183/500 [=========>....................] - ETA: 5:25 - loss: 1.8590 - regression_loss: 1.3880 - classification_loss: 0.4711\n",
      "184/500 [==========>...................] - ETA: 5:24 - loss: 1.8645 - regression_loss: 1.3926 - classification_loss: 0.4719\n",
      "185/500 [==========>...................] - ETA: 5:23 - loss: 1.8645 - regression_loss: 1.3927 - classification_loss: 0.4718\n",
      "186/500 [==========>...................] - ETA: 5:21 - loss: 1.8614 - regression_loss: 1.3899 - classification_loss: 0.4715\n",
      "187/500 [==========>...................] - ETA: 5:21 - loss: 1.8592 - regression_loss: 1.3878 - classification_loss: 0.4714\n",
      "188/500 [==========>...................] - ETA: 5:20 - loss: 1.8666 - regression_loss: 1.3921 - classification_loss: 0.4745\n",
      "189/500 [==========>...................] - ETA: 5:19 - loss: 1.8647 - regression_loss: 1.3912 - classification_loss: 0.4735\n",
      "190/500 [==========>...................] - ETA: 5:18 - loss: 1.8665 - regression_loss: 1.3934 - classification_loss: 0.4731\n",
      "191/500 [==========>...................] - ETA: 5:17 - loss: 1.8666 - regression_loss: 1.3937 - classification_loss: 0.4729\n",
      "192/500 [==========>...................] - ETA: 5:15 - loss: 1.8667 - regression_loss: 1.3925 - classification_loss: 0.4741\n",
      "193/500 [==========>...................] - ETA: 5:14 - loss: 1.8646 - regression_loss: 1.3915 - classification_loss: 0.4731\n",
      "194/500 [==========>...................] - ETA: 5:13 - loss: 1.8650 - regression_loss: 1.3918 - classification_loss: 0.4732\n",
      "195/500 [==========>...................] - ETA: 5:13 - loss: 1.8638 - regression_loss: 1.3911 - classification_loss: 0.4727\n",
      "196/500 [==========>...................] - ETA: 5:12 - loss: 1.8686 - regression_loss: 1.3953 - classification_loss: 0.4732\n",
      "197/500 [==========>...................] - ETA: 5:10 - loss: 1.8670 - regression_loss: 1.3945 - classification_loss: 0.4725\n",
      "198/500 [==========>...................] - ETA: 5:09 - loss: 1.8701 - regression_loss: 1.3966 - classification_loss: 0.4735\n",
      "199/500 [==========>...................] - ETA: 5:08 - loss: 1.8681 - regression_loss: 1.3948 - classification_loss: 0.4733\n",
      "200/500 [===========>..................] - ETA: 5:07 - loss: 1.8662 - regression_loss: 1.3931 - classification_loss: 0.4731\n",
      "201/500 [===========>..................] - ETA: 5:06 - loss: 1.8650 - regression_loss: 1.3925 - classification_loss: 0.4725\n",
      "202/500 [===========>..................] - ETA: 5:05 - loss: 1.8638 - regression_loss: 1.3919 - classification_loss: 0.4719\n",
      "203/500 [===========>..................] - ETA: 5:04 - loss: 1.8628 - regression_loss: 1.3914 - classification_loss: 0.4714\n",
      "204/500 [===========>..................] - ETA: 5:03 - loss: 1.8608 - regression_loss: 1.3899 - classification_loss: 0.4710\n",
      "205/500 [===========>..................] - ETA: 5:02 - loss: 1.8623 - regression_loss: 1.3914 - classification_loss: 0.4709\n",
      "206/500 [===========>..................] - ETA: 5:01 - loss: 1.8615 - regression_loss: 1.3905 - classification_loss: 0.4710\n",
      "207/500 [===========>..................] - ETA: 5:00 - loss: 1.8633 - regression_loss: 1.3924 - classification_loss: 0.4710\n",
      "208/500 [===========>..................] - ETA: 4:59 - loss: 1.8665 - regression_loss: 1.3946 - classification_loss: 0.4719\n",
      "209/500 [===========>..................] - ETA: 4:58 - loss: 1.8672 - regression_loss: 1.3960 - classification_loss: 0.4711\n",
      "210/500 [===========>..................] - ETA: 4:57 - loss: 1.8674 - regression_loss: 1.3960 - classification_loss: 0.4714\n",
      "211/500 [===========>..................] - ETA: 4:56 - loss: 1.8688 - regression_loss: 1.3969 - classification_loss: 0.4719\n",
      "212/500 [===========>..................] - ETA: 4:55 - loss: 1.8663 - regression_loss: 1.3951 - classification_loss: 0.4712\n",
      "213/500 [===========>..................] - ETA: 4:54 - loss: 1.8645 - regression_loss: 1.3941 - classification_loss: 0.4703\n",
      "214/500 [===========>..................] - ETA: 4:53 - loss: 1.8641 - regression_loss: 1.3941 - classification_loss: 0.4701\n",
      "215/500 [===========>..................] - ETA: 4:52 - loss: 1.8626 - regression_loss: 1.3933 - classification_loss: 0.4694\n",
      "216/500 [===========>..................] - ETA: 4:51 - loss: 1.8645 - regression_loss: 1.3952 - classification_loss: 0.4692\n",
      "217/500 [============>.................] - ETA: 4:49 - loss: 1.8681 - regression_loss: 1.3965 - classification_loss: 0.4716\n",
      "218/500 [============>.................] - ETA: 4:48 - loss: 1.8663 - regression_loss: 1.3957 - classification_loss: 0.4706\n",
      "219/500 [============>.................] - ETA: 4:47 - loss: 1.8674 - regression_loss: 1.3969 - classification_loss: 0.4705\n",
      "220/500 [============>.................] - ETA: 4:46 - loss: 1.8694 - regression_loss: 1.3982 - classification_loss: 0.4712\n",
      "221/500 [============>.................] - ETA: 4:45 - loss: 1.8695 - regression_loss: 1.3989 - classification_loss: 0.4706\n",
      "222/500 [============>.................] - ETA: 4:44 - loss: 1.8711 - regression_loss: 1.3998 - classification_loss: 0.4713\n",
      "223/500 [============>.................] - ETA: 4:43 - loss: 1.8696 - regression_loss: 1.3990 - classification_loss: 0.4706\n",
      "224/500 [============>.................] - ETA: 4:42 - loss: 1.8737 - regression_loss: 1.4003 - classification_loss: 0.4734\n",
      "225/500 [============>.................] - ETA: 4:41 - loss: 1.8706 - regression_loss: 1.3977 - classification_loss: 0.4729\n",
      "226/500 [============>.................] - ETA: 4:40 - loss: 1.8684 - regression_loss: 1.3962 - classification_loss: 0.4722\n",
      "227/500 [============>.................] - ETA: 4:39 - loss: 1.8675 - regression_loss: 1.3955 - classification_loss: 0.4720\n",
      "228/500 [============>.................] - ETA: 4:38 - loss: 1.8674 - regression_loss: 1.3956 - classification_loss: 0.4718\n",
      "229/500 [============>.................] - ETA: 4:37 - loss: 1.8718 - regression_loss: 1.3987 - classification_loss: 0.4731\n",
      "230/500 [============>.................] - ETA: 4:36 - loss: 1.8684 - regression_loss: 1.3965 - classification_loss: 0.4719\n",
      "231/500 [============>.................] - ETA: 4:35 - loss: 1.8685 - regression_loss: 1.3963 - classification_loss: 0.4722\n",
      "232/500 [============>.................] - ETA: 4:34 - loss: 1.8665 - regression_loss: 1.3951 - classification_loss: 0.4714\n",
      "233/500 [============>.................] - ETA: 4:33 - loss: 1.8650 - regression_loss: 1.3941 - classification_loss: 0.4708\n",
      "234/500 [=============>................] - ETA: 4:32 - loss: 1.8645 - regression_loss: 1.3934 - classification_loss: 0.4711\n",
      "235/500 [=============>................] - ETA: 4:30 - loss: 1.8680 - regression_loss: 1.3949 - classification_loss: 0.4731\n",
      "236/500 [=============>................] - ETA: 4:30 - loss: 1.8725 - regression_loss: 1.3977 - classification_loss: 0.4748\n",
      "237/500 [=============>................] - ETA: 4:29 - loss: 1.8743 - regression_loss: 1.3991 - classification_loss: 0.4752\n",
      "238/500 [=============>................] - ETA: 4:27 - loss: 1.8746 - regression_loss: 1.3999 - classification_loss: 0.4747\n",
      "239/500 [=============>................] - ETA: 4:27 - loss: 1.8718 - regression_loss: 1.3977 - classification_loss: 0.4741\n",
      "240/500 [=============>................] - ETA: 4:25 - loss: 1.8699 - regression_loss: 1.3963 - classification_loss: 0.4736\n",
      "241/500 [=============>................] - ETA: 4:24 - loss: 1.8749 - regression_loss: 1.4003 - classification_loss: 0.4747\n",
      "242/500 [=============>................] - ETA: 4:23 - loss: 1.8741 - regression_loss: 1.3994 - classification_loss: 0.4747\n",
      "243/500 [=============>................] - ETA: 4:22 - loss: 1.8726 - regression_loss: 1.3986 - classification_loss: 0.4740\n",
      "244/500 [=============>................] - ETA: 4:21 - loss: 1.8678 - regression_loss: 1.3951 - classification_loss: 0.4727\n",
      "245/500 [=============>................] - ETA: 4:20 - loss: 1.8691 - regression_loss: 1.3965 - classification_loss: 0.4726\n",
      "246/500 [=============>................] - ETA: 4:19 - loss: 1.8673 - regression_loss: 1.3954 - classification_loss: 0.4720\n",
      "247/500 [=============>................] - ETA: 4:18 - loss: 1.8691 - regression_loss: 1.3965 - classification_loss: 0.4726\n",
      "248/500 [=============>................] - ETA: 4:17 - loss: 1.8680 - regression_loss: 1.3955 - classification_loss: 0.4726\n",
      "249/500 [=============>................] - ETA: 4:16 - loss: 1.8659 - regression_loss: 1.3942 - classification_loss: 0.4718\n",
      "250/500 [==============>...............] - ETA: 4:15 - loss: 1.8659 - regression_loss: 1.3948 - classification_loss: 0.4711\n",
      "251/500 [==============>...............] - ETA: 4:14 - loss: 1.8652 - regression_loss: 1.3943 - classification_loss: 0.4709\n",
      "252/500 [==============>...............] - ETA: 4:13 - loss: 1.8638 - regression_loss: 1.3933 - classification_loss: 0.4705\n",
      "253/500 [==============>...............] - ETA: 4:12 - loss: 1.8671 - regression_loss: 1.3961 - classification_loss: 0.4711\n",
      "254/500 [==============>...............] - ETA: 4:11 - loss: 1.8652 - regression_loss: 1.3942 - classification_loss: 0.4711\n",
      "255/500 [==============>...............] - ETA: 4:10 - loss: 1.8630 - regression_loss: 1.3923 - classification_loss: 0.4707\n",
      "256/500 [==============>...............] - ETA: 4:09 - loss: 1.8616 - regression_loss: 1.3909 - classification_loss: 0.4708\n",
      "257/500 [==============>...............] - ETA: 4:08 - loss: 1.8619 - regression_loss: 1.3915 - classification_loss: 0.4705\n",
      "258/500 [==============>...............] - ETA: 4:07 - loss: 1.8639 - regression_loss: 1.3929 - classification_loss: 0.4711\n",
      "259/500 [==============>...............] - ETA: 4:06 - loss: 1.8627 - regression_loss: 1.3918 - classification_loss: 0.4708\n",
      "260/500 [==============>...............] - ETA: 4:05 - loss: 1.8616 - regression_loss: 1.3911 - classification_loss: 0.4705\n",
      "261/500 [==============>...............] - ETA: 4:04 - loss: 1.8636 - regression_loss: 1.3920 - classification_loss: 0.4715\n",
      "262/500 [==============>...............] - ETA: 4:03 - loss: 1.8603 - regression_loss: 1.3895 - classification_loss: 0.4707\n",
      "263/500 [==============>...............] - ETA: 4:02 - loss: 1.8614 - regression_loss: 1.3904 - classification_loss: 0.4710\n",
      "264/500 [==============>...............] - ETA: 4:01 - loss: 1.8622 - regression_loss: 1.3910 - classification_loss: 0.4712\n",
      "265/500 [==============>...............] - ETA: 4:00 - loss: 1.8598 - regression_loss: 1.3891 - classification_loss: 0.4707\n",
      "266/500 [==============>...............] - ETA: 3:58 - loss: 1.8575 - regression_loss: 1.3872 - classification_loss: 0.4702\n",
      "267/500 [===============>..............] - ETA: 3:58 - loss: 1.8590 - regression_loss: 1.3888 - classification_loss: 0.4701\n",
      "268/500 [===============>..............] - ETA: 3:56 - loss: 1.8587 - regression_loss: 1.3885 - classification_loss: 0.4702\n",
      "269/500 [===============>..............] - ETA: 3:55 - loss: 1.8593 - regression_loss: 1.3884 - classification_loss: 0.4709\n",
      "270/500 [===============>..............] - ETA: 3:54 - loss: 1.8623 - regression_loss: 1.3910 - classification_loss: 0.4713\n",
      "271/500 [===============>..............] - ETA: 3:53 - loss: 1.8621 - regression_loss: 1.3911 - classification_loss: 0.4711\n",
      "272/500 [===============>..............] - ETA: 3:52 - loss: 1.8648 - regression_loss: 1.3926 - classification_loss: 0.4723\n",
      "273/500 [===============>..............] - ETA: 3:51 - loss: 1.8653 - regression_loss: 1.3931 - classification_loss: 0.4722\n",
      "274/500 [===============>..............] - ETA: 3:50 - loss: 1.8682 - regression_loss: 1.3944 - classification_loss: 0.4737\n",
      "275/500 [===============>..............] - ETA: 3:49 - loss: 1.8687 - regression_loss: 1.3944 - classification_loss: 0.4742\n",
      "276/500 [===============>..............] - ETA: 3:48 - loss: 1.8698 - regression_loss: 1.3954 - classification_loss: 0.4744\n",
      "277/500 [===============>..............] - ETA: 3:47 - loss: 1.8711 - regression_loss: 1.3966 - classification_loss: 0.4744\n",
      "278/500 [===============>..............] - ETA: 3:46 - loss: 1.8714 - regression_loss: 1.3964 - classification_loss: 0.4751\n",
      "279/500 [===============>..............] - ETA: 3:45 - loss: 1.8724 - regression_loss: 1.3975 - classification_loss: 0.4749\n",
      "280/500 [===============>..............] - ETA: 3:44 - loss: 1.8716 - regression_loss: 1.3969 - classification_loss: 0.4747\n",
      "281/500 [===============>..............] - ETA: 3:43 - loss: 1.8724 - regression_loss: 1.3974 - classification_loss: 0.4750\n",
      "282/500 [===============>..............] - ETA: 3:42 - loss: 1.8713 - regression_loss: 1.3963 - classification_loss: 0.4750\n",
      "283/500 [===============>..............] - ETA: 3:41 - loss: 1.8679 - regression_loss: 1.3937 - classification_loss: 0.4742\n",
      "284/500 [================>.............] - ETA: 3:40 - loss: 1.8716 - regression_loss: 1.3969 - classification_loss: 0.4747\n",
      "285/500 [================>.............] - ETA: 3:39 - loss: 1.8701 - regression_loss: 1.3959 - classification_loss: 0.4742\n",
      "286/500 [================>.............] - ETA: 3:38 - loss: 1.8710 - regression_loss: 1.3970 - classification_loss: 0.4740\n",
      "287/500 [================>.............] - ETA: 3:37 - loss: 1.8722 - regression_loss: 1.3983 - classification_loss: 0.4740\n",
      "288/500 [================>.............] - ETA: 3:36 - loss: 1.8736 - regression_loss: 1.3992 - classification_loss: 0.4744\n",
      "289/500 [================>.............] - ETA: 3:35 - loss: 1.8724 - regression_loss: 1.3986 - classification_loss: 0.4739\n",
      "290/500 [================>.............] - ETA: 3:34 - loss: 1.8713 - regression_loss: 1.3979 - classification_loss: 0.4735\n",
      "291/500 [================>.............] - ETA: 3:33 - loss: 1.8752 - regression_loss: 1.4003 - classification_loss: 0.4748\n",
      "292/500 [================>.............] - ETA: 3:32 - loss: 1.8779 - regression_loss: 1.4028 - classification_loss: 0.4751\n",
      "293/500 [================>.............] - ETA: 3:31 - loss: 1.8786 - regression_loss: 1.4034 - classification_loss: 0.4751\n",
      "294/500 [================>.............] - ETA: 3:30 - loss: 1.8811 - regression_loss: 1.4057 - classification_loss: 0.4754\n",
      "295/500 [================>.............] - ETA: 3:29 - loss: 1.8823 - regression_loss: 1.4065 - classification_loss: 0.4758\n",
      "296/500 [================>.............] - ETA: 3:28 - loss: 1.8830 - regression_loss: 1.4063 - classification_loss: 0.4767\n",
      "297/500 [================>.............] - ETA: 3:27 - loss: 1.8817 - regression_loss: 1.4053 - classification_loss: 0.4764\n",
      "298/500 [================>.............] - ETA: 3:26 - loss: 1.8791 - regression_loss: 1.4036 - classification_loss: 0.4755\n",
      "299/500 [================>.............] - ETA: 3:25 - loss: 1.8791 - regression_loss: 1.4035 - classification_loss: 0.4756\n",
      "300/500 [=================>............] - ETA: 3:24 - loss: 1.8802 - regression_loss: 1.4043 - classification_loss: 0.4759\n",
      "301/500 [=================>............] - ETA: 3:23 - loss: 1.8785 - regression_loss: 1.4029 - classification_loss: 0.4756\n",
      "302/500 [=================>............] - ETA: 3:22 - loss: 1.8766 - regression_loss: 1.4017 - classification_loss: 0.4749\n",
      "303/500 [=================>............] - ETA: 3:21 - loss: 1.8767 - regression_loss: 1.4015 - classification_loss: 0.4752\n",
      "304/500 [=================>............] - ETA: 3:20 - loss: 1.8807 - regression_loss: 1.4039 - classification_loss: 0.4768\n",
      "305/500 [=================>............] - ETA: 3:19 - loss: 1.8852 - regression_loss: 1.4066 - classification_loss: 0.4786\n",
      "306/500 [=================>............] - ETA: 3:18 - loss: 1.8867 - regression_loss: 1.4082 - classification_loss: 0.4785\n",
      "307/500 [=================>............] - ETA: 3:17 - loss: 1.8864 - regression_loss: 1.4082 - classification_loss: 0.4782\n",
      "308/500 [=================>............] - ETA: 3:16 - loss: 1.8850 - regression_loss: 1.4071 - classification_loss: 0.4778\n",
      "309/500 [=================>............] - ETA: 3:15 - loss: 1.8849 - regression_loss: 1.4069 - classification_loss: 0.4780\n",
      "310/500 [=================>............] - ETA: 3:14 - loss: 1.8865 - regression_loss: 1.4085 - classification_loss: 0.4781\n",
      "311/500 [=================>............] - ETA: 3:13 - loss: 1.8855 - regression_loss: 1.4079 - classification_loss: 0.4775\n",
      "312/500 [=================>............] - ETA: 3:12 - loss: 1.8835 - regression_loss: 1.4068 - classification_loss: 0.4767\n",
      "313/500 [=================>............] - ETA: 3:11 - loss: 1.8823 - regression_loss: 1.4055 - classification_loss: 0.4769\n",
      "314/500 [=================>............] - ETA: 3:10 - loss: 1.8806 - regression_loss: 1.4041 - classification_loss: 0.4764\n",
      "315/500 [=================>............] - ETA: 3:09 - loss: 1.8789 - regression_loss: 1.4029 - classification_loss: 0.4760\n",
      "316/500 [=================>............] - ETA: 3:08 - loss: 1.8791 - regression_loss: 1.4029 - classification_loss: 0.4762\n",
      "317/500 [==================>...........] - ETA: 3:07 - loss: 1.8792 - regression_loss: 1.4028 - classification_loss: 0.4764\n",
      "318/500 [==================>...........] - ETA: 3:06 - loss: 1.8810 - regression_loss: 1.4033 - classification_loss: 0.4777\n",
      "319/500 [==================>...........] - ETA: 3:05 - loss: 1.8795 - regression_loss: 1.4020 - classification_loss: 0.4776\n",
      "320/500 [==================>...........] - ETA: 3:04 - loss: 1.8775 - regression_loss: 1.4006 - classification_loss: 0.4768\n",
      "321/500 [==================>...........] - ETA: 3:03 - loss: 1.8764 - regression_loss: 1.3995 - classification_loss: 0.4769\n",
      "322/500 [==================>...........] - ETA: 3:02 - loss: 1.8772 - regression_loss: 1.3994 - classification_loss: 0.4778\n",
      "323/500 [==================>...........] - ETA: 3:00 - loss: 1.8783 - regression_loss: 1.4002 - classification_loss: 0.4780\n",
      "324/500 [==================>...........] - ETA: 2:59 - loss: 1.8792 - regression_loss: 1.4011 - classification_loss: 0.4781\n",
      "325/500 [==================>...........] - ETA: 2:58 - loss: 1.8815 - regression_loss: 1.4026 - classification_loss: 0.4789\n",
      "326/500 [==================>...........] - ETA: 2:57 - loss: 1.8779 - regression_loss: 1.3998 - classification_loss: 0.4781\n",
      "327/500 [==================>...........] - ETA: 2:56 - loss: 1.8794 - regression_loss: 1.4005 - classification_loss: 0.4789\n",
      "328/500 [==================>...........] - ETA: 2:55 - loss: 1.8771 - regression_loss: 1.3988 - classification_loss: 0.4783\n",
      "329/500 [==================>...........] - ETA: 2:54 - loss: 1.8816 - regression_loss: 1.4014 - classification_loss: 0.4802\n",
      "330/500 [==================>...........] - ETA: 2:53 - loss: 1.8815 - regression_loss: 1.4014 - classification_loss: 0.4801\n",
      "331/500 [==================>...........] - ETA: 2:52 - loss: 1.8826 - regression_loss: 1.4023 - classification_loss: 0.4803\n",
      "332/500 [==================>...........] - ETA: 2:51 - loss: 1.8855 - regression_loss: 1.4041 - classification_loss: 0.4814\n",
      "333/500 [==================>...........] - ETA: 2:50 - loss: 1.8845 - regression_loss: 1.4036 - classification_loss: 0.4810\n",
      "334/500 [===================>..........] - ETA: 2:49 - loss: 1.8874 - regression_loss: 1.4054 - classification_loss: 0.4820\n",
      "335/500 [===================>..........] - ETA: 2:48 - loss: 1.8875 - regression_loss: 1.4055 - classification_loss: 0.4820\n",
      "336/500 [===================>..........] - ETA: 2:47 - loss: 1.8913 - regression_loss: 1.4078 - classification_loss: 0.4835\n",
      "337/500 [===================>..........] - ETA: 2:46 - loss: 1.8917 - regression_loss: 1.4080 - classification_loss: 0.4837\n",
      "338/500 [===================>..........] - ETA: 2:45 - loss: 1.8913 - regression_loss: 1.4079 - classification_loss: 0.4834\n",
      "339/500 [===================>..........] - ETA: 2:44 - loss: 1.8907 - regression_loss: 1.4075 - classification_loss: 0.4832\n",
      "340/500 [===================>..........] - ETA: 2:43 - loss: 1.8910 - regression_loss: 1.4076 - classification_loss: 0.4834\n",
      "341/500 [===================>..........] - ETA: 2:42 - loss: 1.8898 - regression_loss: 1.4064 - classification_loss: 0.4834\n",
      "342/500 [===================>..........] - ETA: 2:41 - loss: 1.8894 - regression_loss: 1.4061 - classification_loss: 0.4832\n",
      "343/500 [===================>..........] - ETA: 2:40 - loss: 1.8872 - regression_loss: 1.4044 - classification_loss: 0.4828\n",
      "344/500 [===================>..........] - ETA: 2:39 - loss: 1.8870 - regression_loss: 1.4042 - classification_loss: 0.4828\n",
      "345/500 [===================>..........] - ETA: 2:38 - loss: 1.8886 - regression_loss: 1.4044 - classification_loss: 0.4842\n",
      "346/500 [===================>..........] - ETA: 2:37 - loss: 1.8907 - regression_loss: 1.4058 - classification_loss: 0.4849\n",
      "347/500 [===================>..........] - ETA: 2:36 - loss: 1.8911 - regression_loss: 1.4062 - classification_loss: 0.4849\n",
      "348/500 [===================>..........] - ETA: 2:35 - loss: 1.8923 - regression_loss: 1.4073 - classification_loss: 0.4850\n",
      "349/500 [===================>..........] - ETA: 2:34 - loss: 1.8901 - regression_loss: 1.4058 - classification_loss: 0.4843\n",
      "350/500 [====================>.........] - ETA: 2:33 - loss: 1.8891 - regression_loss: 1.4049 - classification_loss: 0.4842\n",
      "351/500 [====================>.........] - ETA: 2:32 - loss: 1.8870 - regression_loss: 1.4033 - classification_loss: 0.4836\n",
      "352/500 [====================>.........] - ETA: 2:31 - loss: 1.8881 - regression_loss: 1.4041 - classification_loss: 0.4840\n",
      "353/500 [====================>.........] - ETA: 2:30 - loss: 1.8882 - regression_loss: 1.4045 - classification_loss: 0.4837\n",
      "354/500 [====================>.........] - ETA: 2:29 - loss: 1.8891 - regression_loss: 1.4058 - classification_loss: 0.4833\n",
      "355/500 [====================>.........] - ETA: 2:28 - loss: 1.8892 - regression_loss: 1.4061 - classification_loss: 0.4830\n",
      "356/500 [====================>.........] - ETA: 2:27 - loss: 1.8868 - regression_loss: 1.4046 - classification_loss: 0.4821\n",
      "357/500 [====================>.........] - ETA: 2:26 - loss: 1.8861 - regression_loss: 1.4044 - classification_loss: 0.4818\n",
      "358/500 [====================>.........] - ETA: 2:25 - loss: 1.8857 - regression_loss: 1.4039 - classification_loss: 0.4818\n",
      "359/500 [====================>.........] - ETA: 2:24 - loss: 1.8868 - regression_loss: 1.4046 - classification_loss: 0.4822\n",
      "360/500 [====================>.........] - ETA: 2:23 - loss: 1.8862 - regression_loss: 1.4042 - classification_loss: 0.4820\n",
      "361/500 [====================>.........] - ETA: 2:22 - loss: 1.8847 - regression_loss: 1.4029 - classification_loss: 0.4818\n",
      "362/500 [====================>.........] - ETA: 2:21 - loss: 1.8863 - regression_loss: 1.4044 - classification_loss: 0.4819\n",
      "363/500 [====================>.........] - ETA: 2:20 - loss: 1.8865 - regression_loss: 1.4041 - classification_loss: 0.4824\n",
      "364/500 [====================>.........] - ETA: 2:19 - loss: 1.8862 - regression_loss: 1.4038 - classification_loss: 0.4824\n",
      "365/500 [====================>.........] - ETA: 2:18 - loss: 1.8864 - regression_loss: 1.4038 - classification_loss: 0.4826\n",
      "366/500 [====================>.........] - ETA: 2:17 - loss: 1.8856 - regression_loss: 1.4034 - classification_loss: 0.4823\n",
      "367/500 [=====================>........] - ETA: 2:16 - loss: 1.8842 - regression_loss: 1.4025 - classification_loss: 0.4817\n",
      "368/500 [=====================>........] - ETA: 2:15 - loss: 1.8854 - regression_loss: 1.4031 - classification_loss: 0.4823\n",
      "369/500 [=====================>........] - ETA: 2:14 - loss: 1.8877 - regression_loss: 1.4044 - classification_loss: 0.4833\n",
      "370/500 [=====================>........] - ETA: 2:12 - loss: 1.8862 - regression_loss: 1.4038 - classification_loss: 0.4824\n",
      "371/500 [=====================>........] - ETA: 2:11 - loss: 1.8851 - regression_loss: 1.4028 - classification_loss: 0.4822\n",
      "372/500 [=====================>........] - ETA: 2:10 - loss: 1.8854 - regression_loss: 1.4028 - classification_loss: 0.4825\n",
      "373/500 [=====================>........] - ETA: 2:09 - loss: 1.8851 - regression_loss: 1.4026 - classification_loss: 0.4825\n",
      "374/500 [=====================>........] - ETA: 2:08 - loss: 1.8869 - regression_loss: 1.4034 - classification_loss: 0.4835\n",
      "375/500 [=====================>........] - ETA: 2:07 - loss: 1.8895 - regression_loss: 1.4055 - classification_loss: 0.4840\n",
      "376/500 [=====================>........] - ETA: 2:06 - loss: 1.8886 - regression_loss: 1.4046 - classification_loss: 0.4840\n",
      "377/500 [=====================>........] - ETA: 2:05 - loss: 1.8886 - regression_loss: 1.4046 - classification_loss: 0.4840\n",
      "378/500 [=====================>........] - ETA: 2:04 - loss: 1.8898 - regression_loss: 1.4057 - classification_loss: 0.4841\n",
      "379/500 [=====================>........] - ETA: 2:03 - loss: 1.8907 - regression_loss: 1.4065 - classification_loss: 0.4843\n",
      "380/500 [=====================>........] - ETA: 2:02 - loss: 1.8906 - regression_loss: 1.4064 - classification_loss: 0.4842\n",
      "381/500 [=====================>........] - ETA: 2:01 - loss: 1.8900 - regression_loss: 1.4062 - classification_loss: 0.4838\n",
      "382/500 [=====================>........] - ETA: 2:00 - loss: 1.8907 - regression_loss: 1.4069 - classification_loss: 0.4837\n",
      "383/500 [=====================>........] - ETA: 1:59 - loss: 1.8920 - regression_loss: 1.4080 - classification_loss: 0.4840\n",
      "384/500 [======================>.......] - ETA: 1:58 - loss: 1.8922 - regression_loss: 1.4076 - classification_loss: 0.4846\n",
      "385/500 [======================>.......] - ETA: 1:57 - loss: 1.8940 - regression_loss: 1.4088 - classification_loss: 0.4852\n",
      "386/500 [======================>.......] - ETA: 1:56 - loss: 1.8923 - regression_loss: 1.4075 - classification_loss: 0.4848\n",
      "387/500 [======================>.......] - ETA: 1:55 - loss: 1.8938 - regression_loss: 1.4087 - classification_loss: 0.4851\n",
      "388/500 [======================>.......] - ETA: 1:54 - loss: 1.8937 - regression_loss: 1.4089 - classification_loss: 0.4849\n",
      "389/500 [======================>.......] - ETA: 1:53 - loss: 1.8937 - regression_loss: 1.4085 - classification_loss: 0.4852\n",
      "390/500 [======================>.......] - ETA: 1:52 - loss: 1.8969 - regression_loss: 1.4110 - classification_loss: 0.4859\n",
      "391/500 [======================>.......] - ETA: 1:51 - loss: 1.8992 - regression_loss: 1.4126 - classification_loss: 0.4867\n",
      "392/500 [======================>.......] - ETA: 1:50 - loss: 1.8977 - regression_loss: 1.4115 - classification_loss: 0.4862\n",
      "393/500 [======================>.......] - ETA: 1:49 - loss: 1.8966 - regression_loss: 1.4106 - classification_loss: 0.4860\n",
      "394/500 [======================>.......] - ETA: 1:48 - loss: 1.8943 - regression_loss: 1.4090 - classification_loss: 0.4853\n",
      "395/500 [======================>.......] - ETA: 1:47 - loss: 1.8931 - regression_loss: 1.4081 - classification_loss: 0.4850\n",
      "396/500 [======================>.......] - ETA: 1:46 - loss: 1.8920 - regression_loss: 1.4076 - classification_loss: 0.4844\n",
      "397/500 [======================>.......] - ETA: 1:45 - loss: 1.8918 - regression_loss: 1.4073 - classification_loss: 0.4844\n",
      "398/500 [======================>.......] - ETA: 1:44 - loss: 1.8927 - regression_loss: 1.4082 - classification_loss: 0.4845\n",
      "399/500 [======================>.......] - ETA: 1:43 - loss: 1.8921 - regression_loss: 1.4080 - classification_loss: 0.4840\n",
      "400/500 [=======================>......] - ETA: 1:42 - loss: 1.8930 - regression_loss: 1.4090 - classification_loss: 0.4840\n",
      "401/500 [=======================>......] - ETA: 1:41 - loss: 1.8934 - regression_loss: 1.4091 - classification_loss: 0.4843\n",
      "402/500 [=======================>......] - ETA: 1:40 - loss: 1.8941 - regression_loss: 1.4097 - classification_loss: 0.4844\n",
      "403/500 [=======================>......] - ETA: 1:39 - loss: 1.8957 - regression_loss: 1.4105 - classification_loss: 0.4852\n",
      "404/500 [=======================>......] - ETA: 1:38 - loss: 1.8947 - regression_loss: 1.4099 - classification_loss: 0.4848\n",
      "405/500 [=======================>......] - ETA: 1:37 - loss: 1.8934 - regression_loss: 1.4091 - classification_loss: 0.4843\n",
      "406/500 [=======================>......] - ETA: 1:36 - loss: 1.8958 - regression_loss: 1.4107 - classification_loss: 0.4852\n",
      "407/500 [=======================>......] - ETA: 1:35 - loss: 1.8953 - regression_loss: 1.4104 - classification_loss: 0.4849\n",
      "408/500 [=======================>......] - ETA: 1:34 - loss: 1.8941 - regression_loss: 1.4096 - classification_loss: 0.4846\n",
      "409/500 [=======================>......] - ETA: 1:33 - loss: 1.8937 - regression_loss: 1.4091 - classification_loss: 0.4846\n",
      "410/500 [=======================>......] - ETA: 1:32 - loss: 1.8927 - regression_loss: 1.4085 - classification_loss: 0.4843\n",
      "411/500 [=======================>......] - ETA: 1:31 - loss: 1.8927 - regression_loss: 1.4085 - classification_loss: 0.4842\n",
      "412/500 [=======================>......] - ETA: 1:30 - loss: 1.8944 - regression_loss: 1.4097 - classification_loss: 0.4847\n",
      "413/500 [=======================>......] - ETA: 1:29 - loss: 1.8939 - regression_loss: 1.4093 - classification_loss: 0.4846\n",
      "414/500 [=======================>......] - ETA: 1:28 - loss: 1.8943 - regression_loss: 1.4097 - classification_loss: 0.4846\n",
      "415/500 [=======================>......] - ETA: 1:27 - loss: 1.8957 - regression_loss: 1.4109 - classification_loss: 0.4848\n",
      "416/500 [=======================>......] - ETA: 1:26 - loss: 1.8969 - regression_loss: 1.4120 - classification_loss: 0.4848\n",
      "417/500 [========================>.....] - ETA: 1:25 - loss: 1.8962 - regression_loss: 1.4116 - classification_loss: 0.4846\n",
      "418/500 [========================>.....] - ETA: 1:23 - loss: 1.8943 - regression_loss: 1.4100 - classification_loss: 0.4844\n",
      "419/500 [========================>.....] - ETA: 1:22 - loss: 1.8961 - regression_loss: 1.4116 - classification_loss: 0.4845\n",
      "420/500 [========================>.....] - ETA: 1:21 - loss: 1.8990 - regression_loss: 1.4136 - classification_loss: 0.4853\n",
      "421/500 [========================>.....] - ETA: 1:20 - loss: 1.8974 - regression_loss: 1.4125 - classification_loss: 0.4848\n",
      "422/500 [========================>.....] - ETA: 1:19 - loss: 1.8976 - regression_loss: 1.4129 - classification_loss: 0.4848\n",
      "423/500 [========================>.....] - ETA: 1:18 - loss: 1.8984 - regression_loss: 1.4137 - classification_loss: 0.4847\n",
      "424/500 [========================>.....] - ETA: 1:17 - loss: 1.8980 - regression_loss: 1.4136 - classification_loss: 0.4844\n",
      "425/500 [========================>.....] - ETA: 1:16 - loss: 1.8997 - regression_loss: 1.4149 - classification_loss: 0.4847\n",
      "426/500 [========================>.....] - ETA: 1:15 - loss: 1.8999 - regression_loss: 1.4150 - classification_loss: 0.4849\n",
      "427/500 [========================>.....] - ETA: 1:14 - loss: 1.8995 - regression_loss: 1.4147 - classification_loss: 0.4848\n",
      "428/500 [========================>.....] - ETA: 1:13 - loss: 1.9003 - regression_loss: 1.4154 - classification_loss: 0.4849\n",
      "429/500 [========================>.....] - ETA: 1:12 - loss: 1.9022 - regression_loss: 1.4168 - classification_loss: 0.4854\n",
      "430/500 [========================>.....] - ETA: 1:11 - loss: 1.9037 - regression_loss: 1.4177 - classification_loss: 0.4860\n",
      "431/500 [========================>.....] - ETA: 1:10 - loss: 1.9033 - regression_loss: 1.4176 - classification_loss: 0.4857\n",
      "432/500 [========================>.....] - ETA: 1:09 - loss: 1.9072 - regression_loss: 1.4200 - classification_loss: 0.4873\n",
      "433/500 [========================>.....] - ETA: 1:08 - loss: 1.9085 - regression_loss: 1.4210 - classification_loss: 0.4875\n",
      "434/500 [=========================>....] - ETA: 1:07 - loss: 1.9116 - regression_loss: 1.4229 - classification_loss: 0.4887\n",
      "435/500 [=========================>....] - ETA: 1:06 - loss: 1.9097 - regression_loss: 1.4216 - classification_loss: 0.4881\n",
      "436/500 [=========================>....] - ETA: 1:05 - loss: 1.9097 - regression_loss: 1.4213 - classification_loss: 0.4884\n",
      "437/500 [=========================>....] - ETA: 1:04 - loss: 1.9101 - regression_loss: 1.4215 - classification_loss: 0.4887\n",
      "438/500 [=========================>....] - ETA: 1:03 - loss: 1.9100 - regression_loss: 1.4216 - classification_loss: 0.4884\n",
      "439/500 [=========================>....] - ETA: 1:02 - loss: 1.9131 - regression_loss: 1.4235 - classification_loss: 0.4896\n",
      "440/500 [=========================>....] - ETA: 1:01 - loss: 1.9125 - regression_loss: 1.4230 - classification_loss: 0.4895\n",
      "441/500 [=========================>....] - ETA: 1:00 - loss: 1.9118 - regression_loss: 1.4225 - classification_loss: 0.4893\n",
      "442/500 [=========================>....] - ETA: 59s - loss: 1.9123 - regression_loss: 1.4229 - classification_loss: 0.4894 \n",
      "443/500 [=========================>....] - ETA: 58s - loss: 1.9116 - regression_loss: 1.4225 - classification_loss: 0.4891\n",
      "444/500 [=========================>....] - ETA: 57s - loss: 1.9110 - regression_loss: 1.4222 - classification_loss: 0.4887\n",
      "445/500 [=========================>....] - ETA: 56s - loss: 1.9139 - regression_loss: 1.4240 - classification_loss: 0.4898\n",
      "446/500 [=========================>....] - ETA: 55s - loss: 1.9137 - regression_loss: 1.4239 - classification_loss: 0.4898\n",
      "447/500 [=========================>....] - ETA: 54s - loss: 1.9132 - regression_loss: 1.4237 - classification_loss: 0.4894\n",
      "448/500 [=========================>....] - ETA: 53s - loss: 1.9124 - regression_loss: 1.4230 - classification_loss: 0.4895\n",
      "449/500 [=========================>....] - ETA: 52s - loss: 1.9134 - regression_loss: 1.4236 - classification_loss: 0.4898\n",
      "450/500 [==========================>...] - ETA: 51s - loss: 1.9132 - regression_loss: 1.4235 - classification_loss: 0.4898\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 1.9151 - regression_loss: 1.4247 - classification_loss: 0.4904\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 1.9150 - regression_loss: 1.4248 - classification_loss: 0.4902\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.9137 - regression_loss: 1.4237 - classification_loss: 0.4900\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.9124 - regression_loss: 1.4228 - classification_loss: 0.4896\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.9122 - regression_loss: 1.4226 - classification_loss: 0.4897\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.9123 - regression_loss: 1.4225 - classification_loss: 0.4898\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.9149 - regression_loss: 1.4240 - classification_loss: 0.4909\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.9172 - regression_loss: 1.4255 - classification_loss: 0.4916\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.9178 - regression_loss: 1.4261 - classification_loss: 0.4916\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.9190 - regression_loss: 1.4269 - classification_loss: 0.4921\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 1.9184 - regression_loss: 1.4264 - classification_loss: 0.4920\n",
      "462/500 [==========================>...] - ETA: 38s - loss: 1.9184 - regression_loss: 1.4259 - classification_loss: 0.4925\n",
      "463/500 [==========================>...] - ETA: 37s - loss: 1.9204 - regression_loss: 1.4279 - classification_loss: 0.4925\n",
      "464/500 [==========================>...] - ETA: 36s - loss: 1.9199 - regression_loss: 1.4276 - classification_loss: 0.4923\n",
      "465/500 [==========================>...] - ETA: 35s - loss: 1.9198 - regression_loss: 1.4277 - classification_loss: 0.4921\n",
      "466/500 [==========================>...] - ETA: 34s - loss: 1.9194 - regression_loss: 1.4273 - classification_loss: 0.4921\n",
      "467/500 [===========================>..] - ETA: 33s - loss: 1.9212 - regression_loss: 1.4281 - classification_loss: 0.4931\n",
      "468/500 [===========================>..] - ETA: 32s - loss: 1.9210 - regression_loss: 1.4279 - classification_loss: 0.4931\n",
      "469/500 [===========================>..] - ETA: 31s - loss: 1.9212 - regression_loss: 1.4283 - classification_loss: 0.4928\n",
      "470/500 [===========================>..] - ETA: 30s - loss: 1.9216 - regression_loss: 1.4288 - classification_loss: 0.4928\n",
      "471/500 [===========================>..] - ETA: 29s - loss: 1.9204 - regression_loss: 1.4278 - classification_loss: 0.4926\n",
      "472/500 [===========================>..] - ETA: 28s - loss: 1.9205 - regression_loss: 1.4279 - classification_loss: 0.4926\n",
      "473/500 [===========================>..] - ETA: 27s - loss: 1.9215 - regression_loss: 1.4284 - classification_loss: 0.4931\n",
      "474/500 [===========================>..] - ETA: 26s - loss: 1.9234 - regression_loss: 1.4301 - classification_loss: 0.4933\n",
      "475/500 [===========================>..] - ETA: 25s - loss: 1.9235 - regression_loss: 1.4299 - classification_loss: 0.4936\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 1.9248 - regression_loss: 1.4310 - classification_loss: 0.4938\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.9247 - regression_loss: 1.4310 - classification_loss: 0.4937\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.9250 - regression_loss: 1.4312 - classification_loss: 0.4938\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.9241 - regression_loss: 1.4308 - classification_loss: 0.4933\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.9245 - regression_loss: 1.4309 - classification_loss: 0.4936\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.9264 - regression_loss: 1.4324 - classification_loss: 0.4940\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.9258 - regression_loss: 1.4319 - classification_loss: 0.4940\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.9267 - regression_loss: 1.4324 - classification_loss: 0.4943\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.9267 - regression_loss: 1.4323 - classification_loss: 0.4944\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.9277 - regression_loss: 1.4325 - classification_loss: 0.4952\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.9273 - regression_loss: 1.4324 - classification_loss: 0.4949\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.9270 - regression_loss: 1.4325 - classification_loss: 0.4946\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.9273 - regression_loss: 1.4325 - classification_loss: 0.4948\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.9282 - regression_loss: 1.4331 - classification_loss: 0.4951\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.9271 - regression_loss: 1.4325 - classification_loss: 0.4946\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.9267 - regression_loss: 1.4321 - classification_loss: 0.4946 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.9265 - regression_loss: 1.4321 - classification_loss: 0.4944\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.9259 - regression_loss: 1.4317 - classification_loss: 0.4941\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.9250 - regression_loss: 1.4311 - classification_loss: 0.4939\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.9238 - regression_loss: 1.4300 - classification_loss: 0.4938\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.9237 - regression_loss: 1.4299 - classification_loss: 0.4937\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.9244 - regression_loss: 1.4304 - classification_loss: 0.4940\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.9233 - regression_loss: 1.4296 - classification_loss: 0.4938\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.9243 - regression_loss: 1.4304 - classification_loss: 0.4939\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9247 - regression_loss: 1.4302 - classification_loss: 0.4945\n",
      "Epoch 00025: saving model to ./snapshots\\resnet50_csv_25.h5\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.999999939225292e-10.\n",
      "\n",
      "500/500 [==============================] - 514s 1s/step - loss: 1.9247 - regression_loss: 1.4302 - classification_loss: 0.4945\n",
      "Epoch 26/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 2.2744 - regression_loss: 1.5726 - classification_loss: 0.7018\n",
      "  2/500 [..............................] - ETA: 4:35 - loss: 1.9329 - regression_loss: 1.4135 - classification_loss: 0.5193\n",
      "  3/500 [..............................] - ETA: 5:46 - loss: 1.9590 - regression_loss: 1.4875 - classification_loss: 0.4715\n",
      "  4/500 [..............................] - ETA: 6:36 - loss: 2.1399 - regression_loss: 1.6170 - classification_loss: 0.5229\n",
      "  5/500 [..............................] - ETA: 7:03 - loss: 2.0899 - regression_loss: 1.5620 - classification_loss: 0.5280\n",
      "  6/500 [..............................] - ETA: 7:14 - loss: 2.0815 - regression_loss: 1.5646 - classification_loss: 0.5168\n",
      "  7/500 [..............................] - ETA: 7:08 - loss: 2.0255 - regression_loss: 1.5402 - classification_loss: 0.4853\n",
      "  8/500 [..............................] - ETA: 7:31 - loss: 1.9969 - regression_loss: 1.5249 - classification_loss: 0.4720\n",
      "  9/500 [..............................] - ETA: 7:34 - loss: 2.0035 - regression_loss: 1.5191 - classification_loss: 0.4844\n",
      " 10/500 [..............................] - ETA: 7:40 - loss: 2.0822 - regression_loss: 1.5801 - classification_loss: 0.5021\n",
      " 11/500 [..............................] - ETA: 7:41 - loss: 2.0373 - regression_loss: 1.5415 - classification_loss: 0.4958\n",
      " 12/500 [..............................] - ETA: 7:51 - loss: 2.0332 - regression_loss: 1.5299 - classification_loss: 0.5032\n",
      " 13/500 [..............................] - ETA: 7:55 - loss: 2.0215 - regression_loss: 1.5187 - classification_loss: 0.5028\n",
      " 14/500 [..............................] - ETA: 7:57 - loss: 2.0740 - regression_loss: 1.5579 - classification_loss: 0.5161\n",
      " 15/500 [..............................] - ETA: 7:57 - loss: 2.0255 - regression_loss: 1.5172 - classification_loss: 0.5083\n",
      " 16/500 [..............................] - ETA: 7:57 - loss: 2.0332 - regression_loss: 1.5079 - classification_loss: 0.5253\n",
      " 17/500 [>.............................] - ETA: 7:56 - loss: 2.0070 - regression_loss: 1.4858 - classification_loss: 0.5212\n",
      " 18/500 [>.............................] - ETA: 7:57 - loss: 2.0243 - regression_loss: 1.4907 - classification_loss: 0.5336\n",
      " 19/500 [>.............................] - ETA: 7:58 - loss: 2.0572 - regression_loss: 1.5150 - classification_loss: 0.5422\n",
      " 20/500 [>.............................] - ETA: 8:00 - loss: 2.0412 - regression_loss: 1.5002 - classification_loss: 0.5410\n",
      " 21/500 [>.............................] - ETA: 7:58 - loss: 2.0086 - regression_loss: 1.4752 - classification_loss: 0.5334\n",
      " 22/500 [>.............................] - ETA: 7:57 - loss: 1.9904 - regression_loss: 1.4651 - classification_loss: 0.5253\n",
      " 23/500 [>.............................] - ETA: 7:56 - loss: 2.0051 - regression_loss: 1.4766 - classification_loss: 0.5286\n",
      " 24/500 [>.............................] - ETA: 7:55 - loss: 2.0026 - regression_loss: 1.4690 - classification_loss: 0.5336\n",
      " 25/500 [>.............................] - ETA: 7:53 - loss: 2.0365 - regression_loss: 1.4874 - classification_loss: 0.5491\n",
      " 26/500 [>.............................] - ETA: 7:55 - loss: 2.0481 - regression_loss: 1.4992 - classification_loss: 0.5490\n",
      " 27/500 [>.............................] - ETA: 7:53 - loss: 2.0350 - regression_loss: 1.4830 - classification_loss: 0.5520\n",
      " 28/500 [>.............................] - ETA: 7:50 - loss: 2.0467 - regression_loss: 1.4906 - classification_loss: 0.5561\n",
      " 29/500 [>.............................] - ETA: 7:52 - loss: 2.0849 - regression_loss: 1.5157 - classification_loss: 0.5693\n",
      " 30/500 [>.............................] - ETA: 7:52 - loss: 2.0584 - regression_loss: 1.4980 - classification_loss: 0.5604\n",
      " 31/500 [>.............................] - ETA: 7:53 - loss: 2.0514 - regression_loss: 1.4956 - classification_loss: 0.5557\n",
      " 32/500 [>.............................] - ETA: 7:54 - loss: 2.0396 - regression_loss: 1.4860 - classification_loss: 0.5536\n",
      " 33/500 [>.............................] - ETA: 7:53 - loss: 2.0348 - regression_loss: 1.4883 - classification_loss: 0.5465\n",
      " 34/500 [=>............................] - ETA: 7:52 - loss: 2.0090 - regression_loss: 1.4698 - classification_loss: 0.5392\n",
      " 35/500 [=>............................] - ETA: 7:50 - loss: 2.0101 - regression_loss: 1.4711 - classification_loss: 0.5390\n",
      " 36/500 [=>............................] - ETA: 7:46 - loss: 2.0249 - regression_loss: 1.4819 - classification_loss: 0.5430\n",
      " 37/500 [=>............................] - ETA: 7:47 - loss: 2.0358 - regression_loss: 1.4915 - classification_loss: 0.5443\n",
      " 38/500 [=>............................] - ETA: 7:47 - loss: 2.0128 - regression_loss: 1.4746 - classification_loss: 0.5382\n",
      " 39/500 [=>............................] - ETA: 7:47 - loss: 2.0062 - regression_loss: 1.4714 - classification_loss: 0.5348\n",
      " 40/500 [=>............................] - ETA: 7:47 - loss: 2.0118 - regression_loss: 1.4755 - classification_loss: 0.5362\n",
      " 41/500 [=>............................] - ETA: 7:46 - loss: 1.9991 - regression_loss: 1.4677 - classification_loss: 0.5314\n",
      " 42/500 [=>............................] - ETA: 7:45 - loss: 2.0276 - regression_loss: 1.4905 - classification_loss: 0.5370\n",
      " 43/500 [=>............................] - ETA: 7:45 - loss: 2.0348 - regression_loss: 1.4994 - classification_loss: 0.5354\n",
      " 44/500 [=>............................] - ETA: 7:45 - loss: 2.0193 - regression_loss: 1.4884 - classification_loss: 0.5309\n",
      " 45/500 [=>............................] - ETA: 7:43 - loss: 2.0105 - regression_loss: 1.4809 - classification_loss: 0.5296\n",
      " 46/500 [=>............................] - ETA: 7:43 - loss: 2.0254 - regression_loss: 1.4959 - classification_loss: 0.5295\n",
      " 47/500 [=>............................] - ETA: 7:43 - loss: 2.0221 - regression_loss: 1.4950 - classification_loss: 0.5270\n",
      " 48/500 [=>............................] - ETA: 7:39 - loss: 2.0243 - regression_loss: 1.4948 - classification_loss: 0.5295\n",
      " 49/500 [=>............................] - ETA: 7:39 - loss: 2.0086 - regression_loss: 1.4821 - classification_loss: 0.5264\n",
      " 50/500 [==>...........................] - ETA: 7:39 - loss: 2.0102 - regression_loss: 1.4825 - classification_loss: 0.5277\n",
      " 51/500 [==>...........................] - ETA: 7:38 - loss: 1.9966 - regression_loss: 1.4722 - classification_loss: 0.5244\n",
      " 52/500 [==>...........................] - ETA: 7:36 - loss: 1.9817 - regression_loss: 1.4598 - classification_loss: 0.5219\n",
      " 53/500 [==>...........................] - ETA: 7:35 - loss: 1.9684 - regression_loss: 1.4517 - classification_loss: 0.5167\n",
      " 54/500 [==>...........................] - ETA: 7:34 - loss: 1.9698 - regression_loss: 1.4531 - classification_loss: 0.5167\n",
      " 55/500 [==>...........................] - ETA: 7:33 - loss: 1.9696 - regression_loss: 1.4545 - classification_loss: 0.5152\n",
      " 56/500 [==>...........................] - ETA: 7:32 - loss: 1.9529 - regression_loss: 1.4421 - classification_loss: 0.5108\n",
      " 57/500 [==>...........................] - ETA: 7:32 - loss: 1.9510 - regression_loss: 1.4418 - classification_loss: 0.5092\n",
      " 58/500 [==>...........................] - ETA: 7:31 - loss: 1.9613 - regression_loss: 1.4495 - classification_loss: 0.5119\n",
      " 59/500 [==>...........................] - ETA: 7:31 - loss: 1.9668 - regression_loss: 1.4527 - classification_loss: 0.5141\n",
      " 60/500 [==>...........................] - ETA: 7:29 - loss: 1.9633 - regression_loss: 1.4511 - classification_loss: 0.5122\n",
      " 61/500 [==>...........................] - ETA: 7:29 - loss: 1.9669 - regression_loss: 1.4572 - classification_loss: 0.5097\n",
      " 62/500 [==>...........................] - ETA: 7:28 - loss: 1.9622 - regression_loss: 1.4556 - classification_loss: 0.5066\n",
      " 63/500 [==>...........................] - ETA: 7:28 - loss: 1.9584 - regression_loss: 1.4511 - classification_loss: 0.5072\n",
      " 64/500 [==>...........................] - ETA: 7:26 - loss: 1.9578 - regression_loss: 1.4501 - classification_loss: 0.5077\n",
      " 65/500 [==>...........................] - ETA: 7:25 - loss: 1.9567 - regression_loss: 1.4499 - classification_loss: 0.5068\n",
      " 66/500 [==>...........................] - ETA: 7:24 - loss: 1.9567 - regression_loss: 1.4482 - classification_loss: 0.5085\n",
      " 67/500 [===>..........................] - ETA: 7:24 - loss: 1.9544 - regression_loss: 1.4461 - classification_loss: 0.5083\n",
      " 68/500 [===>..........................] - ETA: 7:23 - loss: 1.9415 - regression_loss: 1.4371 - classification_loss: 0.5044\n",
      " 69/500 [===>..........................] - ETA: 7:23 - loss: 1.9364 - regression_loss: 1.4344 - classification_loss: 0.5020\n",
      " 70/500 [===>..........................] - ETA: 7:21 - loss: 1.9369 - regression_loss: 1.4338 - classification_loss: 0.5030\n",
      " 71/500 [===>..........................] - ETA: 7:19 - loss: 1.9215 - regression_loss: 1.4218 - classification_loss: 0.4997\n",
      " 72/500 [===>..........................] - ETA: 7:19 - loss: 1.9250 - regression_loss: 1.4229 - classification_loss: 0.5021\n",
      " 73/500 [===>..........................] - ETA: 7:18 - loss: 1.9204 - regression_loss: 1.4205 - classification_loss: 0.4999\n",
      " 74/500 [===>..........................] - ETA: 7:16 - loss: 1.9094 - regression_loss: 1.4128 - classification_loss: 0.4966\n",
      " 75/500 [===>..........................] - ETA: 7:15 - loss: 1.9001 - regression_loss: 1.4072 - classification_loss: 0.4930\n",
      " 76/500 [===>..........................] - ETA: 7:14 - loss: 1.8988 - regression_loss: 1.4066 - classification_loss: 0.4922\n",
      " 77/500 [===>..........................] - ETA: 7:14 - loss: 1.8962 - regression_loss: 1.4061 - classification_loss: 0.4901\n",
      " 78/500 [===>..........................] - ETA: 7:12 - loss: 1.8913 - regression_loss: 1.4017 - classification_loss: 0.4896\n",
      " 79/500 [===>..........................] - ETA: 7:12 - loss: 1.8898 - regression_loss: 1.3999 - classification_loss: 0.4899\n",
      " 80/500 [===>..........................] - ETA: 7:11 - loss: 1.8887 - regression_loss: 1.3999 - classification_loss: 0.4888\n",
      " 81/500 [===>..........................] - ETA: 7:09 - loss: 1.8861 - regression_loss: 1.3979 - classification_loss: 0.4882\n",
      " 82/500 [===>..........................] - ETA: 7:08 - loss: 1.8820 - regression_loss: 1.3966 - classification_loss: 0.4854\n",
      " 83/500 [===>..........................] - ETA: 7:07 - loss: 1.8792 - regression_loss: 1.3942 - classification_loss: 0.4850\n",
      " 84/500 [====>.........................] - ETA: 7:06 - loss: 1.8913 - regression_loss: 1.4027 - classification_loss: 0.4886\n",
      " 85/500 [====>.........................] - ETA: 7:05 - loss: 1.8848 - regression_loss: 1.3992 - classification_loss: 0.4857\n",
      " 86/500 [====>.........................] - ETA: 7:05 - loss: 1.8871 - regression_loss: 1.4009 - classification_loss: 0.4862\n",
      " 87/500 [====>.........................] - ETA: 7:04 - loss: 1.8770 - regression_loss: 1.3940 - classification_loss: 0.4830\n",
      " 88/500 [====>.........................] - ETA: 7:03 - loss: 1.8681 - regression_loss: 1.3869 - classification_loss: 0.4813\n",
      " 89/500 [====>.........................] - ETA: 7:02 - loss: 1.8688 - regression_loss: 1.3866 - classification_loss: 0.4821\n",
      " 90/500 [====>.........................] - ETA: 7:01 - loss: 1.8693 - regression_loss: 1.3844 - classification_loss: 0.4849\n",
      " 91/500 [====>.........................] - ETA: 6:59 - loss: 1.8659 - regression_loss: 1.3818 - classification_loss: 0.4841\n",
      " 92/500 [====>.........................] - ETA: 6:58 - loss: 1.8616 - regression_loss: 1.3789 - classification_loss: 0.4827\n",
      " 93/500 [====>.........................] - ETA: 6:58 - loss: 1.8612 - regression_loss: 1.3776 - classification_loss: 0.4836\n",
      " 94/500 [====>.........................] - ETA: 6:57 - loss: 1.8558 - regression_loss: 1.3737 - classification_loss: 0.4820\n",
      " 95/500 [====>.........................] - ETA: 6:55 - loss: 1.8494 - regression_loss: 1.3706 - classification_loss: 0.4788\n",
      " 96/500 [====>.........................] - ETA: 6:55 - loss: 1.8508 - regression_loss: 1.3723 - classification_loss: 0.4784\n",
      " 97/500 [====>.........................] - ETA: 6:53 - loss: 1.8515 - regression_loss: 1.3745 - classification_loss: 0.4770\n",
      " 98/500 [====>.........................] - ETA: 6:53 - loss: 1.8608 - regression_loss: 1.3818 - classification_loss: 0.4790\n",
      " 99/500 [====>.........................] - ETA: 6:50 - loss: 1.8555 - regression_loss: 1.3770 - classification_loss: 0.4785\n",
      "100/500 [=====>........................] - ETA: 6:50 - loss: 1.8536 - regression_loss: 1.3761 - classification_loss: 0.4775\n",
      "101/500 [=====>........................] - ETA: 6:49 - loss: 1.8548 - regression_loss: 1.3778 - classification_loss: 0.4770\n",
      "102/500 [=====>........................] - ETA: 6:48 - loss: 1.8540 - regression_loss: 1.3772 - classification_loss: 0.4768\n",
      "103/500 [=====>........................] - ETA: 6:47 - loss: 1.8461 - regression_loss: 1.3716 - classification_loss: 0.4744\n",
      "104/500 [=====>........................] - ETA: 6:46 - loss: 1.8452 - regression_loss: 1.3715 - classification_loss: 0.4737\n",
      "105/500 [=====>........................] - ETA: 6:45 - loss: 1.8425 - regression_loss: 1.3701 - classification_loss: 0.4724\n",
      "106/500 [=====>........................] - ETA: 6:43 - loss: 1.8379 - regression_loss: 1.3658 - classification_loss: 0.4721\n",
      "107/500 [=====>........................] - ETA: 6:42 - loss: 1.8409 - regression_loss: 1.3673 - classification_loss: 0.4736\n",
      "108/500 [=====>........................] - ETA: 6:41 - loss: 1.8513 - regression_loss: 1.3743 - classification_loss: 0.4770\n",
      "109/500 [=====>........................] - ETA: 6:41 - loss: 1.8545 - regression_loss: 1.3768 - classification_loss: 0.4777\n",
      "110/500 [=====>........................] - ETA: 6:39 - loss: 1.8582 - regression_loss: 1.3810 - classification_loss: 0.4772\n",
      "111/500 [=====>........................] - ETA: 6:38 - loss: 1.8572 - regression_loss: 1.3804 - classification_loss: 0.4767\n",
      "112/500 [=====>........................] - ETA: 6:37 - loss: 1.8577 - regression_loss: 1.3807 - classification_loss: 0.4771\n",
      "113/500 [=====>........................] - ETA: 6:35 - loss: 1.8519 - regression_loss: 1.3769 - classification_loss: 0.4750\n",
      "114/500 [=====>........................] - ETA: 6:35 - loss: 1.8554 - regression_loss: 1.3804 - classification_loss: 0.4750\n",
      "115/500 [=====>........................] - ETA: 6:34 - loss: 1.8549 - regression_loss: 1.3803 - classification_loss: 0.4746\n",
      "116/500 [=====>........................] - ETA: 6:33 - loss: 1.8541 - regression_loss: 1.3781 - classification_loss: 0.4761\n",
      "117/500 [======>.......................] - ETA: 6:33 - loss: 1.8521 - regression_loss: 1.3754 - classification_loss: 0.4767\n",
      "118/500 [======>.......................] - ETA: 6:31 - loss: 1.8509 - regression_loss: 1.3749 - classification_loss: 0.4760\n",
      "119/500 [======>.......................] - ETA: 6:31 - loss: 1.8491 - regression_loss: 1.3748 - classification_loss: 0.4743\n",
      "120/500 [======>.......................] - ETA: 6:30 - loss: 1.8479 - regression_loss: 1.3741 - classification_loss: 0.4738\n",
      "121/500 [======>.......................] - ETA: 6:29 - loss: 1.8548 - regression_loss: 1.3778 - classification_loss: 0.4769\n",
      "122/500 [======>.......................] - ETA: 6:28 - loss: 1.8540 - regression_loss: 1.3779 - classification_loss: 0.4761\n",
      "123/500 [======>.......................] - ETA: 6:27 - loss: 1.8590 - regression_loss: 1.3805 - classification_loss: 0.4785\n",
      "124/500 [======>.......................] - ETA: 6:26 - loss: 1.8531 - regression_loss: 1.3762 - classification_loss: 0.4769\n",
      "125/500 [======>.......................] - ETA: 6:25 - loss: 1.8584 - regression_loss: 1.3810 - classification_loss: 0.4775\n",
      "126/500 [======>.......................] - ETA: 6:24 - loss: 1.8581 - regression_loss: 1.3806 - classification_loss: 0.4775\n",
      "127/500 [======>.......................] - ETA: 6:23 - loss: 1.8581 - regression_loss: 1.3814 - classification_loss: 0.4768\n",
      "128/500 [======>.......................] - ETA: 6:22 - loss: 1.8561 - regression_loss: 1.3801 - classification_loss: 0.4760\n",
      "129/500 [======>.......................] - ETA: 6:21 - loss: 1.8562 - regression_loss: 1.3803 - classification_loss: 0.4759\n",
      "130/500 [======>.......................] - ETA: 6:20 - loss: 1.8564 - regression_loss: 1.3799 - classification_loss: 0.4764\n",
      "131/500 [======>.......................] - ETA: 6:19 - loss: 1.8568 - regression_loss: 1.3813 - classification_loss: 0.4754\n",
      "132/500 [======>.......................] - ETA: 6:18 - loss: 1.8543 - regression_loss: 1.3783 - classification_loss: 0.4760\n",
      "133/500 [======>.......................] - ETA: 6:17 - loss: 1.8583 - regression_loss: 1.3812 - classification_loss: 0.4771\n",
      "134/500 [=======>......................] - ETA: 6:16 - loss: 1.8540 - regression_loss: 1.3781 - classification_loss: 0.4759\n",
      "135/500 [=======>......................] - ETA: 6:15 - loss: 1.8546 - regression_loss: 1.3796 - classification_loss: 0.4750\n",
      "136/500 [=======>......................] - ETA: 6:14 - loss: 1.8575 - regression_loss: 1.3819 - classification_loss: 0.4755\n",
      "137/500 [=======>......................] - ETA: 6:13 - loss: 1.8594 - regression_loss: 1.3827 - classification_loss: 0.4767\n",
      "138/500 [=======>......................] - ETA: 6:12 - loss: 1.8579 - regression_loss: 1.3814 - classification_loss: 0.4765\n",
      "139/500 [=======>......................] - ETA: 6:11 - loss: 1.8586 - regression_loss: 1.3827 - classification_loss: 0.4760\n",
      "140/500 [=======>......................] - ETA: 6:09 - loss: 1.8642 - regression_loss: 1.3847 - classification_loss: 0.4795\n",
      "141/500 [=======>......................] - ETA: 6:09 - loss: 1.8692 - regression_loss: 1.3887 - classification_loss: 0.4805\n",
      "142/500 [=======>......................] - ETA: 6:07 - loss: 1.8712 - regression_loss: 1.3908 - classification_loss: 0.4804\n",
      "143/500 [=======>......................] - ETA: 6:06 - loss: 1.8734 - regression_loss: 1.3936 - classification_loss: 0.4799\n",
      "144/500 [=======>......................] - ETA: 6:05 - loss: 1.8795 - regression_loss: 1.3983 - classification_loss: 0.4812\n",
      "145/500 [=======>......................] - ETA: 6:04 - loss: 1.8775 - regression_loss: 1.3974 - classification_loss: 0.4801\n",
      "146/500 [=======>......................] - ETA: 6:03 - loss: 1.8845 - regression_loss: 1.4013 - classification_loss: 0.4832\n",
      "147/500 [=======>......................] - ETA: 6:03 - loss: 1.8820 - regression_loss: 1.4004 - classification_loss: 0.4815\n",
      "148/500 [=======>......................] - ETA: 6:01 - loss: 1.8770 - regression_loss: 1.3968 - classification_loss: 0.4802\n",
      "149/500 [=======>......................] - ETA: 6:00 - loss: 1.8753 - regression_loss: 1.3955 - classification_loss: 0.4798\n",
      "150/500 [========>.....................] - ETA: 5:59 - loss: 1.8738 - regression_loss: 1.3943 - classification_loss: 0.4795\n",
      "151/500 [========>.....................] - ETA: 5:58 - loss: 1.8793 - regression_loss: 1.3989 - classification_loss: 0.4804\n",
      "152/500 [========>.....................] - ETA: 5:57 - loss: 1.8779 - regression_loss: 1.3978 - classification_loss: 0.4801\n",
      "153/500 [========>.....................] - ETA: 5:57 - loss: 1.8758 - regression_loss: 1.3963 - classification_loss: 0.4795\n",
      "154/500 [========>.....................] - ETA: 5:55 - loss: 1.8722 - regression_loss: 1.3936 - classification_loss: 0.4786\n",
      "155/500 [========>.....................] - ETA: 5:55 - loss: 1.8767 - regression_loss: 1.3969 - classification_loss: 0.4798\n",
      "156/500 [========>.....................] - ETA: 5:53 - loss: 1.8745 - regression_loss: 1.3953 - classification_loss: 0.4792\n",
      "157/500 [========>.....................] - ETA: 5:52 - loss: 1.8802 - regression_loss: 1.4002 - classification_loss: 0.4800\n",
      "158/500 [========>.....................] - ETA: 5:51 - loss: 1.8813 - regression_loss: 1.4007 - classification_loss: 0.4806\n",
      "159/500 [========>.....................] - ETA: 5:50 - loss: 1.8886 - regression_loss: 1.4043 - classification_loss: 0.4843\n",
      "160/500 [========>.....................] - ETA: 5:49 - loss: 1.8908 - regression_loss: 1.4066 - classification_loss: 0.4842\n",
      "161/500 [========>.....................] - ETA: 5:48 - loss: 1.8932 - regression_loss: 1.4082 - classification_loss: 0.4850\n",
      "162/500 [========>.....................] - ETA: 5:47 - loss: 1.8997 - regression_loss: 1.4120 - classification_loss: 0.4877\n",
      "163/500 [========>.....................] - ETA: 5:46 - loss: 1.9003 - regression_loss: 1.4128 - classification_loss: 0.4875\n",
      "164/500 [========>.....................] - ETA: 5:45 - loss: 1.9003 - regression_loss: 1.4132 - classification_loss: 0.4871\n",
      "165/500 [========>.....................] - ETA: 5:44 - loss: 1.8990 - regression_loss: 1.4127 - classification_loss: 0.4863\n",
      "166/500 [========>.....................] - ETA: 5:43 - loss: 1.9017 - regression_loss: 1.4145 - classification_loss: 0.4872\n",
      "167/500 [=========>....................] - ETA: 5:42 - loss: 1.8993 - regression_loss: 1.4120 - classification_loss: 0.4873\n",
      "168/500 [=========>....................] - ETA: 5:41 - loss: 1.8981 - regression_loss: 1.4106 - classification_loss: 0.4874\n",
      "169/500 [=========>....................] - ETA: 5:40 - loss: 1.8986 - regression_loss: 1.4108 - classification_loss: 0.4878\n",
      "170/500 [=========>....................] - ETA: 5:39 - loss: 1.9000 - regression_loss: 1.4127 - classification_loss: 0.4873\n",
      "171/500 [=========>....................] - ETA: 5:38 - loss: 1.8947 - regression_loss: 1.4090 - classification_loss: 0.4858\n",
      "172/500 [=========>....................] - ETA: 5:36 - loss: 1.8937 - regression_loss: 1.4068 - classification_loss: 0.4869\n",
      "173/500 [=========>....................] - ETA: 5:35 - loss: 1.8899 - regression_loss: 1.4040 - classification_loss: 0.4859\n",
      "174/500 [=========>....................] - ETA: 5:34 - loss: 1.8907 - regression_loss: 1.4054 - classification_loss: 0.4853\n",
      "175/500 [=========>....................] - ETA: 5:33 - loss: 1.8940 - regression_loss: 1.4072 - classification_loss: 0.4868\n",
      "176/500 [=========>....................] - ETA: 5:32 - loss: 1.8948 - regression_loss: 1.4087 - classification_loss: 0.4861\n",
      "177/500 [=========>....................] - ETA: 5:31 - loss: 1.8975 - regression_loss: 1.4103 - classification_loss: 0.4872\n",
      "178/500 [=========>....................] - ETA: 5:30 - loss: 1.8983 - regression_loss: 1.4116 - classification_loss: 0.4867\n",
      "179/500 [=========>....................] - ETA: 5:29 - loss: 1.9013 - regression_loss: 1.4141 - classification_loss: 0.4872\n",
      "180/500 [=========>....................] - ETA: 5:28 - loss: 1.9010 - regression_loss: 1.4140 - classification_loss: 0.4870\n",
      "181/500 [=========>....................] - ETA: 5:27 - loss: 1.9020 - regression_loss: 1.4152 - classification_loss: 0.4868\n",
      "182/500 [=========>....................] - ETA: 5:26 - loss: 1.9077 - regression_loss: 1.4202 - classification_loss: 0.4875\n",
      "183/500 [=========>....................] - ETA: 5:25 - loss: 1.9061 - regression_loss: 1.4191 - classification_loss: 0.4869\n",
      "184/500 [==========>...................] - ETA: 5:24 - loss: 1.9086 - regression_loss: 1.4209 - classification_loss: 0.4877\n",
      "185/500 [==========>...................] - ETA: 5:23 - loss: 1.9045 - regression_loss: 1.4178 - classification_loss: 0.4867\n",
      "186/500 [==========>...................] - ETA: 5:22 - loss: 1.9086 - regression_loss: 1.4209 - classification_loss: 0.4877\n",
      "187/500 [==========>...................] - ETA: 5:21 - loss: 1.9106 - regression_loss: 1.4227 - classification_loss: 0.4879\n",
      "188/500 [==========>...................] - ETA: 5:20 - loss: 1.9089 - regression_loss: 1.4224 - classification_loss: 0.4865\n",
      "189/500 [==========>...................] - ETA: 5:19 - loss: 1.9059 - regression_loss: 1.4199 - classification_loss: 0.4860\n",
      "190/500 [==========>...................] - ETA: 5:18 - loss: 1.9102 - regression_loss: 1.4230 - classification_loss: 0.4872\n",
      "191/500 [==========>...................] - ETA: 5:17 - loss: 1.9112 - regression_loss: 1.4240 - classification_loss: 0.4873\n",
      "192/500 [==========>...................] - ETA: 5:16 - loss: 1.9119 - regression_loss: 1.4241 - classification_loss: 0.4878\n",
      "193/500 [==========>...................] - ETA: 5:15 - loss: 1.9070 - regression_loss: 1.4208 - classification_loss: 0.4863\n",
      "194/500 [==========>...................] - ETA: 5:14 - loss: 1.9095 - regression_loss: 1.4231 - classification_loss: 0.4865\n",
      "195/500 [==========>...................] - ETA: 5:13 - loss: 1.9089 - regression_loss: 1.4222 - classification_loss: 0.4867\n",
      "196/500 [==========>...................] - ETA: 5:12 - loss: 1.9050 - regression_loss: 1.4195 - classification_loss: 0.4855\n",
      "197/500 [==========>...................] - ETA: 5:11 - loss: 1.9058 - regression_loss: 1.4209 - classification_loss: 0.4849\n",
      "198/500 [==========>...................] - ETA: 5:10 - loss: 1.9081 - regression_loss: 1.4227 - classification_loss: 0.4854\n",
      "199/500 [==========>...................] - ETA: 5:09 - loss: 1.9083 - regression_loss: 1.4233 - classification_loss: 0.4850\n",
      "200/500 [===========>..................] - ETA: 5:08 - loss: 1.9101 - regression_loss: 1.4244 - classification_loss: 0.4857\n",
      "201/500 [===========>..................] - ETA: 5:07 - loss: 1.9095 - regression_loss: 1.4245 - classification_loss: 0.4849\n",
      "202/500 [===========>..................] - ETA: 5:05 - loss: 1.9078 - regression_loss: 1.4229 - classification_loss: 0.4849\n",
      "203/500 [===========>..................] - ETA: 5:04 - loss: 1.9077 - regression_loss: 1.4232 - classification_loss: 0.4844\n",
      "204/500 [===========>..................] - ETA: 5:03 - loss: 1.9061 - regression_loss: 1.4220 - classification_loss: 0.4841\n",
      "205/500 [===========>..................] - ETA: 5:02 - loss: 1.9023 - regression_loss: 1.4192 - classification_loss: 0.4830\n",
      "206/500 [===========>..................] - ETA: 5:01 - loss: 1.9083 - regression_loss: 1.4231 - classification_loss: 0.4852\n",
      "207/500 [===========>..................] - ETA: 5:00 - loss: 1.9046 - regression_loss: 1.4205 - classification_loss: 0.4841\n",
      "208/500 [===========>..................] - ETA: 4:59 - loss: 1.9050 - regression_loss: 1.4205 - classification_loss: 0.4845\n",
      "209/500 [===========>..................] - ETA: 4:58 - loss: 1.9107 - regression_loss: 1.4244 - classification_loss: 0.4862\n",
      "210/500 [===========>..................] - ETA: 4:57 - loss: 1.9113 - regression_loss: 1.4254 - classification_loss: 0.4859\n",
      "211/500 [===========>..................] - ETA: 4:56 - loss: 1.9090 - regression_loss: 1.4237 - classification_loss: 0.4852\n",
      "212/500 [===========>..................] - ETA: 4:55 - loss: 1.9066 - regression_loss: 1.4225 - classification_loss: 0.4841\n",
      "213/500 [===========>..................] - ETA: 4:54 - loss: 1.9036 - regression_loss: 1.4205 - classification_loss: 0.4831\n",
      "214/500 [===========>..................] - ETA: 4:53 - loss: 1.9073 - regression_loss: 1.4227 - classification_loss: 0.4846\n",
      "215/500 [===========>..................] - ETA: 4:52 - loss: 1.9039 - regression_loss: 1.4202 - classification_loss: 0.4837\n",
      "216/500 [===========>..................] - ETA: 4:51 - loss: 1.9026 - regression_loss: 1.4190 - classification_loss: 0.4836\n",
      "217/500 [============>.................] - ETA: 4:50 - loss: 1.9044 - regression_loss: 1.4211 - classification_loss: 0.4832\n",
      "218/500 [============>.................] - ETA: 4:49 - loss: 1.9057 - regression_loss: 1.4220 - classification_loss: 0.4837\n",
      "219/500 [============>.................] - ETA: 4:48 - loss: 1.9099 - regression_loss: 1.4249 - classification_loss: 0.4849\n",
      "220/500 [============>.................] - ETA: 4:47 - loss: 1.9056 - regression_loss: 1.4218 - classification_loss: 0.4838\n",
      "221/500 [============>.................] - ETA: 4:46 - loss: 1.9050 - regression_loss: 1.4216 - classification_loss: 0.4835\n",
      "222/500 [============>.................] - ETA: 4:45 - loss: 1.9040 - regression_loss: 1.4208 - classification_loss: 0.4832\n",
      "223/500 [============>.................] - ETA: 4:44 - loss: 1.8996 - regression_loss: 1.4178 - classification_loss: 0.4818\n",
      "224/500 [============>.................] - ETA: 4:43 - loss: 1.9026 - regression_loss: 1.4197 - classification_loss: 0.4830\n",
      "225/500 [============>.................] - ETA: 4:42 - loss: 1.9010 - regression_loss: 1.4185 - classification_loss: 0.4825\n",
      "226/500 [============>.................] - ETA: 4:41 - loss: 1.9015 - regression_loss: 1.4185 - classification_loss: 0.4830\n",
      "227/500 [============>.................] - ETA: 4:40 - loss: 1.9012 - regression_loss: 1.4186 - classification_loss: 0.4827\n",
      "228/500 [============>.................] - ETA: 4:39 - loss: 1.9007 - regression_loss: 1.4184 - classification_loss: 0.4823\n",
      "229/500 [============>.................] - ETA: 4:38 - loss: 1.8990 - regression_loss: 1.4171 - classification_loss: 0.4819\n",
      "230/500 [============>.................] - ETA: 4:37 - loss: 1.8941 - regression_loss: 1.4134 - classification_loss: 0.4807\n",
      "231/500 [============>.................] - ETA: 4:36 - loss: 1.8924 - regression_loss: 1.4116 - classification_loss: 0.4809\n",
      "232/500 [============>.................] - ETA: 4:35 - loss: 1.8942 - regression_loss: 1.4132 - classification_loss: 0.4810\n",
      "233/500 [============>.................] - ETA: 4:34 - loss: 1.8928 - regression_loss: 1.4122 - classification_loss: 0.4805\n",
      "234/500 [=============>................] - ETA: 4:33 - loss: 1.8927 - regression_loss: 1.4129 - classification_loss: 0.4799\n",
      "235/500 [=============>................] - ETA: 4:32 - loss: 1.8906 - regression_loss: 1.4107 - classification_loss: 0.4799\n",
      "236/500 [=============>................] - ETA: 4:31 - loss: 1.8883 - regression_loss: 1.4093 - classification_loss: 0.4790\n",
      "237/500 [=============>................] - ETA: 4:30 - loss: 1.8870 - regression_loss: 1.4084 - classification_loss: 0.4786\n",
      "238/500 [=============>................] - ETA: 4:29 - loss: 1.8848 - regression_loss: 1.4070 - classification_loss: 0.4778\n",
      "239/500 [=============>................] - ETA: 4:28 - loss: 1.8850 - regression_loss: 1.4066 - classification_loss: 0.4784\n",
      "240/500 [=============>................] - ETA: 4:27 - loss: 1.8823 - regression_loss: 1.4050 - classification_loss: 0.4773\n",
      "241/500 [=============>................] - ETA: 4:26 - loss: 1.8830 - regression_loss: 1.4056 - classification_loss: 0.4774\n",
      "242/500 [=============>................] - ETA: 4:25 - loss: 1.8827 - regression_loss: 1.4060 - classification_loss: 0.4767\n",
      "243/500 [=============>................] - ETA: 4:24 - loss: 1.8840 - regression_loss: 1.4071 - classification_loss: 0.4769\n",
      "244/500 [=============>................] - ETA: 4:23 - loss: 1.8857 - regression_loss: 1.4074 - classification_loss: 0.4783\n",
      "245/500 [=============>................] - ETA: 4:22 - loss: 1.8856 - regression_loss: 1.4073 - classification_loss: 0.4783\n",
      "246/500 [=============>................] - ETA: 4:21 - loss: 1.8819 - regression_loss: 1.4040 - classification_loss: 0.4779\n",
      "247/500 [=============>................] - ETA: 4:20 - loss: 1.8819 - regression_loss: 1.4045 - classification_loss: 0.4774\n",
      "248/500 [=============>................] - ETA: 4:19 - loss: 1.8812 - regression_loss: 1.4043 - classification_loss: 0.4769\n",
      "249/500 [=============>................] - ETA: 4:18 - loss: 1.8791 - regression_loss: 1.4032 - classification_loss: 0.4759\n",
      "250/500 [==============>...............] - ETA: 4:17 - loss: 1.8795 - regression_loss: 1.4036 - classification_loss: 0.4759\n",
      "251/500 [==============>...............] - ETA: 4:16 - loss: 1.8792 - regression_loss: 1.4033 - classification_loss: 0.4759\n",
      "252/500 [==============>...............] - ETA: 4:15 - loss: 1.8800 - regression_loss: 1.4041 - classification_loss: 0.4758\n",
      "253/500 [==============>...............] - ETA: 4:14 - loss: 1.8806 - regression_loss: 1.4046 - classification_loss: 0.4761\n",
      "254/500 [==============>...............] - ETA: 4:13 - loss: 1.8809 - regression_loss: 1.4047 - classification_loss: 0.4762\n",
      "255/500 [==============>...............] - ETA: 4:12 - loss: 1.8819 - regression_loss: 1.4055 - classification_loss: 0.4764\n",
      "256/500 [==============>...............] - ETA: 4:11 - loss: 1.8851 - regression_loss: 1.4054 - classification_loss: 0.4797\n",
      "257/500 [==============>...............] - ETA: 4:10 - loss: 1.8874 - regression_loss: 1.4070 - classification_loss: 0.4804\n",
      "258/500 [==============>...............] - ETA: 4:09 - loss: 1.8886 - regression_loss: 1.4080 - classification_loss: 0.4806\n",
      "259/500 [==============>...............] - ETA: 4:08 - loss: 1.8899 - regression_loss: 1.4090 - classification_loss: 0.4808\n",
      "260/500 [==============>...............] - ETA: 4:07 - loss: 1.8873 - regression_loss: 1.4072 - classification_loss: 0.4801\n",
      "261/500 [==============>...............] - ETA: 4:06 - loss: 1.8859 - regression_loss: 1.4064 - classification_loss: 0.4794\n",
      "262/500 [==============>...............] - ETA: 4:05 - loss: 1.8872 - regression_loss: 1.4083 - classification_loss: 0.4789\n",
      "263/500 [==============>...............] - ETA: 4:04 - loss: 1.8869 - regression_loss: 1.4081 - classification_loss: 0.4788\n",
      "264/500 [==============>...............] - ETA: 4:03 - loss: 1.8877 - regression_loss: 1.4079 - classification_loss: 0.4798\n",
      "265/500 [==============>...............] - ETA: 4:02 - loss: 1.8874 - regression_loss: 1.4070 - classification_loss: 0.4804\n",
      "266/500 [==============>...............] - ETA: 4:01 - loss: 1.8869 - regression_loss: 1.4066 - classification_loss: 0.4803\n",
      "267/500 [===============>..............] - ETA: 4:00 - loss: 1.8874 - regression_loss: 1.4070 - classification_loss: 0.4804\n",
      "268/500 [===============>..............] - ETA: 3:58 - loss: 1.8876 - regression_loss: 1.4074 - classification_loss: 0.4802\n",
      "269/500 [===============>..............] - ETA: 3:58 - loss: 1.8910 - regression_loss: 1.4097 - classification_loss: 0.4813\n",
      "270/500 [===============>..............] - ETA: 3:56 - loss: 1.8917 - regression_loss: 1.4100 - classification_loss: 0.4817\n",
      "271/500 [===============>..............] - ETA: 3:55 - loss: 1.8886 - regression_loss: 1.4079 - classification_loss: 0.4807\n",
      "272/500 [===============>..............] - ETA: 3:54 - loss: 1.8863 - regression_loss: 1.4063 - classification_loss: 0.4800\n",
      "273/500 [===============>..............] - ETA: 3:53 - loss: 1.8869 - regression_loss: 1.4068 - classification_loss: 0.4800\n",
      "274/500 [===============>..............] - ETA: 3:52 - loss: 1.8864 - regression_loss: 1.4064 - classification_loss: 0.4801\n",
      "275/500 [===============>..............] - ETA: 3:51 - loss: 1.8864 - regression_loss: 1.4061 - classification_loss: 0.4803\n",
      "276/500 [===============>..............] - ETA: 3:50 - loss: 1.8878 - regression_loss: 1.4066 - classification_loss: 0.4812\n",
      "277/500 [===============>..............] - ETA: 3:49 - loss: 1.8878 - regression_loss: 1.4064 - classification_loss: 0.4814\n",
      "278/500 [===============>..............] - ETA: 3:48 - loss: 1.8867 - regression_loss: 1.4057 - classification_loss: 0.4810\n",
      "279/500 [===============>..............] - ETA: 3:47 - loss: 1.8857 - regression_loss: 1.4051 - classification_loss: 0.4806\n",
      "280/500 [===============>..............] - ETA: 3:46 - loss: 1.8852 - regression_loss: 1.4046 - classification_loss: 0.4806\n",
      "281/500 [===============>..............] - ETA: 3:45 - loss: 1.8860 - regression_loss: 1.4057 - classification_loss: 0.4803\n",
      "282/500 [===============>..............] - ETA: 3:44 - loss: 1.8865 - regression_loss: 1.4066 - classification_loss: 0.4800\n",
      "283/500 [===============>..............] - ETA: 3:43 - loss: 1.8836 - regression_loss: 1.4045 - classification_loss: 0.4791\n",
      "284/500 [================>.............] - ETA: 3:42 - loss: 1.8862 - regression_loss: 1.4063 - classification_loss: 0.4799\n",
      "285/500 [================>.............] - ETA: 3:41 - loss: 1.8861 - regression_loss: 1.4063 - classification_loss: 0.4798\n",
      "286/500 [================>.............] - ETA: 3:40 - loss: 1.8837 - regression_loss: 1.4046 - classification_loss: 0.4791\n",
      "287/500 [================>.............] - ETA: 3:39 - loss: 1.8829 - regression_loss: 1.4044 - classification_loss: 0.4785\n",
      "288/500 [================>.............] - ETA: 3:38 - loss: 1.8840 - regression_loss: 1.4053 - classification_loss: 0.4787\n",
      "289/500 [================>.............] - ETA: 3:37 - loss: 1.8819 - regression_loss: 1.4040 - classification_loss: 0.4779\n",
      "290/500 [================>.............] - ETA: 3:36 - loss: 1.8784 - regression_loss: 1.4011 - classification_loss: 0.4773\n",
      "291/500 [================>.............] - ETA: 3:35 - loss: 1.8764 - regression_loss: 1.3996 - classification_loss: 0.4767\n",
      "292/500 [================>.............] - ETA: 3:33 - loss: 1.8756 - regression_loss: 1.3993 - classification_loss: 0.4763\n",
      "293/500 [================>.............] - ETA: 3:32 - loss: 1.8761 - regression_loss: 1.4001 - classification_loss: 0.4760\n",
      "294/500 [================>.............] - ETA: 3:31 - loss: 1.8740 - regression_loss: 1.3987 - classification_loss: 0.4753\n",
      "295/500 [================>.............] - ETA: 3:30 - loss: 1.8771 - regression_loss: 1.4001 - classification_loss: 0.4771\n",
      "296/500 [================>.............] - ETA: 3:29 - loss: 1.8776 - regression_loss: 1.4004 - classification_loss: 0.4772\n",
      "297/500 [================>.............] - ETA: 3:28 - loss: 1.8788 - regression_loss: 1.4008 - classification_loss: 0.4780\n",
      "298/500 [================>.............] - ETA: 3:27 - loss: 1.8783 - regression_loss: 1.4005 - classification_loss: 0.4778\n",
      "299/500 [================>.............] - ETA: 3:26 - loss: 1.8800 - regression_loss: 1.4020 - classification_loss: 0.4780\n",
      "300/500 [=================>............] - ETA: 3:25 - loss: 1.8820 - regression_loss: 1.4041 - classification_loss: 0.4779\n",
      "301/500 [=================>............] - ETA: 3:24 - loss: 1.8831 - regression_loss: 1.4052 - classification_loss: 0.4779\n",
      "302/500 [=================>............] - ETA: 3:23 - loss: 1.8826 - regression_loss: 1.4046 - classification_loss: 0.4779\n",
      "303/500 [=================>............] - ETA: 3:22 - loss: 1.8803 - regression_loss: 1.4030 - classification_loss: 0.4774\n",
      "304/500 [=================>............] - ETA: 3:21 - loss: 1.8774 - regression_loss: 1.4007 - classification_loss: 0.4767\n",
      "305/500 [=================>............] - ETA: 3:20 - loss: 1.8773 - regression_loss: 1.4009 - classification_loss: 0.4764\n",
      "306/500 [=================>............] - ETA: 3:19 - loss: 1.8763 - regression_loss: 1.4001 - classification_loss: 0.4762\n",
      "307/500 [=================>............] - ETA: 3:18 - loss: 1.8784 - regression_loss: 1.4011 - classification_loss: 0.4773\n",
      "308/500 [=================>............] - ETA: 3:17 - loss: 1.8784 - regression_loss: 1.4014 - classification_loss: 0.4770\n",
      "309/500 [=================>............] - ETA: 3:16 - loss: 1.8770 - regression_loss: 1.4003 - classification_loss: 0.4766\n",
      "310/500 [=================>............] - ETA: 3:15 - loss: 1.8748 - regression_loss: 1.3985 - classification_loss: 0.4763\n",
      "311/500 [=================>............] - ETA: 3:14 - loss: 1.8740 - regression_loss: 1.3985 - classification_loss: 0.4755\n",
      "312/500 [=================>............] - ETA: 3:13 - loss: 1.8740 - regression_loss: 1.3984 - classification_loss: 0.4756\n",
      "313/500 [=================>............] - ETA: 3:12 - loss: 1.8751 - regression_loss: 1.3994 - classification_loss: 0.4756\n",
      "314/500 [=================>............] - ETA: 3:10 - loss: 1.8750 - regression_loss: 1.3991 - classification_loss: 0.4760\n",
      "315/500 [=================>............] - ETA: 3:10 - loss: 1.8732 - regression_loss: 1.3980 - classification_loss: 0.4753\n",
      "316/500 [=================>............] - ETA: 3:09 - loss: 1.8709 - regression_loss: 1.3962 - classification_loss: 0.4748\n",
      "317/500 [==================>...........] - ETA: 3:07 - loss: 1.8714 - regression_loss: 1.3960 - classification_loss: 0.4754\n",
      "318/500 [==================>...........] - ETA: 3:06 - loss: 1.8730 - regression_loss: 1.3975 - classification_loss: 0.4755\n",
      "319/500 [==================>...........] - ETA: 3:05 - loss: 1.8740 - regression_loss: 1.3986 - classification_loss: 0.4754\n",
      "320/500 [==================>...........] - ETA: 3:04 - loss: 1.8754 - regression_loss: 1.3999 - classification_loss: 0.4755\n",
      "321/500 [==================>...........] - ETA: 3:03 - loss: 1.8770 - regression_loss: 1.4011 - classification_loss: 0.4759\n",
      "322/500 [==================>...........] - ETA: 3:02 - loss: 1.8759 - regression_loss: 1.4002 - classification_loss: 0.4757\n",
      "323/500 [==================>...........] - ETA: 3:02 - loss: 1.8742 - regression_loss: 1.3989 - classification_loss: 0.4754\n",
      "324/500 [==================>...........] - ETA: 3:01 - loss: 1.8724 - regression_loss: 1.3975 - classification_loss: 0.4749\n",
      "325/500 [==================>...........] - ETA: 2:59 - loss: 1.8714 - regression_loss: 1.3969 - classification_loss: 0.4745\n",
      "326/500 [==================>...........] - ETA: 2:58 - loss: 1.8713 - regression_loss: 1.3971 - classification_loss: 0.4742\n",
      "327/500 [==================>...........] - ETA: 2:57 - loss: 1.8711 - regression_loss: 1.3971 - classification_loss: 0.4740\n",
      "328/500 [==================>...........] - ETA: 2:56 - loss: 1.8687 - regression_loss: 1.3954 - classification_loss: 0.4734\n",
      "329/500 [==================>...........] - ETA: 2:55 - loss: 1.8678 - regression_loss: 1.3947 - classification_loss: 0.4731\n",
      "330/500 [==================>...........] - ETA: 2:54 - loss: 1.8650 - regression_loss: 1.3926 - classification_loss: 0.4724\n",
      "331/500 [==================>...........] - ETA: 2:53 - loss: 1.8681 - regression_loss: 1.3950 - classification_loss: 0.4731\n",
      "332/500 [==================>...........] - ETA: 2:52 - loss: 1.8696 - regression_loss: 1.3961 - classification_loss: 0.4735\n",
      "333/500 [==================>...........] - ETA: 2:51 - loss: 1.8704 - regression_loss: 1.3966 - classification_loss: 0.4738\n",
      "334/500 [===================>..........] - ETA: 2:50 - loss: 1.8710 - regression_loss: 1.3967 - classification_loss: 0.4743\n",
      "335/500 [===================>..........] - ETA: 2:49 - loss: 1.8722 - regression_loss: 1.3972 - classification_loss: 0.4749\n",
      "336/500 [===================>..........] - ETA: 2:48 - loss: 1.8713 - regression_loss: 1.3961 - classification_loss: 0.4751\n",
      "337/500 [===================>..........] - ETA: 2:47 - loss: 1.8704 - regression_loss: 1.3953 - classification_loss: 0.4751\n",
      "338/500 [===================>..........] - ETA: 2:46 - loss: 1.8711 - regression_loss: 1.3954 - classification_loss: 0.4757\n",
      "339/500 [===================>..........] - ETA: 2:45 - loss: 1.8725 - regression_loss: 1.3962 - classification_loss: 0.4763\n",
      "340/500 [===================>..........] - ETA: 2:44 - loss: 1.8728 - regression_loss: 1.3967 - classification_loss: 0.4761\n",
      "341/500 [===================>..........] - ETA: 2:43 - loss: 1.8728 - regression_loss: 1.3971 - classification_loss: 0.4758\n",
      "342/500 [===================>..........] - ETA: 2:42 - loss: 1.8726 - regression_loss: 1.3969 - classification_loss: 0.4757\n",
      "343/500 [===================>..........] - ETA: 2:41 - loss: 1.8726 - regression_loss: 1.3968 - classification_loss: 0.4757\n",
      "344/500 [===================>..........] - ETA: 2:40 - loss: 1.8731 - regression_loss: 1.3969 - classification_loss: 0.4762\n",
      "345/500 [===================>..........] - ETA: 2:39 - loss: 1.8705 - regression_loss: 1.3951 - classification_loss: 0.4754\n",
      "346/500 [===================>..........] - ETA: 2:38 - loss: 1.8707 - regression_loss: 1.3951 - classification_loss: 0.4756\n",
      "347/500 [===================>..........] - ETA: 2:37 - loss: 1.8745 - regression_loss: 1.3979 - classification_loss: 0.4766\n",
      "348/500 [===================>..........] - ETA: 2:36 - loss: 1.8757 - regression_loss: 1.3990 - classification_loss: 0.4768\n",
      "349/500 [===================>..........] - ETA: 2:35 - loss: 1.8746 - regression_loss: 1.3982 - classification_loss: 0.4764\n",
      "350/500 [====================>.........] - ETA: 2:34 - loss: 1.8752 - regression_loss: 1.3991 - classification_loss: 0.4761\n",
      "351/500 [====================>.........] - ETA: 2:33 - loss: 1.8736 - regression_loss: 1.3978 - classification_loss: 0.4758\n",
      "352/500 [====================>.........] - ETA: 2:32 - loss: 1.8746 - regression_loss: 1.3985 - classification_loss: 0.4761\n",
      "353/500 [====================>.........] - ETA: 2:31 - loss: 1.8740 - regression_loss: 1.3981 - classification_loss: 0.4759\n",
      "354/500 [====================>.........] - ETA: 2:30 - loss: 1.8739 - regression_loss: 1.3980 - classification_loss: 0.4759\n",
      "355/500 [====================>.........] - ETA: 2:28 - loss: 1.8766 - regression_loss: 1.3999 - classification_loss: 0.4767\n",
      "356/500 [====================>.........] - ETA: 2:27 - loss: 1.8774 - regression_loss: 1.4008 - classification_loss: 0.4766\n",
      "357/500 [====================>.........] - ETA: 2:26 - loss: 1.8776 - regression_loss: 1.4010 - classification_loss: 0.4767\n",
      "358/500 [====================>.........] - ETA: 2:25 - loss: 1.8792 - regression_loss: 1.4025 - classification_loss: 0.4767\n",
      "359/500 [====================>.........] - ETA: 2:24 - loss: 1.8783 - regression_loss: 1.4020 - classification_loss: 0.4763\n",
      "360/500 [====================>.........] - ETA: 2:23 - loss: 1.8765 - regression_loss: 1.4006 - classification_loss: 0.4759\n",
      "361/500 [====================>.........] - ETA: 2:22 - loss: 1.8769 - regression_loss: 1.4010 - classification_loss: 0.4759\n",
      "362/500 [====================>.........] - ETA: 2:21 - loss: 1.8753 - regression_loss: 1.3997 - classification_loss: 0.4756\n",
      "363/500 [====================>.........] - ETA: 2:20 - loss: 1.8750 - regression_loss: 1.3999 - classification_loss: 0.4751\n",
      "364/500 [====================>.........] - ETA: 2:19 - loss: 1.8747 - regression_loss: 1.3998 - classification_loss: 0.4750\n",
      "365/500 [====================>.........] - ETA: 2:18 - loss: 1.8741 - regression_loss: 1.3993 - classification_loss: 0.4747\n",
      "366/500 [====================>.........] - ETA: 2:17 - loss: 1.8744 - regression_loss: 1.3998 - classification_loss: 0.4747\n",
      "367/500 [=====================>........] - ETA: 2:16 - loss: 1.8732 - regression_loss: 1.3986 - classification_loss: 0.4747\n",
      "368/500 [=====================>........] - ETA: 2:15 - loss: 1.8734 - regression_loss: 1.3989 - classification_loss: 0.4746\n",
      "369/500 [=====================>........] - ETA: 2:14 - loss: 1.8739 - regression_loss: 1.3992 - classification_loss: 0.4747\n",
      "370/500 [=====================>........] - ETA: 2:13 - loss: 1.8738 - regression_loss: 1.3992 - classification_loss: 0.4746\n",
      "371/500 [=====================>........] - ETA: 2:12 - loss: 1.8724 - regression_loss: 1.3980 - classification_loss: 0.4744\n",
      "372/500 [=====================>........] - ETA: 2:11 - loss: 1.8719 - regression_loss: 1.3977 - classification_loss: 0.4742\n",
      "373/500 [=====================>........] - ETA: 2:10 - loss: 1.8714 - regression_loss: 1.3976 - classification_loss: 0.4739\n",
      "374/500 [=====================>........] - ETA: 2:09 - loss: 1.8699 - regression_loss: 1.3965 - classification_loss: 0.4734\n",
      "375/500 [=====================>........] - ETA: 2:08 - loss: 1.8702 - regression_loss: 1.3969 - classification_loss: 0.4733\n",
      "376/500 [=====================>........] - ETA: 2:07 - loss: 1.8716 - regression_loss: 1.3975 - classification_loss: 0.4741\n",
      "377/500 [=====================>........] - ETA: 2:06 - loss: 1.8712 - regression_loss: 1.3973 - classification_loss: 0.4739\n",
      "378/500 [=====================>........] - ETA: 2:05 - loss: 1.8707 - regression_loss: 1.3966 - classification_loss: 0.4742\n",
      "379/500 [=====================>........] - ETA: 2:04 - loss: 1.8701 - regression_loss: 1.3962 - classification_loss: 0.4739\n",
      "380/500 [=====================>........] - ETA: 2:03 - loss: 1.8698 - regression_loss: 1.3958 - classification_loss: 0.4740\n",
      "381/500 [=====================>........] - ETA: 2:02 - loss: 1.8687 - regression_loss: 1.3952 - classification_loss: 0.4736\n",
      "382/500 [=====================>........] - ETA: 2:01 - loss: 1.8678 - regression_loss: 1.3944 - classification_loss: 0.4734\n",
      "383/500 [=====================>........] - ETA: 2:00 - loss: 1.8712 - regression_loss: 1.3966 - classification_loss: 0.4746\n",
      "384/500 [======================>.......] - ETA: 1:59 - loss: 1.8711 - regression_loss: 1.3966 - classification_loss: 0.4744\n",
      "385/500 [======================>.......] - ETA: 1:58 - loss: 1.8696 - regression_loss: 1.3955 - classification_loss: 0.4741\n",
      "386/500 [======================>.......] - ETA: 1:57 - loss: 1.8702 - regression_loss: 1.3961 - classification_loss: 0.4741\n",
      "387/500 [======================>.......] - ETA: 1:56 - loss: 1.8703 - regression_loss: 1.3961 - classification_loss: 0.4742\n",
      "388/500 [======================>.......] - ETA: 1:55 - loss: 1.8721 - regression_loss: 1.3968 - classification_loss: 0.4753\n",
      "389/500 [======================>.......] - ETA: 1:54 - loss: 1.8724 - regression_loss: 1.3973 - classification_loss: 0.4752\n",
      "390/500 [======================>.......] - ETA: 1:53 - loss: 1.8722 - regression_loss: 1.3973 - classification_loss: 0.4749\n",
      "391/500 [======================>.......] - ETA: 1:52 - loss: 1.8727 - regression_loss: 1.3978 - classification_loss: 0.4749\n",
      "392/500 [======================>.......] - ETA: 1:51 - loss: 1.8727 - regression_loss: 1.3980 - classification_loss: 0.4747\n",
      "393/500 [======================>.......] - ETA: 1:50 - loss: 1.8712 - regression_loss: 1.3969 - classification_loss: 0.4744\n",
      "394/500 [======================>.......] - ETA: 1:49 - loss: 1.8704 - regression_loss: 1.3962 - classification_loss: 0.4742\n",
      "395/500 [======================>.......] - ETA: 1:47 - loss: 1.8683 - regression_loss: 1.3949 - classification_loss: 0.4734\n",
      "396/500 [======================>.......] - ETA: 1:46 - loss: 1.8692 - regression_loss: 1.3959 - classification_loss: 0.4733\n",
      "397/500 [======================>.......] - ETA: 1:45 - loss: 1.8710 - regression_loss: 1.3969 - classification_loss: 0.4741\n",
      "398/500 [======================>.......] - ETA: 1:44 - loss: 1.8696 - regression_loss: 1.3957 - classification_loss: 0.4739\n",
      "399/500 [======================>.......] - ETA: 1:43 - loss: 1.8688 - regression_loss: 1.3952 - classification_loss: 0.4737\n",
      "400/500 [=======================>......] - ETA: 1:42 - loss: 1.8693 - regression_loss: 1.3956 - classification_loss: 0.4738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401/500 [=======================>......] - ETA: 1:41 - loss: 1.8686 - regression_loss: 1.3949 - classification_loss: 0.4736\n",
      "402/500 [=======================>......] - ETA: 1:40 - loss: 1.8673 - regression_loss: 1.3941 - classification_loss: 0.4733\n",
      "403/500 [=======================>......] - ETA: 1:39 - loss: 1.8671 - regression_loss: 1.3939 - classification_loss: 0.4732\n",
      "404/500 [=======================>......] - ETA: 1:38 - loss: 1.8656 - regression_loss: 1.3928 - classification_loss: 0.4728\n",
      "405/500 [=======================>......] - ETA: 1:37 - loss: 1.8640 - regression_loss: 1.3915 - classification_loss: 0.4725\n",
      "406/500 [=======================>......] - ETA: 1:36 - loss: 1.8620 - regression_loss: 1.3899 - classification_loss: 0.4721\n",
      "407/500 [=======================>......] - ETA: 1:35 - loss: 1.8647 - regression_loss: 1.3917 - classification_loss: 0.4731\n",
      "408/500 [=======================>......] - ETA: 1:34 - loss: 1.8636 - regression_loss: 1.3908 - classification_loss: 0.4727\n",
      "409/500 [=======================>......] - ETA: 1:33 - loss: 1.8644 - regression_loss: 1.3912 - classification_loss: 0.4732\n",
      "410/500 [=======================>......] - ETA: 1:32 - loss: 1.8649 - regression_loss: 1.3918 - classification_loss: 0.4732\n",
      "411/500 [=======================>......] - ETA: 1:31 - loss: 1.8641 - regression_loss: 1.3909 - classification_loss: 0.4732\n",
      "412/500 [=======================>......] - ETA: 1:30 - loss: 1.8640 - regression_loss: 1.3910 - classification_loss: 0.4730\n",
      "413/500 [=======================>......] - ETA: 1:29 - loss: 1.8630 - regression_loss: 1.3903 - classification_loss: 0.4727\n",
      "414/500 [=======================>......] - ETA: 1:28 - loss: 1.8609 - regression_loss: 1.3887 - classification_loss: 0.4722\n",
      "415/500 [=======================>......] - ETA: 1:27 - loss: 1.8622 - regression_loss: 1.3896 - classification_loss: 0.4726\n",
      "416/500 [=======================>......] - ETA: 1:26 - loss: 1.8610 - regression_loss: 1.3888 - classification_loss: 0.4722\n",
      "417/500 [========================>.....] - ETA: 1:25 - loss: 1.8615 - regression_loss: 1.3888 - classification_loss: 0.4727\n",
      "418/500 [========================>.....] - ETA: 1:24 - loss: 1.8608 - regression_loss: 1.3880 - classification_loss: 0.4729\n",
      "419/500 [========================>.....] - ETA: 1:23 - loss: 1.8602 - regression_loss: 1.3876 - classification_loss: 0.4726\n",
      "420/500 [========================>.....] - ETA: 1:22 - loss: 1.8597 - regression_loss: 1.3874 - classification_loss: 0.4723\n",
      "421/500 [========================>.....] - ETA: 1:21 - loss: 1.8590 - regression_loss: 1.3866 - classification_loss: 0.4724\n",
      "422/500 [========================>.....] - ETA: 1:20 - loss: 1.8596 - regression_loss: 1.3867 - classification_loss: 0.4729\n",
      "423/500 [========================>.....] - ETA: 1:19 - loss: 1.8593 - regression_loss: 1.3866 - classification_loss: 0.4727\n",
      "424/500 [========================>.....] - ETA: 1:18 - loss: 1.8598 - regression_loss: 1.3870 - classification_loss: 0.4729\n",
      "425/500 [========================>.....] - ETA: 1:17 - loss: 1.8613 - regression_loss: 1.3883 - classification_loss: 0.4731\n",
      "426/500 [========================>.....] - ETA: 1:16 - loss: 1.8615 - regression_loss: 1.3887 - classification_loss: 0.4728\n",
      "427/500 [========================>.....] - ETA: 1:14 - loss: 1.8603 - regression_loss: 1.3880 - classification_loss: 0.4723\n",
      "428/500 [========================>.....] - ETA: 1:13 - loss: 1.8590 - regression_loss: 1.3871 - classification_loss: 0.4719\n",
      "429/500 [========================>.....] - ETA: 1:12 - loss: 1.8578 - regression_loss: 1.3863 - classification_loss: 0.4714\n",
      "430/500 [========================>.....] - ETA: 1:11 - loss: 1.8584 - regression_loss: 1.3869 - classification_loss: 0.4715\n",
      "431/500 [========================>.....] - ETA: 1:10 - loss: 1.8575 - regression_loss: 1.3864 - classification_loss: 0.4710\n",
      "432/500 [========================>.....] - ETA: 1:09 - loss: 1.8577 - regression_loss: 1.3866 - classification_loss: 0.4711\n",
      "433/500 [========================>.....] - ETA: 1:08 - loss: 1.8589 - regression_loss: 1.3876 - classification_loss: 0.4713\n",
      "434/500 [=========================>....] - ETA: 1:07 - loss: 1.8578 - regression_loss: 1.3870 - classification_loss: 0.4709\n",
      "435/500 [=========================>....] - ETA: 1:06 - loss: 1.8592 - regression_loss: 1.3882 - classification_loss: 0.4710\n",
      "436/500 [=========================>....] - ETA: 1:05 - loss: 1.8607 - regression_loss: 1.3892 - classification_loss: 0.4715\n",
      "437/500 [=========================>....] - ETA: 1:04 - loss: 1.8601 - regression_loss: 1.3891 - classification_loss: 0.4711\n",
      "438/500 [=========================>....] - ETA: 1:03 - loss: 1.8602 - regression_loss: 1.3891 - classification_loss: 0.4711\n",
      "439/500 [=========================>....] - ETA: 1:02 - loss: 1.8594 - regression_loss: 1.3887 - classification_loss: 0.4707\n",
      "440/500 [=========================>....] - ETA: 1:01 - loss: 1.8590 - regression_loss: 1.3884 - classification_loss: 0.4706\n",
      "441/500 [=========================>....] - ETA: 1:00 - loss: 1.8588 - regression_loss: 1.3884 - classification_loss: 0.4705\n",
      "442/500 [=========================>....] - ETA: 59s - loss: 1.8587 - regression_loss: 1.3885 - classification_loss: 0.4702 \n",
      "443/500 [=========================>....] - ETA: 58s - loss: 1.8612 - regression_loss: 1.3896 - classification_loss: 0.4716\n",
      "444/500 [=========================>....] - ETA: 57s - loss: 1.8605 - regression_loss: 1.3891 - classification_loss: 0.4714\n",
      "445/500 [=========================>....] - ETA: 56s - loss: 1.8617 - regression_loss: 1.3898 - classification_loss: 0.4720\n",
      "446/500 [=========================>....] - ETA: 55s - loss: 1.8621 - regression_loss: 1.3900 - classification_loss: 0.4721\n",
      "447/500 [=========================>....] - ETA: 54s - loss: 1.8637 - regression_loss: 1.3909 - classification_loss: 0.4728\n",
      "448/500 [=========================>....] - ETA: 53s - loss: 1.8645 - regression_loss: 1.3910 - classification_loss: 0.4735\n",
      "449/500 [=========================>....] - ETA: 52s - loss: 1.8636 - regression_loss: 1.3900 - classification_loss: 0.4736\n",
      "450/500 [==========================>...] - ETA: 51s - loss: 1.8632 - regression_loss: 1.3898 - classification_loss: 0.4734\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 1.8624 - regression_loss: 1.3894 - classification_loss: 0.4730\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 1.8626 - regression_loss: 1.3895 - classification_loss: 0.4731\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.8632 - regression_loss: 1.3900 - classification_loss: 0.4732\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.8634 - regression_loss: 1.3900 - classification_loss: 0.4734\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.8653 - regression_loss: 1.3917 - classification_loss: 0.4736\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.8655 - regression_loss: 1.3917 - classification_loss: 0.4737\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.8654 - regression_loss: 1.3918 - classification_loss: 0.4736\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.8635 - regression_loss: 1.3903 - classification_loss: 0.4731\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.8625 - regression_loss: 1.3896 - classification_loss: 0.4729\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.8628 - regression_loss: 1.3899 - classification_loss: 0.4728\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 1.8639 - regression_loss: 1.3908 - classification_loss: 0.4731\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 1.8636 - regression_loss: 1.3904 - classification_loss: 0.4731\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 1.8641 - regression_loss: 1.3904 - classification_loss: 0.4737\n",
      "464/500 [==========================>...] - ETA: 36s - loss: 1.8636 - regression_loss: 1.3900 - classification_loss: 0.4736\n",
      "465/500 [==========================>...] - ETA: 35s - loss: 1.8658 - regression_loss: 1.3918 - classification_loss: 0.4740\n",
      "466/500 [==========================>...] - ETA: 34s - loss: 1.8654 - regression_loss: 1.3919 - classification_loss: 0.4735\n",
      "467/500 [===========================>..] - ETA: 33s - loss: 1.8644 - regression_loss: 1.3911 - classification_loss: 0.4733\n",
      "468/500 [===========================>..] - ETA: 32s - loss: 1.8645 - regression_loss: 1.3913 - classification_loss: 0.4732\n",
      "469/500 [===========================>..] - ETA: 31s - loss: 1.8621 - regression_loss: 1.3895 - classification_loss: 0.4727\n",
      "470/500 [===========================>..] - ETA: 30s - loss: 1.8648 - regression_loss: 1.3911 - classification_loss: 0.4737\n",
      "471/500 [===========================>..] - ETA: 29s - loss: 1.8643 - regression_loss: 1.3910 - classification_loss: 0.4733\n",
      "472/500 [===========================>..] - ETA: 28s - loss: 1.8654 - regression_loss: 1.3922 - classification_loss: 0.4732\n",
      "473/500 [===========================>..] - ETA: 27s - loss: 1.8644 - regression_loss: 1.3914 - classification_loss: 0.4730\n",
      "474/500 [===========================>..] - ETA: 26s - loss: 1.8644 - regression_loss: 1.3912 - classification_loss: 0.4733\n",
      "475/500 [===========================>..] - ETA: 25s - loss: 1.8639 - regression_loss: 1.3906 - classification_loss: 0.4733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "476/500 [===========================>..] - ETA: 24s - loss: 1.8661 - regression_loss: 1.3918 - classification_loss: 0.4742\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.8672 - regression_loss: 1.3925 - classification_loss: 0.4747\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.8692 - regression_loss: 1.3939 - classification_loss: 0.4753\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.8709 - regression_loss: 1.3944 - classification_loss: 0.4765\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.8696 - regression_loss: 1.3935 - classification_loss: 0.4761\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.8707 - regression_loss: 1.3945 - classification_loss: 0.4762\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.8707 - regression_loss: 1.3947 - classification_loss: 0.4760\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.8690 - regression_loss: 1.3935 - classification_loss: 0.4755\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.8681 - regression_loss: 1.3927 - classification_loss: 0.4753\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.8688 - regression_loss: 1.3935 - classification_loss: 0.4753\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.8695 - regression_loss: 1.3942 - classification_loss: 0.4753\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.8694 - regression_loss: 1.3943 - classification_loss: 0.4751\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.8694 - regression_loss: 1.3940 - classification_loss: 0.4754\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.8716 - regression_loss: 1.3961 - classification_loss: 0.4755\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.8700 - regression_loss: 1.3950 - classification_loss: 0.4750\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.8690 - regression_loss: 1.3944 - classification_loss: 0.4745 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.8682 - regression_loss: 1.3938 - classification_loss: 0.4744\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.8691 - regression_loss: 1.3942 - classification_loss: 0.4749\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.8718 - regression_loss: 1.3960 - classification_loss: 0.4758\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.8720 - regression_loss: 1.3963 - classification_loss: 0.4757\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.8720 - regression_loss: 1.3965 - classification_loss: 0.4755\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.8735 - regression_loss: 1.3975 - classification_loss: 0.4760\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.8736 - regression_loss: 1.3978 - classification_loss: 0.4758\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.8729 - regression_loss: 1.3971 - classification_loss: 0.4757\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.8728 - regression_loss: 1.3971 - classification_loss: 0.4757\n",
      "Epoch 00026: saving model to ./snapshots\\resnet50_csv_26.h5\n",
      "\n",
      "500/500 [==============================] - 516s 1s/step - loss: 1.8728 - regression_loss: 1.3971 - classification_loss: 0.4757\n",
      "Epoch 27/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 1.5348 - regression_loss: 1.1777 - classification_loss: 0.3571\n",
      "  2/500 [..............................] - ETA: 4:29 - loss: 1.6810 - regression_loss: 1.2939 - classification_loss: 0.3871\n",
      "  3/500 [..............................] - ETA: 5:42 - loss: 1.8920 - regression_loss: 1.4375 - classification_loss: 0.4545\n",
      "  4/500 [..............................] - ETA: 6:33 - loss: 1.7243 - regression_loss: 1.3133 - classification_loss: 0.4110\n",
      "  5/500 [..............................] - ETA: 7:00 - loss: 1.8180 - regression_loss: 1.3852 - classification_loss: 0.4329\n",
      "  6/500 [..............................] - ETA: 7:10 - loss: 1.7964 - regression_loss: 1.3634 - classification_loss: 0.4330\n",
      "  7/500 [..............................] - ETA: 7:25 - loss: 1.8898 - regression_loss: 1.4306 - classification_loss: 0.4593\n",
      "  8/500 [..............................] - ETA: 7:29 - loss: 1.8772 - regression_loss: 1.4280 - classification_loss: 0.4492\n",
      "  9/500 [..............................] - ETA: 7:37 - loss: 1.9683 - regression_loss: 1.4797 - classification_loss: 0.4886\n",
      " 10/500 [..............................] - ETA: 7:39 - loss: 1.9065 - regression_loss: 1.4428 - classification_loss: 0.4637\n",
      " 11/500 [..............................] - ETA: 7:44 - loss: 1.8950 - regression_loss: 1.4239 - classification_loss: 0.4710\n",
      " 12/500 [..............................] - ETA: 7:45 - loss: 1.9465 - regression_loss: 1.4529 - classification_loss: 0.4936\n",
      " 13/500 [..............................] - ETA: 7:48 - loss: 1.9507 - regression_loss: 1.4720 - classification_loss: 0.4788\n",
      " 14/500 [..............................] - ETA: 7:44 - loss: 1.9741 - regression_loss: 1.4883 - classification_loss: 0.4858\n",
      " 15/500 [..............................] - ETA: 7:46 - loss: 1.9430 - regression_loss: 1.4604 - classification_loss: 0.4825\n",
      " 16/500 [..............................] - ETA: 7:53 - loss: 1.9102 - regression_loss: 1.4320 - classification_loss: 0.4782\n",
      " 17/500 [>.............................] - ETA: 7:54 - loss: 1.9766 - regression_loss: 1.4822 - classification_loss: 0.4944\n",
      " 18/500 [>.............................] - ETA: 7:47 - loss: 1.9665 - regression_loss: 1.4699 - classification_loss: 0.4966\n",
      " 19/500 [>.............................] - ETA: 7:48 - loss: 1.9566 - regression_loss: 1.4665 - classification_loss: 0.4901\n",
      " 20/500 [>.............................] - ETA: 7:51 - loss: 1.9493 - regression_loss: 1.4556 - classification_loss: 0.4937\n",
      " 21/500 [>.............................] - ETA: 7:50 - loss: 1.9356 - regression_loss: 1.4456 - classification_loss: 0.4900\n",
      " 22/500 [>.............................] - ETA: 7:51 - loss: 1.9226 - regression_loss: 1.4387 - classification_loss: 0.4839\n",
      " 23/500 [>.............................] - ETA: 7:50 - loss: 1.9135 - regression_loss: 1.4256 - classification_loss: 0.4879\n",
      " 24/500 [>.............................] - ETA: 7:49 - loss: 1.9156 - regression_loss: 1.4263 - classification_loss: 0.4893\n",
      " 25/500 [>.............................] - ETA: 7:50 - loss: 1.8991 - regression_loss: 1.4083 - classification_loss: 0.4908\n",
      " 26/500 [>.............................] - ETA: 7:51 - loss: 1.9113 - regression_loss: 1.4193 - classification_loss: 0.4920\n",
      " 27/500 [>.............................] - ETA: 7:50 - loss: 1.9530 - regression_loss: 1.4505 - classification_loss: 0.5026\n",
      " 28/500 [>.............................] - ETA: 7:51 - loss: 1.9455 - regression_loss: 1.4488 - classification_loss: 0.4966\n",
      " 29/500 [>.............................] - ETA: 7:48 - loss: 1.9354 - regression_loss: 1.4414 - classification_loss: 0.4940\n",
      " 30/500 [>.............................] - ETA: 7:49 - loss: 1.9451 - regression_loss: 1.4493 - classification_loss: 0.4957\n",
      " 31/500 [>.............................] - ETA: 7:48 - loss: 1.9243 - regression_loss: 1.4351 - classification_loss: 0.4893\n",
      " 32/500 [>.............................] - ETA: 7:47 - loss: 1.9210 - regression_loss: 1.4296 - classification_loss: 0.4914\n",
      " 33/500 [>.............................] - ETA: 7:47 - loss: 1.9010 - regression_loss: 1.4158 - classification_loss: 0.4852\n",
      " 34/500 [=>............................] - ETA: 7:47 - loss: 1.9008 - regression_loss: 1.4126 - classification_loss: 0.4882\n",
      " 35/500 [=>............................] - ETA: 7:46 - loss: 1.8964 - regression_loss: 1.4092 - classification_loss: 0.4872\n",
      " 36/500 [=>............................] - ETA: 7:42 - loss: 1.8886 - regression_loss: 1.4044 - classification_loss: 0.4842\n",
      " 37/500 [=>............................] - ETA: 7:43 - loss: 1.8951 - regression_loss: 1.4073 - classification_loss: 0.4878\n",
      " 38/500 [=>............................] - ETA: 7:42 - loss: 1.9006 - regression_loss: 1.4129 - classification_loss: 0.4877\n",
      " 39/500 [=>............................] - ETA: 7:41 - loss: 1.8798 - regression_loss: 1.3989 - classification_loss: 0.4809\n",
      " 40/500 [=>............................] - ETA: 7:39 - loss: 1.8749 - regression_loss: 1.3976 - classification_loss: 0.4773\n",
      " 41/500 [=>............................] - ETA: 7:41 - loss: 1.8768 - regression_loss: 1.4013 - classification_loss: 0.4755\n",
      " 42/500 [=>............................] - ETA: 7:40 - loss: 1.8853 - regression_loss: 1.4063 - classification_loss: 0.4790\n",
      " 43/500 [=>............................] - ETA: 7:39 - loss: 1.9008 - regression_loss: 1.4191 - classification_loss: 0.4817\n",
      " 44/500 [=>............................] - ETA: 7:39 - loss: 1.9215 - regression_loss: 1.4298 - classification_loss: 0.4917\n",
      " 45/500 [=>............................] - ETA: 7:39 - loss: 1.9357 - regression_loss: 1.4431 - classification_loss: 0.4926\n",
      " 46/500 [=>............................] - ETA: 7:37 - loss: 1.9492 - regression_loss: 1.4550 - classification_loss: 0.4942\n",
      " 47/500 [=>............................] - ETA: 7:36 - loss: 1.9311 - regression_loss: 1.4409 - classification_loss: 0.4902\n",
      " 48/500 [=>............................] - ETA: 7:36 - loss: 1.9166 - regression_loss: 1.4297 - classification_loss: 0.4869\n",
      " 49/500 [=>............................] - ETA: 7:36 - loss: 1.9195 - regression_loss: 1.4324 - classification_loss: 0.4871\n",
      " 50/500 [==>...........................] - ETA: 7:35 - loss: 1.9069 - regression_loss: 1.4235 - classification_loss: 0.4833\n",
      " 51/500 [==>...........................] - ETA: 7:35 - loss: 1.9051 - regression_loss: 1.4239 - classification_loss: 0.4813\n",
      " 52/500 [==>...........................] - ETA: 7:34 - loss: 1.9054 - regression_loss: 1.4246 - classification_loss: 0.4808\n",
      " 53/500 [==>...........................] - ETA: 7:33 - loss: 1.8869 - regression_loss: 1.4115 - classification_loss: 0.4754\n",
      " 54/500 [==>...........................] - ETA: 7:32 - loss: 1.8773 - regression_loss: 1.4047 - classification_loss: 0.4726\n",
      " 55/500 [==>...........................] - ETA: 7:29 - loss: 1.8749 - regression_loss: 1.3998 - classification_loss: 0.4751\n",
      " 56/500 [==>...........................] - ETA: 7:28 - loss: 1.8588 - regression_loss: 1.3864 - classification_loss: 0.4724\n",
      " 57/500 [==>...........................] - ETA: 7:27 - loss: 1.8609 - regression_loss: 1.3904 - classification_loss: 0.4705\n",
      " 58/500 [==>...........................] - ETA: 7:26 - loss: 1.8575 - regression_loss: 1.3880 - classification_loss: 0.4695\n",
      " 59/500 [==>...........................] - ETA: 7:25 - loss: 1.8593 - regression_loss: 1.3901 - classification_loss: 0.4692\n",
      " 60/500 [==>...........................] - ETA: 7:42 - loss: 1.8514 - regression_loss: 1.3851 - classification_loss: 0.4663\n",
      " 61/500 [==>...........................] - ETA: 7:42 - loss: 1.8450 - regression_loss: 1.3808 - classification_loss: 0.4642\n",
      " 62/500 [==>...........................] - ETA: 7:40 - loss: 1.8529 - regression_loss: 1.3883 - classification_loss: 0.4647\n",
      " 63/500 [==>...........................] - ETA: 7:39 - loss: 1.8494 - regression_loss: 1.3858 - classification_loss: 0.4636\n",
      " 64/500 [==>...........................] - ETA: 7:37 - loss: 1.8689 - regression_loss: 1.3997 - classification_loss: 0.4692\n",
      " 65/500 [==>...........................] - ETA: 7:36 - loss: 1.8663 - regression_loss: 1.4004 - classification_loss: 0.4659\n",
      " 66/500 [==>...........................] - ETA: 7:35 - loss: 1.8610 - regression_loss: 1.3978 - classification_loss: 0.4632\n",
      " 67/500 [===>..........................] - ETA: 7:33 - loss: 1.8644 - regression_loss: 1.4009 - classification_loss: 0.4635\n",
      " 68/500 [===>..........................] - ETA: 7:33 - loss: 1.8601 - regression_loss: 1.3955 - classification_loss: 0.4646\n",
      " 69/500 [===>..........................] - ETA: 7:31 - loss: 1.8548 - regression_loss: 1.3925 - classification_loss: 0.4624\n",
      " 70/500 [===>..........................] - ETA: 7:30 - loss: 1.8547 - regression_loss: 1.3924 - classification_loss: 0.4623\n",
      " 71/500 [===>..........................] - ETA: 7:29 - loss: 1.8626 - regression_loss: 1.3976 - classification_loss: 0.4650\n",
      " 72/500 [===>..........................] - ETA: 7:28 - loss: 1.8665 - regression_loss: 1.4022 - classification_loss: 0.4642\n",
      " 73/500 [===>..........................] - ETA: 7:27 - loss: 1.8776 - regression_loss: 1.4081 - classification_loss: 0.4695\n",
      " 74/500 [===>..........................] - ETA: 7:26 - loss: 1.8746 - regression_loss: 1.4040 - classification_loss: 0.4706\n",
      " 75/500 [===>..........................] - ETA: 7:25 - loss: 1.8809 - regression_loss: 1.4088 - classification_loss: 0.4721\n",
      " 76/500 [===>..........................] - ETA: 7:24 - loss: 1.8757 - regression_loss: 1.4051 - classification_loss: 0.4706\n",
      " 77/500 [===>..........................] - ETA: 7:22 - loss: 1.8686 - regression_loss: 1.4003 - classification_loss: 0.4683\n",
      " 78/500 [===>..........................] - ETA: 7:21 - loss: 1.8883 - regression_loss: 1.4151 - classification_loss: 0.4733\n",
      " 79/500 [===>..........................] - ETA: 7:21 - loss: 1.8817 - regression_loss: 1.4099 - classification_loss: 0.4717\n",
      " 80/500 [===>..........................] - ETA: 7:19 - loss: 1.8819 - regression_loss: 1.4099 - classification_loss: 0.4720\n",
      " 81/500 [===>..........................] - ETA: 7:18 - loss: 1.8754 - regression_loss: 1.4059 - classification_loss: 0.4695\n",
      " 82/500 [===>..........................] - ETA: 7:17 - loss: 1.8692 - regression_loss: 1.4018 - classification_loss: 0.4673\n",
      " 83/500 [===>..........................] - ETA: 7:16 - loss: 1.8662 - regression_loss: 1.3996 - classification_loss: 0.4666\n",
      " 84/500 [====>.........................] - ETA: 7:15 - loss: 1.8629 - regression_loss: 1.3979 - classification_loss: 0.4650\n",
      " 85/500 [====>.........................] - ETA: 7:14 - loss: 1.8615 - regression_loss: 1.3985 - classification_loss: 0.4630\n",
      " 86/500 [====>.........................] - ETA: 7:13 - loss: 1.8627 - regression_loss: 1.4003 - classification_loss: 0.4623\n",
      " 87/500 [====>.........................] - ETA: 7:12 - loss: 1.8649 - regression_loss: 1.4015 - classification_loss: 0.4634\n",
      " 88/500 [====>.........................] - ETA: 7:11 - loss: 1.8629 - regression_loss: 1.4000 - classification_loss: 0.4629\n",
      " 89/500 [====>.........................] - ETA: 7:10 - loss: 1.8551 - regression_loss: 1.3939 - classification_loss: 0.4612\n",
      " 90/500 [====>.........................] - ETA: 7:09 - loss: 1.8568 - regression_loss: 1.3963 - classification_loss: 0.4606\n",
      " 91/500 [====>.........................] - ETA: 7:08 - loss: 1.8556 - regression_loss: 1.3953 - classification_loss: 0.4603\n",
      " 92/500 [====>.........................] - ETA: 7:07 - loss: 1.8524 - regression_loss: 1.3937 - classification_loss: 0.4587\n",
      " 93/500 [====>.........................] - ETA: 7:06 - loss: 1.8453 - regression_loss: 1.3881 - classification_loss: 0.4573\n",
      " 94/500 [====>.........................] - ETA: 7:05 - loss: 1.8485 - regression_loss: 1.3893 - classification_loss: 0.4592\n",
      " 95/500 [====>.........................] - ETA: 7:03 - loss: 1.8494 - regression_loss: 1.3911 - classification_loss: 0.4583\n",
      " 96/500 [====>.........................] - ETA: 7:02 - loss: 1.8504 - regression_loss: 1.3914 - classification_loss: 0.4590\n",
      " 97/500 [====>.........................] - ETA: 7:01 - loss: 1.8497 - regression_loss: 1.3920 - classification_loss: 0.4577\n",
      " 98/500 [====>.........................] - ETA: 7:00 - loss: 1.8573 - regression_loss: 1.3949 - classification_loss: 0.4624\n",
      " 99/500 [====>.........................] - ETA: 6:58 - loss: 1.8553 - regression_loss: 1.3943 - classification_loss: 0.4609\n",
      "100/500 [=====>........................] - ETA: 6:57 - loss: 1.8617 - regression_loss: 1.3979 - classification_loss: 0.4638\n",
      "101/500 [=====>........................] - ETA: 6:56 - loss: 1.8593 - regression_loss: 1.3963 - classification_loss: 0.4629\n",
      "102/500 [=====>........................] - ETA: 6:55 - loss: 1.8552 - regression_loss: 1.3940 - classification_loss: 0.4612\n",
      "103/500 [=====>........................] - ETA: 6:54 - loss: 1.8522 - regression_loss: 1.3914 - classification_loss: 0.4608\n",
      "104/500 [=====>........................] - ETA: 6:53 - loss: 1.8533 - regression_loss: 1.3913 - classification_loss: 0.4621\n",
      "105/500 [=====>........................] - ETA: 6:52 - loss: 1.8592 - regression_loss: 1.3960 - classification_loss: 0.4632\n",
      "106/500 [=====>........................] - ETA: 6:51 - loss: 1.8636 - regression_loss: 1.4008 - classification_loss: 0.4628\n",
      "107/500 [=====>........................] - ETA: 6:50 - loss: 1.8689 - regression_loss: 1.4050 - classification_loss: 0.4639\n",
      "108/500 [=====>........................] - ETA: 6:49 - loss: 1.8730 - regression_loss: 1.4091 - classification_loss: 0.4639\n",
      "109/500 [=====>........................] - ETA: 6:49 - loss: 1.8673 - regression_loss: 1.4054 - classification_loss: 0.4618\n",
      "110/500 [=====>........................] - ETA: 6:48 - loss: 1.8598 - regression_loss: 1.4003 - classification_loss: 0.4595\n",
      "111/500 [=====>........................] - ETA: 6:46 - loss: 1.8606 - regression_loss: 1.4001 - classification_loss: 0.4605\n",
      "112/500 [=====>........................] - ETA: 6:45 - loss: 1.8553 - regression_loss: 1.3970 - classification_loss: 0.4583\n",
      "113/500 [=====>........................] - ETA: 6:44 - loss: 1.8547 - regression_loss: 1.3960 - classification_loss: 0.4587\n",
      "114/500 [=====>........................] - ETA: 6:43 - loss: 1.8504 - regression_loss: 1.3933 - classification_loss: 0.4572\n",
      "115/500 [=====>........................] - ETA: 6:42 - loss: 1.8501 - regression_loss: 1.3927 - classification_loss: 0.4574\n",
      "116/500 [=====>........................] - ETA: 6:41 - loss: 1.8437 - regression_loss: 1.3877 - classification_loss: 0.4560\n",
      "117/500 [======>.......................] - ETA: 6:40 - loss: 1.8516 - regression_loss: 1.3934 - classification_loss: 0.4582\n",
      "118/500 [======>.......................] - ETA: 6:39 - loss: 1.8487 - regression_loss: 1.3917 - classification_loss: 0.4569\n",
      "119/500 [======>.......................] - ETA: 6:37 - loss: 1.8559 - regression_loss: 1.3977 - classification_loss: 0.4582\n",
      "120/500 [======>.......................] - ETA: 6:36 - loss: 1.8577 - regression_loss: 1.3984 - classification_loss: 0.4593\n",
      "121/500 [======>.......................] - ETA: 6:35 - loss: 1.8529 - regression_loss: 1.3947 - classification_loss: 0.4582\n",
      "122/500 [======>.......................] - ETA: 6:34 - loss: 1.8526 - regression_loss: 1.3953 - classification_loss: 0.4574\n",
      "123/500 [======>.......................] - ETA: 6:33 - loss: 1.8564 - regression_loss: 1.3976 - classification_loss: 0.4588\n",
      "124/500 [======>.......................] - ETA: 6:32 - loss: 1.8615 - regression_loss: 1.4011 - classification_loss: 0.4604\n",
      "125/500 [======>.......................] - ETA: 6:31 - loss: 1.8623 - regression_loss: 1.4016 - classification_loss: 0.4606\n",
      "126/500 [======>.......................] - ETA: 6:30 - loss: 1.8630 - regression_loss: 1.4037 - classification_loss: 0.4594\n",
      "127/500 [======>.......................] - ETA: 6:29 - loss: 1.8592 - regression_loss: 1.4002 - classification_loss: 0.4590\n",
      "128/500 [======>.......................] - ETA: 6:28 - loss: 1.8581 - regression_loss: 1.3990 - classification_loss: 0.4591\n",
      "129/500 [======>.......................] - ETA: 6:27 - loss: 1.8629 - regression_loss: 1.4014 - classification_loss: 0.4614\n",
      "130/500 [======>.......................] - ETA: 6:26 - loss: 1.8602 - regression_loss: 1.4000 - classification_loss: 0.4602\n",
      "131/500 [======>.......................] - ETA: 6:25 - loss: 1.8607 - regression_loss: 1.4010 - classification_loss: 0.4597\n",
      "132/500 [======>.......................] - ETA: 6:23 - loss: 1.8638 - regression_loss: 1.4049 - classification_loss: 0.4588\n",
      "133/500 [======>.......................] - ETA: 6:22 - loss: 1.8629 - regression_loss: 1.4042 - classification_loss: 0.4587\n",
      "134/500 [=======>......................] - ETA: 6:21 - loss: 1.8608 - regression_loss: 1.4031 - classification_loss: 0.4577\n",
      "135/500 [=======>......................] - ETA: 6:20 - loss: 1.8624 - regression_loss: 1.4041 - classification_loss: 0.4583\n",
      "136/500 [=======>......................] - ETA: 6:19 - loss: 1.8593 - regression_loss: 1.4021 - classification_loss: 0.4571\n",
      "137/500 [=======>......................] - ETA: 6:18 - loss: 1.8585 - regression_loss: 1.4016 - classification_loss: 0.4570\n",
      "138/500 [=======>......................] - ETA: 6:17 - loss: 1.8622 - regression_loss: 1.4042 - classification_loss: 0.4579\n",
      "139/500 [=======>......................] - ETA: 6:16 - loss: 1.8633 - regression_loss: 1.4045 - classification_loss: 0.4587\n",
      "140/500 [=======>......................] - ETA: 6:15 - loss: 1.8704 - regression_loss: 1.4080 - classification_loss: 0.4624\n",
      "141/500 [=======>......................] - ETA: 6:14 - loss: 1.8677 - regression_loss: 1.4054 - classification_loss: 0.4623\n",
      "142/500 [=======>......................] - ETA: 6:13 - loss: 1.8736 - regression_loss: 1.4099 - classification_loss: 0.4637\n",
      "143/500 [=======>......................] - ETA: 6:12 - loss: 1.8757 - regression_loss: 1.4115 - classification_loss: 0.4641\n",
      "144/500 [=======>......................] - ETA: 6:11 - loss: 1.8814 - regression_loss: 1.4155 - classification_loss: 0.4658\n",
      "145/500 [=======>......................] - ETA: 6:10 - loss: 1.8844 - regression_loss: 1.4177 - classification_loss: 0.4667\n",
      "146/500 [=======>......................] - ETA: 6:09 - loss: 1.8886 - regression_loss: 1.4201 - classification_loss: 0.4685\n",
      "147/500 [=======>......................] - ETA: 6:08 - loss: 1.8908 - regression_loss: 1.4222 - classification_loss: 0.4686\n",
      "148/500 [=======>......................] - ETA: 6:07 - loss: 1.8881 - regression_loss: 1.4200 - classification_loss: 0.4681\n",
      "149/500 [=======>......................] - ETA: 6:06 - loss: 1.8950 - regression_loss: 1.4249 - classification_loss: 0.4701\n",
      "150/500 [========>.....................] - ETA: 6:05 - loss: 1.8883 - regression_loss: 1.4192 - classification_loss: 0.4691\n",
      "151/500 [========>.....................] - ETA: 6:04 - loss: 1.8879 - regression_loss: 1.4196 - classification_loss: 0.4683\n",
      "152/500 [========>.....................] - ETA: 6:03 - loss: 1.8845 - regression_loss: 1.4174 - classification_loss: 0.4671\n",
      "153/500 [========>.....................] - ETA: 6:02 - loss: 1.8830 - regression_loss: 1.4157 - classification_loss: 0.4673\n",
      "154/500 [========>.....................] - ETA: 6:01 - loss: 1.8844 - regression_loss: 1.4161 - classification_loss: 0.4682\n",
      "155/500 [========>.....................] - ETA: 5:59 - loss: 1.8787 - regression_loss: 1.4118 - classification_loss: 0.4669\n",
      "156/500 [========>.....................] - ETA: 5:58 - loss: 1.8806 - regression_loss: 1.4133 - classification_loss: 0.4673\n",
      "157/500 [========>.....................] - ETA: 5:57 - loss: 1.8798 - regression_loss: 1.4127 - classification_loss: 0.4672\n",
      "158/500 [========>.....................] - ETA: 5:56 - loss: 1.8749 - regression_loss: 1.4092 - classification_loss: 0.4657\n",
      "159/500 [========>.....................] - ETA: 5:55 - loss: 1.8807 - regression_loss: 1.4135 - classification_loss: 0.4672\n",
      "160/500 [========>.....................] - ETA: 5:54 - loss: 1.8814 - regression_loss: 1.4129 - classification_loss: 0.4684\n",
      "161/500 [========>.....................] - ETA: 5:53 - loss: 1.8815 - regression_loss: 1.4134 - classification_loss: 0.4680\n",
      "162/500 [========>.....................] - ETA: 5:52 - loss: 1.8831 - regression_loss: 1.4138 - classification_loss: 0.4693\n",
      "163/500 [========>.....................] - ETA: 5:51 - loss: 1.8799 - regression_loss: 1.4108 - classification_loss: 0.4691\n",
      "164/500 [========>.....................] - ETA: 5:50 - loss: 1.8875 - regression_loss: 1.4158 - classification_loss: 0.4717\n",
      "165/500 [========>.....................] - ETA: 5:49 - loss: 1.8881 - regression_loss: 1.4167 - classification_loss: 0.4714\n",
      "166/500 [========>.....................] - ETA: 5:47 - loss: 1.8866 - regression_loss: 1.4165 - classification_loss: 0.4702\n",
      "167/500 [=========>....................] - ETA: 5:46 - loss: 1.8891 - regression_loss: 1.4177 - classification_loss: 0.4714\n",
      "168/500 [=========>....................] - ETA: 5:45 - loss: 1.8910 - regression_loss: 1.4192 - classification_loss: 0.4718\n",
      "169/500 [=========>....................] - ETA: 5:44 - loss: 1.8864 - regression_loss: 1.4158 - classification_loss: 0.4706\n",
      "170/500 [=========>....................] - ETA: 5:43 - loss: 1.8819 - regression_loss: 1.4127 - classification_loss: 0.4692\n",
      "171/500 [=========>....................] - ETA: 5:42 - loss: 1.8828 - regression_loss: 1.4122 - classification_loss: 0.4706\n",
      "172/500 [=========>....................] - ETA: 5:41 - loss: 1.8775 - regression_loss: 1.4082 - classification_loss: 0.4693\n",
      "173/500 [=========>....................] - ETA: 5:40 - loss: 1.8811 - regression_loss: 1.4108 - classification_loss: 0.4703\n",
      "174/500 [=========>....................] - ETA: 5:39 - loss: 1.8772 - regression_loss: 1.4077 - classification_loss: 0.4696\n",
      "175/500 [=========>....................] - ETA: 5:38 - loss: 1.8769 - regression_loss: 1.4068 - classification_loss: 0.4701\n",
      "176/500 [=========>....................] - ETA: 5:37 - loss: 1.8765 - regression_loss: 1.4065 - classification_loss: 0.4700\n",
      "177/500 [=========>....................] - ETA: 5:36 - loss: 1.8792 - regression_loss: 1.4082 - classification_loss: 0.4710\n",
      "178/500 [=========>....................] - ETA: 5:35 - loss: 1.8824 - regression_loss: 1.4101 - classification_loss: 0.4723\n",
      "179/500 [=========>....................] - ETA: 5:33 - loss: 1.8843 - regression_loss: 1.4110 - classification_loss: 0.4733\n",
      "180/500 [=========>....................] - ETA: 5:32 - loss: 1.8851 - regression_loss: 1.4120 - classification_loss: 0.4731\n",
      "181/500 [=========>....................] - ETA: 5:31 - loss: 1.8846 - regression_loss: 1.4110 - classification_loss: 0.4736\n",
      "182/500 [=========>....................] - ETA: 5:30 - loss: 1.8813 - regression_loss: 1.4085 - classification_loss: 0.4728\n",
      "183/500 [=========>....................] - ETA: 5:29 - loss: 1.8841 - regression_loss: 1.4105 - classification_loss: 0.4737\n",
      "184/500 [==========>...................] - ETA: 5:28 - loss: 1.8871 - regression_loss: 1.4130 - classification_loss: 0.4741\n",
      "185/500 [==========>...................] - ETA: 5:27 - loss: 1.8877 - regression_loss: 1.4137 - classification_loss: 0.4740\n",
      "186/500 [==========>...................] - ETA: 5:26 - loss: 1.8910 - regression_loss: 1.4170 - classification_loss: 0.4740\n",
      "187/500 [==========>...................] - ETA: 5:25 - loss: 1.8912 - regression_loss: 1.4172 - classification_loss: 0.4740\n",
      "188/500 [==========>...................] - ETA: 5:24 - loss: 1.8939 - regression_loss: 1.4198 - classification_loss: 0.4741\n",
      "189/500 [==========>...................] - ETA: 5:23 - loss: 1.8954 - regression_loss: 1.4205 - classification_loss: 0.4749\n",
      "190/500 [==========>...................] - ETA: 5:22 - loss: 1.8939 - regression_loss: 1.4189 - classification_loss: 0.4750\n",
      "191/500 [==========>...................] - ETA: 5:21 - loss: 1.8942 - regression_loss: 1.4198 - classification_loss: 0.4744\n",
      "192/500 [==========>...................] - ETA: 5:21 - loss: 1.8922 - regression_loss: 1.4177 - classification_loss: 0.4745\n",
      "193/500 [==========>...................] - ETA: 5:19 - loss: 1.8881 - regression_loss: 1.4146 - classification_loss: 0.4735\n",
      "194/500 [==========>...................] - ETA: 5:18 - loss: 1.8915 - regression_loss: 1.4174 - classification_loss: 0.4741\n",
      "195/500 [==========>...................] - ETA: 5:18 - loss: 1.8952 - regression_loss: 1.4189 - classification_loss: 0.4763\n",
      "196/500 [==========>...................] - ETA: 5:16 - loss: 1.8959 - regression_loss: 1.4187 - classification_loss: 0.4772\n",
      "197/500 [==========>...................] - ETA: 5:15 - loss: 1.8915 - regression_loss: 1.4152 - classification_loss: 0.4763\n",
      "198/500 [==========>...................] - ETA: 5:14 - loss: 1.8911 - regression_loss: 1.4141 - classification_loss: 0.4770\n",
      "199/500 [==========>...................] - ETA: 5:13 - loss: 1.8878 - regression_loss: 1.4116 - classification_loss: 0.4762\n",
      "200/500 [===========>..................] - ETA: 5:12 - loss: 1.8890 - regression_loss: 1.4130 - classification_loss: 0.4760\n",
      "201/500 [===========>..................] - ETA: 5:11 - loss: 1.8888 - regression_loss: 1.4131 - classification_loss: 0.4757\n",
      "202/500 [===========>..................] - ETA: 5:10 - loss: 1.8923 - regression_loss: 1.4153 - classification_loss: 0.4770\n",
      "203/500 [===========>..................] - ETA: 5:09 - loss: 1.8953 - regression_loss: 1.4179 - classification_loss: 0.4774\n",
      "204/500 [===========>..................] - ETA: 5:08 - loss: 1.8976 - regression_loss: 1.4197 - classification_loss: 0.4780\n",
      "205/500 [===========>..................] - ETA: 5:07 - loss: 1.8965 - regression_loss: 1.4189 - classification_loss: 0.4776\n",
      "206/500 [===========>..................] - ETA: 5:06 - loss: 1.8925 - regression_loss: 1.4158 - classification_loss: 0.4767\n",
      "207/500 [===========>..................] - ETA: 5:05 - loss: 1.8945 - regression_loss: 1.4178 - classification_loss: 0.4766\n",
      "208/500 [===========>..................] - ETA: 5:03 - loss: 1.8926 - regression_loss: 1.4166 - classification_loss: 0.4759\n",
      "209/500 [===========>..................] - ETA: 5:02 - loss: 1.8948 - regression_loss: 1.4186 - classification_loss: 0.4762\n",
      "210/500 [===========>..................] - ETA: 5:01 - loss: 1.8982 - regression_loss: 1.4216 - classification_loss: 0.4766\n",
      "211/500 [===========>..................] - ETA: 5:00 - loss: 1.8992 - regression_loss: 1.4216 - classification_loss: 0.4776\n",
      "212/500 [===========>..................] - ETA: 4:59 - loss: 1.8986 - regression_loss: 1.4211 - classification_loss: 0.4775\n",
      "213/500 [===========>..................] - ETA: 4:58 - loss: 1.9003 - regression_loss: 1.4226 - classification_loss: 0.4776\n",
      "214/500 [===========>..................] - ETA: 4:57 - loss: 1.9039 - regression_loss: 1.4250 - classification_loss: 0.4789\n",
      "215/500 [===========>..................] - ETA: 4:56 - loss: 1.9045 - regression_loss: 1.4259 - classification_loss: 0.4787\n",
      "216/500 [===========>..................] - ETA: 4:55 - loss: 1.9093 - regression_loss: 1.4299 - classification_loss: 0.4794\n",
      "217/500 [============>.................] - ETA: 4:54 - loss: 1.9134 - regression_loss: 1.4314 - classification_loss: 0.4820\n",
      "218/500 [============>.................] - ETA: 4:53 - loss: 1.9139 - regression_loss: 1.4322 - classification_loss: 0.4817\n",
      "219/500 [============>.................] - ETA: 4:52 - loss: 1.9156 - regression_loss: 1.4337 - classification_loss: 0.4819\n",
      "220/500 [============>.................] - ETA: 4:51 - loss: 1.9145 - regression_loss: 1.4325 - classification_loss: 0.4820\n",
      "221/500 [============>.................] - ETA: 4:50 - loss: 1.9191 - regression_loss: 1.4357 - classification_loss: 0.4834\n",
      "222/500 [============>.................] - ETA: 4:49 - loss: 1.9161 - regression_loss: 1.4330 - classification_loss: 0.4831\n",
      "223/500 [============>.................] - ETA: 4:48 - loss: 1.9160 - regression_loss: 1.4322 - classification_loss: 0.4838\n",
      "224/500 [============>.................] - ETA: 4:47 - loss: 1.9167 - regression_loss: 1.4325 - classification_loss: 0.4842\n",
      "225/500 [============>.................] - ETA: 4:46 - loss: 1.9133 - regression_loss: 1.4299 - classification_loss: 0.4834\n",
      "226/500 [============>.................] - ETA: 4:45 - loss: 1.9138 - regression_loss: 1.4304 - classification_loss: 0.4834\n",
      "227/500 [============>.................] - ETA: 4:44 - loss: 1.9136 - regression_loss: 1.4304 - classification_loss: 0.4832\n",
      "228/500 [============>.................] - ETA: 4:43 - loss: 1.9173 - regression_loss: 1.4319 - classification_loss: 0.4853\n",
      "229/500 [============>.................] - ETA: 4:42 - loss: 1.9172 - regression_loss: 1.4318 - classification_loss: 0.4854\n",
      "230/500 [============>.................] - ETA: 4:41 - loss: 1.9196 - regression_loss: 1.4336 - classification_loss: 0.4860\n",
      "231/500 [============>.................] - ETA: 4:40 - loss: 1.9172 - regression_loss: 1.4315 - classification_loss: 0.4857\n",
      "232/500 [============>.................] - ETA: 4:39 - loss: 1.9175 - regression_loss: 1.4323 - classification_loss: 0.4851\n",
      "233/500 [============>.................] - ETA: 4:38 - loss: 1.9169 - regression_loss: 1.4321 - classification_loss: 0.4848\n",
      "234/500 [=============>................] - ETA: 4:37 - loss: 1.9201 - regression_loss: 1.4340 - classification_loss: 0.4860\n",
      "235/500 [=============>................] - ETA: 4:36 - loss: 1.9225 - regression_loss: 1.4356 - classification_loss: 0.4869\n",
      "236/500 [=============>................] - ETA: 4:35 - loss: 1.9203 - regression_loss: 1.4339 - classification_loss: 0.4863\n",
      "237/500 [=============>................] - ETA: 4:34 - loss: 1.9211 - regression_loss: 1.4341 - classification_loss: 0.4871\n",
      "238/500 [=============>................] - ETA: 4:33 - loss: 1.9225 - regression_loss: 1.4354 - classification_loss: 0.4871\n",
      "239/500 [=============>................] - ETA: 4:32 - loss: 1.9210 - regression_loss: 1.4344 - classification_loss: 0.4866\n",
      "240/500 [=============>................] - ETA: 4:30 - loss: 1.9198 - regression_loss: 1.4330 - classification_loss: 0.4868\n",
      "241/500 [=============>................] - ETA: 4:29 - loss: 1.9170 - regression_loss: 1.4308 - classification_loss: 0.4862\n",
      "242/500 [=============>................] - ETA: 4:28 - loss: 1.9198 - regression_loss: 1.4329 - classification_loss: 0.4870\n",
      "243/500 [=============>................] - ETA: 4:27 - loss: 1.9214 - regression_loss: 1.4346 - classification_loss: 0.4868\n",
      "244/500 [=============>................] - ETA: 4:26 - loss: 1.9221 - regression_loss: 1.4352 - classification_loss: 0.4869\n",
      "245/500 [=============>................] - ETA: 4:25 - loss: 1.9237 - regression_loss: 1.4368 - classification_loss: 0.4869\n",
      "246/500 [=============>................] - ETA: 4:24 - loss: 1.9290 - regression_loss: 1.4401 - classification_loss: 0.4889\n",
      "247/500 [=============>................] - ETA: 4:23 - loss: 1.9260 - regression_loss: 1.4383 - classification_loss: 0.4877\n",
      "248/500 [=============>................] - ETA: 4:22 - loss: 1.9249 - regression_loss: 1.4377 - classification_loss: 0.4872\n",
      "249/500 [=============>................] - ETA: 4:21 - loss: 1.9222 - regression_loss: 1.4358 - classification_loss: 0.4864\n",
      "250/500 [==============>...............] - ETA: 4:20 - loss: 1.9214 - regression_loss: 1.4353 - classification_loss: 0.4861\n",
      "251/500 [==============>...............] - ETA: 4:19 - loss: 1.9215 - regression_loss: 1.4355 - classification_loss: 0.4860\n",
      "252/500 [==============>...............] - ETA: 4:18 - loss: 1.9212 - regression_loss: 1.4357 - classification_loss: 0.4855\n",
      "253/500 [==============>...............] - ETA: 4:16 - loss: 1.9223 - regression_loss: 1.4366 - classification_loss: 0.4857\n",
      "254/500 [==============>...............] - ETA: 4:15 - loss: 1.9213 - regression_loss: 1.4357 - classification_loss: 0.4856\n",
      "255/500 [==============>...............] - ETA: 4:14 - loss: 1.9229 - regression_loss: 1.4374 - classification_loss: 0.4855\n",
      "256/500 [==============>...............] - ETA: 4:13 - loss: 1.9248 - regression_loss: 1.4390 - classification_loss: 0.4858\n",
      "257/500 [==============>...............] - ETA: 4:12 - loss: 1.9229 - regression_loss: 1.4375 - classification_loss: 0.4854\n",
      "258/500 [==============>...............] - ETA: 4:11 - loss: 1.9280 - regression_loss: 1.4409 - classification_loss: 0.4872\n",
      "259/500 [==============>...............] - ETA: 4:10 - loss: 1.9283 - regression_loss: 1.4409 - classification_loss: 0.4874\n",
      "260/500 [==============>...............] - ETA: 4:09 - loss: 1.9255 - regression_loss: 1.4389 - classification_loss: 0.4866\n",
      "261/500 [==============>...............] - ETA: 4:08 - loss: 1.9238 - regression_loss: 1.4381 - classification_loss: 0.4857\n",
      "262/500 [==============>...............] - ETA: 4:07 - loss: 1.9237 - regression_loss: 1.4381 - classification_loss: 0.4856\n",
      "263/500 [==============>...............] - ETA: 4:06 - loss: 1.9242 - regression_loss: 1.4383 - classification_loss: 0.4859\n",
      "264/500 [==============>...............] - ETA: 4:05 - loss: 1.9234 - regression_loss: 1.4378 - classification_loss: 0.4856\n",
      "265/500 [==============>...............] - ETA: 4:04 - loss: 1.9212 - regression_loss: 1.4364 - classification_loss: 0.4848\n",
      "266/500 [==============>...............] - ETA: 4:03 - loss: 1.9228 - regression_loss: 1.4367 - classification_loss: 0.4861\n",
      "267/500 [===============>..............] - ETA: 4:02 - loss: 1.9201 - regression_loss: 1.4349 - classification_loss: 0.4852\n",
      "268/500 [===============>..............] - ETA: 4:01 - loss: 1.9192 - regression_loss: 1.4343 - classification_loss: 0.4849\n",
      "269/500 [===============>..............] - ETA: 4:00 - loss: 1.9197 - regression_loss: 1.4352 - classification_loss: 0.4845\n",
      "270/500 [===============>..............] - ETA: 3:59 - loss: 1.9200 - regression_loss: 1.4358 - classification_loss: 0.4842\n",
      "271/500 [===============>..............] - ETA: 3:58 - loss: 1.9165 - regression_loss: 1.4331 - classification_loss: 0.4835\n",
      "272/500 [===============>..............] - ETA: 3:57 - loss: 1.9136 - regression_loss: 1.4306 - classification_loss: 0.4830\n",
      "273/500 [===============>..............] - ETA: 3:55 - loss: 1.9138 - regression_loss: 1.4302 - classification_loss: 0.4836\n",
      "274/500 [===============>..............] - ETA: 3:54 - loss: 1.9127 - regression_loss: 1.4288 - classification_loss: 0.4839\n",
      "275/500 [===============>..............] - ETA: 3:53 - loss: 1.9117 - regression_loss: 1.4278 - classification_loss: 0.4839\n",
      "276/500 [===============>..............] - ETA: 3:52 - loss: 1.9121 - regression_loss: 1.4275 - classification_loss: 0.4845\n",
      "277/500 [===============>..............] - ETA: 3:51 - loss: 1.9132 - regression_loss: 1.4279 - classification_loss: 0.4853\n",
      "278/500 [===============>..............] - ETA: 3:50 - loss: 1.9161 - regression_loss: 1.4300 - classification_loss: 0.4861\n",
      "279/500 [===============>..............] - ETA: 3:49 - loss: 1.9133 - regression_loss: 1.4280 - classification_loss: 0.4853\n",
      "280/500 [===============>..............] - ETA: 3:48 - loss: 1.9148 - regression_loss: 1.4296 - classification_loss: 0.4852\n",
      "281/500 [===============>..............] - ETA: 3:49 - loss: 1.9132 - regression_loss: 1.4285 - classification_loss: 0.4847\n",
      "282/500 [===============>..............] - ETA: 3:48 - loss: 1.9134 - regression_loss: 1.4284 - classification_loss: 0.4850\n",
      "283/500 [===============>..............] - ETA: 3:47 - loss: 1.9124 - regression_loss: 1.4277 - classification_loss: 0.4847\n",
      "284/500 [================>.............] - ETA: 3:46 - loss: 1.9110 - regression_loss: 1.4265 - classification_loss: 0.4845\n",
      "285/500 [================>.............] - ETA: 3:45 - loss: 1.9125 - regression_loss: 1.4275 - classification_loss: 0.4850\n",
      "286/500 [================>.............] - ETA: 3:44 - loss: 1.9132 - regression_loss: 1.4286 - classification_loss: 0.4847\n",
      "287/500 [================>.............] - ETA: 3:43 - loss: 1.9113 - regression_loss: 1.4271 - classification_loss: 0.4841\n",
      "288/500 [================>.............] - ETA: 3:42 - loss: 1.9105 - regression_loss: 1.4270 - classification_loss: 0.4835\n",
      "289/500 [================>.............] - ETA: 3:41 - loss: 1.9110 - regression_loss: 1.4273 - classification_loss: 0.4837\n",
      "290/500 [================>.............] - ETA: 3:39 - loss: 1.9121 - regression_loss: 1.4277 - classification_loss: 0.4844\n",
      "291/500 [================>.............] - ETA: 3:38 - loss: 1.9091 - regression_loss: 1.4252 - classification_loss: 0.4839\n",
      "292/500 [================>.............] - ETA: 3:37 - loss: 1.9121 - regression_loss: 1.4278 - classification_loss: 0.4843\n",
      "293/500 [================>.............] - ETA: 3:36 - loss: 1.9140 - regression_loss: 1.4290 - classification_loss: 0.4850\n",
      "294/500 [================>.............] - ETA: 3:35 - loss: 1.9154 - regression_loss: 1.4294 - classification_loss: 0.4861\n",
      "295/500 [================>.............] - ETA: 3:34 - loss: 1.9166 - regression_loss: 1.4305 - classification_loss: 0.4862\n",
      "296/500 [================>.............] - ETA: 3:33 - loss: 1.9158 - regression_loss: 1.4303 - classification_loss: 0.4854\n",
      "297/500 [================>.............] - ETA: 3:32 - loss: 1.9169 - regression_loss: 1.4315 - classification_loss: 0.4854\n",
      "298/500 [================>.............] - ETA: 3:31 - loss: 1.9160 - regression_loss: 1.4306 - classification_loss: 0.4854\n",
      "299/500 [================>.............] - ETA: 3:30 - loss: 1.9174 - regression_loss: 1.4315 - classification_loss: 0.4859\n",
      "300/500 [=================>............] - ETA: 3:29 - loss: 1.9190 - regression_loss: 1.4324 - classification_loss: 0.4866\n",
      "301/500 [=================>............] - ETA: 3:28 - loss: 1.9193 - regression_loss: 1.4334 - classification_loss: 0.4859\n",
      "302/500 [=================>............] - ETA: 3:27 - loss: 1.9211 - regression_loss: 1.4343 - classification_loss: 0.4868\n",
      "303/500 [=================>............] - ETA: 3:26 - loss: 1.9224 - regression_loss: 1.4349 - classification_loss: 0.4874\n",
      "304/500 [=================>............] - ETA: 3:25 - loss: 1.9216 - regression_loss: 1.4348 - classification_loss: 0.4868\n",
      "305/500 [=================>............] - ETA: 3:24 - loss: 1.9227 - regression_loss: 1.4356 - classification_loss: 0.4871\n",
      "306/500 [=================>............] - ETA: 3:23 - loss: 1.9227 - regression_loss: 1.4359 - classification_loss: 0.4868\n",
      "307/500 [=================>............] - ETA: 3:21 - loss: 1.9239 - regression_loss: 1.4370 - classification_loss: 0.4869\n",
      "308/500 [=================>............] - ETA: 3:20 - loss: 1.9228 - regression_loss: 1.4363 - classification_loss: 0.4865\n",
      "309/500 [=================>............] - ETA: 3:19 - loss: 1.9226 - regression_loss: 1.4365 - classification_loss: 0.4861\n",
      "310/500 [=================>............] - ETA: 3:18 - loss: 1.9205 - regression_loss: 1.4349 - classification_loss: 0.4856\n",
      "311/500 [=================>............] - ETA: 3:17 - loss: 1.9205 - regression_loss: 1.4351 - classification_loss: 0.4854\n",
      "312/500 [=================>............] - ETA: 3:16 - loss: 1.9186 - regression_loss: 1.4336 - classification_loss: 0.4851\n",
      "313/500 [=================>............] - ETA: 3:15 - loss: 1.9178 - regression_loss: 1.4333 - classification_loss: 0.4845\n",
      "314/500 [=================>............] - ETA: 3:14 - loss: 1.9181 - regression_loss: 1.4329 - classification_loss: 0.4852\n",
      "315/500 [=================>............] - ETA: 3:13 - loss: 1.9180 - regression_loss: 1.4328 - classification_loss: 0.4852\n",
      "316/500 [=================>............] - ETA: 3:12 - loss: 1.9157 - regression_loss: 1.4307 - classification_loss: 0.4850\n",
      "317/500 [==================>...........] - ETA: 3:11 - loss: 1.9141 - regression_loss: 1.4296 - classification_loss: 0.4845\n",
      "318/500 [==================>...........] - ETA: 3:10 - loss: 1.9129 - regression_loss: 1.4287 - classification_loss: 0.4842\n",
      "319/500 [==================>...........] - ETA: 3:09 - loss: 1.9126 - regression_loss: 1.4286 - classification_loss: 0.4840\n",
      "320/500 [==================>...........] - ETA: 3:08 - loss: 1.9125 - regression_loss: 1.4282 - classification_loss: 0.4843\n",
      "321/500 [==================>...........] - ETA: 3:07 - loss: 1.9148 - regression_loss: 1.4305 - classification_loss: 0.4843\n",
      "322/500 [==================>...........] - ETA: 3:05 - loss: 1.9154 - regression_loss: 1.4306 - classification_loss: 0.4848\n",
      "323/500 [==================>...........] - ETA: 3:04 - loss: 1.9161 - regression_loss: 1.4306 - classification_loss: 0.4855\n",
      "324/500 [==================>...........] - ETA: 3:03 - loss: 1.9159 - regression_loss: 1.4302 - classification_loss: 0.4856\n",
      "325/500 [==================>...........] - ETA: 3:02 - loss: 1.9185 - regression_loss: 1.4328 - classification_loss: 0.4857\n",
      "326/500 [==================>...........] - ETA: 3:01 - loss: 1.9189 - regression_loss: 1.4333 - classification_loss: 0.4857\n",
      "327/500 [==================>...........] - ETA: 3:00 - loss: 1.9184 - regression_loss: 1.4327 - classification_loss: 0.4857\n",
      "328/500 [==================>...........] - ETA: 2:59 - loss: 1.9184 - regression_loss: 1.4326 - classification_loss: 0.4858\n",
      "329/500 [==================>...........] - ETA: 2:58 - loss: 1.9188 - regression_loss: 1.4323 - classification_loss: 0.4865\n",
      "330/500 [==================>...........] - ETA: 2:57 - loss: 1.9178 - regression_loss: 1.4313 - classification_loss: 0.4864\n",
      "331/500 [==================>...........] - ETA: 2:56 - loss: 1.9178 - regression_loss: 1.4310 - classification_loss: 0.4867\n",
      "332/500 [==================>...........] - ETA: 2:55 - loss: 1.9172 - regression_loss: 1.4301 - classification_loss: 0.4870\n",
      "333/500 [==================>...........] - ETA: 2:54 - loss: 1.9197 - regression_loss: 1.4321 - classification_loss: 0.4877\n",
      "334/500 [===================>..........] - ETA: 2:53 - loss: 1.9181 - regression_loss: 1.4306 - classification_loss: 0.4875\n",
      "335/500 [===================>..........] - ETA: 2:52 - loss: 1.9155 - regression_loss: 1.4284 - classification_loss: 0.4870\n",
      "336/500 [===================>..........] - ETA: 2:51 - loss: 1.9138 - regression_loss: 1.4273 - classification_loss: 0.4865\n",
      "337/500 [===================>..........] - ETA: 2:50 - loss: 1.9133 - regression_loss: 1.4271 - classification_loss: 0.4862\n",
      "338/500 [===================>..........] - ETA: 2:49 - loss: 1.9133 - regression_loss: 1.4269 - classification_loss: 0.4864\n",
      "339/500 [===================>..........] - ETA: 2:47 - loss: 1.9120 - regression_loss: 1.4263 - classification_loss: 0.4857\n",
      "340/500 [===================>..........] - ETA: 2:46 - loss: 1.9108 - regression_loss: 1.4257 - classification_loss: 0.4852\n",
      "341/500 [===================>..........] - ETA: 2:45 - loss: 1.9085 - regression_loss: 1.4236 - classification_loss: 0.4849\n",
      "342/500 [===================>..........] - ETA: 2:44 - loss: 1.9090 - regression_loss: 1.4240 - classification_loss: 0.4850\n",
      "343/500 [===================>..........] - ETA: 2:43 - loss: 1.9112 - regression_loss: 1.4257 - classification_loss: 0.4855\n",
      "344/500 [===================>..........] - ETA: 2:42 - loss: 1.9103 - regression_loss: 1.4246 - classification_loss: 0.4856\n",
      "345/500 [===================>..........] - ETA: 2:41 - loss: 1.9113 - regression_loss: 1.4251 - classification_loss: 0.4862\n",
      "346/500 [===================>..........] - ETA: 2:40 - loss: 1.9086 - regression_loss: 1.4229 - classification_loss: 0.4857\n",
      "347/500 [===================>..........] - ETA: 2:39 - loss: 1.9078 - regression_loss: 1.4227 - classification_loss: 0.4851\n",
      "348/500 [===================>..........] - ETA: 2:38 - loss: 1.9108 - regression_loss: 1.4244 - classification_loss: 0.4864\n",
      "349/500 [===================>..........] - ETA: 2:37 - loss: 1.9139 - regression_loss: 1.4261 - classification_loss: 0.4877\n",
      "350/500 [====================>.........] - ETA: 2:36 - loss: 1.9147 - regression_loss: 1.4266 - classification_loss: 0.4881\n",
      "351/500 [====================>.........] - ETA: 2:35 - loss: 1.9168 - regression_loss: 1.4280 - classification_loss: 0.4888\n",
      "352/500 [====================>.........] - ETA: 2:34 - loss: 1.9171 - regression_loss: 1.4281 - classification_loss: 0.4890\n",
      "353/500 [====================>.........] - ETA: 2:33 - loss: 1.9173 - regression_loss: 1.4282 - classification_loss: 0.4891\n",
      "354/500 [====================>.........] - ETA: 2:32 - loss: 1.9163 - regression_loss: 1.4274 - classification_loss: 0.4889\n",
      "355/500 [====================>.........] - ETA: 2:31 - loss: 1.9144 - regression_loss: 1.4261 - classification_loss: 0.4883\n",
      "356/500 [====================>.........] - ETA: 2:30 - loss: 1.9135 - regression_loss: 1.4256 - classification_loss: 0.4879\n",
      "357/500 [====================>.........] - ETA: 2:29 - loss: 1.9131 - regression_loss: 1.4256 - classification_loss: 0.4875\n",
      "358/500 [====================>.........] - ETA: 2:28 - loss: 1.9136 - regression_loss: 1.4263 - classification_loss: 0.4874\n",
      "359/500 [====================>.........] - ETA: 2:27 - loss: 1.9160 - regression_loss: 1.4276 - classification_loss: 0.4884\n",
      "360/500 [====================>.........] - ETA: 2:26 - loss: 1.9163 - regression_loss: 1.4280 - classification_loss: 0.4883\n",
      "361/500 [====================>.........] - ETA: 2:24 - loss: 1.9179 - regression_loss: 1.4285 - classification_loss: 0.4894\n",
      "362/500 [====================>.........] - ETA: 2:23 - loss: 1.9184 - regression_loss: 1.4294 - classification_loss: 0.4890\n",
      "363/500 [====================>.........] - ETA: 2:22 - loss: 1.9187 - regression_loss: 1.4295 - classification_loss: 0.4891\n",
      "364/500 [====================>.........] - ETA: 2:21 - loss: 1.9171 - regression_loss: 1.4285 - classification_loss: 0.4886\n",
      "365/500 [====================>.........] - ETA: 2:20 - loss: 1.9167 - regression_loss: 1.4284 - classification_loss: 0.4883\n",
      "366/500 [====================>.........] - ETA: 2:19 - loss: 1.9166 - regression_loss: 1.4285 - classification_loss: 0.4881\n",
      "367/500 [=====================>........] - ETA: 2:18 - loss: 1.9164 - regression_loss: 1.4284 - classification_loss: 0.4880\n",
      "368/500 [=====================>........] - ETA: 2:17 - loss: 1.9140 - regression_loss: 1.4269 - classification_loss: 0.4870\n",
      "369/500 [=====================>........] - ETA: 2:16 - loss: 1.9124 - regression_loss: 1.4256 - classification_loss: 0.4868\n",
      "370/500 [=====================>........] - ETA: 2:15 - loss: 1.9147 - regression_loss: 1.4270 - classification_loss: 0.4877\n",
      "371/500 [=====================>........] - ETA: 2:14 - loss: 1.9162 - regression_loss: 1.4285 - classification_loss: 0.4877\n",
      "372/500 [=====================>........] - ETA: 2:13 - loss: 1.9170 - regression_loss: 1.4291 - classification_loss: 0.4879\n",
      "373/500 [=====================>........] - ETA: 2:12 - loss: 1.9168 - regression_loss: 1.4290 - classification_loss: 0.4878\n",
      "374/500 [=====================>........] - ETA: 2:11 - loss: 1.9184 - regression_loss: 1.4304 - classification_loss: 0.4880\n",
      "375/500 [=====================>........] - ETA: 2:10 - loss: 1.9206 - regression_loss: 1.4324 - classification_loss: 0.4883\n",
      "376/500 [=====================>........] - ETA: 2:09 - loss: 1.9207 - regression_loss: 1.4318 - classification_loss: 0.4889\n",
      "377/500 [=====================>........] - ETA: 2:08 - loss: 1.9187 - regression_loss: 1.4303 - classification_loss: 0.4884\n",
      "378/500 [=====================>........] - ETA: 2:07 - loss: 1.9188 - regression_loss: 1.4306 - classification_loss: 0.4882\n",
      "379/500 [=====================>........] - ETA: 2:06 - loss: 1.9199 - regression_loss: 1.4316 - classification_loss: 0.4883\n",
      "380/500 [=====================>........] - ETA: 2:05 - loss: 1.9198 - regression_loss: 1.4313 - classification_loss: 0.4885\n",
      "381/500 [=====================>........] - ETA: 2:04 - loss: 1.9185 - regression_loss: 1.4301 - classification_loss: 0.4884\n",
      "382/500 [=====================>........] - ETA: 2:02 - loss: 1.9161 - regression_loss: 1.4284 - classification_loss: 0.4877\n",
      "383/500 [=====================>........] - ETA: 2:01 - loss: 1.9145 - regression_loss: 1.4274 - classification_loss: 0.4871\n",
      "384/500 [======================>.......] - ETA: 2:00 - loss: 1.9152 - regression_loss: 1.4277 - classification_loss: 0.4875\n",
      "385/500 [======================>.......] - ETA: 1:59 - loss: 1.9145 - regression_loss: 1.4263 - classification_loss: 0.4881\n",
      "386/500 [======================>.......] - ETA: 1:58 - loss: 1.9153 - regression_loss: 1.4270 - classification_loss: 0.4883\n",
      "387/500 [======================>.......] - ETA: 1:57 - loss: 1.9142 - regression_loss: 1.4264 - classification_loss: 0.4879\n",
      "388/500 [======================>.......] - ETA: 1:56 - loss: 1.9138 - regression_loss: 1.4262 - classification_loss: 0.4876\n",
      "389/500 [======================>.......] - ETA: 1:55 - loss: 1.9145 - regression_loss: 1.4272 - classification_loss: 0.4873\n",
      "390/500 [======================>.......] - ETA: 1:54 - loss: 1.9133 - regression_loss: 1.4263 - classification_loss: 0.4870\n",
      "391/500 [======================>.......] - ETA: 1:53 - loss: 1.9105 - regression_loss: 1.4241 - classification_loss: 0.4864\n",
      "392/500 [======================>.......] - ETA: 1:52 - loss: 1.9109 - regression_loss: 1.4246 - classification_loss: 0.4863\n",
      "393/500 [======================>.......] - ETA: 1:51 - loss: 1.9093 - regression_loss: 1.4235 - classification_loss: 0.4858\n",
      "394/500 [======================>.......] - ETA: 1:50 - loss: 1.9083 - regression_loss: 1.4231 - classification_loss: 0.4852\n",
      "395/500 [======================>.......] - ETA: 1:49 - loss: 1.9065 - regression_loss: 1.4216 - classification_loss: 0.4849\n",
      "396/500 [======================>.......] - ETA: 1:48 - loss: 1.9061 - regression_loss: 1.4212 - classification_loss: 0.4849\n",
      "397/500 [======================>.......] - ETA: 1:47 - loss: 1.9058 - regression_loss: 1.4208 - classification_loss: 0.4850\n",
      "398/500 [======================>.......] - ETA: 1:46 - loss: 1.9056 - regression_loss: 1.4205 - classification_loss: 0.4850\n",
      "399/500 [======================>.......] - ETA: 1:45 - loss: 1.9074 - regression_loss: 1.4216 - classification_loss: 0.4857\n",
      "400/500 [=======================>......] - ETA: 1:44 - loss: 1.9066 - regression_loss: 1.4208 - classification_loss: 0.4858\n",
      "401/500 [=======================>......] - ETA: 1:43 - loss: 1.9084 - regression_loss: 1.4223 - classification_loss: 0.4861\n",
      "402/500 [=======================>......] - ETA: 1:42 - loss: 1.9099 - regression_loss: 1.4235 - classification_loss: 0.4864\n",
      "403/500 [=======================>......] - ETA: 1:41 - loss: 1.9100 - regression_loss: 1.4241 - classification_loss: 0.4859\n",
      "404/500 [=======================>......] - ETA: 1:40 - loss: 1.9092 - regression_loss: 1.4235 - classification_loss: 0.4857\n",
      "405/500 [=======================>......] - ETA: 1:38 - loss: 1.9079 - regression_loss: 1.4227 - classification_loss: 0.4852\n",
      "406/500 [=======================>......] - ETA: 1:37 - loss: 1.9077 - regression_loss: 1.4222 - classification_loss: 0.4855\n",
      "407/500 [=======================>......] - ETA: 1:36 - loss: 1.9072 - regression_loss: 1.4215 - classification_loss: 0.4858\n",
      "408/500 [=======================>......] - ETA: 1:35 - loss: 1.9072 - regression_loss: 1.4215 - classification_loss: 0.4858\n",
      "409/500 [=======================>......] - ETA: 1:34 - loss: 1.9073 - regression_loss: 1.4217 - classification_loss: 0.4856\n",
      "410/500 [=======================>......] - ETA: 1:33 - loss: 1.9054 - regression_loss: 1.4203 - classification_loss: 0.4851\n",
      "411/500 [=======================>......] - ETA: 1:32 - loss: 1.9043 - regression_loss: 1.4194 - classification_loss: 0.4849\n",
      "412/500 [=======================>......] - ETA: 1:31 - loss: 1.9052 - regression_loss: 1.4203 - classification_loss: 0.4849\n",
      "413/500 [=======================>......] - ETA: 1:30 - loss: 1.9037 - regression_loss: 1.4191 - classification_loss: 0.4845\n",
      "414/500 [=======================>......] - ETA: 1:29 - loss: 1.9041 - regression_loss: 1.4194 - classification_loss: 0.4847\n",
      "415/500 [=======================>......] - ETA: 1:28 - loss: 1.9020 - regression_loss: 1.4179 - classification_loss: 0.4840\n",
      "416/500 [=======================>......] - ETA: 1:27 - loss: 1.9024 - regression_loss: 1.4182 - classification_loss: 0.4842\n",
      "417/500 [========================>.....] - ETA: 1:26 - loss: 1.9027 - regression_loss: 1.4187 - classification_loss: 0.4840\n",
      "418/500 [========================>.....] - ETA: 1:25 - loss: 1.9017 - regression_loss: 1.4180 - classification_loss: 0.4837\n",
      "419/500 [========================>.....] - ETA: 1:24 - loss: 1.9000 - regression_loss: 1.4164 - classification_loss: 0.4836\n",
      "420/500 [========================>.....] - ETA: 1:23 - loss: 1.8996 - regression_loss: 1.4161 - classification_loss: 0.4835\n",
      "421/500 [========================>.....] - ETA: 1:22 - loss: 1.8989 - regression_loss: 1.4154 - classification_loss: 0.4835\n",
      "422/500 [========================>.....] - ETA: 1:21 - loss: 1.9000 - regression_loss: 1.4160 - classification_loss: 0.4840\n",
      "423/500 [========================>.....] - ETA: 1:20 - loss: 1.8990 - regression_loss: 1.4154 - classification_loss: 0.4835\n",
      "424/500 [========================>.....] - ETA: 1:19 - loss: 1.8981 - regression_loss: 1.4151 - classification_loss: 0.4830\n",
      "425/500 [========================>.....] - ETA: 1:18 - loss: 1.8985 - regression_loss: 1.4158 - classification_loss: 0.4827\n",
      "426/500 [========================>.....] - ETA: 1:17 - loss: 1.8972 - regression_loss: 1.4147 - classification_loss: 0.4825\n",
      "427/500 [========================>.....] - ETA: 1:15 - loss: 1.8977 - regression_loss: 1.4150 - classification_loss: 0.4828\n",
      "428/500 [========================>.....] - ETA: 1:14 - loss: 1.8976 - regression_loss: 1.4150 - classification_loss: 0.4826\n",
      "429/500 [========================>.....] - ETA: 1:13 - loss: 1.8970 - regression_loss: 1.4146 - classification_loss: 0.4824\n",
      "430/500 [========================>.....] - ETA: 1:12 - loss: 1.8969 - regression_loss: 1.4147 - classification_loss: 0.4822\n",
      "431/500 [========================>.....] - ETA: 1:11 - loss: 1.8967 - regression_loss: 1.4145 - classification_loss: 0.4822\n",
      "432/500 [========================>.....] - ETA: 1:10 - loss: 1.8972 - regression_loss: 1.4145 - classification_loss: 0.4826\n",
      "433/500 [========================>.....] - ETA: 1:09 - loss: 1.8973 - regression_loss: 1.4148 - classification_loss: 0.4824\n",
      "434/500 [=========================>....] - ETA: 1:08 - loss: 1.8966 - regression_loss: 1.4144 - classification_loss: 0.4822\n",
      "435/500 [=========================>....] - ETA: 1:07 - loss: 1.8961 - regression_loss: 1.4140 - classification_loss: 0.4820\n",
      "436/500 [=========================>....] - ETA: 1:06 - loss: 1.8987 - regression_loss: 1.4157 - classification_loss: 0.4830\n",
      "437/500 [=========================>....] - ETA: 1:05 - loss: 1.8968 - regression_loss: 1.4144 - classification_loss: 0.4824\n",
      "438/500 [=========================>....] - ETA: 1:04 - loss: 1.8969 - regression_loss: 1.4146 - classification_loss: 0.4823\n",
      "439/500 [=========================>....] - ETA: 1:03 - loss: 1.8990 - regression_loss: 1.4161 - classification_loss: 0.4829\n",
      "440/500 [=========================>....] - ETA: 1:02 - loss: 1.8981 - regression_loss: 1.4155 - classification_loss: 0.4826\n",
      "441/500 [=========================>....] - ETA: 1:01 - loss: 1.8969 - regression_loss: 1.4148 - classification_loss: 0.4821\n",
      "442/500 [=========================>....] - ETA: 1:00 - loss: 1.8963 - regression_loss: 1.4142 - classification_loss: 0.4821\n",
      "443/500 [=========================>....] - ETA: 59s - loss: 1.8959 - regression_loss: 1.4139 - classification_loss: 0.4820 \n",
      "444/500 [=========================>....] - ETA: 58s - loss: 1.8945 - regression_loss: 1.4130 - classification_loss: 0.4815\n",
      "445/500 [=========================>....] - ETA: 57s - loss: 1.8931 - regression_loss: 1.4121 - classification_loss: 0.4810\n",
      "446/500 [=========================>....] - ETA: 56s - loss: 1.8943 - regression_loss: 1.4130 - classification_loss: 0.4813\n",
      "447/500 [=========================>....] - ETA: 55s - loss: 1.8930 - regression_loss: 1.4122 - classification_loss: 0.4809\n",
      "448/500 [=========================>....] - ETA: 54s - loss: 1.8919 - regression_loss: 1.4116 - classification_loss: 0.4804\n",
      "449/500 [=========================>....] - ETA: 53s - loss: 1.8945 - regression_loss: 1.4132 - classification_loss: 0.4812\n",
      "450/500 [==========================>...] - ETA: 52s - loss: 1.8934 - regression_loss: 1.4125 - classification_loss: 0.4809\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 1.8941 - regression_loss: 1.4128 - classification_loss: 0.4813\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 1.8966 - regression_loss: 1.4144 - classification_loss: 0.4822\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.8974 - regression_loss: 1.4148 - classification_loss: 0.4825\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.8974 - regression_loss: 1.4147 - classification_loss: 0.4827\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.8985 - regression_loss: 1.4153 - classification_loss: 0.4832\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.8976 - regression_loss: 1.4149 - classification_loss: 0.4828\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.8976 - regression_loss: 1.4148 - classification_loss: 0.4827\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.8991 - regression_loss: 1.4159 - classification_loss: 0.4832\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.8991 - regression_loss: 1.4157 - classification_loss: 0.4834\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.8994 - regression_loss: 1.4159 - classification_loss: 0.4835\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 1.8985 - regression_loss: 1.4152 - classification_loss: 0.4833\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 1.9015 - regression_loss: 1.4172 - classification_loss: 0.4843\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 1.8994 - regression_loss: 1.4158 - classification_loss: 0.4837\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 1.8992 - regression_loss: 1.4153 - classification_loss: 0.4840\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 1.8988 - regression_loss: 1.4153 - classification_loss: 0.4836\n",
      "466/500 [==========================>...] - ETA: 35s - loss: 1.8973 - regression_loss: 1.4140 - classification_loss: 0.4833\n",
      "467/500 [===========================>..] - ETA: 34s - loss: 1.8976 - regression_loss: 1.4143 - classification_loss: 0.4833\n",
      "468/500 [===========================>..] - ETA: 33s - loss: 1.8970 - regression_loss: 1.4137 - classification_loss: 0.4832\n",
      "469/500 [===========================>..] - ETA: 32s - loss: 1.8977 - regression_loss: 1.4144 - classification_loss: 0.4832\n",
      "470/500 [===========================>..] - ETA: 31s - loss: 1.8985 - regression_loss: 1.4151 - classification_loss: 0.4833\n",
      "471/500 [===========================>..] - ETA: 30s - loss: 1.8971 - regression_loss: 1.4139 - classification_loss: 0.4831\n",
      "472/500 [===========================>..] - ETA: 29s - loss: 1.8969 - regression_loss: 1.4140 - classification_loss: 0.4829\n",
      "473/500 [===========================>..] - ETA: 28s - loss: 1.8975 - regression_loss: 1.4146 - classification_loss: 0.4829\n",
      "474/500 [===========================>..] - ETA: 27s - loss: 1.8972 - regression_loss: 1.4145 - classification_loss: 0.4827\n",
      "475/500 [===========================>..] - ETA: 26s - loss: 1.8964 - regression_loss: 1.4142 - classification_loss: 0.4822\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 1.8963 - regression_loss: 1.4141 - classification_loss: 0.4821\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.8971 - regression_loss: 1.4147 - classification_loss: 0.4824\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.8973 - regression_loss: 1.4148 - classification_loss: 0.4826\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.8967 - regression_loss: 1.4140 - classification_loss: 0.4826\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.8950 - regression_loss: 1.4129 - classification_loss: 0.4821\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.8950 - regression_loss: 1.4129 - classification_loss: 0.4821\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.8935 - regression_loss: 1.4118 - classification_loss: 0.4817\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.8936 - regression_loss: 1.4119 - classification_loss: 0.4817\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.8919 - regression_loss: 1.4108 - classification_loss: 0.4812\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.8920 - regression_loss: 1.4107 - classification_loss: 0.4813\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.8933 - regression_loss: 1.4117 - classification_loss: 0.4816\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.8926 - regression_loss: 1.4112 - classification_loss: 0.4814\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.8921 - regression_loss: 1.4109 - classification_loss: 0.4812\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.8908 - regression_loss: 1.4101 - classification_loss: 0.4807\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.8892 - regression_loss: 1.4090 - classification_loss: 0.4802\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.8892 - regression_loss: 1.4089 - classification_loss: 0.4803 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.8903 - regression_loss: 1.4097 - classification_loss: 0.4806\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.8902 - regression_loss: 1.4097 - classification_loss: 0.4805\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.8894 - regression_loss: 1.4093 - classification_loss: 0.4801\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.8889 - regression_loss: 1.4090 - classification_loss: 0.4799\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.8915 - regression_loss: 1.4108 - classification_loss: 0.4807\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.8909 - regression_loss: 1.4105 - classification_loss: 0.4804\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.8899 - regression_loss: 1.4093 - classification_loss: 0.4806\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.8905 - regression_loss: 1.4100 - classification_loss: 0.4805\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.8908 - regression_loss: 1.4102 - classification_loss: 0.4806\n",
      "Epoch 00027: saving model to ./snapshots\\resnet50_csv_27.h5\n",
      "\n",
      "500/500 [==============================] - 520s 1s/step - loss: 1.8908 - regression_loss: 1.4102 - classification_loss: 0.4806\n",
      "Epoch 28/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 1.4544 - regression_loss: 1.0086 - classification_loss: 0.4458\n",
      "  2/500 [..............................] - ETA: 3:39 - loss: 1.6372 - regression_loss: 1.1412 - classification_loss: 0.4961\n",
      "  3/500 [..............................] - ETA: 5:24 - loss: 1.9710 - regression_loss: 1.3294 - classification_loss: 0.6416\n",
      "  4/500 [..............................] - ETA: 6:05 - loss: 1.7357 - regression_loss: 1.1838 - classification_loss: 0.5518\n",
      "  5/500 [..............................] - ETA: 6:42 - loss: 1.8411 - regression_loss: 1.2781 - classification_loss: 0.5630\n",
      "  6/500 [..............................] - ETA: 6:55 - loss: 1.7839 - regression_loss: 1.2664 - classification_loss: 0.5175\n",
      "  7/500 [..............................] - ETA: 7:05 - loss: 1.8406 - regression_loss: 1.3410 - classification_loss: 0.4996\n",
      "  8/500 [..............................] - ETA: 7:12 - loss: 1.8054 - regression_loss: 1.3161 - classification_loss: 0.4893\n",
      "  9/500 [..............................] - ETA: 7:17 - loss: 1.8252 - regression_loss: 1.3248 - classification_loss: 0.5005\n",
      " 10/500 [..............................] - ETA: 7:22 - loss: 1.8661 - regression_loss: 1.3746 - classification_loss: 0.4914\n",
      " 11/500 [..............................] - ETA: 7:25 - loss: 1.8261 - regression_loss: 1.3426 - classification_loss: 0.4835\n",
      " 12/500 [..............................] - ETA: 7:27 - loss: 1.8634 - regression_loss: 1.3778 - classification_loss: 0.4856\n",
      " 13/500 [..............................] - ETA: 7:32 - loss: 1.8664 - regression_loss: 1.3823 - classification_loss: 0.4842\n",
      " 14/500 [..............................] - ETA: 7:36 - loss: 1.9234 - regression_loss: 1.4204 - classification_loss: 0.5030\n",
      " 15/500 [..............................] - ETA: 7:37 - loss: 1.8895 - regression_loss: 1.3959 - classification_loss: 0.4936\n",
      " 16/500 [..............................] - ETA: 7:34 - loss: 1.8728 - regression_loss: 1.3804 - classification_loss: 0.4924\n",
      " 17/500 [>.............................] - ETA: 7:36 - loss: 1.9238 - regression_loss: 1.4152 - classification_loss: 0.5086\n",
      " 18/500 [>.............................] - ETA: 7:34 - loss: 1.9418 - regression_loss: 1.4350 - classification_loss: 0.5068\n",
      " 19/500 [>.............................] - ETA: 7:37 - loss: 1.9821 - regression_loss: 1.4648 - classification_loss: 0.5173\n",
      " 20/500 [>.............................] - ETA: 7:32 - loss: 1.9603 - regression_loss: 1.4442 - classification_loss: 0.5161\n",
      " 21/500 [>.............................] - ETA: 7:31 - loss: 1.9270 - regression_loss: 1.4183 - classification_loss: 0.5087\n",
      " 22/500 [>.............................] - ETA: 7:35 - loss: 1.9308 - regression_loss: 1.4208 - classification_loss: 0.5100\n",
      " 23/500 [>.............................] - ETA: 7:36 - loss: 1.9108 - regression_loss: 1.4073 - classification_loss: 0.5034\n",
      " 24/500 [>.............................] - ETA: 7:37 - loss: 1.9206 - regression_loss: 1.4159 - classification_loss: 0.5047\n",
      " 25/500 [>.............................] - ETA: 7:36 - loss: 1.9005 - regression_loss: 1.4055 - classification_loss: 0.4950\n",
      " 26/500 [>.............................] - ETA: 7:36 - loss: 1.8928 - regression_loss: 1.4006 - classification_loss: 0.4922\n",
      " 27/500 [>.............................] - ETA: 7:37 - loss: 1.8560 - regression_loss: 1.3738 - classification_loss: 0.4822\n",
      " 28/500 [>.............................] - ETA: 7:36 - loss: 1.8826 - regression_loss: 1.3851 - classification_loss: 0.4975\n",
      " 29/500 [>.............................] - ETA: 7:36 - loss: 1.9025 - regression_loss: 1.4055 - classification_loss: 0.4970\n",
      " 30/500 [>.............................] - ETA: 7:36 - loss: 1.8899 - regression_loss: 1.3925 - classification_loss: 0.4974\n",
      " 31/500 [>.............................] - ETA: 7:37 - loss: 1.8880 - regression_loss: 1.3920 - classification_loss: 0.4960\n",
      " 32/500 [>.............................] - ETA: 7:38 - loss: 1.8611 - regression_loss: 1.3699 - classification_loss: 0.4912\n",
      " 33/500 [>.............................] - ETA: 7:37 - loss: 1.8550 - regression_loss: 1.3661 - classification_loss: 0.4889\n",
      " 34/500 [=>............................] - ETA: 7:37 - loss: 1.8874 - regression_loss: 1.3715 - classification_loss: 0.5160\n",
      " 35/500 [=>............................] - ETA: 7:36 - loss: 1.8830 - regression_loss: 1.3709 - classification_loss: 0.5120\n",
      " 36/500 [=>............................] - ETA: 7:37 - loss: 1.8718 - regression_loss: 1.3639 - classification_loss: 0.5079\n",
      " 37/500 [=>............................] - ETA: 7:36 - loss: 1.8729 - regression_loss: 1.3658 - classification_loss: 0.5071\n",
      " 38/500 [=>............................] - ETA: 7:35 - loss: 1.8971 - regression_loss: 1.3776 - classification_loss: 0.5195\n",
      " 39/500 [=>............................] - ETA: 7:35 - loss: 1.8912 - regression_loss: 1.3760 - classification_loss: 0.5153\n",
      " 40/500 [=>............................] - ETA: 7:34 - loss: 1.9209 - regression_loss: 1.3970 - classification_loss: 0.5239\n",
      " 41/500 [=>............................] - ETA: 7:34 - loss: 1.9438 - regression_loss: 1.4166 - classification_loss: 0.5272\n",
      " 42/500 [=>............................] - ETA: 7:34 - loss: 1.9481 - regression_loss: 1.4218 - classification_loss: 0.5263\n",
      " 43/500 [=>............................] - ETA: 7:34 - loss: 1.9479 - regression_loss: 1.4235 - classification_loss: 0.5243\n",
      " 44/500 [=>............................] - ETA: 7:32 - loss: 1.9505 - regression_loss: 1.4265 - classification_loss: 0.5240\n",
      " 45/500 [=>............................] - ETA: 7:33 - loss: 1.9604 - regression_loss: 1.4368 - classification_loss: 0.5236\n",
      " 46/500 [=>............................] - ETA: 7:32 - loss: 1.9595 - regression_loss: 1.4343 - classification_loss: 0.5252\n",
      " 47/500 [=>............................] - ETA: 7:32 - loss: 1.9447 - regression_loss: 1.4238 - classification_loss: 0.5209\n",
      " 48/500 [=>............................] - ETA: 7:29 - loss: 1.9533 - regression_loss: 1.4263 - classification_loss: 0.5270\n",
      " 49/500 [=>............................] - ETA: 7:29 - loss: 1.9392 - regression_loss: 1.4175 - classification_loss: 0.5217\n",
      " 50/500 [==>...........................] - ETA: 7:26 - loss: 1.9270 - regression_loss: 1.4099 - classification_loss: 0.5171\n",
      " 51/500 [==>...........................] - ETA: 7:27 - loss: 1.9341 - regression_loss: 1.4136 - classification_loss: 0.5205\n",
      " 52/500 [==>...........................] - ETA: 7:27 - loss: 1.9423 - regression_loss: 1.4189 - classification_loss: 0.5235\n",
      " 53/500 [==>...........................] - ETA: 7:26 - loss: 1.9374 - regression_loss: 1.4144 - classification_loss: 0.5230\n",
      " 54/500 [==>...........................] - ETA: 7:25 - loss: 1.9275 - regression_loss: 1.4085 - classification_loss: 0.5190\n",
      " 55/500 [==>...........................] - ETA: 7:25 - loss: 1.9298 - regression_loss: 1.4094 - classification_loss: 0.5204\n",
      " 56/500 [==>...........................] - ETA: 7:24 - loss: 1.9217 - regression_loss: 1.4019 - classification_loss: 0.5198\n",
      " 57/500 [==>...........................] - ETA: 7:23 - loss: 1.9084 - regression_loss: 1.3919 - classification_loss: 0.5165\n",
      " 58/500 [==>...........................] - ETA: 7:22 - loss: 1.9044 - regression_loss: 1.3892 - classification_loss: 0.5151\n",
      " 59/500 [==>...........................] - ETA: 7:22 - loss: 1.9160 - regression_loss: 1.3979 - classification_loss: 0.5181\n",
      " 60/500 [==>...........................] - ETA: 7:21 - loss: 1.9025 - regression_loss: 1.3877 - classification_loss: 0.5149\n",
      " 61/500 [==>...........................] - ETA: 7:20 - loss: 1.8928 - regression_loss: 1.3825 - classification_loss: 0.5103\n",
      " 62/500 [==>...........................] - ETA: 7:20 - loss: 1.9069 - regression_loss: 1.3906 - classification_loss: 0.5163\n",
      " 63/500 [==>...........................] - ETA: 7:19 - loss: 1.9061 - regression_loss: 1.3934 - classification_loss: 0.5128\n",
      " 64/500 [==>...........................] - ETA: 7:18 - loss: 1.8977 - regression_loss: 1.3893 - classification_loss: 0.5084\n",
      " 65/500 [==>...........................] - ETA: 7:17 - loss: 1.8908 - regression_loss: 1.3841 - classification_loss: 0.5067\n",
      " 66/500 [==>...........................] - ETA: 7:16 - loss: 1.8801 - regression_loss: 1.3760 - classification_loss: 0.5041\n",
      " 67/500 [===>..........................] - ETA: 7:15 - loss: 1.8738 - regression_loss: 1.3701 - classification_loss: 0.5037\n",
      " 68/500 [===>..........................] - ETA: 7:14 - loss: 1.8811 - regression_loss: 1.3732 - classification_loss: 0.5079\n",
      " 69/500 [===>..........................] - ETA: 7:13 - loss: 1.8910 - regression_loss: 1.3799 - classification_loss: 0.5111\n",
      " 70/500 [===>..........................] - ETA: 7:12 - loss: 1.8791 - regression_loss: 1.3718 - classification_loss: 0.5073\n",
      " 71/500 [===>..........................] - ETA: 7:11 - loss: 1.8746 - regression_loss: 1.3663 - classification_loss: 0.5083\n",
      " 72/500 [===>..........................] - ETA: 7:10 - loss: 1.8758 - regression_loss: 1.3691 - classification_loss: 0.5067\n",
      " 73/500 [===>..........................] - ETA: 7:10 - loss: 1.8784 - regression_loss: 1.3728 - classification_loss: 0.5057\n",
      " 74/500 [===>..........................] - ETA: 7:09 - loss: 1.8742 - regression_loss: 1.3689 - classification_loss: 0.5053\n",
      " 75/500 [===>..........................] - ETA: 7:07 - loss: 1.8609 - regression_loss: 1.3599 - classification_loss: 0.5010\n",
      " 76/500 [===>..........................] - ETA: 7:06 - loss: 1.8533 - regression_loss: 1.3557 - classification_loss: 0.4975\n",
      " 77/500 [===>..........................] - ETA: 7:06 - loss: 1.8551 - regression_loss: 1.3582 - classification_loss: 0.4968\n",
      " 78/500 [===>..........................] - ETA: 7:05 - loss: 1.8490 - regression_loss: 1.3548 - classification_loss: 0.4942\n",
      " 79/500 [===>..........................] - ETA: 7:04 - loss: 1.8411 - regression_loss: 1.3487 - classification_loss: 0.4925\n",
      " 80/500 [===>..........................] - ETA: 7:03 - loss: 1.8432 - regression_loss: 1.3505 - classification_loss: 0.4926\n",
      " 81/500 [===>..........................] - ETA: 7:02 - loss: 1.8343 - regression_loss: 1.3437 - classification_loss: 0.4906\n",
      " 82/500 [===>..........................] - ETA: 7:00 - loss: 1.8360 - regression_loss: 1.3464 - classification_loss: 0.4895\n",
      " 83/500 [===>..........................] - ETA: 6:59 - loss: 1.8252 - regression_loss: 1.3389 - classification_loss: 0.4863\n",
      " 84/500 [====>.........................] - ETA: 6:59 - loss: 1.8156 - regression_loss: 1.3317 - classification_loss: 0.4840\n",
      " 85/500 [====>.........................] - ETA: 6:58 - loss: 1.8130 - regression_loss: 1.3313 - classification_loss: 0.4816\n",
      " 86/500 [====>.........................] - ETA: 6:57 - loss: 1.8151 - regression_loss: 1.3311 - classification_loss: 0.4840\n",
      " 87/500 [====>.........................] - ETA: 6:55 - loss: 1.8081 - regression_loss: 1.3258 - classification_loss: 0.4823\n",
      " 88/500 [====>.........................] - ETA: 6:55 - loss: 1.8064 - regression_loss: 1.3239 - classification_loss: 0.4825\n",
      " 89/500 [====>.........................] - ETA: 6:53 - loss: 1.8041 - regression_loss: 1.3230 - classification_loss: 0.4811\n",
      " 90/500 [====>.........................] - ETA: 6:52 - loss: 1.8063 - regression_loss: 1.3254 - classification_loss: 0.4809\n",
      " 91/500 [====>.........................] - ETA: 6:51 - loss: 1.8089 - regression_loss: 1.3267 - classification_loss: 0.4822\n",
      " 92/500 [====>.........................] - ETA: 6:50 - loss: 1.8130 - regression_loss: 1.3291 - classification_loss: 0.4839\n",
      " 93/500 [====>.........................] - ETA: 6:49 - loss: 1.8053 - regression_loss: 1.3243 - classification_loss: 0.4810\n",
      " 94/500 [====>.........................] - ETA: 6:48 - loss: 1.7976 - regression_loss: 1.3190 - classification_loss: 0.4786\n",
      " 95/500 [====>.........................] - ETA: 6:47 - loss: 1.8002 - regression_loss: 1.3229 - classification_loss: 0.4773\n",
      " 96/500 [====>.........................] - ETA: 6:47 - loss: 1.7991 - regression_loss: 1.3232 - classification_loss: 0.4759\n",
      " 97/500 [====>.........................] - ETA: 6:46 - loss: 1.7992 - regression_loss: 1.3223 - classification_loss: 0.4769\n",
      " 98/500 [====>.........................] - ETA: 6:45 - loss: 1.8040 - regression_loss: 1.3280 - classification_loss: 0.4760\n",
      " 99/500 [====>.........................] - ETA: 6:44 - loss: 1.8014 - regression_loss: 1.3258 - classification_loss: 0.4756\n",
      "100/500 [=====>........................] - ETA: 6:43 - loss: 1.8156 - regression_loss: 1.3351 - classification_loss: 0.4805\n",
      "101/500 [=====>........................] - ETA: 6:42 - loss: 1.8187 - regression_loss: 1.3381 - classification_loss: 0.4807\n",
      "102/500 [=====>........................] - ETA: 6:41 - loss: 1.8138 - regression_loss: 1.3351 - classification_loss: 0.4787\n",
      "103/500 [=====>........................] - ETA: 6:40 - loss: 1.8094 - regression_loss: 1.3318 - classification_loss: 0.4776\n",
      "104/500 [=====>........................] - ETA: 6:40 - loss: 1.8150 - regression_loss: 1.3354 - classification_loss: 0.4796\n",
      "105/500 [=====>........................] - ETA: 6:39 - loss: 1.8168 - regression_loss: 1.3379 - classification_loss: 0.4789\n",
      "106/500 [=====>........................] - ETA: 6:38 - loss: 1.8229 - regression_loss: 1.3428 - classification_loss: 0.4801\n",
      "107/500 [=====>........................] - ETA: 6:37 - loss: 1.8244 - regression_loss: 1.3452 - classification_loss: 0.4792\n",
      "108/500 [=====>........................] - ETA: 6:36 - loss: 1.8308 - regression_loss: 1.3491 - classification_loss: 0.4817\n",
      "109/500 [=====>........................] - ETA: 6:34 - loss: 1.8330 - regression_loss: 1.3504 - classification_loss: 0.4826\n",
      "110/500 [=====>........................] - ETA: 6:34 - loss: 1.8333 - regression_loss: 1.3515 - classification_loss: 0.4819\n",
      "111/500 [=====>........................] - ETA: 6:33 - loss: 1.8297 - regression_loss: 1.3483 - classification_loss: 0.4813\n",
      "112/500 [=====>........................] - ETA: 6:32 - loss: 1.8260 - regression_loss: 1.3466 - classification_loss: 0.4795\n",
      "113/500 [=====>........................] - ETA: 6:32 - loss: 1.8334 - regression_loss: 1.3524 - classification_loss: 0.4810\n",
      "114/500 [=====>........................] - ETA: 6:31 - loss: 1.8347 - regression_loss: 1.3535 - classification_loss: 0.4812\n",
      "115/500 [=====>........................] - ETA: 6:30 - loss: 1.8275 - regression_loss: 1.3486 - classification_loss: 0.4790\n",
      "116/500 [=====>........................] - ETA: 6:29 - loss: 1.8252 - regression_loss: 1.3466 - classification_loss: 0.4785\n",
      "117/500 [======>.......................] - ETA: 6:28 - loss: 1.8336 - regression_loss: 1.3531 - classification_loss: 0.4805\n",
      "118/500 [======>.......................] - ETA: 6:27 - loss: 1.8315 - regression_loss: 1.3525 - classification_loss: 0.4790\n",
      "119/500 [======>.......................] - ETA: 6:26 - loss: 1.8303 - regression_loss: 1.3516 - classification_loss: 0.4786\n",
      "120/500 [======>.......................] - ETA: 6:25 - loss: 1.8271 - regression_loss: 1.3496 - classification_loss: 0.4775\n",
      "121/500 [======>.......................] - ETA: 6:23 - loss: 1.8300 - regression_loss: 1.3505 - classification_loss: 0.4795\n",
      "122/500 [======>.......................] - ETA: 6:23 - loss: 1.8239 - regression_loss: 1.3462 - classification_loss: 0.4777\n",
      "123/500 [======>.......................] - ETA: 6:22 - loss: 1.8222 - regression_loss: 1.3445 - classification_loss: 0.4777\n",
      "124/500 [======>.......................] - ETA: 6:21 - loss: 1.8250 - regression_loss: 1.3474 - classification_loss: 0.4776\n",
      "125/500 [======>.......................] - ETA: 6:20 - loss: 1.8201 - regression_loss: 1.3433 - classification_loss: 0.4768\n",
      "126/500 [======>.......................] - ETA: 6:19 - loss: 1.8246 - regression_loss: 1.3471 - classification_loss: 0.4775\n",
      "127/500 [======>.......................] - ETA: 6:17 - loss: 1.8265 - regression_loss: 1.3493 - classification_loss: 0.4772\n",
      "128/500 [======>.......................] - ETA: 6:17 - loss: 1.8300 - regression_loss: 1.3523 - classification_loss: 0.4777\n",
      "129/500 [======>.......................] - ETA: 6:16 - loss: 1.8301 - regression_loss: 1.3525 - classification_loss: 0.4776\n",
      "130/500 [======>.......................] - ETA: 6:14 - loss: 1.8272 - regression_loss: 1.3512 - classification_loss: 0.4760\n",
      "131/500 [======>.......................] - ETA: 6:13 - loss: 1.8285 - regression_loss: 1.3527 - classification_loss: 0.4758\n",
      "132/500 [======>.......................] - ETA: 6:12 - loss: 1.8312 - regression_loss: 1.3547 - classification_loss: 0.4765\n",
      "133/500 [======>.......................] - ETA: 6:11 - loss: 1.8314 - regression_loss: 1.3547 - classification_loss: 0.4767\n",
      "134/500 [=======>......................] - ETA: 6:10 - loss: 1.8308 - regression_loss: 1.3544 - classification_loss: 0.4764\n",
      "135/500 [=======>......................] - ETA: 6:09 - loss: 1.8328 - regression_loss: 1.3560 - classification_loss: 0.4767\n",
      "136/500 [=======>......................] - ETA: 6:08 - loss: 1.8397 - regression_loss: 1.3616 - classification_loss: 0.4780\n",
      "137/500 [=======>......................] - ETA: 6:07 - loss: 1.8437 - regression_loss: 1.3638 - classification_loss: 0.4799\n",
      "138/500 [=======>......................] - ETA: 6:06 - loss: 1.8448 - regression_loss: 1.3634 - classification_loss: 0.4814\n",
      "139/500 [=======>......................] - ETA: 6:05 - loss: 1.8430 - regression_loss: 1.3618 - classification_loss: 0.4812\n",
      "140/500 [=======>......................] - ETA: 6:04 - loss: 1.8466 - regression_loss: 1.3634 - classification_loss: 0.4832\n",
      "141/500 [=======>......................] - ETA: 6:03 - loss: 1.8451 - regression_loss: 1.3618 - classification_loss: 0.4833\n",
      "142/500 [=======>......................] - ETA: 6:02 - loss: 1.8431 - regression_loss: 1.3607 - classification_loss: 0.4825\n",
      "143/500 [=======>......................] - ETA: 6:01 - loss: 1.8397 - regression_loss: 1.3589 - classification_loss: 0.4808\n",
      "144/500 [=======>......................] - ETA: 6:00 - loss: 1.8400 - regression_loss: 1.3605 - classification_loss: 0.4795\n",
      "145/500 [=======>......................] - ETA: 5:59 - loss: 1.8412 - regression_loss: 1.3617 - classification_loss: 0.4794\n",
      "146/500 [=======>......................] - ETA: 5:58 - loss: 1.8411 - regression_loss: 1.3624 - classification_loss: 0.4787\n",
      "147/500 [=======>......................] - ETA: 5:57 - loss: 1.8499 - regression_loss: 1.3676 - classification_loss: 0.4823\n",
      "148/500 [=======>......................] - ETA: 5:57 - loss: 1.8479 - regression_loss: 1.3670 - classification_loss: 0.4809\n",
      "149/500 [=======>......................] - ETA: 5:56 - loss: 1.8538 - regression_loss: 1.3702 - classification_loss: 0.4836\n",
      "150/500 [========>.....................] - ETA: 5:54 - loss: 1.8509 - regression_loss: 1.3692 - classification_loss: 0.4816\n",
      "151/500 [========>.....................] - ETA: 5:54 - loss: 1.8470 - regression_loss: 1.3672 - classification_loss: 0.4798\n",
      "152/500 [========>.....................] - ETA: 5:52 - loss: 1.8528 - regression_loss: 1.3712 - classification_loss: 0.4816\n",
      "153/500 [========>.....................] - ETA: 5:52 - loss: 1.8541 - regression_loss: 1.3734 - classification_loss: 0.4807\n",
      "154/500 [========>.....................] - ETA: 5:51 - loss: 1.8571 - regression_loss: 1.3750 - classification_loss: 0.4821\n",
      "155/500 [========>.....................] - ETA: 5:49 - loss: 1.8536 - regression_loss: 1.3707 - classification_loss: 0.4830\n",
      "156/500 [========>.....................] - ETA: 5:48 - loss: 1.8521 - regression_loss: 1.3699 - classification_loss: 0.4821\n",
      "157/500 [========>.....................] - ETA: 5:48 - loss: 1.8489 - regression_loss: 1.3680 - classification_loss: 0.4809\n",
      "158/500 [========>.....................] - ETA: 5:46 - loss: 1.8512 - regression_loss: 1.3692 - classification_loss: 0.4820\n",
      "159/500 [========>.....................] - ETA: 5:45 - loss: 1.8537 - regression_loss: 1.3707 - classification_loss: 0.4830\n",
      "160/500 [========>.....................] - ETA: 5:44 - loss: 1.8571 - regression_loss: 1.3728 - classification_loss: 0.4843\n",
      "161/500 [========>.....................] - ETA: 5:43 - loss: 1.8519 - regression_loss: 1.3684 - classification_loss: 0.4836\n",
      "162/500 [========>.....................] - ETA: 5:42 - loss: 1.8527 - regression_loss: 1.3697 - classification_loss: 0.4830\n",
      "163/500 [========>.....................] - ETA: 5:41 - loss: 1.8605 - regression_loss: 1.3756 - classification_loss: 0.4850\n",
      "164/500 [========>.....................] - ETA: 5:40 - loss: 1.8634 - regression_loss: 1.3780 - classification_loss: 0.4854\n",
      "165/500 [========>.....................] - ETA: 5:39 - loss: 1.8600 - regression_loss: 1.3756 - classification_loss: 0.4844\n",
      "166/500 [========>.....................] - ETA: 5:38 - loss: 1.8597 - regression_loss: 1.3756 - classification_loss: 0.4840\n",
      "167/500 [=========>....................] - ETA: 5:37 - loss: 1.8560 - regression_loss: 1.3731 - classification_loss: 0.4829\n",
      "168/500 [=========>....................] - ETA: 5:41 - loss: 1.8490 - regression_loss: 1.3676 - classification_loss: 0.4813\n",
      "169/500 [=========>....................] - ETA: 5:40 - loss: 1.8495 - regression_loss: 1.3673 - classification_loss: 0.4822\n",
      "170/500 [=========>....................] - ETA: 5:39 - loss: 1.8531 - regression_loss: 1.3695 - classification_loss: 0.4836\n",
      "171/500 [=========>....................] - ETA: 5:38 - loss: 1.8576 - regression_loss: 1.3734 - classification_loss: 0.4842\n",
      "172/500 [=========>....................] - ETA: 5:37 - loss: 1.8626 - regression_loss: 1.3765 - classification_loss: 0.4861\n",
      "173/500 [=========>....................] - ETA: 5:36 - loss: 1.8582 - regression_loss: 1.3736 - classification_loss: 0.4846\n",
      "174/500 [=========>....................] - ETA: 5:34 - loss: 1.8530 - regression_loss: 1.3702 - classification_loss: 0.4828\n",
      "175/500 [=========>....................] - ETA: 5:33 - loss: 1.8565 - regression_loss: 1.3718 - classification_loss: 0.4847\n",
      "176/500 [=========>....................] - ETA: 5:32 - loss: 1.8562 - regression_loss: 1.3721 - classification_loss: 0.4841\n",
      "177/500 [=========>....................] - ETA: 5:32 - loss: 1.8555 - regression_loss: 1.3712 - classification_loss: 0.4844\n",
      "178/500 [=========>....................] - ETA: 5:31 - loss: 1.8529 - regression_loss: 1.3697 - classification_loss: 0.4832\n",
      "179/500 [=========>....................] - ETA: 5:30 - loss: 1.8566 - regression_loss: 1.3731 - classification_loss: 0.4834\n",
      "180/500 [=========>....................] - ETA: 5:29 - loss: 1.8583 - regression_loss: 1.3752 - classification_loss: 0.4830\n",
      "181/500 [=========>....................] - ETA: 5:27 - loss: 1.8582 - regression_loss: 1.3759 - classification_loss: 0.4823\n",
      "182/500 [=========>....................] - ETA: 5:26 - loss: 1.8574 - regression_loss: 1.3753 - classification_loss: 0.4821\n",
      "183/500 [=========>....................] - ETA: 5:25 - loss: 1.8542 - regression_loss: 1.3733 - classification_loss: 0.4809\n",
      "184/500 [==========>...................] - ETA: 5:24 - loss: 1.8541 - regression_loss: 1.3738 - classification_loss: 0.4803\n",
      "185/500 [==========>...................] - ETA: 5:23 - loss: 1.8545 - regression_loss: 1.3745 - classification_loss: 0.4801\n",
      "186/500 [==========>...................] - ETA: 5:22 - loss: 1.8533 - regression_loss: 1.3734 - classification_loss: 0.4798\n",
      "187/500 [==========>...................] - ETA: 5:21 - loss: 1.8521 - regression_loss: 1.3731 - classification_loss: 0.4791\n",
      "188/500 [==========>...................] - ETA: 5:20 - loss: 1.8577 - regression_loss: 1.3773 - classification_loss: 0.4803\n",
      "189/500 [==========>...................] - ETA: 5:19 - loss: 1.8574 - regression_loss: 1.3776 - classification_loss: 0.4798\n",
      "190/500 [==========>...................] - ETA: 5:18 - loss: 1.8525 - regression_loss: 1.3741 - classification_loss: 0.4785\n",
      "191/500 [==========>...................] - ETA: 5:17 - loss: 1.8582 - regression_loss: 1.3782 - classification_loss: 0.4800\n",
      "192/500 [==========>...................] - ETA: 5:16 - loss: 1.8556 - regression_loss: 1.3768 - classification_loss: 0.4788\n",
      "193/500 [==========>...................] - ETA: 5:15 - loss: 1.8539 - regression_loss: 1.3753 - classification_loss: 0.4786\n",
      "194/500 [==========>...................] - ETA: 5:14 - loss: 1.8513 - regression_loss: 1.3737 - classification_loss: 0.4776\n",
      "195/500 [==========>...................] - ETA: 5:13 - loss: 1.8500 - regression_loss: 1.3730 - classification_loss: 0.4770\n",
      "196/500 [==========>...................] - ETA: 5:12 - loss: 1.8548 - regression_loss: 1.3763 - classification_loss: 0.4786\n",
      "197/500 [==========>...................] - ETA: 5:11 - loss: 1.8543 - regression_loss: 1.3768 - classification_loss: 0.4775\n",
      "198/500 [==========>...................] - ETA: 5:10 - loss: 1.8557 - regression_loss: 1.3782 - classification_loss: 0.4775\n",
      "199/500 [==========>...................] - ETA: 5:09 - loss: 1.8550 - regression_loss: 1.3776 - classification_loss: 0.4774\n",
      "200/500 [===========>..................] - ETA: 5:08 - loss: 1.8528 - regression_loss: 1.3759 - classification_loss: 0.4769\n",
      "201/500 [===========>..................] - ETA: 5:07 - loss: 1.8556 - regression_loss: 1.3781 - classification_loss: 0.4775\n",
      "202/500 [===========>..................] - ETA: 5:06 - loss: 1.8605 - regression_loss: 1.3811 - classification_loss: 0.4794\n",
      "203/500 [===========>..................] - ETA: 5:04 - loss: 1.8627 - regression_loss: 1.3826 - classification_loss: 0.4801\n",
      "204/500 [===========>..................] - ETA: 5:04 - loss: 1.8612 - regression_loss: 1.3822 - classification_loss: 0.4790\n",
      "205/500 [===========>..................] - ETA: 5:03 - loss: 1.8608 - regression_loss: 1.3822 - classification_loss: 0.4787\n",
      "206/500 [===========>..................] - ETA: 5:02 - loss: 1.8598 - regression_loss: 1.3814 - classification_loss: 0.4784\n",
      "207/500 [===========>..................] - ETA: 5:01 - loss: 1.8604 - regression_loss: 1.3820 - classification_loss: 0.4784\n",
      "208/500 [===========>..................] - ETA: 5:00 - loss: 1.8643 - regression_loss: 1.3844 - classification_loss: 0.4799\n",
      "209/500 [===========>..................] - ETA: 4:59 - loss: 1.8646 - regression_loss: 1.3843 - classification_loss: 0.4803\n",
      "210/500 [===========>..................] - ETA: 4:57 - loss: 1.8624 - regression_loss: 1.3829 - classification_loss: 0.4795\n",
      "211/500 [===========>..................] - ETA: 4:56 - loss: 1.8613 - regression_loss: 1.3812 - classification_loss: 0.4802\n",
      "212/500 [===========>..................] - ETA: 4:55 - loss: 1.8630 - regression_loss: 1.3822 - classification_loss: 0.4808\n",
      "213/500 [===========>..................] - ETA: 4:54 - loss: 1.8656 - regression_loss: 1.3838 - classification_loss: 0.4818\n",
      "214/500 [===========>..................] - ETA: 4:53 - loss: 1.8660 - regression_loss: 1.3842 - classification_loss: 0.4817\n",
      "215/500 [===========>..................] - ETA: 4:52 - loss: 1.8643 - regression_loss: 1.3834 - classification_loss: 0.4809\n",
      "216/500 [===========>..................] - ETA: 4:51 - loss: 1.8657 - regression_loss: 1.3842 - classification_loss: 0.4815\n",
      "217/500 [============>.................] - ETA: 4:50 - loss: 1.8625 - regression_loss: 1.3820 - classification_loss: 0.4805\n",
      "218/500 [============>.................] - ETA: 4:49 - loss: 1.8612 - regression_loss: 1.3803 - classification_loss: 0.4809\n",
      "219/500 [============>.................] - ETA: 4:48 - loss: 1.8618 - regression_loss: 1.3807 - classification_loss: 0.4811\n",
      "220/500 [============>.................] - ETA: 4:47 - loss: 1.8586 - regression_loss: 1.3781 - classification_loss: 0.4805\n",
      "221/500 [============>.................] - ETA: 4:46 - loss: 1.8634 - regression_loss: 1.3812 - classification_loss: 0.4822\n",
      "222/500 [============>.................] - ETA: 4:45 - loss: 1.8690 - regression_loss: 1.3847 - classification_loss: 0.4843\n",
      "223/500 [============>.................] - ETA: 4:44 - loss: 1.8661 - regression_loss: 1.3829 - classification_loss: 0.4833\n",
      "224/500 [============>.................] - ETA: 4:43 - loss: 1.8651 - regression_loss: 1.3824 - classification_loss: 0.4827\n",
      "225/500 [============>.................] - ETA: 4:42 - loss: 1.8678 - regression_loss: 1.3838 - classification_loss: 0.4840\n",
      "226/500 [============>.................] - ETA: 4:41 - loss: 1.8695 - regression_loss: 1.3849 - classification_loss: 0.4847\n",
      "227/500 [============>.................] - ETA: 4:40 - loss: 1.8690 - regression_loss: 1.3840 - classification_loss: 0.4850\n",
      "228/500 [============>.................] - ETA: 4:39 - loss: 1.8689 - regression_loss: 1.3843 - classification_loss: 0.4846\n",
      "229/500 [============>.................] - ETA: 4:38 - loss: 1.8717 - regression_loss: 1.3867 - classification_loss: 0.4850\n",
      "230/500 [============>.................] - ETA: 4:37 - loss: 1.8697 - regression_loss: 1.3852 - classification_loss: 0.4844\n",
      "231/500 [============>.................] - ETA: 4:36 - loss: 1.8714 - regression_loss: 1.3871 - classification_loss: 0.4843\n",
      "232/500 [============>.................] - ETA: 4:35 - loss: 1.8729 - regression_loss: 1.3883 - classification_loss: 0.4847\n",
      "233/500 [============>.................] - ETA: 4:34 - loss: 1.8709 - regression_loss: 1.3872 - classification_loss: 0.4837\n",
      "234/500 [=============>................] - ETA: 4:32 - loss: 1.8713 - regression_loss: 1.3880 - classification_loss: 0.4833\n",
      "235/500 [=============>................] - ETA: 4:32 - loss: 1.8740 - regression_loss: 1.3896 - classification_loss: 0.4845\n",
      "236/500 [=============>................] - ETA: 4:31 - loss: 1.8752 - regression_loss: 1.3902 - classification_loss: 0.4851\n",
      "237/500 [=============>................] - ETA: 4:30 - loss: 1.8731 - regression_loss: 1.3888 - classification_loss: 0.4844\n",
      "238/500 [=============>................] - ETA: 4:29 - loss: 1.8769 - regression_loss: 1.3924 - classification_loss: 0.4845\n",
      "239/500 [=============>................] - ETA: 4:28 - loss: 1.8764 - regression_loss: 1.3926 - classification_loss: 0.4838\n",
      "240/500 [=============>................] - ETA: 4:26 - loss: 1.8796 - regression_loss: 1.3946 - classification_loss: 0.4850\n",
      "241/500 [=============>................] - ETA: 4:25 - loss: 1.8779 - regression_loss: 1.3936 - classification_loss: 0.4843\n",
      "242/500 [=============>................] - ETA: 4:24 - loss: 1.8801 - regression_loss: 1.3959 - classification_loss: 0.4842\n",
      "243/500 [=============>................] - ETA: 4:23 - loss: 1.8826 - regression_loss: 1.3969 - classification_loss: 0.4857\n",
      "244/500 [=============>................] - ETA: 4:22 - loss: 1.8803 - regression_loss: 1.3955 - classification_loss: 0.4848\n",
      "245/500 [=============>................] - ETA: 4:21 - loss: 1.8826 - regression_loss: 1.3976 - classification_loss: 0.4850\n",
      "246/500 [=============>................] - ETA: 4:20 - loss: 1.8790 - regression_loss: 1.3950 - classification_loss: 0.4840\n",
      "247/500 [=============>................] - ETA: 4:19 - loss: 1.8792 - regression_loss: 1.3950 - classification_loss: 0.4841\n",
      "248/500 [=============>................] - ETA: 4:18 - loss: 1.8798 - regression_loss: 1.3952 - classification_loss: 0.4845\n",
      "249/500 [=============>................] - ETA: 4:17 - loss: 1.8791 - regression_loss: 1.3948 - classification_loss: 0.4843\n",
      "250/500 [==============>...............] - ETA: 4:16 - loss: 1.8828 - regression_loss: 1.3984 - classification_loss: 0.4844\n",
      "251/500 [==============>...............] - ETA: 4:15 - loss: 1.8823 - regression_loss: 1.3978 - classification_loss: 0.4844\n",
      "252/500 [==============>...............] - ETA: 4:14 - loss: 1.8810 - regression_loss: 1.3971 - classification_loss: 0.4838\n",
      "253/500 [==============>...............] - ETA: 4:13 - loss: 1.8806 - regression_loss: 1.3969 - classification_loss: 0.4838\n",
      "254/500 [==============>...............] - ETA: 4:12 - loss: 1.8786 - regression_loss: 1.3957 - classification_loss: 0.4829\n",
      "255/500 [==============>...............] - ETA: 4:11 - loss: 1.8793 - regression_loss: 1.3959 - classification_loss: 0.4834\n",
      "256/500 [==============>...............] - ETA: 4:10 - loss: 1.8815 - regression_loss: 1.3979 - classification_loss: 0.4836\n",
      "257/500 [==============>...............] - ETA: 4:09 - loss: 1.8831 - regression_loss: 1.3993 - classification_loss: 0.4838\n",
      "258/500 [==============>...............] - ETA: 4:08 - loss: 1.8856 - regression_loss: 1.4006 - classification_loss: 0.4849\n",
      "259/500 [==============>...............] - ETA: 4:07 - loss: 1.8845 - regression_loss: 1.4002 - classification_loss: 0.4843\n",
      "260/500 [==============>...............] - ETA: 4:06 - loss: 1.8860 - regression_loss: 1.4002 - classification_loss: 0.4858\n",
      "261/500 [==============>...............] - ETA: 4:05 - loss: 1.8872 - regression_loss: 1.4003 - classification_loss: 0.4869\n",
      "262/500 [==============>...............] - ETA: 4:04 - loss: 1.8871 - regression_loss: 1.4009 - classification_loss: 0.4862\n",
      "263/500 [==============>...............] - ETA: 4:03 - loss: 1.8863 - regression_loss: 1.3998 - classification_loss: 0.4865\n",
      "264/500 [==============>...............] - ETA: 4:01 - loss: 1.8896 - regression_loss: 1.4011 - classification_loss: 0.4886\n",
      "265/500 [==============>...............] - ETA: 4:01 - loss: 1.8934 - regression_loss: 1.4032 - classification_loss: 0.4902\n",
      "266/500 [==============>...............] - ETA: 3:59 - loss: 1.8965 - regression_loss: 1.4059 - classification_loss: 0.4907\n",
      "267/500 [===============>..............] - ETA: 3:58 - loss: 1.8966 - regression_loss: 1.4060 - classification_loss: 0.4906\n",
      "268/500 [===============>..............] - ETA: 3:57 - loss: 1.8966 - regression_loss: 1.4061 - classification_loss: 0.4906\n",
      "269/500 [===============>..............] - ETA: 3:56 - loss: 1.8973 - regression_loss: 1.4067 - classification_loss: 0.4905\n",
      "270/500 [===============>..............] - ETA: 3:55 - loss: 1.8993 - regression_loss: 1.4087 - classification_loss: 0.4906\n",
      "271/500 [===============>..............] - ETA: 3:54 - loss: 1.8985 - regression_loss: 1.4082 - classification_loss: 0.4903\n",
      "272/500 [===============>..............] - ETA: 3:53 - loss: 1.8983 - regression_loss: 1.4080 - classification_loss: 0.4903\n",
      "273/500 [===============>..............] - ETA: 3:52 - loss: 1.8959 - regression_loss: 1.4062 - classification_loss: 0.4896\n",
      "274/500 [===============>..............] - ETA: 3:51 - loss: 1.8937 - regression_loss: 1.4046 - classification_loss: 0.4891\n",
      "275/500 [===============>..............] - ETA: 3:50 - loss: 1.8903 - regression_loss: 1.4022 - classification_loss: 0.4881\n",
      "276/500 [===============>..............] - ETA: 3:49 - loss: 1.8884 - regression_loss: 1.4011 - classification_loss: 0.4873\n",
      "277/500 [===============>..............] - ETA: 3:48 - loss: 1.8869 - regression_loss: 1.3999 - classification_loss: 0.4870\n",
      "278/500 [===============>..............] - ETA: 3:47 - loss: 1.8888 - regression_loss: 1.4010 - classification_loss: 0.4877\n",
      "279/500 [===============>..............] - ETA: 3:46 - loss: 1.8904 - regression_loss: 1.4017 - classification_loss: 0.4887\n",
      "280/500 [===============>..............] - ETA: 3:45 - loss: 1.8925 - regression_loss: 1.4031 - classification_loss: 0.4894\n",
      "281/500 [===============>..............] - ETA: 3:44 - loss: 1.8924 - regression_loss: 1.4026 - classification_loss: 0.4898\n",
      "282/500 [===============>..............] - ETA: 3:43 - loss: 1.8922 - regression_loss: 1.4029 - classification_loss: 0.4893\n",
      "283/500 [===============>..............] - ETA: 3:42 - loss: 1.8919 - regression_loss: 1.4027 - classification_loss: 0.4892\n",
      "284/500 [================>.............] - ETA: 3:41 - loss: 1.8938 - regression_loss: 1.4037 - classification_loss: 0.4902\n",
      "285/500 [================>.............] - ETA: 3:40 - loss: 1.8979 - regression_loss: 1.4063 - classification_loss: 0.4916\n",
      "286/500 [================>.............] - ETA: 3:39 - loss: 1.8996 - regression_loss: 1.4076 - classification_loss: 0.4921\n",
      "287/500 [================>.............] - ETA: 3:38 - loss: 1.8968 - regression_loss: 1.4055 - classification_loss: 0.4913\n",
      "288/500 [================>.............] - ETA: 3:37 - loss: 1.8999 - regression_loss: 1.4072 - classification_loss: 0.4927\n",
      "289/500 [================>.............] - ETA: 3:36 - loss: 1.9012 - regression_loss: 1.4076 - classification_loss: 0.4936\n",
      "290/500 [================>.............] - ETA: 3:35 - loss: 1.9019 - regression_loss: 1.4081 - classification_loss: 0.4938\n",
      "291/500 [================>.............] - ETA: 3:34 - loss: 1.9021 - regression_loss: 1.4087 - classification_loss: 0.4933\n",
      "292/500 [================>.............] - ETA: 3:33 - loss: 1.9008 - regression_loss: 1.4079 - classification_loss: 0.4929\n",
      "293/500 [================>.............] - ETA: 3:32 - loss: 1.9020 - regression_loss: 1.4091 - classification_loss: 0.4928\n",
      "294/500 [================>.............] - ETA: 3:31 - loss: 1.9008 - regression_loss: 1.4083 - classification_loss: 0.4925\n",
      "295/500 [================>.............] - ETA: 3:30 - loss: 1.8991 - regression_loss: 1.4072 - classification_loss: 0.4919\n",
      "296/500 [================>.............] - ETA: 3:29 - loss: 1.8985 - regression_loss: 1.4071 - classification_loss: 0.4915\n",
      "297/500 [================>.............] - ETA: 3:28 - loss: 1.9024 - regression_loss: 1.4093 - classification_loss: 0.4931\n",
      "298/500 [================>.............] - ETA: 3:26 - loss: 1.9011 - regression_loss: 1.4081 - classification_loss: 0.4929\n",
      "299/500 [================>.............] - ETA: 3:25 - loss: 1.9056 - regression_loss: 1.4111 - classification_loss: 0.4945\n",
      "300/500 [=================>............] - ETA: 3:24 - loss: 1.9053 - regression_loss: 1.4111 - classification_loss: 0.4942\n",
      "301/500 [=================>............] - ETA: 3:23 - loss: 1.9049 - regression_loss: 1.4107 - classification_loss: 0.4943\n",
      "302/500 [=================>............] - ETA: 3:22 - loss: 1.9040 - regression_loss: 1.4101 - classification_loss: 0.4939\n",
      "303/500 [=================>............] - ETA: 3:21 - loss: 1.9054 - regression_loss: 1.4117 - classification_loss: 0.4937\n",
      "304/500 [=================>............] - ETA: 3:20 - loss: 1.9089 - regression_loss: 1.4144 - classification_loss: 0.4945\n",
      "305/500 [=================>............] - ETA: 3:19 - loss: 1.9075 - regression_loss: 1.4136 - classification_loss: 0.4939\n",
      "306/500 [=================>............] - ETA: 3:18 - loss: 1.9084 - regression_loss: 1.4140 - classification_loss: 0.4944\n",
      "307/500 [=================>............] - ETA: 3:17 - loss: 1.9084 - regression_loss: 1.4142 - classification_loss: 0.4942\n",
      "308/500 [=================>............] - ETA: 3:16 - loss: 1.9070 - regression_loss: 1.4133 - classification_loss: 0.4937\n",
      "309/500 [=================>............] - ETA: 3:15 - loss: 1.9075 - regression_loss: 1.4134 - classification_loss: 0.4940\n",
      "310/500 [=================>............] - ETA: 3:14 - loss: 1.9072 - regression_loss: 1.4138 - classification_loss: 0.4934\n",
      "311/500 [=================>............] - ETA: 3:13 - loss: 1.9068 - regression_loss: 1.4135 - classification_loss: 0.4933\n",
      "312/500 [=================>............] - ETA: 3:12 - loss: 1.9064 - regression_loss: 1.4135 - classification_loss: 0.4930\n",
      "313/500 [=================>............] - ETA: 3:11 - loss: 1.9096 - regression_loss: 1.4153 - classification_loss: 0.4942\n",
      "314/500 [=================>............] - ETA: 3:10 - loss: 1.9079 - regression_loss: 1.4140 - classification_loss: 0.4939\n",
      "315/500 [=================>............] - ETA: 3:09 - loss: 1.9080 - regression_loss: 1.4141 - classification_loss: 0.4939\n",
      "316/500 [=================>............] - ETA: 3:08 - loss: 1.9059 - regression_loss: 1.4127 - classification_loss: 0.4933\n",
      "317/500 [==================>...........] - ETA: 3:07 - loss: 1.9050 - regression_loss: 1.4116 - classification_loss: 0.4934\n",
      "318/500 [==================>...........] - ETA: 3:06 - loss: 1.9042 - regression_loss: 1.4108 - classification_loss: 0.4933\n",
      "319/500 [==================>...........] - ETA: 3:05 - loss: 1.9047 - regression_loss: 1.4114 - classification_loss: 0.4932\n",
      "320/500 [==================>...........] - ETA: 3:04 - loss: 1.9039 - regression_loss: 1.4111 - classification_loss: 0.4928\n",
      "321/500 [==================>...........] - ETA: 3:03 - loss: 1.9023 - regression_loss: 1.4100 - classification_loss: 0.4924\n",
      "322/500 [==================>...........] - ETA: 3:02 - loss: 1.9004 - regression_loss: 1.4087 - classification_loss: 0.4917\n",
      "323/500 [==================>...........] - ETA: 3:01 - loss: 1.8996 - regression_loss: 1.4081 - classification_loss: 0.4915\n",
      "324/500 [==================>...........] - ETA: 2:59 - loss: 1.8994 - regression_loss: 1.4084 - classification_loss: 0.4910\n",
      "325/500 [==================>...........] - ETA: 2:58 - loss: 1.8972 - regression_loss: 1.4067 - classification_loss: 0.4905\n",
      "326/500 [==================>...........] - ETA: 2:57 - loss: 1.8948 - regression_loss: 1.4049 - classification_loss: 0.4899\n",
      "327/500 [==================>...........] - ETA: 2:56 - loss: 1.8941 - regression_loss: 1.4041 - classification_loss: 0.4899\n",
      "328/500 [==================>...........] - ETA: 2:55 - loss: 1.8959 - regression_loss: 1.4055 - classification_loss: 0.4904\n",
      "329/500 [==================>...........] - ETA: 2:54 - loss: 1.8972 - regression_loss: 1.4067 - classification_loss: 0.4905\n",
      "330/500 [==================>...........] - ETA: 2:53 - loss: 1.8963 - regression_loss: 1.4062 - classification_loss: 0.4901\n",
      "331/500 [==================>...........] - ETA: 2:52 - loss: 1.8974 - regression_loss: 1.4070 - classification_loss: 0.4904\n",
      "332/500 [==================>...........] - ETA: 2:51 - loss: 1.8978 - regression_loss: 1.4074 - classification_loss: 0.4904\n",
      "333/500 [==================>...........] - ETA: 2:50 - loss: 1.8978 - regression_loss: 1.4077 - classification_loss: 0.4901\n",
      "334/500 [===================>..........] - ETA: 2:49 - loss: 1.8987 - regression_loss: 1.4085 - classification_loss: 0.4902\n",
      "335/500 [===================>..........] - ETA: 2:48 - loss: 1.8981 - regression_loss: 1.4079 - classification_loss: 0.4902\n",
      "336/500 [===================>..........] - ETA: 2:47 - loss: 1.8989 - regression_loss: 1.4080 - classification_loss: 0.4909\n",
      "337/500 [===================>..........] - ETA: 2:46 - loss: 1.9011 - regression_loss: 1.4095 - classification_loss: 0.4916\n",
      "338/500 [===================>..........] - ETA: 2:45 - loss: 1.9009 - regression_loss: 1.4096 - classification_loss: 0.4913\n",
      "339/500 [===================>..........] - ETA: 2:44 - loss: 1.9005 - regression_loss: 1.4094 - classification_loss: 0.4912\n",
      "340/500 [===================>..........] - ETA: 2:43 - loss: 1.8989 - regression_loss: 1.4082 - classification_loss: 0.4907\n",
      "341/500 [===================>..........] - ETA: 2:42 - loss: 1.8979 - regression_loss: 1.4070 - classification_loss: 0.4909\n",
      "342/500 [===================>..........] - ETA: 2:41 - loss: 1.8967 - regression_loss: 1.4061 - classification_loss: 0.4906\n",
      "343/500 [===================>..........] - ETA: 2:40 - loss: 1.8997 - regression_loss: 1.4080 - classification_loss: 0.4917\n",
      "344/500 [===================>..........] - ETA: 2:39 - loss: 1.8994 - regression_loss: 1.4078 - classification_loss: 0.4915\n",
      "345/500 [===================>..........] - ETA: 2:38 - loss: 1.9009 - regression_loss: 1.4090 - classification_loss: 0.4919\n",
      "346/500 [===================>..........] - ETA: 2:37 - loss: 1.9003 - regression_loss: 1.4085 - classification_loss: 0.4919\n",
      "347/500 [===================>..........] - ETA: 2:36 - loss: 1.8999 - regression_loss: 1.4084 - classification_loss: 0.4916\n",
      "348/500 [===================>..........] - ETA: 2:35 - loss: 1.9018 - regression_loss: 1.4092 - classification_loss: 0.4926\n",
      "349/500 [===================>..........] - ETA: 2:34 - loss: 1.9011 - regression_loss: 1.4089 - classification_loss: 0.4922\n",
      "350/500 [====================>.........] - ETA: 2:33 - loss: 1.9016 - regression_loss: 1.4095 - classification_loss: 0.4921\n",
      "351/500 [====================>.........] - ETA: 2:32 - loss: 1.9009 - regression_loss: 1.4090 - classification_loss: 0.4919\n",
      "352/500 [====================>.........] - ETA: 2:31 - loss: 1.9040 - regression_loss: 1.4113 - classification_loss: 0.4927\n",
      "353/500 [====================>.........] - ETA: 2:30 - loss: 1.9036 - regression_loss: 1.4111 - classification_loss: 0.4925\n",
      "354/500 [====================>.........] - ETA: 2:29 - loss: 1.9032 - regression_loss: 1.4108 - classification_loss: 0.4924\n",
      "355/500 [====================>.........] - ETA: 2:28 - loss: 1.9037 - regression_loss: 1.4114 - classification_loss: 0.4923\n",
      "356/500 [====================>.........] - ETA: 2:27 - loss: 1.9026 - regression_loss: 1.4104 - classification_loss: 0.4922\n",
      "357/500 [====================>.........] - ETA: 2:26 - loss: 1.9017 - regression_loss: 1.4095 - classification_loss: 0.4922\n",
      "358/500 [====================>.........] - ETA: 2:25 - loss: 1.9009 - regression_loss: 1.4091 - classification_loss: 0.4918\n",
      "359/500 [====================>.........] - ETA: 2:24 - loss: 1.8996 - regression_loss: 1.4082 - classification_loss: 0.4914\n",
      "360/500 [====================>.........] - ETA: 2:23 - loss: 1.8992 - regression_loss: 1.4084 - classification_loss: 0.4908\n",
      "361/500 [====================>.........] - ETA: 2:22 - loss: 1.9003 - regression_loss: 1.4091 - classification_loss: 0.4912\n",
      "362/500 [====================>.........] - ETA: 2:21 - loss: 1.8986 - regression_loss: 1.4079 - classification_loss: 0.4907\n",
      "363/500 [====================>.........] - ETA: 2:20 - loss: 1.8981 - regression_loss: 1.4074 - classification_loss: 0.4907\n",
      "364/500 [====================>.........] - ETA: 2:19 - loss: 1.8998 - regression_loss: 1.4092 - classification_loss: 0.4906\n",
      "365/500 [====================>.........] - ETA: 2:18 - loss: 1.8993 - regression_loss: 1.4087 - classification_loss: 0.4905\n",
      "366/500 [====================>.........] - ETA: 2:17 - loss: 1.9017 - regression_loss: 1.4105 - classification_loss: 0.4912\n",
      "367/500 [=====================>........] - ETA: 2:16 - loss: 1.9025 - regression_loss: 1.4115 - classification_loss: 0.4910\n",
      "368/500 [=====================>........] - ETA: 2:15 - loss: 1.9015 - regression_loss: 1.4109 - classification_loss: 0.4905\n",
      "369/500 [=====================>........] - ETA: 2:14 - loss: 1.9024 - regression_loss: 1.4110 - classification_loss: 0.4914\n",
      "370/500 [=====================>........] - ETA: 2:13 - loss: 1.9017 - regression_loss: 1.4105 - classification_loss: 0.4912\n",
      "371/500 [=====================>........] - ETA: 2:12 - loss: 1.9023 - regression_loss: 1.4112 - classification_loss: 0.4911\n",
      "372/500 [=====================>........] - ETA: 2:11 - loss: 1.9026 - regression_loss: 1.4111 - classification_loss: 0.4915\n",
      "373/500 [=====================>........] - ETA: 2:10 - loss: 1.9014 - regression_loss: 1.4106 - classification_loss: 0.4908\n",
      "374/500 [=====================>........] - ETA: 2:09 - loss: 1.9012 - regression_loss: 1.4099 - classification_loss: 0.4912\n",
      "375/500 [=====================>........] - ETA: 2:08 - loss: 1.9017 - regression_loss: 1.4105 - classification_loss: 0.4912\n",
      "376/500 [=====================>........] - ETA: 2:07 - loss: 1.9004 - regression_loss: 1.4094 - classification_loss: 0.4910\n",
      "377/500 [=====================>........] - ETA: 2:06 - loss: 1.8994 - regression_loss: 1.4089 - classification_loss: 0.4905\n",
      "378/500 [=====================>........] - ETA: 2:05 - loss: 1.8977 - regression_loss: 1.4077 - classification_loss: 0.4901\n",
      "379/500 [=====================>........] - ETA: 2:04 - loss: 1.8992 - regression_loss: 1.4092 - classification_loss: 0.4899\n",
      "380/500 [=====================>........] - ETA: 2:02 - loss: 1.8986 - regression_loss: 1.4090 - classification_loss: 0.4896\n",
      "381/500 [=====================>........] - ETA: 2:01 - loss: 1.8989 - regression_loss: 1.4090 - classification_loss: 0.4899\n",
      "382/500 [=====================>........] - ETA: 2:00 - loss: 1.8987 - regression_loss: 1.4088 - classification_loss: 0.4899\n",
      "383/500 [=====================>........] - ETA: 1:59 - loss: 1.9009 - regression_loss: 1.4102 - classification_loss: 0.4908\n",
      "384/500 [======================>.......] - ETA: 1:58 - loss: 1.9012 - regression_loss: 1.4103 - classification_loss: 0.4909\n",
      "385/500 [======================>.......] - ETA: 1:57 - loss: 1.9006 - regression_loss: 1.4099 - classification_loss: 0.4907\n",
      "386/500 [======================>.......] - ETA: 1:56 - loss: 1.8988 - regression_loss: 1.4085 - classification_loss: 0.4903\n",
      "387/500 [======================>.......] - ETA: 1:55 - loss: 1.8999 - regression_loss: 1.4090 - classification_loss: 0.4909\n",
      "388/500 [======================>.......] - ETA: 1:54 - loss: 1.8986 - regression_loss: 1.4082 - classification_loss: 0.4904\n",
      "389/500 [======================>.......] - ETA: 1:53 - loss: 1.8990 - regression_loss: 1.4085 - classification_loss: 0.4904\n",
      "390/500 [======================>.......] - ETA: 1:52 - loss: 1.8983 - regression_loss: 1.4084 - classification_loss: 0.4900\n",
      "391/500 [======================>.......] - ETA: 1:51 - loss: 1.8958 - regression_loss: 1.4065 - classification_loss: 0.4892\n",
      "392/500 [======================>.......] - ETA: 1:50 - loss: 1.8968 - regression_loss: 1.4077 - classification_loss: 0.4891\n",
      "393/500 [======================>.......] - ETA: 1:49 - loss: 1.8969 - regression_loss: 1.4080 - classification_loss: 0.4889\n",
      "394/500 [======================>.......] - ETA: 1:48 - loss: 1.8981 - regression_loss: 1.4091 - classification_loss: 0.4890\n",
      "395/500 [======================>.......] - ETA: 1:47 - loss: 1.8968 - regression_loss: 1.4080 - classification_loss: 0.4888\n",
      "396/500 [======================>.......] - ETA: 1:46 - loss: 1.8980 - regression_loss: 1.4090 - classification_loss: 0.4891\n",
      "397/500 [======================>.......] - ETA: 1:45 - loss: 1.8960 - regression_loss: 1.4076 - classification_loss: 0.4884\n",
      "398/500 [======================>.......] - ETA: 1:44 - loss: 1.8982 - regression_loss: 1.4092 - classification_loss: 0.4889\n",
      "399/500 [======================>.......] - ETA: 1:43 - loss: 1.8982 - regression_loss: 1.4090 - classification_loss: 0.4892\n",
      "400/500 [=======================>......] - ETA: 1:42 - loss: 1.9011 - regression_loss: 1.4111 - classification_loss: 0.4900\n",
      "401/500 [=======================>......] - ETA: 1:41 - loss: 1.9007 - regression_loss: 1.4111 - classification_loss: 0.4896\n",
      "402/500 [=======================>......] - ETA: 1:40 - loss: 1.9015 - regression_loss: 1.4115 - classification_loss: 0.4900\n",
      "403/500 [=======================>......] - ETA: 1:39 - loss: 1.9024 - regression_loss: 1.4117 - classification_loss: 0.4907\n",
      "404/500 [=======================>......] - ETA: 1:38 - loss: 1.9032 - regression_loss: 1.4123 - classification_loss: 0.4909\n",
      "405/500 [=======================>......] - ETA: 1:37 - loss: 1.9025 - regression_loss: 1.4117 - classification_loss: 0.4908\n",
      "406/500 [=======================>......] - ETA: 1:36 - loss: 1.9031 - regression_loss: 1.4122 - classification_loss: 0.4909\n",
      "407/500 [=======================>......] - ETA: 1:35 - loss: 1.9032 - regression_loss: 1.4123 - classification_loss: 0.4909\n",
      "408/500 [=======================>......] - ETA: 1:34 - loss: 1.9059 - regression_loss: 1.4143 - classification_loss: 0.4916\n",
      "409/500 [=======================>......] - ETA: 1:33 - loss: 1.9051 - regression_loss: 1.4136 - classification_loss: 0.4915\n",
      "410/500 [=======================>......] - ETA: 1:32 - loss: 1.9053 - regression_loss: 1.4137 - classification_loss: 0.4916\n",
      "411/500 [=======================>......] - ETA: 1:31 - loss: 1.9029 - regression_loss: 1.4121 - classification_loss: 0.4908\n",
      "412/500 [=======================>......] - ETA: 1:30 - loss: 1.9022 - regression_loss: 1.4117 - classification_loss: 0.4905\n",
      "413/500 [=======================>......] - ETA: 1:29 - loss: 1.9015 - regression_loss: 1.4115 - classification_loss: 0.4900\n",
      "414/500 [=======================>......] - ETA: 1:28 - loss: 1.9017 - regression_loss: 1.4115 - classification_loss: 0.4902\n",
      "415/500 [=======================>......] - ETA: 1:27 - loss: 1.9013 - regression_loss: 1.4114 - classification_loss: 0.4899\n",
      "416/500 [=======================>......] - ETA: 1:26 - loss: 1.9009 - regression_loss: 1.4113 - classification_loss: 0.4896\n",
      "417/500 [========================>.....] - ETA: 1:25 - loss: 1.9020 - regression_loss: 1.4124 - classification_loss: 0.4895\n",
      "418/500 [========================>.....] - ETA: 1:24 - loss: 1.9042 - regression_loss: 1.4134 - classification_loss: 0.4909\n",
      "419/500 [========================>.....] - ETA: 1:23 - loss: 1.9041 - regression_loss: 1.4133 - classification_loss: 0.4908\n",
      "420/500 [========================>.....] - ETA: 1:22 - loss: 1.9030 - regression_loss: 1.4127 - classification_loss: 0.4902\n",
      "421/500 [========================>.....] - ETA: 1:21 - loss: 1.9013 - regression_loss: 1.4115 - classification_loss: 0.4898\n",
      "422/500 [========================>.....] - ETA: 1:20 - loss: 1.9012 - regression_loss: 1.4114 - classification_loss: 0.4898\n",
      "423/500 [========================>.....] - ETA: 1:19 - loss: 1.9010 - regression_loss: 1.4115 - classification_loss: 0.4895\n",
      "424/500 [========================>.....] - ETA: 1:18 - loss: 1.9011 - regression_loss: 1.4114 - classification_loss: 0.4896\n",
      "425/500 [========================>.....] - ETA: 1:17 - loss: 1.8997 - regression_loss: 1.4103 - classification_loss: 0.4894\n",
      "426/500 [========================>.....] - ETA: 1:16 - loss: 1.9001 - regression_loss: 1.4100 - classification_loss: 0.4901\n",
      "427/500 [========================>.....] - ETA: 1:14 - loss: 1.8979 - regression_loss: 1.4082 - classification_loss: 0.4896\n",
      "428/500 [========================>.....] - ETA: 1:13 - loss: 1.8969 - regression_loss: 1.4076 - classification_loss: 0.4892\n",
      "429/500 [========================>.....] - ETA: 1:12 - loss: 1.8962 - regression_loss: 1.4072 - classification_loss: 0.4890\n",
      "430/500 [========================>.....] - ETA: 1:11 - loss: 1.8964 - regression_loss: 1.4075 - classification_loss: 0.4889\n",
      "431/500 [========================>.....] - ETA: 1:10 - loss: 1.8970 - regression_loss: 1.4079 - classification_loss: 0.4891\n",
      "432/500 [========================>.....] - ETA: 1:09 - loss: 1.8979 - regression_loss: 1.4084 - classification_loss: 0.4895\n",
      "433/500 [========================>.....] - ETA: 1:08 - loss: 1.8997 - regression_loss: 1.4095 - classification_loss: 0.4901\n",
      "434/500 [=========================>....] - ETA: 1:07 - loss: 1.8982 - regression_loss: 1.4082 - classification_loss: 0.4900\n",
      "435/500 [=========================>....] - ETA: 1:06 - loss: 1.8961 - regression_loss: 1.4066 - classification_loss: 0.4895\n",
      "436/500 [=========================>....] - ETA: 1:05 - loss: 1.8957 - regression_loss: 1.4066 - classification_loss: 0.4891\n",
      "437/500 [=========================>....] - ETA: 1:04 - loss: 1.8966 - regression_loss: 1.4075 - classification_loss: 0.4891\n",
      "438/500 [=========================>....] - ETA: 1:03 - loss: 1.8957 - regression_loss: 1.4071 - classification_loss: 0.4886\n",
      "439/500 [=========================>....] - ETA: 1:02 - loss: 1.8956 - regression_loss: 1.4072 - classification_loss: 0.4884\n",
      "440/500 [=========================>....] - ETA: 1:01 - loss: 1.8961 - regression_loss: 1.4078 - classification_loss: 0.4882\n",
      "441/500 [=========================>....] - ETA: 1:00 - loss: 1.8950 - regression_loss: 1.4070 - classification_loss: 0.4880\n",
      "442/500 [=========================>....] - ETA: 59s - loss: 1.8950 - regression_loss: 1.4072 - classification_loss: 0.4878 \n",
      "443/500 [=========================>....] - ETA: 58s - loss: 1.8947 - regression_loss: 1.4072 - classification_loss: 0.4874\n",
      "444/500 [=========================>....] - ETA: 57s - loss: 1.8940 - regression_loss: 1.4070 - classification_loss: 0.4870\n",
      "445/500 [=========================>....] - ETA: 56s - loss: 1.8969 - regression_loss: 1.4087 - classification_loss: 0.4882\n",
      "446/500 [=========================>....] - ETA: 55s - loss: 1.8951 - regression_loss: 1.4074 - classification_loss: 0.4877\n",
      "447/500 [=========================>....] - ETA: 54s - loss: 1.8937 - regression_loss: 1.4064 - classification_loss: 0.4873\n",
      "448/500 [=========================>....] - ETA: 53s - loss: 1.8939 - regression_loss: 1.4069 - classification_loss: 0.4870\n",
      "449/500 [=========================>....] - ETA: 52s - loss: 1.8975 - regression_loss: 1.4093 - classification_loss: 0.4882\n",
      "450/500 [==========================>...] - ETA: 51s - loss: 1.8961 - regression_loss: 1.4083 - classification_loss: 0.4878\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 1.8963 - regression_loss: 1.4085 - classification_loss: 0.4878\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 1.8961 - regression_loss: 1.4087 - classification_loss: 0.4874\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.8946 - regression_loss: 1.4078 - classification_loss: 0.4868\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.8928 - regression_loss: 1.4066 - classification_loss: 0.4862\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.8924 - regression_loss: 1.4062 - classification_loss: 0.4862\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.8921 - regression_loss: 1.4061 - classification_loss: 0.4861\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.8922 - regression_loss: 1.4058 - classification_loss: 0.4864\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.8934 - regression_loss: 1.4066 - classification_loss: 0.4868\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.8934 - regression_loss: 1.4066 - classification_loss: 0.4867\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.8940 - regression_loss: 1.4067 - classification_loss: 0.4873\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 1.8953 - regression_loss: 1.4074 - classification_loss: 0.4879\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 1.8948 - regression_loss: 1.4072 - classification_loss: 0.4875\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 1.8933 - regression_loss: 1.4061 - classification_loss: 0.4872\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 1.8922 - regression_loss: 1.4055 - classification_loss: 0.4867\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 1.8946 - regression_loss: 1.4070 - classification_loss: 0.4876\n",
      "466/500 [==========================>...] - ETA: 34s - loss: 1.8934 - regression_loss: 1.4062 - classification_loss: 0.4872\n",
      "467/500 [===========================>..] - ETA: 33s - loss: 1.8936 - regression_loss: 1.4066 - classification_loss: 0.4870\n",
      "468/500 [===========================>..] - ETA: 32s - loss: 1.8940 - regression_loss: 1.4063 - classification_loss: 0.4876\n",
      "469/500 [===========================>..] - ETA: 31s - loss: 1.8947 - regression_loss: 1.4072 - classification_loss: 0.4876\n",
      "470/500 [===========================>..] - ETA: 30s - loss: 1.8959 - regression_loss: 1.4082 - classification_loss: 0.4878\n",
      "471/500 [===========================>..] - ETA: 29s - loss: 1.8966 - regression_loss: 1.4088 - classification_loss: 0.4878\n",
      "472/500 [===========================>..] - ETA: 28s - loss: 1.8962 - regression_loss: 1.4087 - classification_loss: 0.4875\n",
      "473/500 [===========================>..] - ETA: 27s - loss: 1.8964 - regression_loss: 1.4090 - classification_loss: 0.4874\n",
      "474/500 [===========================>..] - ETA: 26s - loss: 1.8953 - regression_loss: 1.4086 - classification_loss: 0.4868\n",
      "475/500 [===========================>..] - ETA: 25s - loss: 1.8951 - regression_loss: 1.4082 - classification_loss: 0.4869\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 1.8961 - regression_loss: 1.4091 - classification_loss: 0.4871\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.8956 - regression_loss: 1.4087 - classification_loss: 0.4868\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.8965 - regression_loss: 1.4096 - classification_loss: 0.4869\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.8958 - regression_loss: 1.4092 - classification_loss: 0.4866\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.8970 - regression_loss: 1.4103 - classification_loss: 0.4867\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.8972 - regression_loss: 1.4107 - classification_loss: 0.4864\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.8955 - regression_loss: 1.4094 - classification_loss: 0.4860\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.8947 - regression_loss: 1.4088 - classification_loss: 0.4858\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.8950 - regression_loss: 1.4085 - classification_loss: 0.4865\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.8953 - regression_loss: 1.4092 - classification_loss: 0.4861\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.8958 - regression_loss: 1.4096 - classification_loss: 0.4862\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.8956 - regression_loss: 1.4093 - classification_loss: 0.4863\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.8956 - regression_loss: 1.4096 - classification_loss: 0.4861\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.8950 - regression_loss: 1.4093 - classification_loss: 0.4858\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.8945 - regression_loss: 1.4090 - classification_loss: 0.4855\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.8939 - regression_loss: 1.4087 - classification_loss: 0.4852 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.8953 - regression_loss: 1.4095 - classification_loss: 0.4858\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.8954 - regression_loss: 1.4098 - classification_loss: 0.4856\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.8956 - regression_loss: 1.4097 - classification_loss: 0.4859\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.8941 - regression_loss: 1.4087 - classification_loss: 0.4853\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.8945 - regression_loss: 1.4091 - classification_loss: 0.4853\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.8937 - regression_loss: 1.4088 - classification_loss: 0.4849\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.8924 - regression_loss: 1.4080 - classification_loss: 0.4844\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.8919 - regression_loss: 1.4079 - classification_loss: 0.4840\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.8916 - regression_loss: 1.4076 - classification_loss: 0.4840\n",
      "Epoch 00028: saving model to ./snapshots\\resnet50_csv_28.h5\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 9.999999717180686e-11.\n",
      "\n",
      "500/500 [==============================] - 515s 1s/step - loss: 1.8916 - regression_loss: 1.4076 - classification_loss: 0.4840\n",
      "Epoch 29/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 2.0452 - regression_loss: 1.5322 - classification_loss: 0.5130\n",
      "  2/500 [..............................] - ETA: 4:29 - loss: 2.1694 - regression_loss: 1.6262 - classification_loss: 0.5432\n",
      "  3/500 [..............................] - ETA: 5:41 - loss: 2.0462 - regression_loss: 1.5330 - classification_loss: 0.5132\n",
      "  4/500 [..............................] - ETA: 6:18 - loss: 1.9038 - regression_loss: 1.4443 - classification_loss: 0.4596\n",
      "  5/500 [..............................] - ETA: 6:39 - loss: 1.7429 - regression_loss: 1.2993 - classification_loss: 0.4436\n",
      "  6/500 [..............................] - ETA: 7:01 - loss: 1.7211 - regression_loss: 1.2745 - classification_loss: 0.4466\n",
      "  7/500 [..............................] - ETA: 6:53 - loss: 1.7164 - regression_loss: 1.2740 - classification_loss: 0.4424\n",
      "  8/500 [..............................] - ETA: 7:07 - loss: 1.7089 - regression_loss: 1.2666 - classification_loss: 0.4423\n",
      "  9/500 [..............................] - ETA: 7:17 - loss: 1.6878 - regression_loss: 1.2437 - classification_loss: 0.4441\n",
      " 10/500 [..............................] - ETA: 7:25 - loss: 1.6408 - regression_loss: 1.2032 - classification_loss: 0.4376\n",
      " 11/500 [..............................] - ETA: 7:28 - loss: 1.7693 - regression_loss: 1.2969 - classification_loss: 0.4723\n",
      " 12/500 [..............................] - ETA: 7:29 - loss: 1.7283 - regression_loss: 1.2782 - classification_loss: 0.4501\n",
      " 13/500 [..............................] - ETA: 7:31 - loss: 1.7242 - regression_loss: 1.2721 - classification_loss: 0.4521\n",
      " 14/500 [..............................] - ETA: 7:36 - loss: 1.8052 - regression_loss: 1.3248 - classification_loss: 0.4804\n",
      " 15/500 [..............................] - ETA: 7:36 - loss: 1.8070 - regression_loss: 1.3249 - classification_loss: 0.4821\n",
      " 16/500 [..............................] - ETA: 7:37 - loss: 1.8073 - regression_loss: 1.3225 - classification_loss: 0.4848\n",
      " 17/500 [>.............................] - ETA: 7:39 - loss: 1.7808 - regression_loss: 1.3064 - classification_loss: 0.4744\n",
      " 18/500 [>.............................] - ETA: 7:42 - loss: 1.7767 - regression_loss: 1.3012 - classification_loss: 0.4755\n",
      " 19/500 [>.............................] - ETA: 7:41 - loss: 1.7771 - regression_loss: 1.3124 - classification_loss: 0.4647\n",
      " 20/500 [>.............................] - ETA: 7:44 - loss: 1.7796 - regression_loss: 1.3185 - classification_loss: 0.4611\n",
      " 21/500 [>.............................] - ETA: 7:43 - loss: 1.7708 - regression_loss: 1.3177 - classification_loss: 0.4532\n",
      " 22/500 [>.............................] - ETA: 7:45 - loss: 1.7651 - regression_loss: 1.3089 - classification_loss: 0.4562\n",
      " 23/500 [>.............................] - ETA: 7:46 - loss: 1.7721 - regression_loss: 1.3133 - classification_loss: 0.4588\n",
      " 24/500 [>.............................] - ETA: 7:45 - loss: 1.7832 - regression_loss: 1.3274 - classification_loss: 0.4557\n",
      " 25/500 [>.............................] - ETA: 7:40 - loss: 1.7631 - regression_loss: 1.3097 - classification_loss: 0.4534\n",
      " 26/500 [>.............................] - ETA: 7:44 - loss: 1.8038 - regression_loss: 1.3381 - classification_loss: 0.4657\n",
      " 27/500 [>.............................] - ETA: 7:43 - loss: 1.8044 - regression_loss: 1.3360 - classification_loss: 0.4685\n",
      " 28/500 [>.............................] - ETA: 7:37 - loss: 1.8405 - regression_loss: 1.3615 - classification_loss: 0.4790\n",
      " 29/500 [>.............................] - ETA: 7:40 - loss: 1.8327 - regression_loss: 1.3555 - classification_loss: 0.4772\n",
      " 30/500 [>.............................] - ETA: 7:39 - loss: 1.8121 - regression_loss: 1.3388 - classification_loss: 0.4733\n",
      " 31/500 [>.............................] - ETA: 7:40 - loss: 1.8233 - regression_loss: 1.3473 - classification_loss: 0.4759\n",
      " 32/500 [>.............................] - ETA: 7:39 - loss: 1.8402 - regression_loss: 1.3637 - classification_loss: 0.4764\n",
      " 33/500 [>.............................] - ETA: 7:40 - loss: 1.8437 - regression_loss: 1.3685 - classification_loss: 0.4752\n",
      " 34/500 [=>............................] - ETA: 7:39 - loss: 1.8305 - regression_loss: 1.3629 - classification_loss: 0.4676\n",
      " 35/500 [=>............................] - ETA: 7:38 - loss: 1.8265 - regression_loss: 1.3631 - classification_loss: 0.4634\n",
      " 36/500 [=>............................] - ETA: 7:37 - loss: 1.8214 - regression_loss: 1.3600 - classification_loss: 0.4614\n",
      " 37/500 [=>............................] - ETA: 7:38 - loss: 1.8248 - regression_loss: 1.3610 - classification_loss: 0.4638\n",
      " 38/500 [=>............................] - ETA: 7:38 - loss: 1.8438 - regression_loss: 1.3774 - classification_loss: 0.4664\n",
      " 39/500 [=>............................] - ETA: 7:36 - loss: 1.8456 - regression_loss: 1.3746 - classification_loss: 0.4710\n",
      " 40/500 [=>............................] - ETA: 7:35 - loss: 1.8237 - regression_loss: 1.3586 - classification_loss: 0.4651\n",
      " 41/500 [=>............................] - ETA: 7:34 - loss: 1.8249 - regression_loss: 1.3608 - classification_loss: 0.4641\n",
      " 42/500 [=>............................] - ETA: 7:33 - loss: 1.8155 - regression_loss: 1.3548 - classification_loss: 0.4607\n",
      " 43/500 [=>............................] - ETA: 7:32 - loss: 1.8104 - regression_loss: 1.3528 - classification_loss: 0.4576\n",
      " 44/500 [=>............................] - ETA: 7:30 - loss: 1.8144 - regression_loss: 1.3530 - classification_loss: 0.4615\n",
      " 45/500 [=>............................] - ETA: 7:30 - loss: 1.8145 - regression_loss: 1.3553 - classification_loss: 0.4592\n",
      " 46/500 [=>............................] - ETA: 7:30 - loss: 1.8148 - regression_loss: 1.3557 - classification_loss: 0.4591\n",
      " 47/500 [=>............................] - ETA: 7:26 - loss: 1.8101 - regression_loss: 1.3514 - classification_loss: 0.4587\n",
      " 48/500 [=>............................] - ETA: 7:27 - loss: 1.8256 - regression_loss: 1.3626 - classification_loss: 0.4630\n",
      " 49/500 [=>............................] - ETA: 7:26 - loss: 1.8243 - regression_loss: 1.3591 - classification_loss: 0.4652\n",
      " 50/500 [==>...........................] - ETA: 7:25 - loss: 1.8265 - regression_loss: 1.3615 - classification_loss: 0.4650\n",
      " 51/500 [==>...........................] - ETA: 7:24 - loss: 1.8272 - regression_loss: 1.3611 - classification_loss: 0.4661\n",
      " 52/500 [==>...........................] - ETA: 7:21 - loss: 1.8201 - regression_loss: 1.3568 - classification_loss: 0.4634\n",
      " 53/500 [==>...........................] - ETA: 7:22 - loss: 1.8227 - regression_loss: 1.3602 - classification_loss: 0.4625\n",
      " 54/500 [==>...........................] - ETA: 7:22 - loss: 1.8397 - regression_loss: 1.3719 - classification_loss: 0.4678\n",
      " 55/500 [==>...........................] - ETA: 7:21 - loss: 1.8505 - regression_loss: 1.3818 - classification_loss: 0.4687\n",
      " 56/500 [==>...........................] - ETA: 7:21 - loss: 1.8633 - regression_loss: 1.3909 - classification_loss: 0.4724\n",
      " 57/500 [==>...........................] - ETA: 7:21 - loss: 1.8659 - regression_loss: 1.3926 - classification_loss: 0.4733\n",
      " 58/500 [==>...........................] - ETA: 7:18 - loss: 1.8711 - regression_loss: 1.3935 - classification_loss: 0.4776\n",
      " 59/500 [==>...........................] - ETA: 7:18 - loss: 1.8740 - regression_loss: 1.3935 - classification_loss: 0.4805\n",
      " 60/500 [==>...........................] - ETA: 7:17 - loss: 1.8716 - regression_loss: 1.3910 - classification_loss: 0.4805\n",
      " 61/500 [==>...........................] - ETA: 7:16 - loss: 1.8844 - regression_loss: 1.3986 - classification_loss: 0.4858\n",
      " 62/500 [==>...........................] - ETA: 7:15 - loss: 1.8890 - regression_loss: 1.4039 - classification_loss: 0.4850\n",
      " 63/500 [==>...........................] - ETA: 7:14 - loss: 1.8742 - regression_loss: 1.3928 - classification_loss: 0.4814\n",
      " 64/500 [==>...........................] - ETA: 7:13 - loss: 1.8823 - regression_loss: 1.3986 - classification_loss: 0.4837\n",
      " 65/500 [==>...........................] - ETA: 7:12 - loss: 1.8743 - regression_loss: 1.3919 - classification_loss: 0.4824\n",
      " 66/500 [==>...........................] - ETA: 7:11 - loss: 1.8817 - regression_loss: 1.3947 - classification_loss: 0.4870\n",
      " 67/500 [===>..........................] - ETA: 7:10 - loss: 1.8788 - regression_loss: 1.3941 - classification_loss: 0.4847\n",
      " 68/500 [===>..........................] - ETA: 7:09 - loss: 1.8814 - regression_loss: 1.3950 - classification_loss: 0.4864\n",
      " 69/500 [===>..........................] - ETA: 7:09 - loss: 1.8802 - regression_loss: 1.3944 - classification_loss: 0.4858\n",
      " 70/500 [===>..........................] - ETA: 7:08 - loss: 1.8743 - regression_loss: 1.3908 - classification_loss: 0.4835\n",
      " 71/500 [===>..........................] - ETA: 7:07 - loss: 1.8761 - regression_loss: 1.3912 - classification_loss: 0.4849\n",
      " 72/500 [===>..........................] - ETA: 7:07 - loss: 1.8940 - regression_loss: 1.4018 - classification_loss: 0.4922\n",
      " 73/500 [===>..........................] - ETA: 7:06 - loss: 1.8897 - regression_loss: 1.4000 - classification_loss: 0.4897\n",
      " 74/500 [===>..........................] - ETA: 7:05 - loss: 1.8921 - regression_loss: 1.4012 - classification_loss: 0.4909\n",
      " 75/500 [===>..........................] - ETA: 7:03 - loss: 1.8829 - regression_loss: 1.3945 - classification_loss: 0.4884\n",
      " 76/500 [===>..........................] - ETA: 7:02 - loss: 1.8902 - regression_loss: 1.3995 - classification_loss: 0.4906\n",
      " 77/500 [===>..........................] - ETA: 7:02 - loss: 1.8792 - regression_loss: 1.3919 - classification_loss: 0.4873\n",
      " 78/500 [===>..........................] - ETA: 7:01 - loss: 1.8740 - regression_loss: 1.3886 - classification_loss: 0.4854\n",
      " 79/500 [===>..........................] - ETA: 7:00 - loss: 1.8744 - regression_loss: 1.3902 - classification_loss: 0.4843\n",
      " 80/500 [===>..........................] - ETA: 6:59 - loss: 1.8657 - regression_loss: 1.3841 - classification_loss: 0.4816\n",
      " 81/500 [===>..........................] - ETA: 6:59 - loss: 1.8661 - regression_loss: 1.3842 - classification_loss: 0.4819\n",
      " 82/500 [===>..........................] - ETA: 6:58 - loss: 1.8668 - regression_loss: 1.3836 - classification_loss: 0.4831\n",
      " 83/500 [===>..........................] - ETA: 6:58 - loss: 1.8656 - regression_loss: 1.3834 - classification_loss: 0.4821\n",
      " 84/500 [====>.........................] - ETA: 6:57 - loss: 1.8770 - regression_loss: 1.3881 - classification_loss: 0.4889\n",
      " 85/500 [====>.........................] - ETA: 6:56 - loss: 1.8779 - regression_loss: 1.3887 - classification_loss: 0.4892\n",
      " 86/500 [====>.........................] - ETA: 6:55 - loss: 1.8738 - regression_loss: 1.3848 - classification_loss: 0.4890\n",
      " 87/500 [====>.........................] - ETA: 6:55 - loss: 1.8624 - regression_loss: 1.3755 - classification_loss: 0.4869\n",
      " 88/500 [====>.........................] - ETA: 6:53 - loss: 1.8553 - regression_loss: 1.3705 - classification_loss: 0.4849\n",
      " 89/500 [====>.........................] - ETA: 6:53 - loss: 1.8533 - regression_loss: 1.3685 - classification_loss: 0.4848\n",
      " 90/500 [====>.........................] - ETA: 6:52 - loss: 1.8534 - regression_loss: 1.3690 - classification_loss: 0.4844\n",
      " 91/500 [====>.........................] - ETA: 6:51 - loss: 1.8490 - regression_loss: 1.3669 - classification_loss: 0.4821\n",
      " 92/500 [====>.........................] - ETA: 6:50 - loss: 1.8545 - regression_loss: 1.3691 - classification_loss: 0.4853\n",
      " 93/500 [====>.........................] - ETA: 6:49 - loss: 1.8577 - regression_loss: 1.3692 - classification_loss: 0.4885\n",
      " 94/500 [====>.........................] - ETA: 6:48 - loss: 1.8584 - regression_loss: 1.3683 - classification_loss: 0.4901\n",
      " 95/500 [====>.........................] - ETA: 6:48 - loss: 1.8642 - regression_loss: 1.3720 - classification_loss: 0.4921\n",
      " 96/500 [====>.........................] - ETA: 6:47 - loss: 1.8703 - regression_loss: 1.3754 - classification_loss: 0.4948\n",
      " 97/500 [====>.........................] - ETA: 6:46 - loss: 1.8665 - regression_loss: 1.3732 - classification_loss: 0.4934\n",
      " 98/500 [====>.........................] - ETA: 6:45 - loss: 1.8695 - regression_loss: 1.3743 - classification_loss: 0.4951\n",
      " 99/500 [====>.........................] - ETA: 6:44 - loss: 1.8744 - regression_loss: 1.3782 - classification_loss: 0.4961\n",
      "100/500 [=====>........................] - ETA: 6:44 - loss: 1.8771 - regression_loss: 1.3820 - classification_loss: 0.4952\n",
      "101/500 [=====>........................] - ETA: 6:43 - loss: 1.8714 - regression_loss: 1.3781 - classification_loss: 0.4933\n",
      "102/500 [=====>........................] - ETA: 6:41 - loss: 1.8683 - regression_loss: 1.3760 - classification_loss: 0.4923\n",
      "103/500 [=====>........................] - ETA: 6:41 - loss: 1.8723 - regression_loss: 1.3793 - classification_loss: 0.4930\n",
      "104/500 [=====>........................] - ETA: 6:40 - loss: 1.8771 - regression_loss: 1.3823 - classification_loss: 0.4948\n",
      "105/500 [=====>........................] - ETA: 6:39 - loss: 1.8818 - regression_loss: 1.3835 - classification_loss: 0.4983\n",
      "106/500 [=====>........................] - ETA: 6:38 - loss: 1.8757 - regression_loss: 1.3796 - classification_loss: 0.4960\n",
      "107/500 [=====>........................] - ETA: 6:38 - loss: 1.8799 - regression_loss: 1.3824 - classification_loss: 0.4975\n",
      "108/500 [=====>........................] - ETA: 6:37 - loss: 1.8782 - regression_loss: 1.3815 - classification_loss: 0.4967\n",
      "109/500 [=====>........................] - ETA: 6:36 - loss: 1.8766 - regression_loss: 1.3798 - classification_loss: 0.4968\n",
      "110/500 [=====>........................] - ETA: 6:35 - loss: 1.8827 - regression_loss: 1.3843 - classification_loss: 0.4985\n",
      "111/500 [=====>........................] - ETA: 6:34 - loss: 1.8838 - regression_loss: 1.3860 - classification_loss: 0.4978\n",
      "112/500 [=====>........................] - ETA: 6:33 - loss: 1.8784 - regression_loss: 1.3823 - classification_loss: 0.4961\n",
      "113/500 [=====>........................] - ETA: 6:32 - loss: 1.8828 - regression_loss: 1.3859 - classification_loss: 0.4969\n",
      "114/500 [=====>........................] - ETA: 6:31 - loss: 1.8766 - regression_loss: 1.3809 - classification_loss: 0.4958\n",
      "115/500 [=====>........................] - ETA: 6:30 - loss: 1.8781 - regression_loss: 1.3826 - classification_loss: 0.4955\n",
      "116/500 [=====>........................] - ETA: 6:29 - loss: 1.8732 - regression_loss: 1.3793 - classification_loss: 0.4938\n",
      "117/500 [======>.......................] - ETA: 6:28 - loss: 1.8714 - regression_loss: 1.3788 - classification_loss: 0.4926\n",
      "118/500 [======>.......................] - ETA: 6:27 - loss: 1.8819 - regression_loss: 1.3867 - classification_loss: 0.4953\n",
      "119/500 [======>.......................] - ETA: 6:26 - loss: 1.8827 - regression_loss: 1.3881 - classification_loss: 0.4946\n",
      "120/500 [======>.......................] - ETA: 6:24 - loss: 1.8796 - regression_loss: 1.3847 - classification_loss: 0.4950\n",
      "121/500 [======>.......................] - ETA: 6:24 - loss: 1.8800 - regression_loss: 1.3850 - classification_loss: 0.4950\n",
      "122/500 [======>.......................] - ETA: 6:23 - loss: 1.8800 - regression_loss: 1.3856 - classification_loss: 0.4944\n",
      "123/500 [======>.......................] - ETA: 6:22 - loss: 1.8758 - regression_loss: 1.3821 - classification_loss: 0.4937\n",
      "124/500 [======>.......................] - ETA: 6:21 - loss: 1.8773 - regression_loss: 1.3844 - classification_loss: 0.4928\n",
      "125/500 [======>.......................] - ETA: 6:21 - loss: 1.8784 - regression_loss: 1.3854 - classification_loss: 0.4930\n",
      "126/500 [======>.......................] - ETA: 6:19 - loss: 1.8761 - regression_loss: 1.3836 - classification_loss: 0.4925\n",
      "127/500 [======>.......................] - ETA: 6:18 - loss: 1.8711 - regression_loss: 1.3796 - classification_loss: 0.4915\n",
      "128/500 [======>.......................] - ETA: 6:17 - loss: 1.8770 - regression_loss: 1.3838 - classification_loss: 0.4933\n",
      "129/500 [======>.......................] - ETA: 6:17 - loss: 1.8830 - regression_loss: 1.3874 - classification_loss: 0.4956\n",
      "130/500 [======>.......................] - ETA: 6:16 - loss: 1.8879 - regression_loss: 1.3911 - classification_loss: 0.4969\n",
      "131/500 [======>.......................] - ETA: 6:15 - loss: 1.8882 - regression_loss: 1.3924 - classification_loss: 0.4958\n",
      "132/500 [======>.......................] - ETA: 6:14 - loss: 1.8903 - regression_loss: 1.3944 - classification_loss: 0.4959\n",
      "133/500 [======>.......................] - ETA: 6:12 - loss: 1.8906 - regression_loss: 1.3941 - classification_loss: 0.4965\n",
      "134/500 [=======>......................] - ETA: 6:12 - loss: 1.8942 - regression_loss: 1.3968 - classification_loss: 0.4974\n",
      "135/500 [=======>......................] - ETA: 6:11 - loss: 1.9014 - regression_loss: 1.4033 - classification_loss: 0.4981\n",
      "136/500 [=======>......................] - ETA: 6:10 - loss: 1.8994 - regression_loss: 1.4022 - classification_loss: 0.4972\n",
      "137/500 [=======>......................] - ETA: 6:09 - loss: 1.8948 - regression_loss: 1.3983 - classification_loss: 0.4966\n",
      "138/500 [=======>......................] - ETA: 6:08 - loss: 1.8943 - regression_loss: 1.3973 - classification_loss: 0.4970\n",
      "139/500 [=======>......................] - ETA: 6:06 - loss: 1.9001 - regression_loss: 1.4008 - classification_loss: 0.4993\n",
      "140/500 [=======>......................] - ETA: 6:06 - loss: 1.8977 - regression_loss: 1.3995 - classification_loss: 0.4982\n",
      "141/500 [=======>......................] - ETA: 6:05 - loss: 1.9025 - regression_loss: 1.4039 - classification_loss: 0.4986\n",
      "142/500 [=======>......................] - ETA: 6:04 - loss: 1.9090 - regression_loss: 1.4083 - classification_loss: 0.5007\n",
      "143/500 [=======>......................] - ETA: 6:03 - loss: 1.9095 - regression_loss: 1.4075 - classification_loss: 0.5020\n",
      "144/500 [=======>......................] - ETA: 6:02 - loss: 1.9087 - regression_loss: 1.4061 - classification_loss: 0.5026\n",
      "145/500 [=======>......................] - ETA: 6:01 - loss: 1.9114 - regression_loss: 1.4076 - classification_loss: 0.5038\n",
      "146/500 [=======>......................] - ETA: 5:59 - loss: 1.9164 - regression_loss: 1.4117 - classification_loss: 0.5046\n",
      "147/500 [=======>......................] - ETA: 5:59 - loss: 1.9160 - regression_loss: 1.4108 - classification_loss: 0.5052\n",
      "148/500 [=======>......................] - ETA: 5:58 - loss: 1.9179 - regression_loss: 1.4128 - classification_loss: 0.5051\n",
      "149/500 [=======>......................] - ETA: 5:57 - loss: 1.9202 - regression_loss: 1.4143 - classification_loss: 0.5059\n",
      "150/500 [========>.....................] - ETA: 5:56 - loss: 1.9133 - regression_loss: 1.4092 - classification_loss: 0.5041\n",
      "151/500 [========>.....................] - ETA: 5:55 - loss: 1.9120 - regression_loss: 1.4082 - classification_loss: 0.5037\n",
      "152/500 [========>.....................] - ETA: 5:54 - loss: 1.9123 - regression_loss: 1.4084 - classification_loss: 0.5039\n",
      "153/500 [========>.....................] - ETA: 5:53 - loss: 1.9195 - regression_loss: 1.4130 - classification_loss: 0.5065\n",
      "154/500 [========>.....................] - ETA: 5:52 - loss: 1.9213 - regression_loss: 1.4145 - classification_loss: 0.5069\n",
      "155/500 [========>.....................] - ETA: 5:51 - loss: 1.9198 - regression_loss: 1.4133 - classification_loss: 0.5065\n",
      "156/500 [========>.....................] - ETA: 5:50 - loss: 1.9195 - regression_loss: 1.4125 - classification_loss: 0.5070\n",
      "157/500 [========>.....................] - ETA: 5:49 - loss: 1.9210 - regression_loss: 1.4146 - classification_loss: 0.5064\n",
      "158/500 [========>.....................] - ETA: 5:49 - loss: 1.9191 - regression_loss: 1.4132 - classification_loss: 0.5058\n",
      "159/500 [========>.....................] - ETA: 5:48 - loss: 1.9137 - regression_loss: 1.4093 - classification_loss: 0.5044\n",
      "160/500 [========>.....................] - ETA: 5:47 - loss: 1.9087 - regression_loss: 1.4062 - classification_loss: 0.5025\n",
      "161/500 [========>.....................] - ETA: 5:46 - loss: 1.9037 - regression_loss: 1.4024 - classification_loss: 0.5013\n",
      "162/500 [========>.....................] - ETA: 5:45 - loss: 1.9011 - regression_loss: 1.4013 - classification_loss: 0.4998\n",
      "163/500 [========>.....................] - ETA: 5:44 - loss: 1.8983 - regression_loss: 1.3994 - classification_loss: 0.4988\n",
      "164/500 [========>.....................] - ETA: 5:42 - loss: 1.9017 - regression_loss: 1.4021 - classification_loss: 0.4996\n",
      "165/500 [========>.....................] - ETA: 5:41 - loss: 1.9067 - regression_loss: 1.4068 - classification_loss: 0.4999\n",
      "166/500 [========>.....................] - ETA: 5:40 - loss: 1.9051 - regression_loss: 1.4062 - classification_loss: 0.4989\n",
      "167/500 [=========>....................] - ETA: 5:39 - loss: 1.9051 - regression_loss: 1.4064 - classification_loss: 0.4986\n",
      "168/500 [=========>....................] - ETA: 5:38 - loss: 1.9033 - regression_loss: 1.4057 - classification_loss: 0.4976\n",
      "169/500 [=========>....................] - ETA: 5:38 - loss: 1.9038 - regression_loss: 1.4059 - classification_loss: 0.4979\n",
      "170/500 [=========>....................] - ETA: 5:37 - loss: 1.9059 - regression_loss: 1.4077 - classification_loss: 0.4982\n",
      "171/500 [=========>....................] - ETA: 5:36 - loss: 1.9113 - regression_loss: 1.4107 - classification_loss: 0.5006\n",
      "172/500 [=========>....................] - ETA: 5:35 - loss: 1.9165 - regression_loss: 1.4132 - classification_loss: 0.5033\n",
      "173/500 [=========>....................] - ETA: 5:34 - loss: 1.9180 - regression_loss: 1.4138 - classification_loss: 0.5042\n",
      "174/500 [=========>....................] - ETA: 5:33 - loss: 1.9144 - regression_loss: 1.4113 - classification_loss: 0.5030\n",
      "175/500 [=========>....................] - ETA: 5:32 - loss: 1.9173 - regression_loss: 1.4136 - classification_loss: 0.5038\n",
      "176/500 [=========>....................] - ETA: 5:31 - loss: 1.9142 - regression_loss: 1.4106 - classification_loss: 0.5036\n",
      "177/500 [=========>....................] - ETA: 5:30 - loss: 1.9096 - regression_loss: 1.4072 - classification_loss: 0.5024\n",
      "178/500 [=========>....................] - ETA: 5:29 - loss: 1.9082 - regression_loss: 1.4061 - classification_loss: 0.5022\n",
      "179/500 [=========>....................] - ETA: 5:28 - loss: 1.9063 - regression_loss: 1.4047 - classification_loss: 0.5016\n",
      "180/500 [=========>....................] - ETA: 5:27 - loss: 1.9027 - regression_loss: 1.4021 - classification_loss: 0.5006\n",
      "181/500 [=========>....................] - ETA: 5:26 - loss: 1.9032 - regression_loss: 1.4020 - classification_loss: 0.5012\n",
      "182/500 [=========>....................] - ETA: 5:25 - loss: 1.9094 - regression_loss: 1.4069 - classification_loss: 0.5025\n",
      "183/500 [=========>....................] - ETA: 5:24 - loss: 1.9095 - regression_loss: 1.4072 - classification_loss: 0.5023\n",
      "184/500 [==========>...................] - ETA: 5:23 - loss: 1.9101 - regression_loss: 1.4083 - classification_loss: 0.5018\n",
      "185/500 [==========>...................] - ETA: 5:22 - loss: 1.9071 - regression_loss: 1.4062 - classification_loss: 0.5009\n",
      "186/500 [==========>...................] - ETA: 5:21 - loss: 1.9046 - regression_loss: 1.4052 - classification_loss: 0.4994\n",
      "187/500 [==========>...................] - ETA: 5:20 - loss: 1.9047 - regression_loss: 1.4054 - classification_loss: 0.4993\n",
      "188/500 [==========>...................] - ETA: 5:19 - loss: 1.9014 - regression_loss: 1.4033 - classification_loss: 0.4980\n",
      "189/500 [==========>...................] - ETA: 5:18 - loss: 1.9013 - regression_loss: 1.4033 - classification_loss: 0.4980\n",
      "190/500 [==========>...................] - ETA: 5:17 - loss: 1.9027 - regression_loss: 1.4047 - classification_loss: 0.4981\n",
      "191/500 [==========>...................] - ETA: 5:16 - loss: 1.9018 - regression_loss: 1.4037 - classification_loss: 0.4982\n",
      "192/500 [==========>...................] - ETA: 5:15 - loss: 1.9011 - regression_loss: 1.4039 - classification_loss: 0.4972\n",
      "193/500 [==========>...................] - ETA: 5:14 - loss: 1.9027 - regression_loss: 1.4051 - classification_loss: 0.4977\n",
      "194/500 [==========>...................] - ETA: 5:13 - loss: 1.9055 - regression_loss: 1.4069 - classification_loss: 0.4986\n",
      "195/500 [==========>...................] - ETA: 5:12 - loss: 1.9026 - regression_loss: 1.4053 - classification_loss: 0.4973\n",
      "196/500 [==========>...................] - ETA: 5:11 - loss: 1.9020 - regression_loss: 1.4054 - classification_loss: 0.4967\n",
      "197/500 [==========>...................] - ETA: 5:10 - loss: 1.9028 - regression_loss: 1.4058 - classification_loss: 0.4969\n",
      "198/500 [==========>...................] - ETA: 5:09 - loss: 1.9007 - regression_loss: 1.4039 - classification_loss: 0.4968\n",
      "199/500 [==========>...................] - ETA: 5:08 - loss: 1.9027 - regression_loss: 1.4053 - classification_loss: 0.4974\n",
      "200/500 [===========>..................] - ETA: 5:07 - loss: 1.9029 - regression_loss: 1.4049 - classification_loss: 0.4980\n",
      "201/500 [===========>..................] - ETA: 5:06 - loss: 1.9065 - regression_loss: 1.4079 - classification_loss: 0.4986\n",
      "202/500 [===========>..................] - ETA: 5:05 - loss: 1.9116 - regression_loss: 1.4119 - classification_loss: 0.4997\n",
      "203/500 [===========>..................] - ETA: 5:04 - loss: 1.9143 - regression_loss: 1.4146 - classification_loss: 0.4997\n",
      "204/500 [===========>..................] - ETA: 5:02 - loss: 1.9125 - regression_loss: 1.4136 - classification_loss: 0.4989\n",
      "205/500 [===========>..................] - ETA: 5:01 - loss: 1.9137 - regression_loss: 1.4152 - classification_loss: 0.4985\n",
      "206/500 [===========>..................] - ETA: 5:00 - loss: 1.9142 - regression_loss: 1.4148 - classification_loss: 0.4994\n",
      "207/500 [===========>..................] - ETA: 4:59 - loss: 1.9165 - regression_loss: 1.4167 - classification_loss: 0.4998\n",
      "208/500 [===========>..................] - ETA: 4:58 - loss: 1.9160 - regression_loss: 1.4168 - classification_loss: 0.4992\n",
      "209/500 [===========>..................] - ETA: 4:57 - loss: 1.9156 - regression_loss: 1.4165 - classification_loss: 0.4992\n",
      "210/500 [===========>..................] - ETA: 4:56 - loss: 1.9158 - regression_loss: 1.4170 - classification_loss: 0.4988\n",
      "211/500 [===========>..................] - ETA: 4:55 - loss: 1.9189 - regression_loss: 1.4194 - classification_loss: 0.4995\n",
      "212/500 [===========>..................] - ETA: 4:54 - loss: 1.9196 - regression_loss: 1.4192 - classification_loss: 0.5004\n",
      "213/500 [===========>..................] - ETA: 4:53 - loss: 1.9149 - regression_loss: 1.4156 - classification_loss: 0.4992\n",
      "214/500 [===========>..................] - ETA: 4:52 - loss: 1.9167 - regression_loss: 1.4174 - classification_loss: 0.4993\n",
      "215/500 [===========>..................] - ETA: 4:51 - loss: 1.9184 - regression_loss: 1.4181 - classification_loss: 0.5003\n",
      "216/500 [===========>..................] - ETA: 4:50 - loss: 1.9207 - regression_loss: 1.4198 - classification_loss: 0.5009\n",
      "217/500 [============>.................] - ETA: 4:52 - loss: 1.9191 - regression_loss: 1.4188 - classification_loss: 0.5003\n",
      "218/500 [============>.................] - ETA: 4:51 - loss: 1.9200 - regression_loss: 1.4196 - classification_loss: 0.5004\n",
      "219/500 [============>.................] - ETA: 4:50 - loss: 1.9185 - regression_loss: 1.4183 - classification_loss: 0.5002\n",
      "220/500 [============>.................] - ETA: 4:49 - loss: 1.9186 - regression_loss: 1.4184 - classification_loss: 0.5002\n",
      "221/500 [============>.................] - ETA: 4:48 - loss: 1.9203 - regression_loss: 1.4192 - classification_loss: 0.5011\n",
      "222/500 [============>.................] - ETA: 4:47 - loss: 1.9186 - regression_loss: 1.4183 - classification_loss: 0.5004\n",
      "223/500 [============>.................] - ETA: 4:46 - loss: 1.9214 - regression_loss: 1.4196 - classification_loss: 0.5018\n",
      "224/500 [============>.................] - ETA: 4:45 - loss: 1.9255 - regression_loss: 1.4222 - classification_loss: 0.5033\n",
      "225/500 [============>.................] - ETA: 4:44 - loss: 1.9295 - regression_loss: 1.4245 - classification_loss: 0.5049\n",
      "226/500 [============>.................] - ETA: 4:43 - loss: 1.9273 - regression_loss: 1.4232 - classification_loss: 0.5041\n",
      "227/500 [============>.................] - ETA: 4:42 - loss: 1.9295 - regression_loss: 1.4252 - classification_loss: 0.5044\n",
      "228/500 [============>.................] - ETA: 4:41 - loss: 1.9295 - regression_loss: 1.4249 - classification_loss: 0.5046\n",
      "229/500 [============>.................] - ETA: 4:40 - loss: 1.9298 - regression_loss: 1.4252 - classification_loss: 0.5046\n",
      "230/500 [============>.................] - ETA: 4:39 - loss: 1.9319 - regression_loss: 1.4269 - classification_loss: 0.5050\n",
      "231/500 [============>.................] - ETA: 4:38 - loss: 1.9336 - regression_loss: 1.4283 - classification_loss: 0.5053\n",
      "232/500 [============>.................] - ETA: 4:37 - loss: 1.9348 - regression_loss: 1.4293 - classification_loss: 0.5055\n",
      "233/500 [============>.................] - ETA: 4:36 - loss: 1.9342 - regression_loss: 1.4286 - classification_loss: 0.5056\n",
      "234/500 [=============>................] - ETA: 4:35 - loss: 1.9303 - regression_loss: 1.4253 - classification_loss: 0.5050\n",
      "235/500 [=============>................] - ETA: 4:34 - loss: 1.9329 - regression_loss: 1.4278 - classification_loss: 0.5050\n",
      "236/500 [=============>................] - ETA: 4:33 - loss: 1.9354 - regression_loss: 1.4296 - classification_loss: 0.5058\n",
      "237/500 [=============>................] - ETA: 4:32 - loss: 1.9350 - regression_loss: 1.4300 - classification_loss: 0.5050\n",
      "238/500 [=============>................] - ETA: 4:30 - loss: 1.9357 - regression_loss: 1.4296 - classification_loss: 0.5061\n",
      "239/500 [=============>................] - ETA: 4:29 - loss: 1.9375 - regression_loss: 1.4311 - classification_loss: 0.5064\n",
      "240/500 [=============>................] - ETA: 4:28 - loss: 1.9372 - regression_loss: 1.4310 - classification_loss: 0.5062\n",
      "241/500 [=============>................] - ETA: 4:27 - loss: 1.9368 - regression_loss: 1.4311 - classification_loss: 0.5058\n",
      "242/500 [=============>................] - ETA: 4:26 - loss: 1.9382 - regression_loss: 1.4322 - classification_loss: 0.5060\n",
      "243/500 [=============>................] - ETA: 4:25 - loss: 1.9363 - regression_loss: 1.4311 - classification_loss: 0.5052\n",
      "244/500 [=============>................] - ETA: 4:24 - loss: 1.9366 - regression_loss: 1.4316 - classification_loss: 0.5050\n",
      "245/500 [=============>................] - ETA: 4:23 - loss: 1.9373 - regression_loss: 1.4323 - classification_loss: 0.5050\n",
      "246/500 [=============>................] - ETA: 4:22 - loss: 1.9401 - regression_loss: 1.4333 - classification_loss: 0.5068\n",
      "247/500 [=============>................] - ETA: 4:21 - loss: 1.9382 - regression_loss: 1.4319 - classification_loss: 0.5063\n",
      "248/500 [=============>................] - ETA: 4:20 - loss: 1.9392 - regression_loss: 1.4327 - classification_loss: 0.5065\n",
      "249/500 [=============>................] - ETA: 4:19 - loss: 1.9372 - regression_loss: 1.4312 - classification_loss: 0.5061\n",
      "250/500 [==============>...............] - ETA: 4:18 - loss: 1.9359 - regression_loss: 1.4303 - classification_loss: 0.5057\n",
      "251/500 [==============>...............] - ETA: 4:17 - loss: 1.9386 - regression_loss: 1.4326 - classification_loss: 0.5059\n",
      "252/500 [==============>...............] - ETA: 4:16 - loss: 1.9367 - regression_loss: 1.4315 - classification_loss: 0.5052\n",
      "253/500 [==============>...............] - ETA: 4:15 - loss: 1.9340 - regression_loss: 1.4296 - classification_loss: 0.5044\n",
      "254/500 [==============>...............] - ETA: 4:14 - loss: 1.9329 - regression_loss: 1.4291 - classification_loss: 0.5038\n",
      "255/500 [==============>...............] - ETA: 4:13 - loss: 1.9329 - regression_loss: 1.4295 - classification_loss: 0.5034\n",
      "256/500 [==============>...............] - ETA: 4:12 - loss: 1.9360 - regression_loss: 1.4321 - classification_loss: 0.5038\n",
      "257/500 [==============>...............] - ETA: 4:11 - loss: 1.9377 - regression_loss: 1.4331 - classification_loss: 0.5047\n",
      "258/500 [==============>...............] - ETA: 4:10 - loss: 1.9354 - regression_loss: 1.4310 - classification_loss: 0.5044\n",
      "259/500 [==============>...............] - ETA: 4:08 - loss: 1.9341 - regression_loss: 1.4298 - classification_loss: 0.5043\n",
      "260/500 [==============>...............] - ETA: 4:07 - loss: 1.9331 - regression_loss: 1.4290 - classification_loss: 0.5040\n",
      "261/500 [==============>...............] - ETA: 4:06 - loss: 1.9315 - regression_loss: 1.4279 - classification_loss: 0.5035\n",
      "262/500 [==============>...............] - ETA: 4:05 - loss: 1.9304 - regression_loss: 1.4269 - classification_loss: 0.5035\n",
      "263/500 [==============>...............] - ETA: 4:04 - loss: 1.9345 - regression_loss: 1.4294 - classification_loss: 0.5051\n",
      "264/500 [==============>...............] - ETA: 4:03 - loss: 1.9378 - regression_loss: 1.4310 - classification_loss: 0.5067\n",
      "265/500 [==============>...............] - ETA: 4:02 - loss: 1.9365 - regression_loss: 1.4298 - classification_loss: 0.5067\n",
      "266/500 [==============>...............] - ETA: 4:01 - loss: 1.9376 - regression_loss: 1.4305 - classification_loss: 0.5071\n",
      "267/500 [===============>..............] - ETA: 4:00 - loss: 1.9348 - regression_loss: 1.4285 - classification_loss: 0.5062\n",
      "268/500 [===============>..............] - ETA: 3:59 - loss: 1.9357 - regression_loss: 1.4290 - classification_loss: 0.5067\n",
      "269/500 [===============>..............] - ETA: 3:58 - loss: 1.9340 - regression_loss: 1.4280 - classification_loss: 0.5060\n",
      "270/500 [===============>..............] - ETA: 3:57 - loss: 1.9345 - regression_loss: 1.4282 - classification_loss: 0.5063\n",
      "271/500 [===============>..............] - ETA: 3:56 - loss: 1.9323 - regression_loss: 1.4268 - classification_loss: 0.5055\n",
      "272/500 [===============>..............] - ETA: 3:55 - loss: 1.9357 - regression_loss: 1.4295 - classification_loss: 0.5063\n",
      "273/500 [===============>..............] - ETA: 3:54 - loss: 1.9369 - regression_loss: 1.4306 - classification_loss: 0.5063\n",
      "274/500 [===============>..............] - ETA: 3:53 - loss: 1.9350 - regression_loss: 1.4292 - classification_loss: 0.5059\n",
      "275/500 [===============>..............] - ETA: 3:52 - loss: 1.9338 - regression_loss: 1.4284 - classification_loss: 0.5053\n",
      "276/500 [===============>..............] - ETA: 3:51 - loss: 1.9337 - regression_loss: 1.4287 - classification_loss: 0.5050\n",
      "277/500 [===============>..............] - ETA: 3:50 - loss: 1.9352 - regression_loss: 1.4296 - classification_loss: 0.5056\n",
      "278/500 [===============>..............] - ETA: 3:49 - loss: 1.9351 - regression_loss: 1.4295 - classification_loss: 0.5056\n",
      "279/500 [===============>..............] - ETA: 3:48 - loss: 1.9338 - regression_loss: 1.4286 - classification_loss: 0.5053\n",
      "280/500 [===============>..............] - ETA: 3:47 - loss: 1.9324 - regression_loss: 1.4273 - classification_loss: 0.5051\n",
      "281/500 [===============>..............] - ETA: 3:46 - loss: 1.9329 - regression_loss: 1.4277 - classification_loss: 0.5053\n",
      "282/500 [===============>..............] - ETA: 3:45 - loss: 1.9324 - regression_loss: 1.4269 - classification_loss: 0.5054\n",
      "283/500 [===============>..............] - ETA: 3:44 - loss: 1.9317 - regression_loss: 1.4266 - classification_loss: 0.5051\n",
      "284/500 [================>.............] - ETA: 3:43 - loss: 1.9312 - regression_loss: 1.4258 - classification_loss: 0.5054\n",
      "285/500 [================>.............] - ETA: 3:42 - loss: 1.9349 - regression_loss: 1.4285 - classification_loss: 0.5064\n",
      "286/500 [================>.............] - ETA: 3:40 - loss: 1.9337 - regression_loss: 1.4277 - classification_loss: 0.5061\n",
      "287/500 [================>.............] - ETA: 3:39 - loss: 1.9330 - regression_loss: 1.4273 - classification_loss: 0.5057\n",
      "288/500 [================>.............] - ETA: 3:38 - loss: 1.9353 - regression_loss: 1.4293 - classification_loss: 0.5061\n",
      "289/500 [================>.............] - ETA: 3:37 - loss: 1.9337 - regression_loss: 1.4282 - classification_loss: 0.5055\n",
      "290/500 [================>.............] - ETA: 3:36 - loss: 1.9370 - regression_loss: 1.4307 - classification_loss: 0.5063\n",
      "291/500 [================>.............] - ETA: 3:35 - loss: 1.9373 - regression_loss: 1.4308 - classification_loss: 0.5064\n",
      "292/500 [================>.............] - ETA: 3:34 - loss: 1.9334 - regression_loss: 1.4280 - classification_loss: 0.5053\n",
      "293/500 [================>.............] - ETA: 3:33 - loss: 1.9343 - regression_loss: 1.4289 - classification_loss: 0.5054\n",
      "294/500 [================>.............] - ETA: 3:32 - loss: 1.9391 - regression_loss: 1.4322 - classification_loss: 0.5068\n",
      "295/500 [================>.............] - ETA: 3:31 - loss: 1.9381 - regression_loss: 1.4313 - classification_loss: 0.5068\n",
      "296/500 [================>.............] - ETA: 3:30 - loss: 1.9385 - regression_loss: 1.4310 - classification_loss: 0.5075\n",
      "297/500 [================>.............] - ETA: 3:29 - loss: 1.9395 - regression_loss: 1.4326 - classification_loss: 0.5069\n",
      "298/500 [================>.............] - ETA: 3:28 - loss: 1.9405 - regression_loss: 1.4334 - classification_loss: 0.5070\n",
      "299/500 [================>.............] - ETA: 3:27 - loss: 1.9402 - regression_loss: 1.4336 - classification_loss: 0.5066\n",
      "300/500 [=================>............] - ETA: 3:26 - loss: 1.9392 - regression_loss: 1.4331 - classification_loss: 0.5061\n",
      "301/500 [=================>............] - ETA: 3:25 - loss: 1.9387 - regression_loss: 1.4328 - classification_loss: 0.5058\n",
      "302/500 [=================>............] - ETA: 3:24 - loss: 1.9357 - regression_loss: 1.4308 - classification_loss: 0.5048\n",
      "303/500 [=================>............] - ETA: 3:23 - loss: 1.9362 - regression_loss: 1.4312 - classification_loss: 0.5050\n",
      "304/500 [=================>............] - ETA: 3:22 - loss: 1.9366 - regression_loss: 1.4312 - classification_loss: 0.5055\n",
      "305/500 [=================>............] - ETA: 3:21 - loss: 1.9362 - regression_loss: 1.4305 - classification_loss: 0.5057\n",
      "306/500 [=================>............] - ETA: 3:20 - loss: 1.9355 - regression_loss: 1.4300 - classification_loss: 0.5055\n",
      "307/500 [=================>............] - ETA: 3:19 - loss: 1.9363 - regression_loss: 1.4306 - classification_loss: 0.5056\n",
      "308/500 [=================>............] - ETA: 3:18 - loss: 1.9378 - regression_loss: 1.4320 - classification_loss: 0.5059\n",
      "309/500 [=================>............] - ETA: 3:17 - loss: 1.9377 - regression_loss: 1.4323 - classification_loss: 0.5054\n",
      "310/500 [=================>............] - ETA: 3:16 - loss: 1.9374 - regression_loss: 1.4320 - classification_loss: 0.5055\n",
      "311/500 [=================>............] - ETA: 3:15 - loss: 1.9359 - regression_loss: 1.4309 - classification_loss: 0.5050\n",
      "312/500 [=================>............] - ETA: 3:14 - loss: 1.9339 - regression_loss: 1.4295 - classification_loss: 0.5044\n",
      "313/500 [=================>............] - ETA: 3:12 - loss: 1.9339 - regression_loss: 1.4300 - classification_loss: 0.5039\n",
      "314/500 [=================>............] - ETA: 3:11 - loss: 1.9330 - regression_loss: 1.4295 - classification_loss: 0.5034\n",
      "315/500 [=================>............] - ETA: 3:10 - loss: 1.9329 - regression_loss: 1.4298 - classification_loss: 0.5031\n",
      "316/500 [=================>............] - ETA: 3:09 - loss: 1.9321 - regression_loss: 1.4287 - classification_loss: 0.5034\n",
      "317/500 [==================>...........] - ETA: 3:08 - loss: 1.9292 - regression_loss: 1.4264 - classification_loss: 0.5028\n",
      "318/500 [==================>...........] - ETA: 3:07 - loss: 1.9280 - regression_loss: 1.4252 - classification_loss: 0.5028\n",
      "319/500 [==================>...........] - ETA: 3:06 - loss: 1.9255 - regression_loss: 1.4233 - classification_loss: 0.5022\n",
      "320/500 [==================>...........] - ETA: 3:05 - loss: 1.9254 - regression_loss: 1.4236 - classification_loss: 0.5018\n",
      "321/500 [==================>...........] - ETA: 3:04 - loss: 1.9238 - regression_loss: 1.4226 - classification_loss: 0.5011\n",
      "322/500 [==================>...........] - ETA: 3:03 - loss: 1.9248 - regression_loss: 1.4238 - classification_loss: 0.5011\n",
      "323/500 [==================>...........] - ETA: 3:02 - loss: 1.9252 - regression_loss: 1.4240 - classification_loss: 0.5012\n",
      "324/500 [==================>...........] - ETA: 3:01 - loss: 1.9248 - regression_loss: 1.4243 - classification_loss: 0.5005\n",
      "325/500 [==================>...........] - ETA: 3:00 - loss: 1.9259 - regression_loss: 1.4251 - classification_loss: 0.5008\n",
      "326/500 [==================>...........] - ETA: 2:59 - loss: 1.9258 - regression_loss: 1.4253 - classification_loss: 0.5005\n",
      "327/500 [==================>...........] - ETA: 2:58 - loss: 1.9257 - regression_loss: 1.4249 - classification_loss: 0.5008\n",
      "328/500 [==================>...........] - ETA: 2:57 - loss: 1.9247 - regression_loss: 1.4241 - classification_loss: 0.5006\n",
      "329/500 [==================>...........] - ETA: 2:56 - loss: 1.9248 - regression_loss: 1.4244 - classification_loss: 0.5003\n",
      "330/500 [==================>...........] - ETA: 2:55 - loss: 1.9231 - regression_loss: 1.4235 - classification_loss: 0.4996\n",
      "331/500 [==================>...........] - ETA: 2:54 - loss: 1.9240 - regression_loss: 1.4245 - classification_loss: 0.4994\n",
      "332/500 [==================>...........] - ETA: 2:53 - loss: 1.9264 - regression_loss: 1.4263 - classification_loss: 0.5001\n",
      "333/500 [==================>...........] - ETA: 2:52 - loss: 1.9259 - regression_loss: 1.4260 - classification_loss: 0.4999\n",
      "334/500 [===================>..........] - ETA: 2:51 - loss: 1.9285 - regression_loss: 1.4278 - classification_loss: 0.5007\n",
      "335/500 [===================>..........] - ETA: 2:50 - loss: 1.9287 - regression_loss: 1.4281 - classification_loss: 0.5007\n",
      "336/500 [===================>..........] - ETA: 2:49 - loss: 1.9260 - regression_loss: 1.4258 - classification_loss: 0.5001\n",
      "337/500 [===================>..........] - ETA: 2:48 - loss: 1.9281 - regression_loss: 1.4268 - classification_loss: 0.5013\n",
      "338/500 [===================>..........] - ETA: 2:47 - loss: 1.9256 - regression_loss: 1.4248 - classification_loss: 0.5008\n",
      "339/500 [===================>..........] - ETA: 2:46 - loss: 1.9265 - regression_loss: 1.4255 - classification_loss: 0.5009\n",
      "340/500 [===================>..........] - ETA: 2:44 - loss: 1.9264 - regression_loss: 1.4254 - classification_loss: 0.5010\n",
      "341/500 [===================>..........] - ETA: 2:43 - loss: 1.9267 - regression_loss: 1.4260 - classification_loss: 0.5007\n",
      "342/500 [===================>..........] - ETA: 2:42 - loss: 1.9254 - regression_loss: 1.4250 - classification_loss: 0.5004\n",
      "343/500 [===================>..........] - ETA: 2:41 - loss: 1.9267 - regression_loss: 1.4258 - classification_loss: 0.5009\n",
      "344/500 [===================>..........] - ETA: 2:40 - loss: 1.9257 - regression_loss: 1.4251 - classification_loss: 0.5005\n",
      "345/500 [===================>..........] - ETA: 2:39 - loss: 1.9282 - regression_loss: 1.4263 - classification_loss: 0.5019\n",
      "346/500 [===================>..........] - ETA: 2:38 - loss: 1.9262 - regression_loss: 1.4252 - classification_loss: 0.5010\n",
      "347/500 [===================>..........] - ETA: 2:37 - loss: 1.9278 - regression_loss: 1.4263 - classification_loss: 0.5016\n",
      "348/500 [===================>..........] - ETA: 2:36 - loss: 1.9264 - regression_loss: 1.4251 - classification_loss: 0.5013\n",
      "349/500 [===================>..........] - ETA: 2:35 - loss: 1.9233 - regression_loss: 1.4227 - classification_loss: 0.5006\n",
      "350/500 [====================>.........] - ETA: 2:34 - loss: 1.9241 - regression_loss: 1.4235 - classification_loss: 0.5006\n",
      "351/500 [====================>.........] - ETA: 2:33 - loss: 1.9244 - regression_loss: 1.4233 - classification_loss: 0.5011\n",
      "352/500 [====================>.........] - ETA: 2:32 - loss: 1.9221 - regression_loss: 1.4217 - classification_loss: 0.5004\n",
      "353/500 [====================>.........] - ETA: 2:31 - loss: 1.9230 - regression_loss: 1.4223 - classification_loss: 0.5007\n",
      "354/500 [====================>.........] - ETA: 2:30 - loss: 1.9237 - regression_loss: 1.4227 - classification_loss: 0.5010\n",
      "355/500 [====================>.........] - ETA: 2:29 - loss: 1.9232 - regression_loss: 1.4226 - classification_loss: 0.5006\n",
      "356/500 [====================>.........] - ETA: 2:28 - loss: 1.9221 - regression_loss: 1.4218 - classification_loss: 0.5003\n",
      "357/500 [====================>.........] - ETA: 2:27 - loss: 1.9218 - regression_loss: 1.4218 - classification_loss: 0.5000\n",
      "358/500 [====================>.........] - ETA: 2:26 - loss: 1.9223 - regression_loss: 1.4215 - classification_loss: 0.5007\n",
      "359/500 [====================>.........] - ETA: 2:25 - loss: 1.9228 - regression_loss: 1.4222 - classification_loss: 0.5006\n",
      "360/500 [====================>.........] - ETA: 2:24 - loss: 1.9234 - regression_loss: 1.4227 - classification_loss: 0.5007\n",
      "361/500 [====================>.........] - ETA: 2:23 - loss: 1.9237 - regression_loss: 1.4231 - classification_loss: 0.5006\n",
      "362/500 [====================>.........] - ETA: 2:22 - loss: 1.9217 - regression_loss: 1.4218 - classification_loss: 0.4999\n",
      "363/500 [====================>.........] - ETA: 2:21 - loss: 1.9202 - regression_loss: 1.4210 - classification_loss: 0.4992\n",
      "364/500 [====================>.........] - ETA: 2:20 - loss: 1.9192 - regression_loss: 1.4204 - classification_loss: 0.4988\n",
      "365/500 [====================>.........] - ETA: 2:19 - loss: 1.9178 - regression_loss: 1.4194 - classification_loss: 0.4984\n",
      "366/500 [====================>.........] - ETA: 2:18 - loss: 1.9166 - regression_loss: 1.4187 - classification_loss: 0.4979\n",
      "367/500 [=====================>........] - ETA: 2:17 - loss: 1.9145 - regression_loss: 1.4174 - classification_loss: 0.4971\n",
      "368/500 [=====================>........] - ETA: 2:16 - loss: 1.9137 - regression_loss: 1.4169 - classification_loss: 0.4969\n",
      "369/500 [=====================>........] - ETA: 2:14 - loss: 1.9130 - regression_loss: 1.4163 - classification_loss: 0.4966\n",
      "370/500 [=====================>........] - ETA: 2:13 - loss: 1.9125 - regression_loss: 1.4161 - classification_loss: 0.4963\n",
      "371/500 [=====================>........] - ETA: 2:12 - loss: 1.9117 - regression_loss: 1.4155 - classification_loss: 0.4962\n",
      "372/500 [=====================>........] - ETA: 2:11 - loss: 1.9123 - regression_loss: 1.4159 - classification_loss: 0.4964\n",
      "373/500 [=====================>........] - ETA: 2:10 - loss: 1.9096 - regression_loss: 1.4137 - classification_loss: 0.4959\n",
      "374/500 [=====================>........] - ETA: 2:09 - loss: 1.9066 - regression_loss: 1.4115 - classification_loss: 0.4951\n",
      "375/500 [=====================>........] - ETA: 2:08 - loss: 1.9072 - regression_loss: 1.4118 - classification_loss: 0.4954\n",
      "376/500 [=====================>........] - ETA: 2:07 - loss: 1.9077 - regression_loss: 1.4123 - classification_loss: 0.4953\n",
      "377/500 [=====================>........] - ETA: 2:06 - loss: 1.9080 - regression_loss: 1.4126 - classification_loss: 0.4954\n",
      "378/500 [=====================>........] - ETA: 2:05 - loss: 1.9073 - regression_loss: 1.4122 - classification_loss: 0.4950\n",
      "379/500 [=====================>........] - ETA: 2:04 - loss: 1.9071 - regression_loss: 1.4118 - classification_loss: 0.4953\n",
      "380/500 [=====================>........] - ETA: 2:03 - loss: 1.9067 - regression_loss: 1.4117 - classification_loss: 0.4951\n",
      "381/500 [=====================>........] - ETA: 2:02 - loss: 1.9067 - regression_loss: 1.4117 - classification_loss: 0.4950\n",
      "382/500 [=====================>........] - ETA: 2:01 - loss: 1.9068 - regression_loss: 1.4120 - classification_loss: 0.4948\n",
      "383/500 [=====================>........] - ETA: 2:00 - loss: 1.9050 - regression_loss: 1.4106 - classification_loss: 0.4944\n",
      "384/500 [======================>.......] - ETA: 1:59 - loss: 1.9046 - regression_loss: 1.4103 - classification_loss: 0.4944\n",
      "385/500 [======================>.......] - ETA: 1:58 - loss: 1.9032 - regression_loss: 1.4091 - classification_loss: 0.4941\n",
      "386/500 [======================>.......] - ETA: 1:57 - loss: 1.9048 - regression_loss: 1.4105 - classification_loss: 0.4942\n",
      "387/500 [======================>.......] - ETA: 1:56 - loss: 1.9063 - regression_loss: 1.4118 - classification_loss: 0.4945\n",
      "388/500 [======================>.......] - ETA: 1:55 - loss: 1.9052 - regression_loss: 1.4110 - classification_loss: 0.4942\n",
      "389/500 [======================>.......] - ETA: 1:54 - loss: 1.9074 - regression_loss: 1.4123 - classification_loss: 0.4951\n",
      "390/500 [======================>.......] - ETA: 1:53 - loss: 1.9088 - regression_loss: 1.4134 - classification_loss: 0.4954\n",
      "391/500 [======================>.......] - ETA: 1:52 - loss: 1.9079 - regression_loss: 1.4128 - classification_loss: 0.4950\n",
      "392/500 [======================>.......] - ETA: 1:51 - loss: 1.9069 - regression_loss: 1.4117 - classification_loss: 0.4952\n",
      "393/500 [======================>.......] - ETA: 1:50 - loss: 1.9088 - regression_loss: 1.4131 - classification_loss: 0.4957\n",
      "394/500 [======================>.......] - ETA: 1:49 - loss: 1.9078 - regression_loss: 1.4124 - classification_loss: 0.4953\n",
      "395/500 [======================>.......] - ETA: 1:48 - loss: 1.9085 - regression_loss: 1.4131 - classification_loss: 0.4954\n",
      "396/500 [======================>.......] - ETA: 1:47 - loss: 1.9097 - regression_loss: 1.4139 - classification_loss: 0.4958\n",
      "397/500 [======================>.......] - ETA: 1:46 - loss: 1.9115 - regression_loss: 1.4154 - classification_loss: 0.4961\n",
      "398/500 [======================>.......] - ETA: 1:45 - loss: 1.9094 - regression_loss: 1.4139 - classification_loss: 0.4955\n",
      "399/500 [======================>.......] - ETA: 1:44 - loss: 1.9071 - regression_loss: 1.4122 - classification_loss: 0.4949\n",
      "400/500 [=======================>......] - ETA: 1:43 - loss: 1.9066 - regression_loss: 1.4120 - classification_loss: 0.4945\n",
      "401/500 [=======================>......] - ETA: 1:42 - loss: 1.9065 - regression_loss: 1.4122 - classification_loss: 0.4944\n",
      "402/500 [=======================>......] - ETA: 1:40 - loss: 1.9081 - regression_loss: 1.4133 - classification_loss: 0.4948\n",
      "403/500 [=======================>......] - ETA: 1:39 - loss: 1.9073 - regression_loss: 1.4130 - classification_loss: 0.4943\n",
      "404/500 [=======================>......] - ETA: 1:38 - loss: 1.9076 - regression_loss: 1.4130 - classification_loss: 0.4946\n",
      "405/500 [=======================>......] - ETA: 1:37 - loss: 1.9079 - regression_loss: 1.4132 - classification_loss: 0.4947\n",
      "406/500 [=======================>......] - ETA: 1:36 - loss: 1.9090 - regression_loss: 1.4140 - classification_loss: 0.4951\n",
      "407/500 [=======================>......] - ETA: 1:35 - loss: 1.9099 - regression_loss: 1.4149 - classification_loss: 0.4950\n",
      "408/500 [=======================>......] - ETA: 1:34 - loss: 1.9087 - regression_loss: 1.4141 - classification_loss: 0.4946\n",
      "409/500 [=======================>......] - ETA: 1:33 - loss: 1.9080 - regression_loss: 1.4138 - classification_loss: 0.4943\n",
      "410/500 [=======================>......] - ETA: 1:32 - loss: 1.9090 - regression_loss: 1.4145 - classification_loss: 0.4945\n",
      "411/500 [=======================>......] - ETA: 1:31 - loss: 1.9093 - regression_loss: 1.4144 - classification_loss: 0.4949\n",
      "412/500 [=======================>......] - ETA: 1:30 - loss: 1.9116 - regression_loss: 1.4157 - classification_loss: 0.4958\n",
      "413/500 [=======================>......] - ETA: 1:29 - loss: 1.9117 - regression_loss: 1.4158 - classification_loss: 0.4959\n",
      "414/500 [=======================>......] - ETA: 1:28 - loss: 1.9114 - regression_loss: 1.4158 - classification_loss: 0.4955\n",
      "415/500 [=======================>......] - ETA: 1:27 - loss: 1.9137 - regression_loss: 1.4176 - classification_loss: 0.4961\n",
      "416/500 [=======================>......] - ETA: 1:26 - loss: 1.9152 - regression_loss: 1.4190 - classification_loss: 0.4962\n",
      "417/500 [========================>.....] - ETA: 1:25 - loss: 1.9130 - regression_loss: 1.4175 - classification_loss: 0.4955\n",
      "418/500 [========================>.....] - ETA: 1:24 - loss: 1.9142 - regression_loss: 1.4186 - classification_loss: 0.4956\n",
      "419/500 [========================>.....] - ETA: 1:23 - loss: 1.9130 - regression_loss: 1.4177 - classification_loss: 0.4954\n",
      "420/500 [========================>.....] - ETA: 1:22 - loss: 1.9142 - regression_loss: 1.4186 - classification_loss: 0.4956\n",
      "421/500 [========================>.....] - ETA: 1:21 - loss: 1.9124 - regression_loss: 1.4172 - classification_loss: 0.4953\n",
      "422/500 [========================>.....] - ETA: 1:20 - loss: 1.9121 - regression_loss: 1.4173 - classification_loss: 0.4948\n",
      "423/500 [========================>.....] - ETA: 1:19 - loss: 1.9128 - regression_loss: 1.4179 - classification_loss: 0.4949\n",
      "424/500 [========================>.....] - ETA: 1:18 - loss: 1.9128 - regression_loss: 1.4182 - classification_loss: 0.4946\n",
      "425/500 [========================>.....] - ETA: 1:17 - loss: 1.9111 - regression_loss: 1.4166 - classification_loss: 0.4944\n",
      "426/500 [========================>.....] - ETA: 1:16 - loss: 1.9107 - regression_loss: 1.4166 - classification_loss: 0.4941\n",
      "427/500 [========================>.....] - ETA: 1:15 - loss: 1.9107 - regression_loss: 1.4166 - classification_loss: 0.4942\n",
      "428/500 [========================>.....] - ETA: 1:14 - loss: 1.9112 - regression_loss: 1.4168 - classification_loss: 0.4944\n",
      "429/500 [========================>.....] - ETA: 1:13 - loss: 1.9129 - regression_loss: 1.4182 - classification_loss: 0.4948\n",
      "430/500 [========================>.....] - ETA: 1:12 - loss: 1.9123 - regression_loss: 1.4176 - classification_loss: 0.4947\n",
      "431/500 [========================>.....] - ETA: 1:11 - loss: 1.9109 - regression_loss: 1.4167 - classification_loss: 0.4941\n",
      "432/500 [========================>.....] - ETA: 1:10 - loss: 1.9105 - regression_loss: 1.4165 - classification_loss: 0.4940\n",
      "433/500 [========================>.....] - ETA: 1:08 - loss: 1.9120 - regression_loss: 1.4178 - classification_loss: 0.4943\n",
      "434/500 [=========================>....] - ETA: 1:07 - loss: 1.9118 - regression_loss: 1.4176 - classification_loss: 0.4942\n",
      "435/500 [=========================>....] - ETA: 1:06 - loss: 1.9121 - regression_loss: 1.4181 - classification_loss: 0.4940\n",
      "436/500 [=========================>....] - ETA: 1:05 - loss: 1.9143 - regression_loss: 1.4191 - classification_loss: 0.4952\n",
      "437/500 [=========================>....] - ETA: 1:04 - loss: 1.9135 - regression_loss: 1.4186 - classification_loss: 0.4949\n",
      "438/500 [=========================>....] - ETA: 1:03 - loss: 1.9140 - regression_loss: 1.4184 - classification_loss: 0.4956\n",
      "439/500 [=========================>....] - ETA: 1:02 - loss: 1.9127 - regression_loss: 1.4175 - classification_loss: 0.4952\n",
      "440/500 [=========================>....] - ETA: 1:01 - loss: 1.9122 - regression_loss: 1.4170 - classification_loss: 0.4951\n",
      "441/500 [=========================>....] - ETA: 1:00 - loss: 1.9119 - regression_loss: 1.4167 - classification_loss: 0.4952\n",
      "442/500 [=========================>....] - ETA: 59s - loss: 1.9104 - regression_loss: 1.4157 - classification_loss: 0.4947 \n",
      "443/500 [=========================>....] - ETA: 58s - loss: 1.9119 - regression_loss: 1.4167 - classification_loss: 0.4951\n",
      "444/500 [=========================>....] - ETA: 57s - loss: 1.9137 - regression_loss: 1.4179 - classification_loss: 0.4958\n",
      "445/500 [=========================>....] - ETA: 56s - loss: 1.9132 - regression_loss: 1.4175 - classification_loss: 0.4956\n",
      "446/500 [=========================>....] - ETA: 55s - loss: 1.9115 - regression_loss: 1.4164 - classification_loss: 0.4951\n",
      "447/500 [=========================>....] - ETA: 54s - loss: 1.9111 - regression_loss: 1.4160 - classification_loss: 0.4951\n",
      "448/500 [=========================>....] - ETA: 53s - loss: 1.9113 - regression_loss: 1.4163 - classification_loss: 0.4950\n",
      "449/500 [=========================>....] - ETA: 52s - loss: 1.9115 - regression_loss: 1.4162 - classification_loss: 0.4952\n",
      "450/500 [==========================>...] - ETA: 51s - loss: 1.9106 - regression_loss: 1.4157 - classification_loss: 0.4949\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 1.9093 - regression_loss: 1.4147 - classification_loss: 0.4946\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 1.9089 - regression_loss: 1.4143 - classification_loss: 0.4947\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.9103 - regression_loss: 1.4152 - classification_loss: 0.4951\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.9103 - regression_loss: 1.4152 - classification_loss: 0.4950\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.9098 - regression_loss: 1.4145 - classification_loss: 0.4953\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.9095 - regression_loss: 1.4141 - classification_loss: 0.4954\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.9089 - regression_loss: 1.4137 - classification_loss: 0.4952\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.9086 - regression_loss: 1.4136 - classification_loss: 0.4950\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.9065 - regression_loss: 1.4122 - classification_loss: 0.4943\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.9073 - regression_loss: 1.4130 - classification_loss: 0.4943\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 1.9068 - regression_loss: 1.4127 - classification_loss: 0.4941\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 1.9050 - regression_loss: 1.4113 - classification_loss: 0.4937\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 1.9052 - regression_loss: 1.4119 - classification_loss: 0.4933\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 1.9043 - regression_loss: 1.4111 - classification_loss: 0.4932\n",
      "465/500 [==========================>...] - ETA: 35s - loss: 1.9036 - regression_loss: 1.4107 - classification_loss: 0.4929\n",
      "466/500 [==========================>...] - ETA: 34s - loss: 1.9034 - regression_loss: 1.4106 - classification_loss: 0.4928\n",
      "467/500 [===========================>..] - ETA: 33s - loss: 1.9038 - regression_loss: 1.4106 - classification_loss: 0.4932\n",
      "468/500 [===========================>..] - ETA: 32s - loss: 1.9043 - regression_loss: 1.4112 - classification_loss: 0.4931\n",
      "469/500 [===========================>..] - ETA: 31s - loss: 1.9057 - regression_loss: 1.4120 - classification_loss: 0.4937\n",
      "470/500 [===========================>..] - ETA: 30s - loss: 1.9066 - regression_loss: 1.4129 - classification_loss: 0.4937\n",
      "471/500 [===========================>..] - ETA: 29s - loss: 1.9049 - regression_loss: 1.4119 - classification_loss: 0.4930\n",
      "472/500 [===========================>..] - ETA: 28s - loss: 1.9052 - regression_loss: 1.4115 - classification_loss: 0.4937\n",
      "473/500 [===========================>..] - ETA: 27s - loss: 1.9048 - regression_loss: 1.4111 - classification_loss: 0.4937\n",
      "474/500 [===========================>..] - ETA: 26s - loss: 1.9047 - regression_loss: 1.4112 - classification_loss: 0.4934\n",
      "475/500 [===========================>..] - ETA: 25s - loss: 1.9064 - regression_loss: 1.4127 - classification_loss: 0.4937\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 1.9066 - regression_loss: 1.4129 - classification_loss: 0.4937\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.9070 - regression_loss: 1.4129 - classification_loss: 0.4942\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.9066 - regression_loss: 1.4126 - classification_loss: 0.4939\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.9069 - regression_loss: 1.4127 - classification_loss: 0.4942\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.9078 - regression_loss: 1.4141 - classification_loss: 0.4938\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.9086 - regression_loss: 1.4145 - classification_loss: 0.4941\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.9100 - regression_loss: 1.4155 - classification_loss: 0.4944\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.9089 - regression_loss: 1.4147 - classification_loss: 0.4941\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.9095 - regression_loss: 1.4152 - classification_loss: 0.4943\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.9096 - regression_loss: 1.4152 - classification_loss: 0.4943\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.9084 - regression_loss: 1.4140 - classification_loss: 0.4943\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.9084 - regression_loss: 1.4141 - classification_loss: 0.4943\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.9082 - regression_loss: 1.4139 - classification_loss: 0.4942\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.9072 - regression_loss: 1.4133 - classification_loss: 0.4939\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.9065 - regression_loss: 1.4125 - classification_loss: 0.4939\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.9058 - regression_loss: 1.4124 - classification_loss: 0.4934 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.9051 - regression_loss: 1.4121 - classification_loss: 0.4930\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.9033 - regression_loss: 1.4107 - classification_loss: 0.4925\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.9028 - regression_loss: 1.4105 - classification_loss: 0.4923\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.9052 - regression_loss: 1.4122 - classification_loss: 0.4930\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.9056 - regression_loss: 1.4129 - classification_loss: 0.4927\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.9052 - regression_loss: 1.4124 - classification_loss: 0.4928\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.9058 - regression_loss: 1.4125 - classification_loss: 0.4932\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.9055 - regression_loss: 1.4125 - classification_loss: 0.4930\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9066 - regression_loss: 1.4131 - classification_loss: 0.4935\n",
      "Epoch 00029: saving model to ./snapshots\\resnet50_csv_29.h5\n",
      "\n",
      "500/500 [==============================] - 515s 1s/step - loss: 1.9066 - regression_loss: 1.4131 - classification_loss: 0.4935\n",
      "Epoch 30/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 2.1617 - regression_loss: 1.4372 - classification_loss: 0.7245\n",
      "  2/500 [..............................] - ETA: 4:04 - loss: 2.4071 - regression_loss: 1.7179 - classification_loss: 0.6892\n",
      "  3/500 [..............................] - ETA: 5:25 - loss: 2.6213 - regression_loss: 1.8557 - classification_loss: 0.7656\n",
      "  4/500 [..............................] - ETA: 6:05 - loss: 2.2858 - regression_loss: 1.6405 - classification_loss: 0.6453\n",
      "  5/500 [..............................] - ETA: 6:38 - loss: 2.1881 - regression_loss: 1.5918 - classification_loss: 0.5964\n",
      "  6/500 [..............................] - ETA: 6:52 - loss: 2.0346 - regression_loss: 1.4655 - classification_loss: 0.5691\n",
      "  7/500 [..............................] - ETA: 7:08 - loss: 2.0160 - regression_loss: 1.4645 - classification_loss: 0.5515\n",
      "  8/500 [..............................] - ETA: 7:21 - loss: 2.0460 - regression_loss: 1.4883 - classification_loss: 0.5576\n",
      "  9/500 [..............................] - ETA: 7:30 - loss: 1.9903 - regression_loss: 1.4554 - classification_loss: 0.5349\n",
      " 10/500 [..............................] - ETA: 7:33 - loss: 2.0351 - regression_loss: 1.4926 - classification_loss: 0.5424\n",
      " 11/500 [..............................] - ETA: 7:35 - loss: 1.9927 - regression_loss: 1.4730 - classification_loss: 0.5197\n",
      " 12/500 [..............................] - ETA: 7:40 - loss: 2.0234 - regression_loss: 1.5061 - classification_loss: 0.5173\n",
      " 13/500 [..............................] - ETA: 7:41 - loss: 2.0343 - regression_loss: 1.5269 - classification_loss: 0.5074\n",
      " 14/500 [..............................] - ETA: 7:45 - loss: 2.0521 - regression_loss: 1.5375 - classification_loss: 0.5147\n",
      " 15/500 [..............................] - ETA: 7:45 - loss: 2.0888 - regression_loss: 1.5686 - classification_loss: 0.5202\n",
      " 16/500 [..............................] - ETA: 7:45 - loss: 2.0491 - regression_loss: 1.5373 - classification_loss: 0.5119\n",
      " 17/500 [>.............................] - ETA: 7:45 - loss: 2.0475 - regression_loss: 1.5375 - classification_loss: 0.5100\n",
      " 18/500 [>.............................] - ETA: 7:47 - loss: 2.0971 - regression_loss: 1.5602 - classification_loss: 0.5368\n",
      " 19/500 [>.............................] - ETA: 7:46 - loss: 2.1199 - regression_loss: 1.5697 - classification_loss: 0.5502\n",
      " 20/500 [>.............................] - ETA: 7:46 - loss: 2.0846 - regression_loss: 1.5448 - classification_loss: 0.5398\n",
      " 21/500 [>.............................] - ETA: 7:50 - loss: 2.0703 - regression_loss: 1.5293 - classification_loss: 0.5410\n",
      " 22/500 [>.............................] - ETA: 7:49 - loss: 2.0761 - regression_loss: 1.5377 - classification_loss: 0.5385\n",
      " 23/500 [>.............................] - ETA: 7:50 - loss: 2.0664 - regression_loss: 1.5298 - classification_loss: 0.5366\n",
      " 24/500 [>.............................] - ETA: 7:49 - loss: 2.0415 - regression_loss: 1.5111 - classification_loss: 0.5303\n",
      " 25/500 [>.............................] - ETA: 7:48 - loss: 2.0778 - regression_loss: 1.5372 - classification_loss: 0.5406\n",
      " 26/500 [>.............................] - ETA: 7:47 - loss: 2.0873 - regression_loss: 1.5435 - classification_loss: 0.5438\n",
      " 27/500 [>.............................] - ETA: 7:48 - loss: 2.0942 - regression_loss: 1.5457 - classification_loss: 0.5485\n",
      " 28/500 [>.............................] - ETA: 7:47 - loss: 2.0604 - regression_loss: 1.5235 - classification_loss: 0.5369\n",
      " 29/500 [>.............................] - ETA: 7:47 - loss: 2.0800 - regression_loss: 1.5318 - classification_loss: 0.5481\n",
      " 30/500 [>.............................] - ETA: 7:48 - loss: 2.0649 - regression_loss: 1.5253 - classification_loss: 0.5396\n",
      " 31/500 [>.............................] - ETA: 7:47 - loss: 2.0578 - regression_loss: 1.5223 - classification_loss: 0.5355\n",
      " 32/500 [>.............................] - ETA: 7:46 - loss: 2.0649 - regression_loss: 1.5277 - classification_loss: 0.5372\n",
      " 33/500 [>.............................] - ETA: 7:42 - loss: 2.0632 - regression_loss: 1.5247 - classification_loss: 0.5385\n",
      " 34/500 [=>............................] - ETA: 7:44 - loss: 2.0554 - regression_loss: 1.5177 - classification_loss: 0.5377\n",
      " 35/500 [=>............................] - ETA: 7:43 - loss: 2.0592 - regression_loss: 1.5259 - classification_loss: 0.5334\n",
      " 36/500 [=>............................] - ETA: 7:42 - loss: 2.0357 - regression_loss: 1.5095 - classification_loss: 0.5262\n",
      " 37/500 [=>............................] - ETA: 7:39 - loss: 2.0221 - regression_loss: 1.4962 - classification_loss: 0.5259\n",
      " 38/500 [=>............................] - ETA: 7:40 - loss: 2.0325 - regression_loss: 1.5007 - classification_loss: 0.5318\n",
      " 39/500 [=>............................] - ETA: 7:39 - loss: 2.0086 - regression_loss: 1.4819 - classification_loss: 0.5268\n",
      " 40/500 [=>............................] - ETA: 7:35 - loss: 2.0102 - regression_loss: 1.4834 - classification_loss: 0.5269\n",
      " 41/500 [=>............................] - ETA: 7:35 - loss: 1.9949 - regression_loss: 1.4736 - classification_loss: 0.5212\n",
      " 42/500 [=>............................] - ETA: 7:35 - loss: 1.9973 - regression_loss: 1.4737 - classification_loss: 0.5236\n",
      " 43/500 [=>............................] - ETA: 7:34 - loss: 1.9897 - regression_loss: 1.4704 - classification_loss: 0.5193\n",
      " 44/500 [=>............................] - ETA: 7:33 - loss: 1.9845 - regression_loss: 1.4680 - classification_loss: 0.5165\n",
      " 45/500 [=>............................] - ETA: 7:32 - loss: 1.9713 - regression_loss: 1.4587 - classification_loss: 0.5126\n",
      " 46/500 [=>............................] - ETA: 7:32 - loss: 1.9774 - regression_loss: 1.4588 - classification_loss: 0.5185\n",
      " 47/500 [=>............................] - ETA: 7:31 - loss: 1.9852 - regression_loss: 1.4630 - classification_loss: 0.5223\n",
      " 48/500 [=>............................] - ETA: 7:30 - loss: 1.9905 - regression_loss: 1.4675 - classification_loss: 0.5230\n",
      " 49/500 [=>............................] - ETA: 7:30 - loss: 2.0050 - regression_loss: 1.4766 - classification_loss: 0.5284\n",
      " 50/500 [==>...........................] - ETA: 7:30 - loss: 2.0073 - regression_loss: 1.4777 - classification_loss: 0.5297\n",
      " 51/500 [==>...........................] - ETA: 7:29 - loss: 2.0026 - regression_loss: 1.4728 - classification_loss: 0.5297\n",
      " 52/500 [==>...........................] - ETA: 7:28 - loss: 1.9952 - regression_loss: 1.4672 - classification_loss: 0.5280\n",
      " 53/500 [==>...........................] - ETA: 7:27 - loss: 1.9860 - regression_loss: 1.4615 - classification_loss: 0.5245\n",
      " 54/500 [==>...........................] - ETA: 7:26 - loss: 1.9972 - regression_loss: 1.4678 - classification_loss: 0.5293\n",
      " 55/500 [==>...........................] - ETA: 7:26 - loss: 2.0032 - regression_loss: 1.4697 - classification_loss: 0.5335\n",
      " 56/500 [==>...........................] - ETA: 7:25 - loss: 1.9911 - regression_loss: 1.4616 - classification_loss: 0.5295\n",
      " 57/500 [==>...........................] - ETA: 7:24 - loss: 1.9867 - regression_loss: 1.4610 - classification_loss: 0.5258\n",
      " 58/500 [==>...........................] - ETA: 7:24 - loss: 1.9875 - regression_loss: 1.4623 - classification_loss: 0.5252\n",
      " 59/500 [==>...........................] - ETA: 7:24 - loss: 1.9861 - regression_loss: 1.4602 - classification_loss: 0.5259\n",
      " 60/500 [==>...........................] - ETA: 7:23 - loss: 1.9904 - regression_loss: 1.4651 - classification_loss: 0.5253\n",
      " 61/500 [==>...........................] - ETA: 7:23 - loss: 2.0000 - regression_loss: 1.4723 - classification_loss: 0.5276\n",
      " 62/500 [==>...........................] - ETA: 7:21 - loss: 2.0027 - regression_loss: 1.4725 - classification_loss: 0.5302\n",
      " 63/500 [==>...........................] - ETA: 7:20 - loss: 1.9975 - regression_loss: 1.4700 - classification_loss: 0.5275\n",
      " 64/500 [==>...........................] - ETA: 7:19 - loss: 2.0024 - regression_loss: 1.4772 - classification_loss: 0.5253\n",
      " 65/500 [==>...........................] - ETA: 7:16 - loss: 1.9942 - regression_loss: 1.4708 - classification_loss: 0.5234\n",
      " 66/500 [==>...........................] - ETA: 7:17 - loss: 1.9940 - regression_loss: 1.4720 - classification_loss: 0.5220\n",
      " 67/500 [===>..........................] - ETA: 7:16 - loss: 2.0048 - regression_loss: 1.4799 - classification_loss: 0.5249\n",
      " 68/500 [===>..........................] - ETA: 7:15 - loss: 2.0089 - regression_loss: 1.4844 - classification_loss: 0.5245\n",
      " 69/500 [===>..........................] - ETA: 7:14 - loss: 1.9995 - regression_loss: 1.4770 - classification_loss: 0.5224\n",
      " 70/500 [===>..........................] - ETA: 7:13 - loss: 1.9900 - regression_loss: 1.4703 - classification_loss: 0.5197\n",
      " 71/500 [===>..........................] - ETA: 7:13 - loss: 1.9997 - regression_loss: 1.4751 - classification_loss: 0.5246\n",
      " 72/500 [===>..........................] - ETA: 7:12 - loss: 2.0074 - regression_loss: 1.4798 - classification_loss: 0.5276\n",
      " 73/500 [===>..........................] - ETA: 7:11 - loss: 2.0026 - regression_loss: 1.4756 - classification_loss: 0.5270\n",
      " 74/500 [===>..........................] - ETA: 7:10 - loss: 1.9969 - regression_loss: 1.4681 - classification_loss: 0.5288\n",
      " 75/500 [===>..........................] - ETA: 7:09 - loss: 1.9955 - regression_loss: 1.4667 - classification_loss: 0.5287\n",
      " 76/500 [===>..........................] - ETA: 7:09 - loss: 2.0047 - regression_loss: 1.4760 - classification_loss: 0.5287\n",
      " 77/500 [===>..........................] - ETA: 7:08 - loss: 2.0044 - regression_loss: 1.4757 - classification_loss: 0.5287\n",
      " 78/500 [===>..........................] - ETA: 7:07 - loss: 2.0053 - regression_loss: 1.4785 - classification_loss: 0.5268\n",
      " 79/500 [===>..........................] - ETA: 7:06 - loss: 2.0152 - regression_loss: 1.4857 - classification_loss: 0.5295\n",
      " 80/500 [===>..........................] - ETA: 7:05 - loss: 2.0052 - regression_loss: 1.4779 - classification_loss: 0.5273\n",
      " 81/500 [===>..........................] - ETA: 7:04 - loss: 2.0014 - regression_loss: 1.4763 - classification_loss: 0.5252\n",
      " 82/500 [===>..........................] - ETA: 7:04 - loss: 1.9942 - regression_loss: 1.4716 - classification_loss: 0.5226\n",
      " 83/500 [===>..........................] - ETA: 7:02 - loss: 1.9990 - regression_loss: 1.4757 - classification_loss: 0.5233\n",
      " 84/500 [====>.........................] - ETA: 7:02 - loss: 2.0068 - regression_loss: 1.4802 - classification_loss: 0.5265\n",
      " 85/500 [====>.........................] - ETA: 7:01 - loss: 2.0099 - regression_loss: 1.4813 - classification_loss: 0.5286\n",
      " 86/500 [====>.........................] - ETA: 6:59 - loss: 2.0044 - regression_loss: 1.4787 - classification_loss: 0.5257\n",
      " 87/500 [====>.........................] - ETA: 6:58 - loss: 2.0142 - regression_loss: 1.4873 - classification_loss: 0.5269\n",
      " 88/500 [====>.........................] - ETA: 6:57 - loss: 2.0110 - regression_loss: 1.4861 - classification_loss: 0.5249\n",
      " 89/500 [====>.........................] - ETA: 6:56 - loss: 2.0057 - regression_loss: 1.4838 - classification_loss: 0.5219\n",
      " 90/500 [====>.........................] - ETA: 6:55 - loss: 2.0103 - regression_loss: 1.4861 - classification_loss: 0.5242\n",
      " 91/500 [====>.........................] - ETA: 6:55 - loss: 2.0067 - regression_loss: 1.4840 - classification_loss: 0.5227\n",
      " 92/500 [====>.........................] - ETA: 6:53 - loss: 1.9963 - regression_loss: 1.4764 - classification_loss: 0.5200\n",
      " 93/500 [====>.........................] - ETA: 6:53 - loss: 1.9947 - regression_loss: 1.4757 - classification_loss: 0.5190\n",
      " 94/500 [====>.........................] - ETA: 6:52 - loss: 1.9906 - regression_loss: 1.4716 - classification_loss: 0.5190\n",
      " 95/500 [====>.........................] - ETA: 6:51 - loss: 1.9850 - regression_loss: 1.4664 - classification_loss: 0.5186\n",
      " 96/500 [====>.........................] - ETA: 6:51 - loss: 1.9823 - regression_loss: 1.4655 - classification_loss: 0.5168\n",
      " 97/500 [====>.........................] - ETA: 6:50 - loss: 1.9822 - regression_loss: 1.4651 - classification_loss: 0.5171\n",
      " 98/500 [====>.........................] - ETA: 6:49 - loss: 1.9751 - regression_loss: 1.4607 - classification_loss: 0.5145\n",
      " 99/500 [====>.........................] - ETA: 6:48 - loss: 1.9679 - regression_loss: 1.4560 - classification_loss: 0.5119\n",
      "100/500 [=====>........................] - ETA: 6:47 - loss: 1.9763 - regression_loss: 1.4615 - classification_loss: 0.5147\n",
      "101/500 [=====>........................] - ETA: 6:46 - loss: 1.9804 - regression_loss: 1.4643 - classification_loss: 0.5161\n",
      "102/500 [=====>........................] - ETA: 6:45 - loss: 1.9790 - regression_loss: 1.4645 - classification_loss: 0.5145\n",
      "103/500 [=====>........................] - ETA: 6:44 - loss: 1.9817 - regression_loss: 1.4683 - classification_loss: 0.5134\n",
      "104/500 [=====>........................] - ETA: 6:43 - loss: 1.9809 - regression_loss: 1.4676 - classification_loss: 0.5132\n",
      "105/500 [=====>........................] - ETA: 6:42 - loss: 1.9744 - regression_loss: 1.4628 - classification_loss: 0.5115\n",
      "106/500 [=====>........................] - ETA: 6:41 - loss: 1.9686 - regression_loss: 1.4591 - classification_loss: 0.5095\n",
      "107/500 [=====>........................] - ETA: 6:41 - loss: 1.9625 - regression_loss: 1.4543 - classification_loss: 0.5082\n",
      "108/500 [=====>........................] - ETA: 6:40 - loss: 1.9637 - regression_loss: 1.4554 - classification_loss: 0.5083\n",
      "109/500 [=====>........................] - ETA: 6:39 - loss: 1.9635 - regression_loss: 1.4559 - classification_loss: 0.5075\n",
      "110/500 [=====>........................] - ETA: 6:39 - loss: 1.9582 - regression_loss: 1.4528 - classification_loss: 0.5054\n",
      "111/500 [=====>........................] - ETA: 6:37 - loss: 1.9520 - regression_loss: 1.4486 - classification_loss: 0.5034\n",
      "112/500 [=====>........................] - ETA: 6:36 - loss: 1.9501 - regression_loss: 1.4464 - classification_loss: 0.5037\n",
      "113/500 [=====>........................] - ETA: 6:35 - loss: 1.9468 - regression_loss: 1.4444 - classification_loss: 0.5024\n",
      "114/500 [=====>........................] - ETA: 6:34 - loss: 1.9404 - regression_loss: 1.4395 - classification_loss: 0.5009\n",
      "115/500 [=====>........................] - ETA: 6:33 - loss: 1.9443 - regression_loss: 1.4418 - classification_loss: 0.5025\n",
      "116/500 [=====>........................] - ETA: 6:32 - loss: 1.9356 - regression_loss: 1.4345 - classification_loss: 0.5012\n",
      "117/500 [======>.......................] - ETA: 6:31 - loss: 1.9357 - regression_loss: 1.4360 - classification_loss: 0.4997\n",
      "118/500 [======>.......................] - ETA: 6:30 - loss: 1.9314 - regression_loss: 1.4327 - classification_loss: 0.4987\n",
      "119/500 [======>.......................] - ETA: 6:29 - loss: 1.9339 - regression_loss: 1.4352 - classification_loss: 0.4987\n",
      "120/500 [======>.......................] - ETA: 6:28 - loss: 1.9265 - regression_loss: 1.4298 - classification_loss: 0.4967\n",
      "121/500 [======>.......................] - ETA: 6:27 - loss: 1.9293 - regression_loss: 1.4319 - classification_loss: 0.4974\n",
      "122/500 [======>.......................] - ETA: 6:27 - loss: 1.9370 - regression_loss: 1.4376 - classification_loss: 0.4994\n",
      "123/500 [======>.......................] - ETA: 6:25 - loss: 1.9363 - regression_loss: 1.4359 - classification_loss: 0.5003\n",
      "124/500 [======>.......................] - ETA: 6:24 - loss: 1.9339 - regression_loss: 1.4347 - classification_loss: 0.4993\n",
      "125/500 [======>.......................] - ETA: 6:23 - loss: 1.9270 - regression_loss: 1.4294 - classification_loss: 0.4976\n",
      "126/500 [======>.......................] - ETA: 6:22 - loss: 1.9240 - regression_loss: 1.4276 - classification_loss: 0.4964\n",
      "127/500 [======>.......................] - ETA: 6:21 - loss: 1.9281 - regression_loss: 1.4311 - classification_loss: 0.4971\n",
      "128/500 [======>.......................] - ETA: 6:20 - loss: 1.9252 - regression_loss: 1.4298 - classification_loss: 0.4954\n",
      "129/500 [======>.......................] - ETA: 6:19 - loss: 1.9317 - regression_loss: 1.4335 - classification_loss: 0.4983\n",
      "130/500 [======>.......................] - ETA: 6:18 - loss: 1.9353 - regression_loss: 1.4366 - classification_loss: 0.4987\n",
      "131/500 [======>.......................] - ETA: 6:17 - loss: 1.9433 - regression_loss: 1.4419 - classification_loss: 0.5014\n",
      "132/500 [======>.......................] - ETA: 6:16 - loss: 1.9495 - regression_loss: 1.4470 - classification_loss: 0.5025\n",
      "133/500 [======>.......................] - ETA: 6:15 - loss: 1.9567 - regression_loss: 1.4517 - classification_loss: 0.5050\n",
      "134/500 [=======>......................] - ETA: 6:14 - loss: 1.9553 - regression_loss: 1.4510 - classification_loss: 0.5044\n",
      "135/500 [=======>......................] - ETA: 6:13 - loss: 1.9521 - regression_loss: 1.4487 - classification_loss: 0.5034\n",
      "136/500 [=======>......................] - ETA: 6:12 - loss: 1.9476 - regression_loss: 1.4456 - classification_loss: 0.5020\n",
      "137/500 [=======>......................] - ETA: 6:10 - loss: 1.9540 - regression_loss: 1.4498 - classification_loss: 0.5042\n",
      "138/500 [=======>......................] - ETA: 6:09 - loss: 1.9487 - regression_loss: 1.4453 - classification_loss: 0.5034\n",
      "139/500 [=======>......................] - ETA: 6:08 - loss: 1.9464 - regression_loss: 1.4440 - classification_loss: 0.5024\n",
      "140/500 [=======>......................] - ETA: 6:07 - loss: 1.9465 - regression_loss: 1.4451 - classification_loss: 0.5014\n",
      "141/500 [=======>......................] - ETA: 6:06 - loss: 1.9434 - regression_loss: 1.4428 - classification_loss: 0.5006\n",
      "142/500 [=======>......................] - ETA: 6:05 - loss: 1.9449 - regression_loss: 1.4450 - classification_loss: 0.4999\n",
      "143/500 [=======>......................] - ETA: 6:04 - loss: 1.9398 - regression_loss: 1.4413 - classification_loss: 0.4985\n",
      "144/500 [=======>......................] - ETA: 6:03 - loss: 1.9412 - regression_loss: 1.4414 - classification_loss: 0.4998\n",
      "145/500 [=======>......................] - ETA: 6:02 - loss: 1.9392 - regression_loss: 1.4394 - classification_loss: 0.4997\n",
      "146/500 [=======>......................] - ETA: 6:01 - loss: 1.9367 - regression_loss: 1.4377 - classification_loss: 0.4991\n",
      "147/500 [=======>......................] - ETA: 6:00 - loss: 1.9376 - regression_loss: 1.4376 - classification_loss: 0.4999\n",
      "148/500 [=======>......................] - ETA: 5:59 - loss: 1.9451 - regression_loss: 1.4419 - classification_loss: 0.5032\n",
      "149/500 [=======>......................] - ETA: 5:58 - loss: 1.9431 - regression_loss: 1.4412 - classification_loss: 0.5019\n",
      "150/500 [========>.....................] - ETA: 5:57 - loss: 1.9412 - regression_loss: 1.4405 - classification_loss: 0.5007\n",
      "151/500 [========>.....................] - ETA: 5:56 - loss: 1.9419 - regression_loss: 1.4416 - classification_loss: 0.5003\n",
      "152/500 [========>.....................] - ETA: 5:54 - loss: 1.9400 - regression_loss: 1.4402 - classification_loss: 0.4998\n",
      "153/500 [========>.....................] - ETA: 5:53 - loss: 1.9390 - regression_loss: 1.4401 - classification_loss: 0.4989\n",
      "154/500 [========>.....................] - ETA: 5:52 - loss: 1.9358 - regression_loss: 1.4384 - classification_loss: 0.4974\n",
      "155/500 [========>.....................] - ETA: 5:51 - loss: 1.9345 - regression_loss: 1.4383 - classification_loss: 0.4962\n",
      "156/500 [========>.....................] - ETA: 5:50 - loss: 1.9317 - regression_loss: 1.4359 - classification_loss: 0.4958\n",
      "157/500 [========>.....................] - ETA: 5:49 - loss: 1.9311 - regression_loss: 1.4357 - classification_loss: 0.4954\n",
      "158/500 [========>.....................] - ETA: 5:48 - loss: 1.9347 - regression_loss: 1.4390 - classification_loss: 0.4957\n",
      "159/500 [========>.....................] - ETA: 5:47 - loss: 1.9341 - regression_loss: 1.4392 - classification_loss: 0.4949\n",
      "160/500 [========>.....................] - ETA: 5:46 - loss: 1.9353 - regression_loss: 1.4411 - classification_loss: 0.4942\n",
      "161/500 [========>.....................] - ETA: 5:45 - loss: 1.9355 - regression_loss: 1.4420 - classification_loss: 0.4936\n",
      "162/500 [========>.....................] - ETA: 5:45 - loss: 1.9414 - regression_loss: 1.4463 - classification_loss: 0.4952\n",
      "163/500 [========>.....................] - ETA: 5:44 - loss: 1.9486 - regression_loss: 1.4512 - classification_loss: 0.4974\n",
      "164/500 [========>.....................] - ETA: 5:42 - loss: 1.9495 - regression_loss: 1.4524 - classification_loss: 0.4971\n",
      "165/500 [========>.....................] - ETA: 5:42 - loss: 1.9479 - regression_loss: 1.4503 - classification_loss: 0.4976\n",
      "166/500 [========>.....................] - ETA: 5:40 - loss: 1.9470 - regression_loss: 1.4494 - classification_loss: 0.4976\n",
      "167/500 [=========>....................] - ETA: 5:39 - loss: 1.9420 - regression_loss: 1.4463 - classification_loss: 0.4957\n",
      "168/500 [=========>....................] - ETA: 5:38 - loss: 1.9469 - regression_loss: 1.4464 - classification_loss: 0.5005\n",
      "169/500 [=========>....................] - ETA: 5:37 - loss: 1.9448 - regression_loss: 1.4452 - classification_loss: 0.4996\n",
      "170/500 [=========>....................] - ETA: 5:36 - loss: 1.9501 - regression_loss: 1.4497 - classification_loss: 0.5004\n",
      "171/500 [=========>....................] - ETA: 5:36 - loss: 1.9561 - regression_loss: 1.4541 - classification_loss: 0.5020\n",
      "172/500 [=========>....................] - ETA: 5:35 - loss: 1.9586 - regression_loss: 1.4551 - classification_loss: 0.5035\n",
      "173/500 [=========>....................] - ETA: 5:34 - loss: 1.9614 - regression_loss: 1.4578 - classification_loss: 0.5037\n",
      "174/500 [=========>....................] - ETA: 5:33 - loss: 1.9574 - regression_loss: 1.4545 - classification_loss: 0.5029\n",
      "175/500 [=========>....................] - ETA: 5:32 - loss: 1.9602 - regression_loss: 1.4567 - classification_loss: 0.5035\n",
      "176/500 [=========>....................] - ETA: 5:31 - loss: 1.9590 - regression_loss: 1.4558 - classification_loss: 0.5032\n",
      "177/500 [=========>....................] - ETA: 5:30 - loss: 1.9553 - regression_loss: 1.4529 - classification_loss: 0.5024\n",
      "178/500 [=========>....................] - ETA: 5:29 - loss: 1.9596 - regression_loss: 1.4568 - classification_loss: 0.5028\n",
      "179/500 [=========>....................] - ETA: 5:28 - loss: 1.9600 - regression_loss: 1.4574 - classification_loss: 0.5026\n",
      "180/500 [=========>....................] - ETA: 5:27 - loss: 1.9614 - regression_loss: 1.4594 - classification_loss: 0.5020\n",
      "181/500 [=========>....................] - ETA: 5:26 - loss: 1.9605 - regression_loss: 1.4583 - classification_loss: 0.5021\n",
      "182/500 [=========>....................] - ETA: 5:25 - loss: 1.9608 - regression_loss: 1.4590 - classification_loss: 0.5018\n",
      "183/500 [=========>....................] - ETA: 5:24 - loss: 1.9624 - regression_loss: 1.4601 - classification_loss: 0.5023\n",
      "184/500 [==========>...................] - ETA: 5:23 - loss: 1.9642 - regression_loss: 1.4613 - classification_loss: 0.5029\n",
      "185/500 [==========>...................] - ETA: 5:22 - loss: 1.9639 - regression_loss: 1.4614 - classification_loss: 0.5025\n",
      "186/500 [==========>...................] - ETA: 5:21 - loss: 1.9675 - regression_loss: 1.4630 - classification_loss: 0.5046\n",
      "187/500 [==========>...................] - ETA: 5:20 - loss: 1.9674 - regression_loss: 1.4630 - classification_loss: 0.5044\n",
      "188/500 [==========>...................] - ETA: 5:19 - loss: 1.9674 - regression_loss: 1.4634 - classification_loss: 0.5040\n",
      "189/500 [==========>...................] - ETA: 5:18 - loss: 1.9671 - regression_loss: 1.4633 - classification_loss: 0.5039\n",
      "190/500 [==========>...................] - ETA: 5:17 - loss: 1.9650 - regression_loss: 1.4621 - classification_loss: 0.5029\n",
      "191/500 [==========>...................] - ETA: 5:16 - loss: 1.9636 - regression_loss: 1.4609 - classification_loss: 0.5027\n",
      "192/500 [==========>...................] - ETA: 5:15 - loss: 1.9621 - regression_loss: 1.4597 - classification_loss: 0.5024\n",
      "193/500 [==========>...................] - ETA: 5:14 - loss: 1.9611 - regression_loss: 1.4588 - classification_loss: 0.5022\n",
      "194/500 [==========>...................] - ETA: 5:12 - loss: 1.9592 - regression_loss: 1.4574 - classification_loss: 0.5018\n",
      "195/500 [==========>...................] - ETA: 5:12 - loss: 1.9548 - regression_loss: 1.4539 - classification_loss: 0.5008\n",
      "196/500 [==========>...................] - ETA: 5:11 - loss: 1.9537 - regression_loss: 1.4533 - classification_loss: 0.5004\n",
      "197/500 [==========>...................] - ETA: 5:09 - loss: 1.9557 - regression_loss: 1.4547 - classification_loss: 0.5009\n",
      "198/500 [==========>...................] - ETA: 5:08 - loss: 1.9570 - regression_loss: 1.4560 - classification_loss: 0.5010\n",
      "199/500 [==========>...................] - ETA: 5:07 - loss: 1.9552 - regression_loss: 1.4546 - classification_loss: 0.5006\n",
      "200/500 [===========>..................] - ETA: 5:06 - loss: 1.9544 - regression_loss: 1.4546 - classification_loss: 0.4998\n",
      "201/500 [===========>..................] - ETA: 5:06 - loss: 1.9552 - regression_loss: 1.4555 - classification_loss: 0.4997\n",
      "202/500 [===========>..................] - ETA: 5:05 - loss: 1.9576 - regression_loss: 1.4575 - classification_loss: 0.5001\n",
      "203/500 [===========>..................] - ETA: 5:04 - loss: 1.9642 - regression_loss: 1.4619 - classification_loss: 0.5023\n",
      "204/500 [===========>..................] - ETA: 5:02 - loss: 1.9674 - regression_loss: 1.4650 - classification_loss: 0.5024\n",
      "205/500 [===========>..................] - ETA: 5:01 - loss: 1.9663 - regression_loss: 1.4641 - classification_loss: 0.5022\n",
      "206/500 [===========>..................] - ETA: 5:00 - loss: 1.9658 - regression_loss: 1.4640 - classification_loss: 0.5018\n",
      "207/500 [===========>..................] - ETA: 4:59 - loss: 1.9628 - regression_loss: 1.4618 - classification_loss: 0.5009\n",
      "208/500 [===========>..................] - ETA: 4:58 - loss: 1.9668 - regression_loss: 1.4646 - classification_loss: 0.5022\n",
      "209/500 [===========>..................] - ETA: 4:57 - loss: 1.9623 - regression_loss: 1.4614 - classification_loss: 0.5009\n",
      "210/500 [===========>..................] - ETA: 4:56 - loss: 1.9579 - regression_loss: 1.4585 - classification_loss: 0.4995\n",
      "211/500 [===========>..................] - ETA: 4:55 - loss: 1.9587 - regression_loss: 1.4595 - classification_loss: 0.4992\n",
      "212/500 [===========>..................] - ETA: 4:54 - loss: 1.9593 - regression_loss: 1.4600 - classification_loss: 0.4993\n",
      "213/500 [===========>..................] - ETA: 4:53 - loss: 1.9632 - regression_loss: 1.4628 - classification_loss: 0.5003\n",
      "214/500 [===========>..................] - ETA: 4:52 - loss: 1.9631 - regression_loss: 1.4623 - classification_loss: 0.5008\n",
      "215/500 [===========>..................] - ETA: 4:51 - loss: 1.9613 - regression_loss: 1.4605 - classification_loss: 0.5007\n",
      "216/500 [===========>..................] - ETA: 4:50 - loss: 1.9632 - regression_loss: 1.4621 - classification_loss: 0.5011\n",
      "217/500 [============>.................] - ETA: 4:49 - loss: 1.9654 - regression_loss: 1.4631 - classification_loss: 0.5023\n",
      "218/500 [============>.................] - ETA: 4:48 - loss: 1.9621 - regression_loss: 1.4608 - classification_loss: 0.5014\n",
      "219/500 [============>.................] - ETA: 4:47 - loss: 1.9617 - regression_loss: 1.4604 - classification_loss: 0.5013\n",
      "220/500 [============>.................] - ETA: 4:46 - loss: 1.9658 - regression_loss: 1.4633 - classification_loss: 0.5025\n",
      "221/500 [============>.................] - ETA: 4:45 - loss: 1.9635 - regression_loss: 1.4617 - classification_loss: 0.5018\n",
      "222/500 [============>.................] - ETA: 4:44 - loss: 1.9662 - regression_loss: 1.4641 - classification_loss: 0.5021\n",
      "223/500 [============>.................] - ETA: 4:43 - loss: 1.9662 - regression_loss: 1.4643 - classification_loss: 0.5019\n",
      "224/500 [============>.................] - ETA: 4:42 - loss: 1.9637 - regression_loss: 1.4623 - classification_loss: 0.5014\n",
      "225/500 [============>.................] - ETA: 4:41 - loss: 1.9637 - regression_loss: 1.4617 - classification_loss: 0.5020\n",
      "226/500 [============>.................] - ETA: 4:40 - loss: 1.9643 - regression_loss: 1.4625 - classification_loss: 0.5017\n",
      "227/500 [============>.................] - ETA: 4:39 - loss: 1.9618 - regression_loss: 1.4605 - classification_loss: 0.5014\n",
      "228/500 [============>.................] - ETA: 4:38 - loss: 1.9618 - regression_loss: 1.4610 - classification_loss: 0.5008\n",
      "229/500 [============>.................] - ETA: 4:37 - loss: 1.9587 - regression_loss: 1.4590 - classification_loss: 0.4998\n",
      "230/500 [============>.................] - ETA: 4:36 - loss: 1.9568 - regression_loss: 1.4574 - classification_loss: 0.4994\n",
      "231/500 [============>.................] - ETA: 4:35 - loss: 1.9545 - regression_loss: 1.4554 - classification_loss: 0.4991\n",
      "232/500 [============>.................] - ETA: 4:34 - loss: 1.9585 - regression_loss: 1.4588 - classification_loss: 0.4997\n",
      "233/500 [============>.................] - ETA: 4:33 - loss: 1.9594 - regression_loss: 1.4598 - classification_loss: 0.4996\n",
      "234/500 [=============>................] - ETA: 4:32 - loss: 1.9568 - regression_loss: 1.4579 - classification_loss: 0.4989\n",
      "235/500 [=============>................] - ETA: 4:31 - loss: 1.9581 - regression_loss: 1.4590 - classification_loss: 0.4991\n",
      "236/500 [=============>................] - ETA: 4:30 - loss: 1.9537 - regression_loss: 1.4558 - classification_loss: 0.4980\n",
      "237/500 [=============>................] - ETA: 4:29 - loss: 1.9559 - regression_loss: 1.4571 - classification_loss: 0.4988\n",
      "238/500 [=============>................] - ETA: 4:28 - loss: 1.9557 - regression_loss: 1.4575 - classification_loss: 0.4982\n",
      "239/500 [=============>................] - ETA: 4:26 - loss: 1.9561 - regression_loss: 1.4573 - classification_loss: 0.4988\n",
      "240/500 [=============>................] - ETA: 4:26 - loss: 1.9564 - regression_loss: 1.4573 - classification_loss: 0.4991\n",
      "241/500 [=============>................] - ETA: 4:25 - loss: 1.9546 - regression_loss: 1.4561 - classification_loss: 0.4985\n",
      "242/500 [=============>................] - ETA: 4:24 - loss: 1.9540 - regression_loss: 1.4556 - classification_loss: 0.4983\n",
      "243/500 [=============>................] - ETA: 4:22 - loss: 1.9518 - regression_loss: 1.4544 - classification_loss: 0.4975\n",
      "244/500 [=============>................] - ETA: 4:22 - loss: 1.9565 - regression_loss: 1.4574 - classification_loss: 0.4990\n",
      "245/500 [=============>................] - ETA: 4:21 - loss: 1.9539 - regression_loss: 1.4557 - classification_loss: 0.4982\n",
      "246/500 [=============>................] - ETA: 4:20 - loss: 1.9510 - regression_loss: 1.4532 - classification_loss: 0.4978\n",
      "247/500 [=============>................] - ETA: 4:19 - loss: 1.9526 - regression_loss: 1.4547 - classification_loss: 0.4980\n",
      "248/500 [=============>................] - ETA: 4:17 - loss: 1.9490 - regression_loss: 1.4520 - classification_loss: 0.4970\n",
      "249/500 [=============>................] - ETA: 4:16 - loss: 1.9479 - regression_loss: 1.4514 - classification_loss: 0.4965\n",
      "250/500 [==============>...............] - ETA: 4:16 - loss: 1.9513 - regression_loss: 1.4543 - classification_loss: 0.4970\n",
      "251/500 [==============>...............] - ETA: 4:15 - loss: 1.9528 - regression_loss: 1.4562 - classification_loss: 0.4966\n",
      "252/500 [==============>...............] - ETA: 4:14 - loss: 1.9500 - regression_loss: 1.4539 - classification_loss: 0.4961\n",
      "253/500 [==============>...............] - ETA: 4:12 - loss: 1.9534 - regression_loss: 1.4552 - classification_loss: 0.4982\n",
      "254/500 [==============>...............] - ETA: 4:11 - loss: 1.9527 - regression_loss: 1.4542 - classification_loss: 0.4985\n",
      "255/500 [==============>...............] - ETA: 4:10 - loss: 1.9529 - regression_loss: 1.4542 - classification_loss: 0.4987\n",
      "256/500 [==============>...............] - ETA: 4:09 - loss: 1.9562 - regression_loss: 1.4568 - classification_loss: 0.4994\n",
      "257/500 [==============>...............] - ETA: 4:08 - loss: 1.9555 - regression_loss: 1.4563 - classification_loss: 0.4992\n",
      "258/500 [==============>...............] - ETA: 4:07 - loss: 1.9558 - regression_loss: 1.4567 - classification_loss: 0.4992\n",
      "259/500 [==============>...............] - ETA: 4:06 - loss: 1.9553 - regression_loss: 1.4563 - classification_loss: 0.4990\n",
      "260/500 [==============>...............] - ETA: 4:05 - loss: 1.9543 - regression_loss: 1.4555 - classification_loss: 0.4987\n",
      "261/500 [==============>...............] - ETA: 4:04 - loss: 1.9550 - regression_loss: 1.4557 - classification_loss: 0.4993\n",
      "262/500 [==============>...............] - ETA: 4:03 - loss: 1.9529 - regression_loss: 1.4547 - classification_loss: 0.4982\n",
      "263/500 [==============>...............] - ETA: 4:02 - loss: 1.9540 - regression_loss: 1.4557 - classification_loss: 0.4983\n",
      "264/500 [==============>...............] - ETA: 4:01 - loss: 1.9565 - regression_loss: 1.4578 - classification_loss: 0.4987\n",
      "265/500 [==============>...............] - ETA: 4:00 - loss: 1.9576 - regression_loss: 1.4586 - classification_loss: 0.4990\n",
      "266/500 [==============>...............] - ETA: 3:59 - loss: 1.9565 - regression_loss: 1.4577 - classification_loss: 0.4987\n",
      "267/500 [===============>..............] - ETA: 3:58 - loss: 1.9596 - regression_loss: 1.4599 - classification_loss: 0.4997\n",
      "268/500 [===============>..............] - ETA: 3:57 - loss: 1.9601 - regression_loss: 1.4597 - classification_loss: 0.5003\n",
      "269/500 [===============>..............] - ETA: 3:56 - loss: 1.9563 - regression_loss: 1.4569 - classification_loss: 0.4994\n",
      "270/500 [===============>..............] - ETA: 3:55 - loss: 1.9535 - regression_loss: 1.4549 - classification_loss: 0.4985\n",
      "271/500 [===============>..............] - ETA: 3:54 - loss: 1.9508 - regression_loss: 1.4533 - classification_loss: 0.4975\n",
      "272/500 [===============>..............] - ETA: 3:53 - loss: 1.9499 - regression_loss: 1.4529 - classification_loss: 0.4969\n",
      "273/500 [===============>..............] - ETA: 3:52 - loss: 1.9515 - regression_loss: 1.4540 - classification_loss: 0.4975\n",
      "274/500 [===============>..............] - ETA: 3:51 - loss: 1.9503 - regression_loss: 1.4531 - classification_loss: 0.4972\n",
      "275/500 [===============>..............] - ETA: 3:50 - loss: 1.9485 - regression_loss: 1.4516 - classification_loss: 0.4969\n",
      "276/500 [===============>..............] - ETA: 3:49 - loss: 1.9457 - regression_loss: 1.4495 - classification_loss: 0.4962\n",
      "277/500 [===============>..............] - ETA: 3:48 - loss: 1.9424 - regression_loss: 1.4472 - classification_loss: 0.4952\n",
      "278/500 [===============>..............] - ETA: 3:47 - loss: 1.9435 - regression_loss: 1.4479 - classification_loss: 0.4956\n",
      "279/500 [===============>..............] - ETA: 3:46 - loss: 1.9431 - regression_loss: 1.4472 - classification_loss: 0.4959\n",
      "280/500 [===============>..............] - ETA: 3:45 - loss: 1.9427 - regression_loss: 1.4475 - classification_loss: 0.4952\n",
      "281/500 [===============>..............] - ETA: 3:44 - loss: 1.9422 - regression_loss: 1.4472 - classification_loss: 0.4951\n",
      "282/500 [===============>..............] - ETA: 3:43 - loss: 1.9442 - regression_loss: 1.4488 - classification_loss: 0.4954\n",
      "283/500 [===============>..............] - ETA: 3:42 - loss: 1.9458 - regression_loss: 1.4495 - classification_loss: 0.4963\n",
      "284/500 [================>.............] - ETA: 3:41 - loss: 1.9472 - regression_loss: 1.4509 - classification_loss: 0.4963\n",
      "285/500 [================>.............] - ETA: 3:40 - loss: 1.9481 - regression_loss: 1.4516 - classification_loss: 0.4965\n",
      "286/500 [================>.............] - ETA: 3:39 - loss: 1.9457 - regression_loss: 1.4498 - classification_loss: 0.4960\n",
      "287/500 [================>.............] - ETA: 3:38 - loss: 1.9468 - regression_loss: 1.4505 - classification_loss: 0.4962\n",
      "288/500 [================>.............] - ETA: 3:37 - loss: 1.9490 - regression_loss: 1.4522 - classification_loss: 0.4968\n",
      "289/500 [================>.............] - ETA: 3:36 - loss: 1.9501 - regression_loss: 1.4533 - classification_loss: 0.4968\n",
      "290/500 [================>.............] - ETA: 3:35 - loss: 1.9496 - regression_loss: 1.4523 - classification_loss: 0.4973\n",
      "291/500 [================>.............] - ETA: 3:34 - loss: 1.9507 - regression_loss: 1.4536 - classification_loss: 0.4971\n",
      "292/500 [================>.............] - ETA: 3:33 - loss: 1.9497 - regression_loss: 1.4530 - classification_loss: 0.4968\n",
      "293/500 [================>.............] - ETA: 3:32 - loss: 1.9490 - regression_loss: 1.4528 - classification_loss: 0.4962\n",
      "294/500 [================>.............] - ETA: 3:31 - loss: 1.9489 - regression_loss: 1.4532 - classification_loss: 0.4957\n",
      "295/500 [================>.............] - ETA: 3:30 - loss: 1.9479 - regression_loss: 1.4526 - classification_loss: 0.4953\n",
      "296/500 [================>.............] - ETA: 3:29 - loss: 1.9476 - regression_loss: 1.4522 - classification_loss: 0.4954\n",
      "297/500 [================>.............] - ETA: 3:28 - loss: 1.9439 - regression_loss: 1.4495 - classification_loss: 0.4943\n",
      "298/500 [================>.............] - ETA: 3:27 - loss: 1.9439 - regression_loss: 1.4499 - classification_loss: 0.4940\n",
      "299/500 [================>.............] - ETA: 3:26 - loss: 1.9416 - regression_loss: 1.4483 - classification_loss: 0.4933\n",
      "300/500 [=================>............] - ETA: 3:25 - loss: 1.9456 - regression_loss: 1.4509 - classification_loss: 0.4947\n",
      "301/500 [=================>............] - ETA: 3:24 - loss: 1.9426 - regression_loss: 1.4484 - classification_loss: 0.4942\n",
      "302/500 [=================>............] - ETA: 3:23 - loss: 1.9420 - regression_loss: 1.4482 - classification_loss: 0.4938\n",
      "303/500 [=================>............] - ETA: 3:22 - loss: 1.9412 - regression_loss: 1.4477 - classification_loss: 0.4935\n",
      "304/500 [=================>............] - ETA: 3:21 - loss: 1.9392 - regression_loss: 1.4464 - classification_loss: 0.4929\n",
      "305/500 [=================>............] - ETA: 3:20 - loss: 1.9381 - regression_loss: 1.4459 - classification_loss: 0.4922\n",
      "306/500 [=================>............] - ETA: 3:19 - loss: 1.9373 - regression_loss: 1.4454 - classification_loss: 0.4919\n",
      "307/500 [=================>............] - ETA: 3:18 - loss: 1.9358 - regression_loss: 1.4440 - classification_loss: 0.4918\n",
      "308/500 [=================>............] - ETA: 3:17 - loss: 1.9355 - regression_loss: 1.4440 - classification_loss: 0.4916\n",
      "309/500 [=================>............] - ETA: 3:16 - loss: 1.9346 - regression_loss: 1.4434 - classification_loss: 0.4911\n",
      "310/500 [=================>............] - ETA: 3:15 - loss: 1.9335 - regression_loss: 1.4430 - classification_loss: 0.4905\n",
      "311/500 [=================>............] - ETA: 3:13 - loss: 1.9313 - regression_loss: 1.4413 - classification_loss: 0.4900\n",
      "312/500 [=================>............] - ETA: 3:12 - loss: 1.9290 - regression_loss: 1.4394 - classification_loss: 0.4896\n",
      "313/500 [=================>............] - ETA: 3:11 - loss: 1.9279 - regression_loss: 1.4388 - classification_loss: 0.4892\n",
      "314/500 [=================>............] - ETA: 3:10 - loss: 1.9282 - regression_loss: 1.4388 - classification_loss: 0.4894\n",
      "315/500 [=================>............] - ETA: 3:09 - loss: 1.9288 - regression_loss: 1.4394 - classification_loss: 0.4894\n",
      "316/500 [=================>............] - ETA: 3:08 - loss: 1.9301 - regression_loss: 1.4401 - classification_loss: 0.4900\n",
      "317/500 [==================>...........] - ETA: 3:07 - loss: 1.9292 - regression_loss: 1.4390 - classification_loss: 0.4902\n",
      "318/500 [==================>...........] - ETA: 3:06 - loss: 1.9312 - regression_loss: 1.4395 - classification_loss: 0.4918\n",
      "319/500 [==================>...........] - ETA: 3:05 - loss: 1.9310 - regression_loss: 1.4392 - classification_loss: 0.4918\n",
      "320/500 [==================>...........] - ETA: 3:04 - loss: 1.9302 - regression_loss: 1.4385 - classification_loss: 0.4917\n",
      "321/500 [==================>...........] - ETA: 3:03 - loss: 1.9278 - regression_loss: 1.4366 - classification_loss: 0.4912\n",
      "322/500 [==================>...........] - ETA: 3:02 - loss: 1.9283 - regression_loss: 1.4368 - classification_loss: 0.4915\n",
      "323/500 [==================>...........] - ETA: 3:01 - loss: 1.9282 - regression_loss: 1.4367 - classification_loss: 0.4915\n",
      "324/500 [==================>...........] - ETA: 3:00 - loss: 1.9286 - regression_loss: 1.4370 - classification_loss: 0.4916\n",
      "325/500 [==================>...........] - ETA: 2:59 - loss: 1.9305 - regression_loss: 1.4382 - classification_loss: 0.4923\n",
      "326/500 [==================>...........] - ETA: 2:58 - loss: 1.9296 - regression_loss: 1.4371 - classification_loss: 0.4925\n",
      "327/500 [==================>...........] - ETA: 2:57 - loss: 1.9270 - regression_loss: 1.4355 - classification_loss: 0.4915\n",
      "328/500 [==================>...........] - ETA: 2:56 - loss: 1.9251 - regression_loss: 1.4344 - classification_loss: 0.4907\n",
      "329/500 [==================>...........] - ETA: 2:55 - loss: 1.9249 - regression_loss: 1.4345 - classification_loss: 0.4904\n",
      "330/500 [==================>...........] - ETA: 2:54 - loss: 1.9245 - regression_loss: 1.4346 - classification_loss: 0.4900\n",
      "331/500 [==================>...........] - ETA: 2:53 - loss: 1.9228 - regression_loss: 1.4335 - classification_loss: 0.4893\n",
      "332/500 [==================>...........] - ETA: 2:52 - loss: 1.9242 - regression_loss: 1.4350 - classification_loss: 0.4892\n",
      "333/500 [==================>...........] - ETA: 2:51 - loss: 1.9239 - regression_loss: 1.4351 - classification_loss: 0.4888\n",
      "334/500 [===================>..........] - ETA: 2:50 - loss: 1.9219 - regression_loss: 1.4337 - classification_loss: 0.4882\n",
      "335/500 [===================>..........] - ETA: 2:49 - loss: 1.9242 - regression_loss: 1.4353 - classification_loss: 0.4889\n",
      "336/500 [===================>..........] - ETA: 2:48 - loss: 1.9265 - regression_loss: 1.4366 - classification_loss: 0.4899\n",
      "337/500 [===================>..........] - ETA: 2:47 - loss: 1.9283 - regression_loss: 1.4379 - classification_loss: 0.4904\n",
      "338/500 [===================>..........] - ETA: 2:46 - loss: 1.9294 - regression_loss: 1.4385 - classification_loss: 0.4909\n",
      "339/500 [===================>..........] - ETA: 2:45 - loss: 1.9295 - regression_loss: 1.4390 - classification_loss: 0.4906\n",
      "340/500 [===================>..........] - ETA: 2:44 - loss: 1.9282 - regression_loss: 1.4379 - classification_loss: 0.4903\n",
      "341/500 [===================>..........] - ETA: 2:43 - loss: 1.9279 - regression_loss: 1.4376 - classification_loss: 0.4903\n",
      "342/500 [===================>..........] - ETA: 2:42 - loss: 1.9312 - regression_loss: 1.4401 - classification_loss: 0.4911\n",
      "343/500 [===================>..........] - ETA: 2:41 - loss: 1.9320 - regression_loss: 1.4406 - classification_loss: 0.4914\n",
      "344/500 [===================>..........] - ETA: 2:39 - loss: 1.9324 - regression_loss: 1.4412 - classification_loss: 0.4912\n",
      "345/500 [===================>..........] - ETA: 2:39 - loss: 1.9331 - regression_loss: 1.4415 - classification_loss: 0.4917\n",
      "346/500 [===================>..........] - ETA: 2:38 - loss: 1.9315 - regression_loss: 1.4403 - classification_loss: 0.4912\n",
      "347/500 [===================>..........] - ETA: 2:37 - loss: 1.9308 - regression_loss: 1.4400 - classification_loss: 0.4908\n",
      "348/500 [===================>..........] - ETA: 2:35 - loss: 1.9293 - regression_loss: 1.4389 - classification_loss: 0.4905\n",
      "349/500 [===================>..........] - ETA: 2:34 - loss: 1.9282 - regression_loss: 1.4382 - classification_loss: 0.4900\n",
      "350/500 [====================>.........] - ETA: 2:33 - loss: 1.9278 - regression_loss: 1.4380 - classification_loss: 0.4899\n",
      "351/500 [====================>.........] - ETA: 2:32 - loss: 1.9276 - regression_loss: 1.4376 - classification_loss: 0.4900\n",
      "352/500 [====================>.........] - ETA: 2:31 - loss: 1.9270 - regression_loss: 1.4373 - classification_loss: 0.4897\n",
      "353/500 [====================>.........] - ETA: 2:30 - loss: 1.9265 - regression_loss: 1.4370 - classification_loss: 0.4895\n",
      "354/500 [====================>.........] - ETA: 2:29 - loss: 1.9265 - regression_loss: 1.4369 - classification_loss: 0.4897\n",
      "355/500 [====================>.........] - ETA: 2:28 - loss: 1.9255 - regression_loss: 1.4361 - classification_loss: 0.4894\n",
      "356/500 [====================>.........] - ETA: 2:27 - loss: 1.9243 - regression_loss: 1.4350 - classification_loss: 0.4893\n",
      "357/500 [====================>.........] - ETA: 2:26 - loss: 1.9233 - regression_loss: 1.4345 - classification_loss: 0.4888\n",
      "358/500 [====================>.........] - ETA: 2:25 - loss: 1.9229 - regression_loss: 1.4338 - classification_loss: 0.4891\n",
      "359/500 [====================>.........] - ETA: 2:24 - loss: 1.9228 - regression_loss: 1.4339 - classification_loss: 0.4889\n",
      "360/500 [====================>.........] - ETA: 2:23 - loss: 1.9229 - regression_loss: 1.4338 - classification_loss: 0.4891\n",
      "361/500 [====================>.........] - ETA: 2:22 - loss: 1.9244 - regression_loss: 1.4351 - classification_loss: 0.4894\n",
      "362/500 [====================>.........] - ETA: 2:21 - loss: 1.9260 - regression_loss: 1.4360 - classification_loss: 0.4901\n",
      "363/500 [====================>.........] - ETA: 2:20 - loss: 1.9274 - regression_loss: 1.4368 - classification_loss: 0.4906\n",
      "364/500 [====================>.........] - ETA: 2:19 - loss: 1.9277 - regression_loss: 1.4372 - classification_loss: 0.4905\n",
      "365/500 [====================>.........] - ETA: 2:18 - loss: 1.9294 - regression_loss: 1.4380 - classification_loss: 0.4913\n",
      "366/500 [====================>.........] - ETA: 2:17 - loss: 1.9289 - regression_loss: 1.4380 - classification_loss: 0.4910\n",
      "367/500 [=====================>........] - ETA: 2:16 - loss: 1.9273 - regression_loss: 1.4368 - classification_loss: 0.4905\n",
      "368/500 [=====================>........] - ETA: 2:15 - loss: 1.9267 - regression_loss: 1.4362 - classification_loss: 0.4905\n",
      "369/500 [=====================>........] - ETA: 2:14 - loss: 1.9249 - regression_loss: 1.4351 - classification_loss: 0.4898\n",
      "370/500 [=====================>........] - ETA: 2:13 - loss: 1.9242 - regression_loss: 1.4348 - classification_loss: 0.4894\n",
      "371/500 [=====================>........] - ETA: 2:12 - loss: 1.9270 - regression_loss: 1.4370 - classification_loss: 0.4900\n",
      "372/500 [=====================>........] - ETA: 2:11 - loss: 1.9265 - regression_loss: 1.4369 - classification_loss: 0.4896\n",
      "373/500 [=====================>........] - ETA: 2:10 - loss: 1.9281 - regression_loss: 1.4384 - classification_loss: 0.4898\n",
      "374/500 [=====================>........] - ETA: 2:09 - loss: 1.9260 - regression_loss: 1.4368 - classification_loss: 0.4892\n",
      "375/500 [=====================>........] - ETA: 2:08 - loss: 1.9251 - regression_loss: 1.4364 - classification_loss: 0.4887\n",
      "376/500 [=====================>........] - ETA: 2:07 - loss: 1.9263 - regression_loss: 1.4373 - classification_loss: 0.4890\n",
      "377/500 [=====================>........] - ETA: 2:06 - loss: 1.9261 - regression_loss: 1.4371 - classification_loss: 0.4890\n",
      "378/500 [=====================>........] - ETA: 2:05 - loss: 1.9234 - regression_loss: 1.4351 - classification_loss: 0.4882\n",
      "379/500 [=====================>........] - ETA: 2:04 - loss: 1.9245 - regression_loss: 1.4361 - classification_loss: 0.4884\n",
      "380/500 [=====================>........] - ETA: 2:03 - loss: 1.9233 - regression_loss: 1.4351 - classification_loss: 0.4883\n",
      "381/500 [=====================>........] - ETA: 2:02 - loss: 1.9225 - regression_loss: 1.4346 - classification_loss: 0.4879\n",
      "382/500 [=====================>........] - ETA: 2:01 - loss: 1.9216 - regression_loss: 1.4340 - classification_loss: 0.4876\n",
      "383/500 [=====================>........] - ETA: 2:00 - loss: 1.9194 - regression_loss: 1.4323 - classification_loss: 0.4871\n",
      "384/500 [======================>.......] - ETA: 1:58 - loss: 1.9197 - regression_loss: 1.4325 - classification_loss: 0.4871\n",
      "385/500 [======================>.......] - ETA: 1:57 - loss: 1.9189 - regression_loss: 1.4321 - classification_loss: 0.4868\n",
      "386/500 [======================>.......] - ETA: 1:56 - loss: 1.9192 - regression_loss: 1.4321 - classification_loss: 0.4871\n",
      "387/500 [======================>.......] - ETA: 1:55 - loss: 1.9183 - regression_loss: 1.4315 - classification_loss: 0.4868\n",
      "388/500 [======================>.......] - ETA: 1:54 - loss: 1.9161 - regression_loss: 1.4300 - classification_loss: 0.4862\n",
      "389/500 [======================>.......] - ETA: 1:53 - loss: 1.9151 - regression_loss: 1.4294 - classification_loss: 0.4857\n",
      "390/500 [======================>.......] - ETA: 1:52 - loss: 1.9162 - regression_loss: 1.4303 - classification_loss: 0.4859\n",
      "391/500 [======================>.......] - ETA: 1:51 - loss: 1.9160 - regression_loss: 1.4299 - classification_loss: 0.4861\n",
      "392/500 [======================>.......] - ETA: 1:50 - loss: 1.9187 - regression_loss: 1.4318 - classification_loss: 0.4869\n",
      "393/500 [======================>.......] - ETA: 1:49 - loss: 1.9188 - regression_loss: 1.4318 - classification_loss: 0.4870\n",
      "394/500 [======================>.......] - ETA: 1:48 - loss: 1.9177 - regression_loss: 1.4311 - classification_loss: 0.4866\n",
      "395/500 [======================>.......] - ETA: 1:47 - loss: 1.9163 - regression_loss: 1.4299 - classification_loss: 0.4864\n",
      "396/500 [======================>.......] - ETA: 1:46 - loss: 1.9148 - regression_loss: 1.4289 - classification_loss: 0.4860\n",
      "397/500 [======================>.......] - ETA: 1:45 - loss: 1.9158 - regression_loss: 1.4293 - classification_loss: 0.4865\n",
      "398/500 [======================>.......] - ETA: 1:44 - loss: 1.9160 - regression_loss: 1.4296 - classification_loss: 0.4864\n",
      "399/500 [======================>.......] - ETA: 1:43 - loss: 1.9162 - regression_loss: 1.4299 - classification_loss: 0.4863\n",
      "400/500 [=======================>......] - ETA: 1:42 - loss: 1.9152 - regression_loss: 1.4291 - classification_loss: 0.4860\n",
      "401/500 [=======================>......] - ETA: 1:41 - loss: 1.9152 - regression_loss: 1.4291 - classification_loss: 0.4861\n",
      "402/500 [=======================>......] - ETA: 1:40 - loss: 1.9135 - regression_loss: 1.4278 - classification_loss: 0.4858\n",
      "403/500 [=======================>......] - ETA: 1:39 - loss: 1.9126 - regression_loss: 1.4269 - classification_loss: 0.4857\n",
      "404/500 [=======================>......] - ETA: 1:38 - loss: 1.9132 - regression_loss: 1.4272 - classification_loss: 0.4860\n",
      "405/500 [=======================>......] - ETA: 1:37 - loss: 1.9118 - regression_loss: 1.4259 - classification_loss: 0.4859\n",
      "406/500 [=======================>......] - ETA: 1:36 - loss: 1.9137 - regression_loss: 1.4271 - classification_loss: 0.4866\n",
      "407/500 [=======================>......] - ETA: 1:35 - loss: 1.9138 - regression_loss: 1.4276 - classification_loss: 0.4862\n",
      "408/500 [=======================>......] - ETA: 1:34 - loss: 1.9126 - regression_loss: 1.4268 - classification_loss: 0.4859\n",
      "409/500 [=======================>......] - ETA: 1:33 - loss: 1.9114 - regression_loss: 1.4258 - classification_loss: 0.4856\n",
      "410/500 [=======================>......] - ETA: 1:32 - loss: 1.9118 - regression_loss: 1.4264 - classification_loss: 0.4854\n",
      "411/500 [=======================>......] - ETA: 1:31 - loss: 1.9126 - regression_loss: 1.4271 - classification_loss: 0.4855\n",
      "412/500 [=======================>......] - ETA: 1:30 - loss: 1.9140 - regression_loss: 1.4274 - classification_loss: 0.4865\n",
      "413/500 [=======================>......] - ETA: 1:29 - loss: 1.9146 - regression_loss: 1.4275 - classification_loss: 0.4871\n",
      "414/500 [=======================>......] - ETA: 1:28 - loss: 1.9157 - regression_loss: 1.4280 - classification_loss: 0.4877\n",
      "415/500 [=======================>......] - ETA: 1:27 - loss: 1.9153 - regression_loss: 1.4279 - classification_loss: 0.4874\n",
      "416/500 [=======================>......] - ETA: 1:26 - loss: 1.9154 - regression_loss: 1.4279 - classification_loss: 0.4876\n",
      "417/500 [========================>.....] - ETA: 1:25 - loss: 1.9140 - regression_loss: 1.4266 - classification_loss: 0.4874\n",
      "418/500 [========================>.....] - ETA: 1:24 - loss: 1.9150 - regression_loss: 1.4274 - classification_loss: 0.4876\n",
      "419/500 [========================>.....] - ETA: 1:23 - loss: 1.9159 - regression_loss: 1.4279 - classification_loss: 0.4881\n",
      "420/500 [========================>.....] - ETA: 1:22 - loss: 1.9162 - regression_loss: 1.4283 - classification_loss: 0.4879\n",
      "421/500 [========================>.....] - ETA: 1:20 - loss: 1.9173 - regression_loss: 1.4291 - classification_loss: 0.4882\n",
      "422/500 [========================>.....] - ETA: 1:19 - loss: 1.9175 - regression_loss: 1.4294 - classification_loss: 0.4881\n",
      "423/500 [========================>.....] - ETA: 1:18 - loss: 1.9181 - regression_loss: 1.4301 - classification_loss: 0.4881\n",
      "424/500 [========================>.....] - ETA: 1:17 - loss: 1.9190 - regression_loss: 1.4305 - classification_loss: 0.4886\n",
      "425/500 [========================>.....] - ETA: 1:16 - loss: 1.9185 - regression_loss: 1.4302 - classification_loss: 0.4882\n",
      "426/500 [========================>.....] - ETA: 1:15 - loss: 1.9200 - regression_loss: 1.4320 - classification_loss: 0.4880\n",
      "427/500 [========================>.....] - ETA: 1:14 - loss: 1.9191 - regression_loss: 1.4316 - classification_loss: 0.4875\n",
      "428/500 [========================>.....] - ETA: 1:13 - loss: 1.9182 - regression_loss: 1.4308 - classification_loss: 0.4874\n",
      "429/500 [========================>.....] - ETA: 1:12 - loss: 1.9170 - regression_loss: 1.4299 - classification_loss: 0.4871\n",
      "430/500 [========================>.....] - ETA: 1:11 - loss: 1.9168 - regression_loss: 1.4300 - classification_loss: 0.4868\n",
      "431/500 [========================>.....] - ETA: 1:10 - loss: 1.9190 - regression_loss: 1.4316 - classification_loss: 0.4874\n",
      "432/500 [========================>.....] - ETA: 1:09 - loss: 1.9190 - regression_loss: 1.4318 - classification_loss: 0.4872\n",
      "433/500 [========================>.....] - ETA: 1:08 - loss: 1.9178 - regression_loss: 1.4308 - classification_loss: 0.4870\n",
      "434/500 [=========================>....] - ETA: 1:07 - loss: 1.9212 - regression_loss: 1.4330 - classification_loss: 0.4883\n",
      "435/500 [=========================>....] - ETA: 1:06 - loss: 1.9208 - regression_loss: 1.4325 - classification_loss: 0.4882\n",
      "436/500 [=========================>....] - ETA: 1:05 - loss: 1.9183 - regression_loss: 1.4308 - classification_loss: 0.4875\n",
      "437/500 [=========================>....] - ETA: 1:04 - loss: 1.9180 - regression_loss: 1.4304 - classification_loss: 0.4876\n",
      "438/500 [=========================>....] - ETA: 1:03 - loss: 1.9178 - regression_loss: 1.4301 - classification_loss: 0.4877\n",
      "439/500 [=========================>....] - ETA: 1:02 - loss: 1.9190 - regression_loss: 1.4312 - classification_loss: 0.4878\n",
      "440/500 [=========================>....] - ETA: 1:01 - loss: 1.9198 - regression_loss: 1.4317 - classification_loss: 0.4881\n",
      "441/500 [=========================>....] - ETA: 1:00 - loss: 1.9211 - regression_loss: 1.4322 - classification_loss: 0.4889\n",
      "442/500 [=========================>....] - ETA: 59s - loss: 1.9208 - regression_loss: 1.4316 - classification_loss: 0.4892 \n",
      "443/500 [=========================>....] - ETA: 58s - loss: 1.9202 - regression_loss: 1.4309 - classification_loss: 0.4892\n",
      "444/500 [=========================>....] - ETA: 57s - loss: 1.9213 - regression_loss: 1.4318 - classification_loss: 0.4895\n",
      "445/500 [=========================>....] - ETA: 56s - loss: 1.9218 - regression_loss: 1.4321 - classification_loss: 0.4897\n",
      "446/500 [=========================>....] - ETA: 55s - loss: 1.9203 - regression_loss: 1.4312 - classification_loss: 0.4892\n",
      "447/500 [=========================>....] - ETA: 54s - loss: 1.9209 - regression_loss: 1.4317 - classification_loss: 0.4892\n",
      "448/500 [=========================>....] - ETA: 53s - loss: 1.9213 - regression_loss: 1.4314 - classification_loss: 0.4899\n",
      "449/500 [=========================>....] - ETA: 52s - loss: 1.9215 - regression_loss: 1.4314 - classification_loss: 0.4901\n",
      "450/500 [==========================>...] - ETA: 51s - loss: 1.9219 - regression_loss: 1.4318 - classification_loss: 0.4901\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 1.9227 - regression_loss: 1.4322 - classification_loss: 0.4905\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 1.9220 - regression_loss: 1.4316 - classification_loss: 0.4903\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.9213 - regression_loss: 1.4312 - classification_loss: 0.4901\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.9224 - regression_loss: 1.4322 - classification_loss: 0.4902\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.9231 - regression_loss: 1.4326 - classification_loss: 0.4906\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.9229 - regression_loss: 1.4326 - classification_loss: 0.4902\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.9238 - regression_loss: 1.4329 - classification_loss: 0.4909\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.9252 - regression_loss: 1.4338 - classification_loss: 0.4915\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.9272 - regression_loss: 1.4346 - classification_loss: 0.4926\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.9281 - regression_loss: 1.4349 - classification_loss: 0.4932\n",
      "461/500 [==========================>...] - ETA: 39s - loss: 1.9287 - regression_loss: 1.4353 - classification_loss: 0.4934\n",
      "462/500 [==========================>...] - ETA: 38s - loss: 1.9303 - regression_loss: 1.4369 - classification_loss: 0.4935\n",
      "463/500 [==========================>...] - ETA: 37s - loss: 1.9316 - regression_loss: 1.4377 - classification_loss: 0.4940\n",
      "464/500 [==========================>...] - ETA: 36s - loss: 1.9301 - regression_loss: 1.4366 - classification_loss: 0.4934\n",
      "465/500 [==========================>...] - ETA: 35s - loss: 1.9292 - regression_loss: 1.4358 - classification_loss: 0.4934\n",
      "466/500 [==========================>...] - ETA: 34s - loss: 1.9287 - regression_loss: 1.4354 - classification_loss: 0.4933\n",
      "467/500 [===========================>..] - ETA: 33s - loss: 1.9278 - regression_loss: 1.4348 - classification_loss: 0.4930\n",
      "468/500 [===========================>..] - ETA: 32s - loss: 1.9290 - regression_loss: 1.4357 - classification_loss: 0.4933\n",
      "469/500 [===========================>..] - ETA: 31s - loss: 1.9280 - regression_loss: 1.4351 - classification_loss: 0.4930\n",
      "470/500 [===========================>..] - ETA: 30s - loss: 1.9285 - regression_loss: 1.4357 - classification_loss: 0.4928\n",
      "471/500 [===========================>..] - ETA: 29s - loss: 1.9295 - regression_loss: 1.4363 - classification_loss: 0.4932\n",
      "472/500 [===========================>..] - ETA: 28s - loss: 1.9296 - regression_loss: 1.4365 - classification_loss: 0.4931\n",
      "473/500 [===========================>..] - ETA: 27s - loss: 1.9316 - regression_loss: 1.4377 - classification_loss: 0.4939\n",
      "474/500 [===========================>..] - ETA: 26s - loss: 1.9304 - regression_loss: 1.4372 - classification_loss: 0.4933\n",
      "475/500 [===========================>..] - ETA: 25s - loss: 1.9305 - regression_loss: 1.4374 - classification_loss: 0.4930\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 1.9317 - regression_loss: 1.4382 - classification_loss: 0.4935\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.9327 - regression_loss: 1.4388 - classification_loss: 0.4939\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.9322 - regression_loss: 1.4379 - classification_loss: 0.4942\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.9346 - regression_loss: 1.4393 - classification_loss: 0.4953\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.9339 - regression_loss: 1.4387 - classification_loss: 0.4953\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.9332 - regression_loss: 1.4380 - classification_loss: 0.4952\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.9331 - regression_loss: 1.4381 - classification_loss: 0.4950\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.9331 - regression_loss: 1.4379 - classification_loss: 0.4952\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.9321 - regression_loss: 1.4370 - classification_loss: 0.4951\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.9307 - regression_loss: 1.4360 - classification_loss: 0.4947\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.9286 - regression_loss: 1.4346 - classification_loss: 0.4941\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.9277 - regression_loss: 1.4340 - classification_loss: 0.4937\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.9278 - regression_loss: 1.4343 - classification_loss: 0.4935\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.9282 - regression_loss: 1.4347 - classification_loss: 0.4935\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.9278 - regression_loss: 1.4343 - classification_loss: 0.4935\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.9260 - regression_loss: 1.4329 - classification_loss: 0.4931 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.9262 - regression_loss: 1.4332 - classification_loss: 0.4930\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.9264 - regression_loss: 1.4336 - classification_loss: 0.4928\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.9275 - regression_loss: 1.4344 - classification_loss: 0.4930\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.9260 - regression_loss: 1.4334 - classification_loss: 0.4926\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.9261 - regression_loss: 1.4335 - classification_loss: 0.4926\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.9270 - regression_loss: 1.4339 - classification_loss: 0.4931\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.9261 - regression_loss: 1.4335 - classification_loss: 0.4926\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.9269 - regression_loss: 1.4343 - classification_loss: 0.4927\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9253 - regression_loss: 1.4331 - classification_loss: 0.4922\n",
      "Epoch 00030: saving model to ./snapshots\\resnet50_csv_30.h5\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.99999943962493e-12.\n",
      "\n",
      "500/500 [==============================] - 514s 1s/step - loss: 1.9253 - regression_loss: 1.4331 - classification_loss: 0.4922\n",
      "Epoch 31/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 2.2644 - regression_loss: 1.7868 - classification_loss: 0.4776\n",
      "  2/500 [..............................] - ETA: 3:40 - loss: 2.0489 - regression_loss: 1.5379 - classification_loss: 0.5109\n",
      "  3/500 [..............................] - ETA: 5:15 - loss: 1.9308 - regression_loss: 1.4886 - classification_loss: 0.4422\n",
      "  4/500 [..............................] - ETA: 5:58 - loss: 1.9440 - regression_loss: 1.4488 - classification_loss: 0.4951\n",
      "  5/500 [..............................] - ETA: 6:23 - loss: 2.0922 - regression_loss: 1.5492 - classification_loss: 0.5430\n",
      "  6/500 [..............................] - ETA: 6:48 - loss: 2.0685 - regression_loss: 1.5432 - classification_loss: 0.5253\n",
      "  7/500 [..............................] - ETA: 7:06 - loss: 2.1647 - regression_loss: 1.6242 - classification_loss: 0.5405\n",
      "  8/500 [..............................] - ETA: 7:12 - loss: 2.0454 - regression_loss: 1.5213 - classification_loss: 0.5242\n",
      "  9/500 [..............................] - ETA: 7:17 - loss: 2.0413 - regression_loss: 1.5233 - classification_loss: 0.5179\n",
      " 10/500 [..............................] - ETA: 7:25 - loss: 2.0506 - regression_loss: 1.5358 - classification_loss: 0.5148\n",
      " 11/500 [..............................] - ETA: 7:33 - loss: 1.9672 - regression_loss: 1.4783 - classification_loss: 0.4889\n",
      " 12/500 [..............................] - ETA: 7:35 - loss: 1.9548 - regression_loss: 1.4717 - classification_loss: 0.4831\n",
      " 13/500 [..............................] - ETA: 7:39 - loss: 1.9439 - regression_loss: 1.4587 - classification_loss: 0.4852\n",
      " 14/500 [..............................] - ETA: 7:40 - loss: 1.8958 - regression_loss: 1.4168 - classification_loss: 0.4791\n",
      " 15/500 [..............................] - ETA: 7:35 - loss: 1.8522 - regression_loss: 1.3785 - classification_loss: 0.4737\n",
      " 16/500 [..............................] - ETA: 7:40 - loss: 1.8684 - regression_loss: 1.3940 - classification_loss: 0.4744\n",
      " 17/500 [>.............................] - ETA: 7:40 - loss: 1.8537 - regression_loss: 1.3888 - classification_loss: 0.4649\n",
      " 18/500 [>.............................] - ETA: 7:40 - loss: 1.8377 - regression_loss: 1.3767 - classification_loss: 0.4610\n",
      " 19/500 [>.............................] - ETA: 7:40 - loss: 1.8116 - regression_loss: 1.3531 - classification_loss: 0.4585\n",
      " 20/500 [>.............................] - ETA: 7:36 - loss: 1.8345 - regression_loss: 1.3691 - classification_loss: 0.4653\n",
      " 21/500 [>.............................] - ETA: 7:40 - loss: 1.9106 - regression_loss: 1.4168 - classification_loss: 0.4939\n",
      " 22/500 [>.............................] - ETA: 7:40 - loss: 1.9277 - regression_loss: 1.4205 - classification_loss: 0.5072\n",
      " 23/500 [>.............................] - ETA: 7:39 - loss: 1.8970 - regression_loss: 1.4021 - classification_loss: 0.4948\n",
      " 24/500 [>.............................] - ETA: 7:39 - loss: 1.8836 - regression_loss: 1.3945 - classification_loss: 0.4891\n",
      " 25/500 [>.............................] - ETA: 7:42 - loss: 1.8771 - regression_loss: 1.3912 - classification_loss: 0.4858\n",
      " 26/500 [>.............................] - ETA: 7:43 - loss: 1.8726 - regression_loss: 1.3900 - classification_loss: 0.4826\n",
      " 27/500 [>.............................] - ETA: 7:42 - loss: 1.8732 - regression_loss: 1.3875 - classification_loss: 0.4857\n",
      " 28/500 [>.............................] - ETA: 7:41 - loss: 1.8975 - regression_loss: 1.4130 - classification_loss: 0.4845\n",
      " 29/500 [>.............................] - ETA: 7:41 - loss: 1.9189 - regression_loss: 1.4298 - classification_loss: 0.4891\n",
      " 30/500 [>.............................] - ETA: 7:42 - loss: 1.9405 - regression_loss: 1.4519 - classification_loss: 0.4886\n",
      " 31/500 [>.............................] - ETA: 7:43 - loss: 1.9569 - regression_loss: 1.4620 - classification_loss: 0.4949\n",
      " 32/500 [>.............................] - ETA: 7:42 - loss: 1.9321 - regression_loss: 1.4450 - classification_loss: 0.4871\n",
      " 33/500 [>.............................] - ETA: 7:38 - loss: 1.9081 - regression_loss: 1.4268 - classification_loss: 0.4814\n",
      " 34/500 [=>............................] - ETA: 7:39 - loss: 1.9284 - regression_loss: 1.4351 - classification_loss: 0.4934\n",
      " 35/500 [=>............................] - ETA: 7:38 - loss: 1.9571 - regression_loss: 1.4521 - classification_loss: 0.5049\n",
      " 36/500 [=>............................] - ETA: 7:38 - loss: 1.9432 - regression_loss: 1.4443 - classification_loss: 0.4990\n",
      " 37/500 [=>............................] - ETA: 7:38 - loss: 1.9661 - regression_loss: 1.4574 - classification_loss: 0.5087\n",
      " 38/500 [=>............................] - ETA: 7:39 - loss: 1.9920 - regression_loss: 1.4748 - classification_loss: 0.5171\n",
      " 39/500 [=>............................] - ETA: 7:39 - loss: 2.0054 - regression_loss: 1.4880 - classification_loss: 0.5174\n",
      " 40/500 [=>............................] - ETA: 7:39 - loss: 1.9838 - regression_loss: 1.4721 - classification_loss: 0.5117\n",
      " 41/500 [=>............................] - ETA: 7:37 - loss: 1.9615 - regression_loss: 1.4538 - classification_loss: 0.5078\n",
      " 42/500 [=>............................] - ETA: 7:37 - loss: 1.9469 - regression_loss: 1.4438 - classification_loss: 0.5031\n",
      " 43/500 [=>............................] - ETA: 7:35 - loss: 1.9657 - regression_loss: 1.4594 - classification_loss: 0.5062\n",
      " 44/500 [=>............................] - ETA: 7:34 - loss: 1.9708 - regression_loss: 1.4646 - classification_loss: 0.5062\n",
      " 45/500 [=>............................] - ETA: 7:35 - loss: 1.9605 - regression_loss: 1.4585 - classification_loss: 0.5020\n",
      " 46/500 [=>............................] - ETA: 7:34 - loss: 1.9728 - regression_loss: 1.4675 - classification_loss: 0.5054\n",
      " 47/500 [=>............................] - ETA: 7:33 - loss: 1.9577 - regression_loss: 1.4555 - classification_loss: 0.5022\n",
      " 48/500 [=>............................] - ETA: 7:33 - loss: 1.9640 - regression_loss: 1.4622 - classification_loss: 0.5018\n",
      " 49/500 [=>............................] - ETA: 7:32 - loss: 1.9601 - regression_loss: 1.4597 - classification_loss: 0.5004\n",
      " 50/500 [==>...........................] - ETA: 7:30 - loss: 1.9601 - regression_loss: 1.4570 - classification_loss: 0.5031\n",
      " 51/500 [==>...........................] - ETA: 7:31 - loss: 1.9530 - regression_loss: 1.4544 - classification_loss: 0.4987\n",
      " 52/500 [==>...........................] - ETA: 7:29 - loss: 1.9539 - regression_loss: 1.4519 - classification_loss: 0.5020\n",
      " 53/500 [==>...........................] - ETA: 7:28 - loss: 1.9367 - regression_loss: 1.4401 - classification_loss: 0.4966\n",
      " 54/500 [==>...........................] - ETA: 7:27 - loss: 1.9520 - regression_loss: 1.4509 - classification_loss: 0.5011\n",
      " 55/500 [==>...........................] - ETA: 7:27 - loss: 1.9363 - regression_loss: 1.4397 - classification_loss: 0.4966\n",
      " 56/500 [==>...........................] - ETA: 7:26 - loss: 1.9358 - regression_loss: 1.4403 - classification_loss: 0.4954\n",
      " 57/500 [==>...........................] - ETA: 7:25 - loss: 1.9310 - regression_loss: 1.4341 - classification_loss: 0.4970\n",
      " 58/500 [==>...........................] - ETA: 7:25 - loss: 1.9315 - regression_loss: 1.4371 - classification_loss: 0.4944\n",
      " 59/500 [==>...........................] - ETA: 7:24 - loss: 1.9259 - regression_loss: 1.4337 - classification_loss: 0.4922\n",
      " 60/500 [==>...........................] - ETA: 7:23 - loss: 1.9353 - regression_loss: 1.4408 - classification_loss: 0.4945\n",
      " 61/500 [==>...........................] - ETA: 7:23 - loss: 1.9501 - regression_loss: 1.4514 - classification_loss: 0.4987\n",
      " 62/500 [==>...........................] - ETA: 7:22 - loss: 1.9441 - regression_loss: 1.4478 - classification_loss: 0.4963\n",
      " 63/500 [==>...........................] - ETA: 7:23 - loss: 1.9500 - regression_loss: 1.4517 - classification_loss: 0.4983\n",
      " 64/500 [==>...........................] - ETA: 7:22 - loss: 1.9524 - regression_loss: 1.4550 - classification_loss: 0.4974\n",
      " 65/500 [==>...........................] - ETA: 7:22 - loss: 1.9738 - regression_loss: 1.4699 - classification_loss: 0.5039\n",
      " 66/500 [==>...........................] - ETA: 7:21 - loss: 1.9686 - regression_loss: 1.4656 - classification_loss: 0.5030\n",
      " 67/500 [===>..........................] - ETA: 7:20 - loss: 1.9604 - regression_loss: 1.4595 - classification_loss: 0.5009\n",
      " 68/500 [===>..........................] - ETA: 7:19 - loss: 1.9631 - regression_loss: 1.4584 - classification_loss: 0.5047\n",
      " 69/500 [===>..........................] - ETA: 7:18 - loss: 1.9545 - regression_loss: 1.4521 - classification_loss: 0.5023\n",
      " 70/500 [===>..........................] - ETA: 7:18 - loss: 1.9443 - regression_loss: 1.4435 - classification_loss: 0.5008\n",
      " 71/500 [===>..........................] - ETA: 7:17 - loss: 1.9487 - regression_loss: 1.4467 - classification_loss: 0.5021\n",
      " 72/500 [===>..........................] - ETA: 7:16 - loss: 1.9490 - regression_loss: 1.4464 - classification_loss: 0.5026\n",
      " 73/500 [===>..........................] - ETA: 7:15 - loss: 1.9436 - regression_loss: 1.4433 - classification_loss: 0.5003\n",
      " 74/500 [===>..........................] - ETA: 7:15 - loss: 1.9487 - regression_loss: 1.4489 - classification_loss: 0.4998\n",
      " 75/500 [===>..........................] - ETA: 7:13 - loss: 1.9551 - regression_loss: 1.4547 - classification_loss: 0.5005\n",
      " 76/500 [===>..........................] - ETA: 7:13 - loss: 1.9536 - regression_loss: 1.4530 - classification_loss: 0.5006\n",
      " 77/500 [===>..........................] - ETA: 7:11 - loss: 1.9560 - regression_loss: 1.4560 - classification_loss: 0.5000\n",
      " 78/500 [===>..........................] - ETA: 7:10 - loss: 1.9539 - regression_loss: 1.4552 - classification_loss: 0.4987\n",
      " 79/500 [===>..........................] - ETA: 7:09 - loss: 1.9579 - regression_loss: 1.4560 - classification_loss: 0.5019\n",
      " 80/500 [===>..........................] - ETA: 7:08 - loss: 1.9524 - regression_loss: 1.4511 - classification_loss: 0.5014\n",
      " 81/500 [===>..........................] - ETA: 7:07 - loss: 1.9508 - regression_loss: 1.4488 - classification_loss: 0.5020\n",
      " 82/500 [===>..........................] - ETA: 7:05 - loss: 1.9495 - regression_loss: 1.4482 - classification_loss: 0.5013\n",
      " 83/500 [===>..........................] - ETA: 7:04 - loss: 1.9486 - regression_loss: 1.4477 - classification_loss: 0.5009\n",
      " 84/500 [====>.........................] - ETA: 7:03 - loss: 1.9507 - regression_loss: 1.4485 - classification_loss: 0.5022\n",
      " 85/500 [====>.........................] - ETA: 7:03 - loss: 1.9448 - regression_loss: 1.4442 - classification_loss: 0.5005\n",
      " 86/500 [====>.........................] - ETA: 7:01 - loss: 1.9395 - regression_loss: 1.4409 - classification_loss: 0.4986\n",
      " 87/500 [====>.........................] - ETA: 7:00 - loss: 1.9344 - regression_loss: 1.4378 - classification_loss: 0.4966\n",
      " 88/500 [====>.........................] - ETA: 7:00 - loss: 1.9377 - regression_loss: 1.4408 - classification_loss: 0.4970\n",
      " 89/500 [====>.........................] - ETA: 6:59 - loss: 1.9356 - regression_loss: 1.4382 - classification_loss: 0.4975\n",
      " 90/500 [====>.........................] - ETA: 6:58 - loss: 1.9400 - regression_loss: 1.4426 - classification_loss: 0.4974\n",
      " 91/500 [====>.........................] - ETA: 6:57 - loss: 1.9374 - regression_loss: 1.4413 - classification_loss: 0.4961\n",
      " 92/500 [====>.........................] - ETA: 6:56 - loss: 1.9334 - regression_loss: 1.4377 - classification_loss: 0.4957\n",
      " 93/500 [====>.........................] - ETA: 6:56 - loss: 1.9349 - regression_loss: 1.4392 - classification_loss: 0.4957\n",
      " 94/500 [====>.........................] - ETA: 6:55 - loss: 1.9368 - regression_loss: 1.4410 - classification_loss: 0.4958\n",
      " 95/500 [====>.........................] - ETA: 6:54 - loss: 1.9339 - regression_loss: 1.4387 - classification_loss: 0.4952\n",
      " 96/500 [====>.........................] - ETA: 6:53 - loss: 1.9432 - regression_loss: 1.4471 - classification_loss: 0.4961\n",
      " 97/500 [====>.........................] - ETA: 6:52 - loss: 1.9364 - regression_loss: 1.4427 - classification_loss: 0.4937\n",
      " 98/500 [====>.........................] - ETA: 6:51 - loss: 1.9399 - regression_loss: 1.4455 - classification_loss: 0.4944\n",
      " 99/500 [====>.........................] - ETA: 6:50 - loss: 1.9319 - regression_loss: 1.4399 - classification_loss: 0.4921\n",
      "100/500 [=====>........................] - ETA: 6:49 - loss: 1.9389 - regression_loss: 1.4438 - classification_loss: 0.4951\n",
      "101/500 [=====>........................] - ETA: 6:48 - loss: 1.9383 - regression_loss: 1.4442 - classification_loss: 0.4941\n",
      "102/500 [=====>........................] - ETA: 6:47 - loss: 1.9347 - regression_loss: 1.4426 - classification_loss: 0.4921\n",
      "103/500 [=====>........................] - ETA: 6:46 - loss: 1.9320 - regression_loss: 1.4402 - classification_loss: 0.4918\n",
      "104/500 [=====>........................] - ETA: 6:44 - loss: 1.9287 - regression_loss: 1.4384 - classification_loss: 0.4903\n",
      "105/500 [=====>........................] - ETA: 6:44 - loss: 1.9300 - regression_loss: 1.4391 - classification_loss: 0.4909\n",
      "106/500 [=====>........................] - ETA: 6:42 - loss: 1.9277 - regression_loss: 1.4376 - classification_loss: 0.4901\n",
      "107/500 [=====>........................] - ETA: 6:42 - loss: 1.9224 - regression_loss: 1.4337 - classification_loss: 0.4887\n",
      "108/500 [=====>........................] - ETA: 6:40 - loss: 1.9181 - regression_loss: 1.4308 - classification_loss: 0.4873\n",
      "109/500 [=====>........................] - ETA: 6:40 - loss: 1.9188 - regression_loss: 1.4313 - classification_loss: 0.4875\n",
      "110/500 [=====>........................] - ETA: 6:38 - loss: 1.9157 - regression_loss: 1.4284 - classification_loss: 0.4873\n",
      "111/500 [=====>........................] - ETA: 6:38 - loss: 1.9172 - regression_loss: 1.4298 - classification_loss: 0.4875\n",
      "112/500 [=====>........................] - ETA: 6:36 - loss: 1.9158 - regression_loss: 1.4296 - classification_loss: 0.4862\n",
      "113/500 [=====>........................] - ETA: 6:35 - loss: 1.9124 - regression_loss: 1.4272 - classification_loss: 0.4851\n",
      "114/500 [=====>........................] - ETA: 6:35 - loss: 1.9093 - regression_loss: 1.4263 - classification_loss: 0.4830\n",
      "115/500 [=====>........................] - ETA: 6:33 - loss: 1.9079 - regression_loss: 1.4248 - classification_loss: 0.4832\n",
      "116/500 [=====>........................] - ETA: 6:32 - loss: 1.9056 - regression_loss: 1.4235 - classification_loss: 0.4821\n",
      "117/500 [======>.......................] - ETA: 6:32 - loss: 1.9022 - regression_loss: 1.4199 - classification_loss: 0.4823\n",
      "118/500 [======>.......................] - ETA: 6:30 - loss: 1.9015 - regression_loss: 1.4185 - classification_loss: 0.4830\n",
      "119/500 [======>.......................] - ETA: 6:30 - loss: 1.9001 - regression_loss: 1.4188 - classification_loss: 0.4813\n",
      "120/500 [======>.......................] - ETA: 6:28 - loss: 1.8985 - regression_loss: 1.4164 - classification_loss: 0.4821\n",
      "121/500 [======>.......................] - ETA: 6:27 - loss: 1.8973 - regression_loss: 1.4161 - classification_loss: 0.4813\n",
      "122/500 [======>.......................] - ETA: 6:26 - loss: 1.8994 - regression_loss: 1.4146 - classification_loss: 0.4848\n",
      "123/500 [======>.......................] - ETA: 6:26 - loss: 1.8954 - regression_loss: 1.4118 - classification_loss: 0.4836\n",
      "124/500 [======>.......................] - ETA: 6:25 - loss: 1.9026 - regression_loss: 1.4171 - classification_loss: 0.4855\n",
      "125/500 [======>.......................] - ETA: 6:24 - loss: 1.9026 - regression_loss: 1.4173 - classification_loss: 0.4853\n",
      "126/500 [======>.......................] - ETA: 6:22 - loss: 1.8960 - regression_loss: 1.4130 - classification_loss: 0.4830\n",
      "127/500 [======>.......................] - ETA: 6:21 - loss: 1.8964 - regression_loss: 1.4137 - classification_loss: 0.4827\n",
      "128/500 [======>.......................] - ETA: 6:20 - loss: 1.8971 - regression_loss: 1.4148 - classification_loss: 0.4823\n",
      "129/500 [======>.......................] - ETA: 6:19 - loss: 1.8942 - regression_loss: 1.4122 - classification_loss: 0.4820\n",
      "130/500 [======>.......................] - ETA: 6:18 - loss: 1.8951 - regression_loss: 1.4134 - classification_loss: 0.4817\n",
      "131/500 [======>.......................] - ETA: 6:17 - loss: 1.8969 - regression_loss: 1.4161 - classification_loss: 0.4808\n",
      "132/500 [======>.......................] - ETA: 6:16 - loss: 1.8996 - regression_loss: 1.4177 - classification_loss: 0.4819\n",
      "133/500 [======>.......................] - ETA: 6:15 - loss: 1.9035 - regression_loss: 1.4202 - classification_loss: 0.4833\n",
      "134/500 [=======>......................] - ETA: 6:14 - loss: 1.9080 - regression_loss: 1.4223 - classification_loss: 0.4858\n",
      "135/500 [=======>......................] - ETA: 6:13 - loss: 1.9075 - regression_loss: 1.4207 - classification_loss: 0.4868\n",
      "136/500 [=======>......................] - ETA: 6:11 - loss: 1.9112 - regression_loss: 1.4234 - classification_loss: 0.4878\n",
      "137/500 [=======>......................] - ETA: 6:11 - loss: 1.9081 - regression_loss: 1.4217 - classification_loss: 0.4863\n",
      "138/500 [=======>......................] - ETA: 6:10 - loss: 1.9186 - regression_loss: 1.4288 - classification_loss: 0.4898\n",
      "139/500 [=======>......................] - ETA: 6:09 - loss: 1.9211 - regression_loss: 1.4313 - classification_loss: 0.4898\n",
      "140/500 [=======>......................] - ETA: 6:08 - loss: 1.9269 - regression_loss: 1.4358 - classification_loss: 0.4911\n",
      "141/500 [=======>......................] - ETA: 6:07 - loss: 1.9280 - regression_loss: 1.4360 - classification_loss: 0.4920\n",
      "142/500 [=======>......................] - ETA: 6:06 - loss: 1.9273 - regression_loss: 1.4360 - classification_loss: 0.4913\n",
      "143/500 [=======>......................] - ETA: 6:05 - loss: 1.9279 - regression_loss: 1.4362 - classification_loss: 0.4917\n",
      "144/500 [=======>......................] - ETA: 6:05 - loss: 1.9296 - regression_loss: 1.4373 - classification_loss: 0.4923\n",
      "145/500 [=======>......................] - ETA: 6:04 - loss: 1.9284 - regression_loss: 1.4368 - classification_loss: 0.4916\n",
      "146/500 [=======>......................] - ETA: 6:03 - loss: 1.9313 - regression_loss: 1.4393 - classification_loss: 0.4919\n",
      "147/500 [=======>......................] - ETA: 6:01 - loss: 1.9282 - regression_loss: 1.4374 - classification_loss: 0.4909\n",
      "148/500 [=======>......................] - ETA: 6:00 - loss: 1.9361 - regression_loss: 1.4431 - classification_loss: 0.4931\n",
      "149/500 [=======>......................] - ETA: 6:00 - loss: 1.9332 - regression_loss: 1.4414 - classification_loss: 0.4918\n",
      "150/500 [========>.....................] - ETA: 5:58 - loss: 1.9340 - regression_loss: 1.4418 - classification_loss: 0.4922\n",
      "151/500 [========>.....................] - ETA: 5:57 - loss: 1.9390 - regression_loss: 1.4458 - classification_loss: 0.4932\n",
      "152/500 [========>.....................] - ETA: 5:57 - loss: 1.9370 - regression_loss: 1.4447 - classification_loss: 0.4923\n",
      "153/500 [========>.....................] - ETA: 5:56 - loss: 1.9371 - regression_loss: 1.4452 - classification_loss: 0.4919\n",
      "154/500 [========>.....................] - ETA: 5:55 - loss: 1.9390 - regression_loss: 1.4462 - classification_loss: 0.4928\n",
      "155/500 [========>.....................] - ETA: 5:54 - loss: 1.9374 - regression_loss: 1.4455 - classification_loss: 0.4920\n",
      "156/500 [========>.....................] - ETA: 5:53 - loss: 1.9345 - regression_loss: 1.4435 - classification_loss: 0.4910\n",
      "157/500 [========>.....................] - ETA: 5:52 - loss: 1.9309 - regression_loss: 1.4415 - classification_loss: 0.4894\n",
      "158/500 [========>.....................] - ETA: 5:51 - loss: 1.9276 - regression_loss: 1.4390 - classification_loss: 0.4886\n",
      "159/500 [========>.....................] - ETA: 5:49 - loss: 1.9219 - regression_loss: 1.4346 - classification_loss: 0.4873\n",
      "160/500 [========>.....................] - ETA: 5:49 - loss: 1.9281 - regression_loss: 1.4390 - classification_loss: 0.4890\n",
      "161/500 [========>.....................] - ETA: 5:47 - loss: 1.9346 - regression_loss: 1.4448 - classification_loss: 0.4897\n",
      "162/500 [========>.....................] - ETA: 5:46 - loss: 1.9346 - regression_loss: 1.4444 - classification_loss: 0.4902\n",
      "163/500 [========>.....................] - ETA: 5:46 - loss: 1.9378 - regression_loss: 1.4472 - classification_loss: 0.4906\n",
      "164/500 [========>.....................] - ETA: 5:44 - loss: 1.9402 - regression_loss: 1.4492 - classification_loss: 0.4909\n",
      "165/500 [========>.....................] - ETA: 5:43 - loss: 1.9406 - regression_loss: 1.4494 - classification_loss: 0.4911\n",
      "166/500 [========>.....................] - ETA: 5:42 - loss: 1.9406 - regression_loss: 1.4495 - classification_loss: 0.4911\n",
      "167/500 [=========>....................] - ETA: 5:41 - loss: 1.9378 - regression_loss: 1.4483 - classification_loss: 0.4894\n",
      "168/500 [=========>....................] - ETA: 5:40 - loss: 1.9398 - regression_loss: 1.4499 - classification_loss: 0.4899\n",
      "169/500 [=========>....................] - ETA: 5:39 - loss: 1.9386 - regression_loss: 1.4494 - classification_loss: 0.4891\n",
      "170/500 [=========>....................] - ETA: 5:38 - loss: 1.9420 - regression_loss: 1.4516 - classification_loss: 0.4904\n",
      "171/500 [=========>....................] - ETA: 5:37 - loss: 1.9415 - regression_loss: 1.4523 - classification_loss: 0.4892\n",
      "172/500 [=========>....................] - ETA: 5:36 - loss: 1.9358 - regression_loss: 1.4478 - classification_loss: 0.4880\n",
      "173/500 [=========>....................] - ETA: 5:35 - loss: 1.9365 - regression_loss: 1.4492 - classification_loss: 0.4873\n",
      "174/500 [=========>....................] - ETA: 5:34 - loss: 1.9386 - regression_loss: 1.4510 - classification_loss: 0.4876\n",
      "175/500 [=========>....................] - ETA: 5:33 - loss: 1.9420 - regression_loss: 1.4526 - classification_loss: 0.4894\n",
      "176/500 [=========>....................] - ETA: 5:32 - loss: 1.9400 - regression_loss: 1.4520 - classification_loss: 0.4880\n",
      "177/500 [=========>....................] - ETA: 5:31 - loss: 1.9418 - regression_loss: 1.4537 - classification_loss: 0.4881\n",
      "178/500 [=========>....................] - ETA: 5:30 - loss: 1.9465 - regression_loss: 1.4565 - classification_loss: 0.4900\n",
      "179/500 [=========>....................] - ETA: 5:29 - loss: 1.9421 - regression_loss: 1.4533 - classification_loss: 0.4888\n",
      "180/500 [=========>....................] - ETA: 5:28 - loss: 1.9413 - regression_loss: 1.4523 - classification_loss: 0.4890\n",
      "181/500 [=========>....................] - ETA: 5:27 - loss: 1.9468 - regression_loss: 1.4563 - classification_loss: 0.4905\n",
      "182/500 [=========>....................] - ETA: 5:26 - loss: 1.9453 - regression_loss: 1.4551 - classification_loss: 0.4902\n",
      "183/500 [=========>....................] - ETA: 5:25 - loss: 1.9418 - regression_loss: 1.4521 - classification_loss: 0.4897\n",
      "184/500 [==========>...................] - ETA: 5:24 - loss: 1.9414 - regression_loss: 1.4521 - classification_loss: 0.4893\n",
      "185/500 [==========>...................] - ETA: 5:23 - loss: 1.9398 - regression_loss: 1.4513 - classification_loss: 0.4884\n",
      "186/500 [==========>...................] - ETA: 5:22 - loss: 1.9412 - regression_loss: 1.4518 - classification_loss: 0.4894\n",
      "187/500 [==========>...................] - ETA: 5:21 - loss: 1.9392 - regression_loss: 1.4497 - classification_loss: 0.4895\n",
      "188/500 [==========>...................] - ETA: 5:20 - loss: 1.9372 - regression_loss: 1.4482 - classification_loss: 0.4889\n",
      "189/500 [==========>...................] - ETA: 5:19 - loss: 1.9351 - regression_loss: 1.4467 - classification_loss: 0.4884\n",
      "190/500 [==========>...................] - ETA: 5:18 - loss: 1.9344 - regression_loss: 1.4466 - classification_loss: 0.4879\n",
      "191/500 [==========>...................] - ETA: 5:16 - loss: 1.9332 - regression_loss: 1.4462 - classification_loss: 0.4871\n",
      "192/500 [==========>...................] - ETA: 5:15 - loss: 1.9377 - regression_loss: 1.4496 - classification_loss: 0.4880\n",
      "193/500 [==========>...................] - ETA: 5:14 - loss: 1.9356 - regression_loss: 1.4482 - classification_loss: 0.4874\n",
      "194/500 [==========>...................] - ETA: 5:13 - loss: 1.9372 - regression_loss: 1.4492 - classification_loss: 0.4880\n",
      "195/500 [==========>...................] - ETA: 5:12 - loss: 1.9408 - regression_loss: 1.4536 - classification_loss: 0.4872\n",
      "196/500 [==========>...................] - ETA: 5:12 - loss: 1.9439 - regression_loss: 1.4564 - classification_loss: 0.4874\n",
      "197/500 [==========>...................] - ETA: 5:10 - loss: 1.9388 - regression_loss: 1.4526 - classification_loss: 0.4862\n",
      "198/500 [==========>...................] - ETA: 5:09 - loss: 1.9392 - regression_loss: 1.4529 - classification_loss: 0.4863\n",
      "199/500 [==========>...................] - ETA: 5:08 - loss: 1.9441 - regression_loss: 1.4557 - classification_loss: 0.4884\n",
      "200/500 [===========>..................] - ETA: 5:07 - loss: 1.9397 - regression_loss: 1.4522 - classification_loss: 0.4874\n",
      "201/500 [===========>..................] - ETA: 5:06 - loss: 1.9379 - regression_loss: 1.4509 - classification_loss: 0.4870\n",
      "202/500 [===========>..................] - ETA: 5:05 - loss: 1.9376 - regression_loss: 1.4504 - classification_loss: 0.4872\n",
      "203/500 [===========>..................] - ETA: 5:04 - loss: 1.9334 - regression_loss: 1.4471 - classification_loss: 0.4863\n",
      "204/500 [===========>..................] - ETA: 5:03 - loss: 1.9331 - regression_loss: 1.4470 - classification_loss: 0.4861\n",
      "205/500 [===========>..................] - ETA: 5:02 - loss: 1.9317 - regression_loss: 1.4465 - classification_loss: 0.4853\n",
      "206/500 [===========>..................] - ETA: 5:01 - loss: 1.9366 - regression_loss: 1.4486 - classification_loss: 0.4880\n",
      "207/500 [===========>..................] - ETA: 5:00 - loss: 1.9393 - regression_loss: 1.4511 - classification_loss: 0.4882\n",
      "208/500 [===========>..................] - ETA: 4:59 - loss: 1.9424 - regression_loss: 1.4530 - classification_loss: 0.4893\n",
      "209/500 [===========>..................] - ETA: 4:58 - loss: 1.9429 - regression_loss: 1.4539 - classification_loss: 0.4890\n",
      "210/500 [===========>..................] - ETA: 4:57 - loss: 1.9423 - regression_loss: 1.4534 - classification_loss: 0.4889\n",
      "211/500 [===========>..................] - ETA: 4:56 - loss: 1.9406 - regression_loss: 1.4524 - classification_loss: 0.4882\n",
      "212/500 [===========>..................] - ETA: 4:55 - loss: 1.9423 - regression_loss: 1.4541 - classification_loss: 0.4882\n",
      "213/500 [===========>..................] - ETA: 4:54 - loss: 1.9393 - regression_loss: 1.4522 - classification_loss: 0.4871\n",
      "214/500 [===========>..................] - ETA: 4:53 - loss: 1.9389 - regression_loss: 1.4516 - classification_loss: 0.4873\n",
      "215/500 [===========>..................] - ETA: 4:52 - loss: 1.9412 - regression_loss: 1.4528 - classification_loss: 0.4885\n",
      "216/500 [===========>..................] - ETA: 4:51 - loss: 1.9425 - regression_loss: 1.4536 - classification_loss: 0.4889\n",
      "217/500 [============>.................] - ETA: 4:50 - loss: 1.9467 - regression_loss: 1.4557 - classification_loss: 0.4910\n",
      "218/500 [============>.................] - ETA: 4:49 - loss: 1.9445 - regression_loss: 1.4543 - classification_loss: 0.4902\n",
      "219/500 [============>.................] - ETA: 4:48 - loss: 1.9428 - regression_loss: 1.4529 - classification_loss: 0.4899\n",
      "220/500 [============>.................] - ETA: 4:47 - loss: 1.9430 - regression_loss: 1.4528 - classification_loss: 0.4902\n",
      "221/500 [============>.................] - ETA: 4:46 - loss: 1.9403 - regression_loss: 1.4509 - classification_loss: 0.4894\n",
      "222/500 [============>.................] - ETA: 4:45 - loss: 1.9389 - regression_loss: 1.4496 - classification_loss: 0.4893\n",
      "223/500 [============>.................] - ETA: 4:44 - loss: 1.9391 - regression_loss: 1.4501 - classification_loss: 0.4890\n",
      "224/500 [============>.................] - ETA: 4:43 - loss: 1.9372 - regression_loss: 1.4487 - classification_loss: 0.4884\n",
      "225/500 [============>.................] - ETA: 4:42 - loss: 1.9389 - regression_loss: 1.4500 - classification_loss: 0.4889\n",
      "226/500 [============>.................] - ETA: 4:41 - loss: 1.9369 - regression_loss: 1.4480 - classification_loss: 0.4889\n",
      "227/500 [============>.................] - ETA: 4:40 - loss: 1.9389 - regression_loss: 1.4495 - classification_loss: 0.4894\n",
      "228/500 [============>.................] - ETA: 4:39 - loss: 1.9410 - regression_loss: 1.4513 - classification_loss: 0.4897\n",
      "229/500 [============>.................] - ETA: 4:38 - loss: 1.9381 - regression_loss: 1.4493 - classification_loss: 0.4888\n",
      "230/500 [============>.................] - ETA: 4:37 - loss: 1.9348 - regression_loss: 1.4467 - classification_loss: 0.4881\n",
      "231/500 [============>.................] - ETA: 4:36 - loss: 1.9366 - regression_loss: 1.4483 - classification_loss: 0.4883\n",
      "232/500 [============>.................] - ETA: 4:35 - loss: 1.9338 - regression_loss: 1.4460 - classification_loss: 0.4878\n",
      "233/500 [============>.................] - ETA: 4:34 - loss: 1.9309 - regression_loss: 1.4440 - classification_loss: 0.4870\n",
      "234/500 [=============>................] - ETA: 4:33 - loss: 1.9286 - regression_loss: 1.4425 - classification_loss: 0.4861\n",
      "235/500 [=============>................] - ETA: 4:32 - loss: 1.9289 - regression_loss: 1.4429 - classification_loss: 0.4860\n",
      "236/500 [=============>................] - ETA: 4:31 - loss: 1.9264 - regression_loss: 1.4411 - classification_loss: 0.4853\n",
      "237/500 [=============>................] - ETA: 4:30 - loss: 1.9244 - regression_loss: 1.4396 - classification_loss: 0.4848\n",
      "238/500 [=============>................] - ETA: 4:29 - loss: 1.9262 - regression_loss: 1.4411 - classification_loss: 0.4851\n",
      "239/500 [=============>................] - ETA: 4:28 - loss: 1.9299 - regression_loss: 1.4426 - classification_loss: 0.4872\n",
      "240/500 [=============>................] - ETA: 4:27 - loss: 1.9287 - regression_loss: 1.4420 - classification_loss: 0.4868\n",
      "241/500 [=============>................] - ETA: 4:26 - loss: 1.9336 - regression_loss: 1.4458 - classification_loss: 0.4878\n",
      "242/500 [=============>................] - ETA: 4:25 - loss: 1.9342 - regression_loss: 1.4460 - classification_loss: 0.4882\n",
      "243/500 [=============>................] - ETA: 4:24 - loss: 1.9314 - regression_loss: 1.4443 - classification_loss: 0.4871\n",
      "244/500 [=============>................] - ETA: 4:23 - loss: 1.9301 - regression_loss: 1.4427 - classification_loss: 0.4874\n",
      "245/500 [=============>................] - ETA: 4:22 - loss: 1.9321 - regression_loss: 1.4433 - classification_loss: 0.4888\n",
      "246/500 [=============>................] - ETA: 4:21 - loss: 1.9313 - regression_loss: 1.4434 - classification_loss: 0.4879\n",
      "247/500 [=============>................] - ETA: 4:20 - loss: 1.9293 - regression_loss: 1.4418 - classification_loss: 0.4875\n",
      "248/500 [=============>................] - ETA: 4:19 - loss: 1.9288 - regression_loss: 1.4413 - classification_loss: 0.4876\n",
      "249/500 [=============>................] - ETA: 4:18 - loss: 1.9280 - regression_loss: 1.4399 - classification_loss: 0.4881\n",
      "250/500 [==============>...............] - ETA: 4:16 - loss: 1.9246 - regression_loss: 1.4375 - classification_loss: 0.4871\n",
      "251/500 [==============>...............] - ETA: 4:15 - loss: 1.9235 - regression_loss: 1.4367 - classification_loss: 0.4868\n",
      "252/500 [==============>...............] - ETA: 4:14 - loss: 1.9217 - regression_loss: 1.4347 - classification_loss: 0.4871\n",
      "253/500 [==============>...............] - ETA: 4:13 - loss: 1.9198 - regression_loss: 1.4334 - classification_loss: 0.4864\n",
      "254/500 [==============>...............] - ETA: 4:12 - loss: 1.9204 - regression_loss: 1.4343 - classification_loss: 0.4862\n",
      "255/500 [==============>...............] - ETA: 4:11 - loss: 1.9221 - regression_loss: 1.4353 - classification_loss: 0.4869\n",
      "256/500 [==============>...............] - ETA: 4:10 - loss: 1.9200 - regression_loss: 1.4333 - classification_loss: 0.4867\n",
      "257/500 [==============>...............] - ETA: 4:09 - loss: 1.9200 - regression_loss: 1.4331 - classification_loss: 0.4869\n",
      "258/500 [==============>...............] - ETA: 4:08 - loss: 1.9202 - regression_loss: 1.4331 - classification_loss: 0.4871\n",
      "259/500 [==============>...............] - ETA: 4:07 - loss: 1.9168 - regression_loss: 1.4309 - classification_loss: 0.4859\n",
      "260/500 [==============>...............] - ETA: 4:06 - loss: 1.9153 - regression_loss: 1.4299 - classification_loss: 0.4855\n",
      "261/500 [==============>...............] - ETA: 4:05 - loss: 1.9135 - regression_loss: 1.4288 - classification_loss: 0.4848\n",
      "262/500 [==============>...............] - ETA: 4:04 - loss: 1.9142 - regression_loss: 1.4291 - classification_loss: 0.4850\n",
      "263/500 [==============>...............] - ETA: 4:03 - loss: 1.9168 - regression_loss: 1.4297 - classification_loss: 0.4871\n",
      "264/500 [==============>...............] - ETA: 4:02 - loss: 1.9176 - regression_loss: 1.4301 - classification_loss: 0.4875\n",
      "265/500 [==============>...............] - ETA: 4:01 - loss: 1.9145 - regression_loss: 1.4277 - classification_loss: 0.4868\n",
      "266/500 [==============>...............] - ETA: 4:00 - loss: 1.9171 - regression_loss: 1.4299 - classification_loss: 0.4872\n",
      "267/500 [===============>..............] - ETA: 3:59 - loss: 1.9195 - regression_loss: 1.4315 - classification_loss: 0.4880\n",
      "268/500 [===============>..............] - ETA: 3:58 - loss: 1.9170 - regression_loss: 1.4298 - classification_loss: 0.4873\n",
      "269/500 [===============>..............] - ETA: 3:57 - loss: 1.9168 - regression_loss: 1.4297 - classification_loss: 0.4871\n",
      "270/500 [===============>..............] - ETA: 3:56 - loss: 1.9190 - regression_loss: 1.4308 - classification_loss: 0.4883\n",
      "271/500 [===============>..............] - ETA: 3:55 - loss: 1.9178 - regression_loss: 1.4297 - classification_loss: 0.4880\n",
      "272/500 [===============>..............] - ETA: 3:54 - loss: 1.9188 - regression_loss: 1.4309 - classification_loss: 0.4879\n",
      "273/500 [===============>..............] - ETA: 3:53 - loss: 1.9169 - regression_loss: 1.4300 - classification_loss: 0.4869\n",
      "274/500 [===============>..............] - ETA: 3:52 - loss: 1.9187 - regression_loss: 1.4310 - classification_loss: 0.4877\n",
      "275/500 [===============>..............] - ETA: 3:51 - loss: 1.9202 - regression_loss: 1.4320 - classification_loss: 0.4882\n",
      "276/500 [===============>..............] - ETA: 3:49 - loss: 1.9178 - regression_loss: 1.4297 - classification_loss: 0.4881\n",
      "277/500 [===============>..............] - ETA: 3:48 - loss: 1.9166 - regression_loss: 1.4291 - classification_loss: 0.4875\n",
      "278/500 [===============>..............] - ETA: 3:47 - loss: 1.9207 - regression_loss: 1.4317 - classification_loss: 0.4890\n",
      "279/500 [===============>..............] - ETA: 3:46 - loss: 1.9199 - regression_loss: 1.4307 - classification_loss: 0.4891\n",
      "280/500 [===============>..............] - ETA: 3:45 - loss: 1.9230 - regression_loss: 1.4325 - classification_loss: 0.4905\n",
      "281/500 [===============>..............] - ETA: 3:44 - loss: 1.9234 - regression_loss: 1.4329 - classification_loss: 0.4905\n",
      "282/500 [===============>..............] - ETA: 3:43 - loss: 1.9271 - regression_loss: 1.4351 - classification_loss: 0.4920\n",
      "283/500 [===============>..............] - ETA: 3:42 - loss: 1.9246 - regression_loss: 1.4334 - classification_loss: 0.4912\n",
      "284/500 [================>.............] - ETA: 3:41 - loss: 1.9217 - regression_loss: 1.4312 - classification_loss: 0.4905\n",
      "285/500 [================>.............] - ETA: 3:40 - loss: 1.9218 - regression_loss: 1.4313 - classification_loss: 0.4904\n",
      "286/500 [================>.............] - ETA: 3:39 - loss: 1.9256 - regression_loss: 1.4340 - classification_loss: 0.4916\n",
      "287/500 [================>.............] - ETA: 3:38 - loss: 1.9250 - regression_loss: 1.4333 - classification_loss: 0.4917\n",
      "288/500 [================>.............] - ETA: 3:37 - loss: 1.9225 - regression_loss: 1.4311 - classification_loss: 0.4914\n",
      "289/500 [================>.............] - ETA: 3:36 - loss: 1.9231 - regression_loss: 1.4311 - classification_loss: 0.4920\n",
      "290/500 [================>.............] - ETA: 3:35 - loss: 1.9207 - regression_loss: 1.4295 - classification_loss: 0.4912\n",
      "291/500 [================>.............] - ETA: 3:34 - loss: 1.9189 - regression_loss: 1.4284 - classification_loss: 0.4905\n",
      "292/500 [================>.............] - ETA: 3:33 - loss: 1.9168 - regression_loss: 1.4271 - classification_loss: 0.4897\n",
      "293/500 [================>.............] - ETA: 3:32 - loss: 1.9175 - regression_loss: 1.4274 - classification_loss: 0.4901\n",
      "294/500 [================>.............] - ETA: 3:31 - loss: 1.9155 - regression_loss: 1.4259 - classification_loss: 0.4896\n",
      "295/500 [================>.............] - ETA: 3:30 - loss: 1.9162 - regression_loss: 1.4267 - classification_loss: 0.4895\n",
      "296/500 [================>.............] - ETA: 3:29 - loss: 1.9195 - regression_loss: 1.4290 - classification_loss: 0.4905\n",
      "297/500 [================>.............] - ETA: 3:28 - loss: 1.9207 - regression_loss: 1.4293 - classification_loss: 0.4913\n",
      "298/500 [================>.............] - ETA: 3:27 - loss: 1.9186 - regression_loss: 1.4279 - classification_loss: 0.4906\n",
      "299/500 [================>.............] - ETA: 3:26 - loss: 1.9178 - regression_loss: 1.4278 - classification_loss: 0.4900\n",
      "300/500 [=================>............] - ETA: 3:25 - loss: 1.9179 - regression_loss: 1.4279 - classification_loss: 0.4900\n",
      "301/500 [=================>............] - ETA: 3:24 - loss: 1.9174 - regression_loss: 1.4278 - classification_loss: 0.4896\n",
      "302/500 [=================>............] - ETA: 3:23 - loss: 1.9175 - regression_loss: 1.4279 - classification_loss: 0.4895\n",
      "303/500 [=================>............] - ETA: 3:22 - loss: 1.9169 - regression_loss: 1.4273 - classification_loss: 0.4896\n",
      "304/500 [=================>............] - ETA: 3:21 - loss: 1.9162 - regression_loss: 1.4269 - classification_loss: 0.4893\n",
      "305/500 [=================>............] - ETA: 3:20 - loss: 1.9154 - regression_loss: 1.4267 - classification_loss: 0.4887\n",
      "306/500 [=================>............] - ETA: 3:19 - loss: 1.9175 - regression_loss: 1.4282 - classification_loss: 0.4893\n",
      "307/500 [=================>............] - ETA: 3:18 - loss: 1.9173 - regression_loss: 1.4282 - classification_loss: 0.4891\n",
      "308/500 [=================>............] - ETA: 3:17 - loss: 1.9163 - regression_loss: 1.4277 - classification_loss: 0.4886\n",
      "309/500 [=================>............] - ETA: 3:16 - loss: 1.9161 - regression_loss: 1.4275 - classification_loss: 0.4886\n",
      "310/500 [=================>............] - ETA: 3:15 - loss: 1.9145 - regression_loss: 1.4266 - classification_loss: 0.4879\n",
      "311/500 [=================>............] - ETA: 3:14 - loss: 1.9124 - regression_loss: 1.4250 - classification_loss: 0.4874\n",
      "312/500 [=================>............] - ETA: 3:13 - loss: 1.9123 - regression_loss: 1.4250 - classification_loss: 0.4873\n",
      "313/500 [=================>............] - ETA: 3:12 - loss: 1.9127 - regression_loss: 1.4253 - classification_loss: 0.4874\n",
      "314/500 [=================>............] - ETA: 3:11 - loss: 1.9123 - regression_loss: 1.4253 - classification_loss: 0.4870\n",
      "315/500 [=================>............] - ETA: 3:10 - loss: 1.9097 - regression_loss: 1.4232 - classification_loss: 0.4864\n",
      "316/500 [=================>............] - ETA: 3:08 - loss: 1.9077 - regression_loss: 1.4215 - classification_loss: 0.4862\n",
      "317/500 [==================>...........] - ETA: 3:08 - loss: 1.9056 - regression_loss: 1.4194 - classification_loss: 0.4861\n",
      "318/500 [==================>...........] - ETA: 3:07 - loss: 1.9050 - regression_loss: 1.4195 - classification_loss: 0.4855\n",
      "319/500 [==================>...........] - ETA: 3:06 - loss: 1.9065 - regression_loss: 1.4205 - classification_loss: 0.4860\n",
      "320/500 [==================>...........] - ETA: 3:05 - loss: 1.9044 - regression_loss: 1.4191 - classification_loss: 0.4852\n",
      "321/500 [==================>...........] - ETA: 3:04 - loss: 1.9066 - regression_loss: 1.4210 - classification_loss: 0.4856\n",
      "322/500 [==================>...........] - ETA: 3:02 - loss: 1.9059 - regression_loss: 1.4206 - classification_loss: 0.4852\n",
      "323/500 [==================>...........] - ETA: 3:01 - loss: 1.9064 - regression_loss: 1.4210 - classification_loss: 0.4853\n",
      "324/500 [==================>...........] - ETA: 3:00 - loss: 1.9084 - regression_loss: 1.4225 - classification_loss: 0.4859\n",
      "325/500 [==================>...........] - ETA: 2:59 - loss: 1.9081 - regression_loss: 1.4226 - classification_loss: 0.4855\n",
      "326/500 [==================>...........] - ETA: 2:58 - loss: 1.9064 - regression_loss: 1.4212 - classification_loss: 0.4851\n",
      "327/500 [==================>...........] - ETA: 2:57 - loss: 1.9066 - regression_loss: 1.4215 - classification_loss: 0.4850\n",
      "328/500 [==================>...........] - ETA: 2:56 - loss: 1.9057 - regression_loss: 1.4209 - classification_loss: 0.4848\n",
      "329/500 [==================>...........] - ETA: 2:55 - loss: 1.9043 - regression_loss: 1.4196 - classification_loss: 0.4848\n",
      "330/500 [==================>...........] - ETA: 2:54 - loss: 1.9049 - regression_loss: 1.4193 - classification_loss: 0.4856\n",
      "331/500 [==================>...........] - ETA: 2:53 - loss: 1.9030 - regression_loss: 1.4178 - classification_loss: 0.4851\n",
      "332/500 [==================>...........] - ETA: 2:52 - loss: 1.9027 - regression_loss: 1.4174 - classification_loss: 0.4852\n",
      "333/500 [==================>...........] - ETA: 2:51 - loss: 1.8994 - regression_loss: 1.4150 - classification_loss: 0.4844\n",
      "334/500 [===================>..........] - ETA: 2:50 - loss: 1.9014 - regression_loss: 1.4160 - classification_loss: 0.4853\n",
      "335/500 [===================>..........] - ETA: 2:50 - loss: 1.9020 - regression_loss: 1.4168 - classification_loss: 0.4853\n",
      "336/500 [===================>..........] - ETA: 2:48 - loss: 1.9024 - regression_loss: 1.4172 - classification_loss: 0.4852\n",
      "337/500 [===================>..........] - ETA: 2:47 - loss: 1.9021 - regression_loss: 1.4170 - classification_loss: 0.4850\n",
      "338/500 [===================>..........] - ETA: 2:46 - loss: 1.9042 - regression_loss: 1.4188 - classification_loss: 0.4854\n",
      "339/500 [===================>..........] - ETA: 2:45 - loss: 1.9054 - regression_loss: 1.4203 - classification_loss: 0.4852\n",
      "340/500 [===================>..........] - ETA: 2:44 - loss: 1.9057 - regression_loss: 1.4209 - classification_loss: 0.4848\n",
      "341/500 [===================>..........] - ETA: 2:43 - loss: 1.9085 - regression_loss: 1.4224 - classification_loss: 0.4861\n",
      "342/500 [===================>..........] - ETA: 2:42 - loss: 1.9081 - regression_loss: 1.4221 - classification_loss: 0.4860\n",
      "343/500 [===================>..........] - ETA: 2:41 - loss: 1.9111 - regression_loss: 1.4248 - classification_loss: 0.4864\n",
      "344/500 [===================>..........] - ETA: 2:40 - loss: 1.9112 - regression_loss: 1.4250 - classification_loss: 0.4862\n",
      "345/500 [===================>..........] - ETA: 2:39 - loss: 1.9113 - regression_loss: 1.4252 - classification_loss: 0.4862\n",
      "346/500 [===================>..........] - ETA: 2:38 - loss: 1.9107 - regression_loss: 1.4246 - classification_loss: 0.4861\n",
      "347/500 [===================>..........] - ETA: 2:37 - loss: 1.9090 - regression_loss: 1.4232 - classification_loss: 0.4858\n",
      "348/500 [===================>..........] - ETA: 2:36 - loss: 1.9089 - regression_loss: 1.4230 - classification_loss: 0.4859\n",
      "349/500 [===================>..........] - ETA: 2:35 - loss: 1.9101 - regression_loss: 1.4241 - classification_loss: 0.4860\n",
      "350/500 [====================>.........] - ETA: 2:34 - loss: 1.9121 - regression_loss: 1.4259 - classification_loss: 0.4863\n",
      "351/500 [====================>.........] - ETA: 2:33 - loss: 1.9144 - regression_loss: 1.4275 - classification_loss: 0.4869\n",
      "352/500 [====================>.........] - ETA: 2:32 - loss: 1.9139 - regression_loss: 1.4274 - classification_loss: 0.4865\n",
      "353/500 [====================>.........] - ETA: 2:31 - loss: 1.9126 - regression_loss: 1.4264 - classification_loss: 0.4862\n",
      "354/500 [====================>.........] - ETA: 2:30 - loss: 1.9108 - regression_loss: 1.4249 - classification_loss: 0.4859\n",
      "355/500 [====================>.........] - ETA: 2:29 - loss: 1.9098 - regression_loss: 1.4239 - classification_loss: 0.4858\n",
      "356/500 [====================>.........] - ETA: 2:28 - loss: 1.9091 - regression_loss: 1.4235 - classification_loss: 0.4856\n",
      "357/500 [====================>.........] - ETA: 2:27 - loss: 1.9106 - regression_loss: 1.4244 - classification_loss: 0.4862\n",
      "358/500 [====================>.........] - ETA: 2:26 - loss: 1.9087 - regression_loss: 1.4227 - classification_loss: 0.4859\n",
      "359/500 [====================>.........] - ETA: 2:25 - loss: 1.9080 - regression_loss: 1.4221 - classification_loss: 0.4859\n",
      "360/500 [====================>.........] - ETA: 2:24 - loss: 1.9058 - regression_loss: 1.4204 - classification_loss: 0.4854\n",
      "361/500 [====================>.........] - ETA: 2:23 - loss: 1.9058 - regression_loss: 1.4207 - classification_loss: 0.4851\n",
      "362/500 [====================>.........] - ETA: 2:22 - loss: 1.9065 - regression_loss: 1.4205 - classification_loss: 0.4860\n",
      "363/500 [====================>.........] - ETA: 2:21 - loss: 1.9072 - regression_loss: 1.4212 - classification_loss: 0.4860\n",
      "364/500 [====================>.........] - ETA: 2:20 - loss: 1.9090 - regression_loss: 1.4225 - classification_loss: 0.4865\n",
      "365/500 [====================>.........] - ETA: 2:19 - loss: 1.9099 - regression_loss: 1.4233 - classification_loss: 0.4866\n",
      "366/500 [====================>.........] - ETA: 2:18 - loss: 1.9088 - regression_loss: 1.4226 - classification_loss: 0.4862\n",
      "367/500 [=====================>........] - ETA: 2:17 - loss: 1.9079 - regression_loss: 1.4221 - classification_loss: 0.4858\n",
      "368/500 [=====================>........] - ETA: 2:16 - loss: 1.9064 - regression_loss: 1.4209 - classification_loss: 0.4855\n",
      "369/500 [=====================>........] - ETA: 2:14 - loss: 1.9070 - regression_loss: 1.4212 - classification_loss: 0.4858\n",
      "370/500 [=====================>........] - ETA: 2:13 - loss: 1.9086 - regression_loss: 1.4224 - classification_loss: 0.4862\n",
      "371/500 [=====================>........] - ETA: 2:12 - loss: 1.9094 - regression_loss: 1.4228 - classification_loss: 0.4866\n",
      "372/500 [=====================>........] - ETA: 2:11 - loss: 1.9077 - regression_loss: 1.4215 - classification_loss: 0.4861\n",
      "373/500 [=====================>........] - ETA: 2:10 - loss: 1.9068 - regression_loss: 1.4206 - classification_loss: 0.4862\n",
      "374/500 [=====================>........] - ETA: 2:09 - loss: 1.9057 - regression_loss: 1.4198 - classification_loss: 0.4859\n",
      "375/500 [=====================>........] - ETA: 2:08 - loss: 1.9063 - regression_loss: 1.4203 - classification_loss: 0.4860\n",
      "376/500 [=====================>........] - ETA: 2:07 - loss: 1.9060 - regression_loss: 1.4201 - classification_loss: 0.4859\n",
      "377/500 [=====================>........] - ETA: 2:06 - loss: 1.9052 - regression_loss: 1.4194 - classification_loss: 0.4858\n",
      "378/500 [=====================>........] - ETA: 2:05 - loss: 1.9047 - regression_loss: 1.4189 - classification_loss: 0.4858\n",
      "379/500 [=====================>........] - ETA: 2:04 - loss: 1.9073 - regression_loss: 1.4201 - classification_loss: 0.4872\n",
      "380/500 [=====================>........] - ETA: 2:03 - loss: 1.9068 - regression_loss: 1.4196 - classification_loss: 0.4872\n",
      "381/500 [=====================>........] - ETA: 2:02 - loss: 1.9084 - regression_loss: 1.4203 - classification_loss: 0.4881\n",
      "382/500 [=====================>........] - ETA: 2:01 - loss: 1.9102 - regression_loss: 1.4217 - classification_loss: 0.4885\n",
      "383/500 [=====================>........] - ETA: 2:00 - loss: 1.9110 - regression_loss: 1.4225 - classification_loss: 0.4886\n",
      "384/500 [======================>.......] - ETA: 1:59 - loss: 1.9111 - regression_loss: 1.4223 - classification_loss: 0.4888\n",
      "385/500 [======================>.......] - ETA: 1:58 - loss: 1.9140 - regression_loss: 1.4241 - classification_loss: 0.4899\n",
      "386/500 [======================>.......] - ETA: 1:57 - loss: 1.9156 - regression_loss: 1.4252 - classification_loss: 0.4904\n",
      "387/500 [======================>.......] - ETA: 1:56 - loss: 1.9191 - regression_loss: 1.4275 - classification_loss: 0.4917\n",
      "388/500 [======================>.......] - ETA: 1:55 - loss: 1.9189 - regression_loss: 1.4274 - classification_loss: 0.4915\n",
      "389/500 [======================>.......] - ETA: 1:54 - loss: 1.9225 - regression_loss: 1.4297 - classification_loss: 0.4928\n",
      "390/500 [======================>.......] - ETA: 1:53 - loss: 1.9232 - regression_loss: 1.4302 - classification_loss: 0.4930\n",
      "391/500 [======================>.......] - ETA: 1:52 - loss: 1.9247 - regression_loss: 1.4313 - classification_loss: 0.4934\n",
      "392/500 [======================>.......] - ETA: 1:51 - loss: 1.9265 - regression_loss: 1.4324 - classification_loss: 0.4940\n",
      "393/500 [======================>.......] - ETA: 1:50 - loss: 1.9253 - regression_loss: 1.4318 - classification_loss: 0.4936\n",
      "394/500 [======================>.......] - ETA: 1:49 - loss: 1.9254 - regression_loss: 1.4316 - classification_loss: 0.4938\n",
      "395/500 [======================>.......] - ETA: 1:48 - loss: 1.9276 - regression_loss: 1.4332 - classification_loss: 0.4944\n",
      "396/500 [======================>.......] - ETA: 1:47 - loss: 1.9278 - regression_loss: 1.4331 - classification_loss: 0.4947\n",
      "397/500 [======================>.......] - ETA: 1:46 - loss: 1.9266 - regression_loss: 1.4324 - classification_loss: 0.4942\n",
      "398/500 [======================>.......] - ETA: 1:45 - loss: 1.9257 - regression_loss: 1.4316 - classification_loss: 0.4941\n",
      "399/500 [======================>.......] - ETA: 1:44 - loss: 1.9253 - regression_loss: 1.4315 - classification_loss: 0.4938\n",
      "400/500 [=======================>......] - ETA: 1:43 - loss: 1.9259 - regression_loss: 1.4323 - classification_loss: 0.4936\n",
      "401/500 [=======================>......] - ETA: 1:42 - loss: 1.9260 - regression_loss: 1.4322 - classification_loss: 0.4938\n",
      "402/500 [=======================>......] - ETA: 1:41 - loss: 1.9267 - regression_loss: 1.4329 - classification_loss: 0.4938\n",
      "403/500 [=======================>......] - ETA: 1:40 - loss: 1.9243 - regression_loss: 1.4313 - classification_loss: 0.4930\n",
      "404/500 [=======================>......] - ETA: 1:39 - loss: 1.9236 - regression_loss: 1.4306 - classification_loss: 0.4930\n",
      "405/500 [=======================>......] - ETA: 1:38 - loss: 1.9229 - regression_loss: 1.4302 - classification_loss: 0.4927\n",
      "406/500 [=======================>......] - ETA: 1:37 - loss: 1.9248 - regression_loss: 1.4313 - classification_loss: 0.4936\n",
      "407/500 [=======================>......] - ETA: 1:36 - loss: 1.9244 - regression_loss: 1.4312 - classification_loss: 0.4932\n",
      "408/500 [=======================>......] - ETA: 1:35 - loss: 1.9225 - regression_loss: 1.4299 - classification_loss: 0.4926\n",
      "409/500 [=======================>......] - ETA: 1:34 - loss: 1.9204 - regression_loss: 1.4283 - classification_loss: 0.4921\n",
      "410/500 [=======================>......] - ETA: 1:33 - loss: 1.9209 - regression_loss: 1.4289 - classification_loss: 0.4920\n",
      "411/500 [=======================>......] - ETA: 1:32 - loss: 1.9221 - regression_loss: 1.4295 - classification_loss: 0.4926\n",
      "412/500 [=======================>......] - ETA: 1:30 - loss: 1.9225 - regression_loss: 1.4300 - classification_loss: 0.4925\n",
      "413/500 [=======================>......] - ETA: 1:29 - loss: 1.9239 - regression_loss: 1.4309 - classification_loss: 0.4930\n",
      "414/500 [=======================>......] - ETA: 1:28 - loss: 1.9245 - regression_loss: 1.4309 - classification_loss: 0.4936\n",
      "415/500 [=======================>......] - ETA: 1:27 - loss: 1.9256 - regression_loss: 1.4315 - classification_loss: 0.4941\n",
      "416/500 [=======================>......] - ETA: 1:26 - loss: 1.9244 - regression_loss: 1.4306 - classification_loss: 0.4938\n",
      "417/500 [========================>.....] - ETA: 1:25 - loss: 1.9231 - regression_loss: 1.4298 - classification_loss: 0.4932\n",
      "418/500 [========================>.....] - ETA: 1:24 - loss: 1.9239 - regression_loss: 1.4301 - classification_loss: 0.4937\n",
      "419/500 [========================>.....] - ETA: 1:23 - loss: 1.9214 - regression_loss: 1.4283 - classification_loss: 0.4931\n",
      "420/500 [========================>.....] - ETA: 1:22 - loss: 1.9222 - regression_loss: 1.4293 - classification_loss: 0.4930\n",
      "421/500 [========================>.....] - ETA: 1:21 - loss: 1.9231 - regression_loss: 1.4300 - classification_loss: 0.4931\n",
      "422/500 [========================>.....] - ETA: 1:20 - loss: 1.9250 - regression_loss: 1.4307 - classification_loss: 0.4943\n",
      "423/500 [========================>.....] - ETA: 1:19 - loss: 1.9235 - regression_loss: 1.4292 - classification_loss: 0.4943\n",
      "424/500 [========================>.....] - ETA: 1:18 - loss: 1.9230 - regression_loss: 1.4289 - classification_loss: 0.4940\n",
      "425/500 [========================>.....] - ETA: 1:17 - loss: 1.9237 - regression_loss: 1.4292 - classification_loss: 0.4945\n",
      "426/500 [========================>.....] - ETA: 1:16 - loss: 1.9234 - regression_loss: 1.4289 - classification_loss: 0.4945\n",
      "427/500 [========================>.....] - ETA: 1:15 - loss: 1.9216 - regression_loss: 1.4279 - classification_loss: 0.4938\n",
      "428/500 [========================>.....] - ETA: 1:14 - loss: 1.9221 - regression_loss: 1.4285 - classification_loss: 0.4937\n",
      "429/500 [========================>.....] - ETA: 1:13 - loss: 1.9213 - regression_loss: 1.4279 - classification_loss: 0.4934\n",
      "430/500 [========================>.....] - ETA: 1:12 - loss: 1.9206 - regression_loss: 1.4274 - classification_loss: 0.4932\n",
      "431/500 [========================>.....] - ETA: 1:11 - loss: 1.9215 - regression_loss: 1.4281 - classification_loss: 0.4934\n",
      "432/500 [========================>.....] - ETA: 1:10 - loss: 1.9222 - regression_loss: 1.4290 - classification_loss: 0.4931\n",
      "433/500 [========================>.....] - ETA: 1:09 - loss: 1.9236 - regression_loss: 1.4304 - classification_loss: 0.4932\n",
      "434/500 [=========================>....] - ETA: 1:08 - loss: 1.9227 - regression_loss: 1.4298 - classification_loss: 0.4929\n",
      "435/500 [=========================>....] - ETA: 1:07 - loss: 1.9235 - regression_loss: 1.4305 - classification_loss: 0.4930\n",
      "436/500 [=========================>....] - ETA: 1:06 - loss: 1.9212 - regression_loss: 1.4290 - classification_loss: 0.4922\n",
      "437/500 [=========================>....] - ETA: 1:05 - loss: 1.9205 - regression_loss: 1.4285 - classification_loss: 0.4920\n",
      "438/500 [=========================>....] - ETA: 1:04 - loss: 1.9188 - regression_loss: 1.4272 - classification_loss: 0.4915\n",
      "439/500 [=========================>....] - ETA: 1:03 - loss: 1.9197 - regression_loss: 1.4278 - classification_loss: 0.4919\n",
      "440/500 [=========================>....] - ETA: 1:02 - loss: 1.9194 - regression_loss: 1.4277 - classification_loss: 0.4918\n",
      "441/500 [=========================>....] - ETA: 1:00 - loss: 1.9186 - regression_loss: 1.4272 - classification_loss: 0.4913\n",
      "442/500 [=========================>....] - ETA: 59s - loss: 1.9189 - regression_loss: 1.4273 - classification_loss: 0.4916 \n",
      "443/500 [=========================>....] - ETA: 58s - loss: 1.9195 - regression_loss: 1.4275 - classification_loss: 0.4920\n",
      "444/500 [=========================>....] - ETA: 57s - loss: 1.9179 - regression_loss: 1.4264 - classification_loss: 0.4915\n",
      "445/500 [=========================>....] - ETA: 56s - loss: 1.9172 - regression_loss: 1.4259 - classification_loss: 0.4913\n",
      "446/500 [=========================>....] - ETA: 55s - loss: 1.9162 - regression_loss: 1.4250 - classification_loss: 0.4912\n",
      "447/500 [=========================>....] - ETA: 54s - loss: 1.9168 - regression_loss: 1.4254 - classification_loss: 0.4913\n",
      "448/500 [=========================>....] - ETA: 53s - loss: 1.9194 - regression_loss: 1.4272 - classification_loss: 0.4922\n",
      "449/500 [=========================>....] - ETA: 52s - loss: 1.9202 - regression_loss: 1.4277 - classification_loss: 0.4924\n",
      "450/500 [==========================>...] - ETA: 51s - loss: 1.9208 - regression_loss: 1.4285 - classification_loss: 0.4923\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 1.9192 - regression_loss: 1.4274 - classification_loss: 0.4918\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 1.9187 - regression_loss: 1.4272 - classification_loss: 0.4915\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.9172 - regression_loss: 1.4260 - classification_loss: 0.4912\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.9159 - regression_loss: 1.4247 - classification_loss: 0.4911\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.9153 - regression_loss: 1.4243 - classification_loss: 0.4909\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.9131 - regression_loss: 1.4226 - classification_loss: 0.4906\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.9117 - regression_loss: 1.4213 - classification_loss: 0.4903\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.9121 - regression_loss: 1.4213 - classification_loss: 0.4908\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.9109 - regression_loss: 1.4204 - classification_loss: 0.4905\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.9114 - regression_loss: 1.4210 - classification_loss: 0.4904\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 1.9113 - regression_loss: 1.4208 - classification_loss: 0.4905\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 1.9127 - regression_loss: 1.4215 - classification_loss: 0.4911\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 1.9119 - regression_loss: 1.4208 - classification_loss: 0.4911\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 1.9117 - regression_loss: 1.4207 - classification_loss: 0.4911\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 1.9108 - regression_loss: 1.4200 - classification_loss: 0.4908\n",
      "466/500 [==========================>...] - ETA: 35s - loss: 1.9115 - regression_loss: 1.4204 - classification_loss: 0.4911\n",
      "467/500 [===========================>..] - ETA: 34s - loss: 1.9105 - regression_loss: 1.4199 - classification_loss: 0.4907\n",
      "468/500 [===========================>..] - ETA: 33s - loss: 1.9104 - regression_loss: 1.4199 - classification_loss: 0.4905\n",
      "469/500 [===========================>..] - ETA: 32s - loss: 1.9107 - regression_loss: 1.4202 - classification_loss: 0.4904\n",
      "470/500 [===========================>..] - ETA: 30s - loss: 1.9111 - regression_loss: 1.4205 - classification_loss: 0.4906\n",
      "471/500 [===========================>..] - ETA: 29s - loss: 1.9097 - regression_loss: 1.4196 - classification_loss: 0.4900\n",
      "472/500 [===========================>..] - ETA: 28s - loss: 1.9088 - regression_loss: 1.4190 - classification_loss: 0.4898\n",
      "473/500 [===========================>..] - ETA: 27s - loss: 1.9082 - regression_loss: 1.4184 - classification_loss: 0.4898\n",
      "474/500 [===========================>..] - ETA: 26s - loss: 1.9092 - regression_loss: 1.4197 - classification_loss: 0.4895\n",
      "475/500 [===========================>..] - ETA: 25s - loss: 1.9077 - regression_loss: 1.4183 - classification_loss: 0.4893\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 1.9072 - regression_loss: 1.4180 - classification_loss: 0.4892\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.9066 - regression_loss: 1.4177 - classification_loss: 0.4889\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.9072 - regression_loss: 1.4176 - classification_loss: 0.4897\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.9068 - regression_loss: 1.4173 - classification_loss: 0.4896\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.9053 - regression_loss: 1.4163 - classification_loss: 0.4890\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.9064 - regression_loss: 1.4174 - classification_loss: 0.4890\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.9059 - regression_loss: 1.4168 - classification_loss: 0.4891\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.9050 - regression_loss: 1.4164 - classification_loss: 0.4886\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.9071 - regression_loss: 1.4178 - classification_loss: 0.4893\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.9086 - regression_loss: 1.4188 - classification_loss: 0.4899\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.9069 - regression_loss: 1.4175 - classification_loss: 0.4894\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.9084 - regression_loss: 1.4187 - classification_loss: 0.4897\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.9101 - regression_loss: 1.4198 - classification_loss: 0.4903\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.9084 - regression_loss: 1.4184 - classification_loss: 0.4900\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.9079 - regression_loss: 1.4179 - classification_loss: 0.4900\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.9078 - regression_loss: 1.4180 - classification_loss: 0.4898 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.9076 - regression_loss: 1.4178 - classification_loss: 0.4898\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.9074 - regression_loss: 1.4178 - classification_loss: 0.4896\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.9082 - regression_loss: 1.4186 - classification_loss: 0.4896\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.9077 - regression_loss: 1.4179 - classification_loss: 0.4898\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.9098 - regression_loss: 1.4191 - classification_loss: 0.4907\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.9100 - regression_loss: 1.4189 - classification_loss: 0.4911\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.9115 - regression_loss: 1.4200 - classification_loss: 0.4915\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.9109 - regression_loss: 1.4195 - classification_loss: 0.4913\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9123 - regression_loss: 1.4207 - classification_loss: 0.4916\n",
      "Epoch 00031: saving model to ./snapshots\\resnet50_csv_31.h5\n",
      "\n",
      "500/500 [==============================] - 517s 1s/step - loss: 1.9123 - regression_loss: 1.4207 - classification_loss: 0.4916\n",
      "Epoch 32/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 1.4026 - regression_loss: 1.1690 - classification_loss: 0.2336\n",
      "  2/500 [..............................] - ETA: 4:27 - loss: 1.4076 - regression_loss: 1.1138 - classification_loss: 0.2939\n",
      "  3/500 [..............................] - ETA: 5:57 - loss: 1.8045 - regression_loss: 1.3054 - classification_loss: 0.4990\n",
      "  4/500 [..............................] - ETA: 6:43 - loss: 1.9841 - regression_loss: 1.4725 - classification_loss: 0.5116\n",
      "  5/500 [..............................] - ETA: 6:59 - loss: 1.9214 - regression_loss: 1.4317 - classification_loss: 0.4898\n",
      "  6/500 [..............................] - ETA: 7:10 - loss: 1.8806 - regression_loss: 1.4063 - classification_loss: 0.4743\n",
      "  7/500 [..............................] - ETA: 7:06 - loss: 1.9433 - regression_loss: 1.4189 - classification_loss: 0.5244\n",
      "  8/500 [..............................] - ETA: 7:28 - loss: 1.9403 - regression_loss: 1.4345 - classification_loss: 0.5057\n",
      "  9/500 [..............................] - ETA: 7:37 - loss: 1.8792 - regression_loss: 1.3888 - classification_loss: 0.4905\n",
      " 10/500 [..............................] - ETA: 7:43 - loss: 1.9126 - regression_loss: 1.4079 - classification_loss: 0.5047\n",
      " 11/500 [..............................] - ETA: 7:34 - loss: 1.8166 - regression_loss: 1.3410 - classification_loss: 0.4756\n",
      " 12/500 [..............................] - ETA: 7:43 - loss: 1.8644 - regression_loss: 1.3748 - classification_loss: 0.4896\n",
      " 13/500 [..............................] - ETA: 7:43 - loss: 1.8349 - regression_loss: 1.3530 - classification_loss: 0.4819\n",
      " 14/500 [..............................] - ETA: 7:43 - loss: 1.7844 - regression_loss: 1.3174 - classification_loss: 0.4669\n",
      " 15/500 [..............................] - ETA: 7:46 - loss: 1.7683 - regression_loss: 1.3082 - classification_loss: 0.4601\n",
      " 16/500 [..............................] - ETA: 7:50 - loss: 1.7410 - regression_loss: 1.2865 - classification_loss: 0.4545\n",
      " 17/500 [>.............................] - ETA: 7:46 - loss: 1.7354 - regression_loss: 1.2836 - classification_loss: 0.4518\n",
      " 18/500 [>.............................] - ETA: 7:49 - loss: 1.7203 - regression_loss: 1.2746 - classification_loss: 0.4457\n",
      " 19/500 [>.............................] - ETA: 7:51 - loss: 1.7356 - regression_loss: 1.2816 - classification_loss: 0.4539\n",
      " 20/500 [>.............................] - ETA: 7:53 - loss: 1.7714 - regression_loss: 1.3102 - classification_loss: 0.4612\n",
      " 21/500 [>.............................] - ETA: 7:52 - loss: 1.7431 - regression_loss: 1.2868 - classification_loss: 0.4563\n",
      " 22/500 [>.............................] - ETA: 7:51 - loss: 1.7161 - regression_loss: 1.2659 - classification_loss: 0.4502\n",
      " 23/500 [>.............................] - ETA: 7:50 - loss: 1.7573 - regression_loss: 1.3000 - classification_loss: 0.4573\n",
      " 24/500 [>.............................] - ETA: 7:46 - loss: 1.8066 - regression_loss: 1.3199 - classification_loss: 0.4866\n",
      " 25/500 [>.............................] - ETA: 7:49 - loss: 1.8159 - regression_loss: 1.3291 - classification_loss: 0.4868\n",
      " 26/500 [>.............................] - ETA: 7:49 - loss: 1.8030 - regression_loss: 1.3229 - classification_loss: 0.4801\n",
      " 27/500 [>.............................] - ETA: 7:50 - loss: 1.8042 - regression_loss: 1.3288 - classification_loss: 0.4754\n",
      " 28/500 [>.............................] - ETA: 7:50 - loss: 1.8175 - regression_loss: 1.3381 - classification_loss: 0.4794\n",
      " 29/500 [>.............................] - ETA: 7:49 - loss: 1.8200 - regression_loss: 1.3368 - classification_loss: 0.4832\n",
      " 30/500 [>.............................] - ETA: 7:44 - loss: 1.8122 - regression_loss: 1.3338 - classification_loss: 0.4784\n",
      " 31/500 [>.............................] - ETA: 7:46 - loss: 1.7908 - regression_loss: 1.3150 - classification_loss: 0.4758\n",
      " 32/500 [>.............................] - ETA: 7:48 - loss: 1.7889 - regression_loss: 1.3119 - classification_loss: 0.4771\n",
      " 33/500 [>.............................] - ETA: 7:46 - loss: 1.8194 - regression_loss: 1.3294 - classification_loss: 0.4900\n",
      " 34/500 [=>............................] - ETA: 7:46 - loss: 1.7959 - regression_loss: 1.3122 - classification_loss: 0.4837\n",
      " 35/500 [=>............................] - ETA: 7:45 - loss: 1.8192 - regression_loss: 1.3294 - classification_loss: 0.4897\n",
      " 36/500 [=>............................] - ETA: 7:44 - loss: 1.8060 - regression_loss: 1.3235 - classification_loss: 0.4825\n",
      " 37/500 [=>............................] - ETA: 8:05 - loss: 1.8014 - regression_loss: 1.3214 - classification_loss: 0.4801\n",
      " 38/500 [=>............................] - ETA: 8:04 - loss: 1.8081 - regression_loss: 1.3274 - classification_loss: 0.4806\n",
      " 39/500 [=>............................] - ETA: 8:05 - loss: 1.8062 - regression_loss: 1.3250 - classification_loss: 0.4812\n",
      " 40/500 [=>............................] - ETA: 8:03 - loss: 1.8090 - regression_loss: 1.3281 - classification_loss: 0.4809\n",
      " 41/500 [=>............................] - ETA: 8:01 - loss: 1.7997 - regression_loss: 1.3247 - classification_loss: 0.4750\n",
      " 42/500 [=>............................] - ETA: 8:00 - loss: 1.7944 - regression_loss: 1.3187 - classification_loss: 0.4757\n",
      " 43/500 [=>............................] - ETA: 8:00 - loss: 1.7871 - regression_loss: 1.3148 - classification_loss: 0.4723\n",
      " 44/500 [=>............................] - ETA: 7:56 - loss: 1.7835 - regression_loss: 1.3116 - classification_loss: 0.4719\n",
      " 45/500 [=>............................] - ETA: 7:56 - loss: 1.7810 - regression_loss: 1.3112 - classification_loss: 0.4697\n",
      " 46/500 [=>............................] - ETA: 7:52 - loss: 1.7925 - regression_loss: 1.3233 - classification_loss: 0.4692\n",
      " 47/500 [=>............................] - ETA: 7:51 - loss: 1.7837 - regression_loss: 1.3163 - classification_loss: 0.4673\n",
      " 48/500 [=>............................] - ETA: 7:51 - loss: 1.7929 - regression_loss: 1.3255 - classification_loss: 0.4674\n",
      " 49/500 [=>............................] - ETA: 7:51 - loss: 1.7818 - regression_loss: 1.3195 - classification_loss: 0.4623\n",
      " 50/500 [==>...........................] - ETA: 7:49 - loss: 1.7836 - regression_loss: 1.3207 - classification_loss: 0.4629\n",
      " 51/500 [==>...........................] - ETA: 7:48 - loss: 1.7908 - regression_loss: 1.3260 - classification_loss: 0.4649\n",
      " 52/500 [==>...........................] - ETA: 7:48 - loss: 1.8018 - regression_loss: 1.3351 - classification_loss: 0.4666\n",
      " 53/500 [==>...........................] - ETA: 7:46 - loss: 1.8055 - regression_loss: 1.3413 - classification_loss: 0.4642\n",
      " 54/500 [==>...........................] - ETA: 7:44 - loss: 1.8083 - regression_loss: 1.3445 - classification_loss: 0.4639\n",
      " 55/500 [==>...........................] - ETA: 7:43 - loss: 1.8185 - regression_loss: 1.3517 - classification_loss: 0.4668\n",
      " 56/500 [==>...........................] - ETA: 7:41 - loss: 1.8310 - regression_loss: 1.3583 - classification_loss: 0.4728\n",
      " 57/500 [==>...........................] - ETA: 7:39 - loss: 1.8444 - regression_loss: 1.3678 - classification_loss: 0.4766\n",
      " 58/500 [==>...........................] - ETA: 7:38 - loss: 1.8385 - regression_loss: 1.3647 - classification_loss: 0.4737\n",
      " 59/500 [==>...........................] - ETA: 7:37 - loss: 1.8477 - regression_loss: 1.3750 - classification_loss: 0.4727\n",
      " 60/500 [==>...........................] - ETA: 7:35 - loss: 1.8431 - regression_loss: 1.3712 - classification_loss: 0.4719\n",
      " 61/500 [==>...........................] - ETA: 7:35 - loss: 1.8405 - regression_loss: 1.3687 - classification_loss: 0.4718\n",
      " 62/500 [==>...........................] - ETA: 7:34 - loss: 1.8324 - regression_loss: 1.3616 - classification_loss: 0.4708\n",
      " 63/500 [==>...........................] - ETA: 7:32 - loss: 1.8265 - regression_loss: 1.3589 - classification_loss: 0.4676\n",
      " 64/500 [==>...........................] - ETA: 7:31 - loss: 1.8128 - regression_loss: 1.3488 - classification_loss: 0.4640\n",
      " 65/500 [==>...........................] - ETA: 7:29 - loss: 1.8226 - regression_loss: 1.3582 - classification_loss: 0.4644\n",
      " 66/500 [==>...........................] - ETA: 7:29 - loss: 1.8188 - regression_loss: 1.3559 - classification_loss: 0.4629\n",
      " 67/500 [===>..........................] - ETA: 7:28 - loss: 1.8182 - regression_loss: 1.3562 - classification_loss: 0.4620\n",
      " 68/500 [===>..........................] - ETA: 7:27 - loss: 1.8133 - regression_loss: 1.3523 - classification_loss: 0.4610\n",
      " 69/500 [===>..........................] - ETA: 7:26 - loss: 1.8148 - regression_loss: 1.3536 - classification_loss: 0.4612\n",
      " 70/500 [===>..........................] - ETA: 7:26 - loss: 1.8134 - regression_loss: 1.3538 - classification_loss: 0.4595\n",
      " 71/500 [===>..........................] - ETA: 7:25 - loss: 1.8123 - regression_loss: 1.3531 - classification_loss: 0.4591\n",
      " 72/500 [===>..........................] - ETA: 7:24 - loss: 1.8020 - regression_loss: 1.3451 - classification_loss: 0.4569\n",
      " 73/500 [===>..........................] - ETA: 7:22 - loss: 1.8032 - regression_loss: 1.3482 - classification_loss: 0.4550\n",
      " 74/500 [===>..........................] - ETA: 7:22 - loss: 1.7915 - regression_loss: 1.3393 - classification_loss: 0.4522\n",
      " 75/500 [===>..........................] - ETA: 7:21 - loss: 1.7868 - regression_loss: 1.3352 - classification_loss: 0.4516\n",
      " 76/500 [===>..........................] - ETA: 7:20 - loss: 1.7904 - regression_loss: 1.3396 - classification_loss: 0.4508\n",
      " 77/500 [===>..........................] - ETA: 7:19 - loss: 1.7857 - regression_loss: 1.3369 - classification_loss: 0.4488\n",
      " 78/500 [===>..........................] - ETA: 7:18 - loss: 1.7826 - regression_loss: 1.3355 - classification_loss: 0.4472\n",
      " 79/500 [===>..........................] - ETA: 7:17 - loss: 1.7818 - regression_loss: 1.3346 - classification_loss: 0.4472\n",
      " 80/500 [===>..........................] - ETA: 7:16 - loss: 1.7848 - regression_loss: 1.3369 - classification_loss: 0.4479\n",
      " 81/500 [===>..........................] - ETA: 7:14 - loss: 1.7780 - regression_loss: 1.3315 - classification_loss: 0.4464\n",
      " 82/500 [===>..........................] - ETA: 7:13 - loss: 1.7912 - regression_loss: 1.3430 - classification_loss: 0.4482\n",
      " 83/500 [===>..........................] - ETA: 7:12 - loss: 1.7892 - regression_loss: 1.3425 - classification_loss: 0.4467\n",
      " 84/500 [====>.........................] - ETA: 7:11 - loss: 1.7893 - regression_loss: 1.3432 - classification_loss: 0.4461\n",
      " 85/500 [====>.........................] - ETA: 7:10 - loss: 1.7876 - regression_loss: 1.3412 - classification_loss: 0.4464\n",
      " 86/500 [====>.........................] - ETA: 7:09 - loss: 1.7847 - regression_loss: 1.3398 - classification_loss: 0.4449\n",
      " 87/500 [====>.........................] - ETA: 7:08 - loss: 1.7852 - regression_loss: 1.3401 - classification_loss: 0.4451\n",
      " 88/500 [====>.........................] - ETA: 7:08 - loss: 1.7975 - regression_loss: 1.3493 - classification_loss: 0.4482\n",
      " 89/500 [====>.........................] - ETA: 7:07 - loss: 1.7988 - regression_loss: 1.3501 - classification_loss: 0.4486\n",
      " 90/500 [====>.........................] - ETA: 7:06 - loss: 1.7941 - regression_loss: 1.3457 - classification_loss: 0.4484\n",
      " 91/500 [====>.........................] - ETA: 7:05 - loss: 1.7937 - regression_loss: 1.3462 - classification_loss: 0.4475\n",
      " 92/500 [====>.........................] - ETA: 7:04 - loss: 1.7937 - regression_loss: 1.3467 - classification_loss: 0.4470\n",
      " 93/500 [====>.........................] - ETA: 7:03 - loss: 1.7928 - regression_loss: 1.3464 - classification_loss: 0.4464\n",
      " 94/500 [====>.........................] - ETA: 7:02 - loss: 1.8018 - regression_loss: 1.3522 - classification_loss: 0.4496\n",
      " 95/500 [====>.........................] - ETA: 7:01 - loss: 1.7936 - regression_loss: 1.3454 - classification_loss: 0.4482\n",
      " 96/500 [====>.........................] - ETA: 7:00 - loss: 1.7949 - regression_loss: 1.3461 - classification_loss: 0.4487\n",
      " 97/500 [====>.........................] - ETA: 6:58 - loss: 1.7858 - regression_loss: 1.3391 - classification_loss: 0.4467\n",
      " 98/500 [====>.........................] - ETA: 6:57 - loss: 1.7814 - regression_loss: 1.3372 - classification_loss: 0.4442\n",
      " 99/500 [====>.........................] - ETA: 6:57 - loss: 1.7926 - regression_loss: 1.3432 - classification_loss: 0.4494\n",
      "100/500 [=====>........................] - ETA: 6:55 - loss: 1.7925 - regression_loss: 1.3441 - classification_loss: 0.4484\n",
      "101/500 [=====>........................] - ETA: 6:54 - loss: 1.8023 - regression_loss: 1.3505 - classification_loss: 0.4519\n",
      "102/500 [=====>........................] - ETA: 6:53 - loss: 1.8074 - regression_loss: 1.3540 - classification_loss: 0.4535\n",
      "103/500 [=====>........................] - ETA: 6:51 - loss: 1.8050 - regression_loss: 1.3524 - classification_loss: 0.4526\n",
      "104/500 [=====>........................] - ETA: 6:50 - loss: 1.7991 - regression_loss: 1.3478 - classification_loss: 0.4513\n",
      "105/500 [=====>........................] - ETA: 6:49 - loss: 1.8051 - regression_loss: 1.3532 - classification_loss: 0.4519\n",
      "106/500 [=====>........................] - ETA: 6:48 - loss: 1.8232 - regression_loss: 1.3644 - classification_loss: 0.4588\n",
      "107/500 [=====>........................] - ETA: 6:46 - loss: 1.8192 - regression_loss: 1.3613 - classification_loss: 0.4579\n",
      "108/500 [=====>........................] - ETA: 6:46 - loss: 1.8197 - regression_loss: 1.3619 - classification_loss: 0.4578\n",
      "109/500 [=====>........................] - ETA: 6:44 - loss: 1.8227 - regression_loss: 1.3643 - classification_loss: 0.4584\n",
      "110/500 [=====>........................] - ETA: 6:43 - loss: 1.8183 - regression_loss: 1.3617 - classification_loss: 0.4566\n",
      "111/500 [=====>........................] - ETA: 6:42 - loss: 1.8185 - regression_loss: 1.3632 - classification_loss: 0.4553\n",
      "112/500 [=====>........................] - ETA: 6:40 - loss: 1.8181 - regression_loss: 1.3635 - classification_loss: 0.4546\n",
      "113/500 [=====>........................] - ETA: 6:39 - loss: 1.8192 - regression_loss: 1.3654 - classification_loss: 0.4538\n",
      "114/500 [=====>........................] - ETA: 6:38 - loss: 1.8234 - regression_loss: 1.3673 - classification_loss: 0.4561\n",
      "115/500 [=====>........................] - ETA: 6:37 - loss: 1.8233 - regression_loss: 1.3668 - classification_loss: 0.4566\n",
      "116/500 [=====>........................] - ETA: 6:36 - loss: 1.8278 - regression_loss: 1.3704 - classification_loss: 0.4574\n",
      "117/500 [======>.......................] - ETA: 6:35 - loss: 1.8279 - regression_loss: 1.3693 - classification_loss: 0.4585\n",
      "118/500 [======>.......................] - ETA: 6:34 - loss: 1.8203 - regression_loss: 1.3645 - classification_loss: 0.4558\n",
      "119/500 [======>.......................] - ETA: 6:33 - loss: 1.8282 - regression_loss: 1.3685 - classification_loss: 0.4597\n",
      "120/500 [======>.......................] - ETA: 6:32 - loss: 1.8276 - regression_loss: 1.3693 - classification_loss: 0.4583\n",
      "121/500 [======>.......................] - ETA: 6:31 - loss: 1.8327 - regression_loss: 1.3714 - classification_loss: 0.4613\n",
      "122/500 [======>.......................] - ETA: 6:30 - loss: 1.8375 - regression_loss: 1.3730 - classification_loss: 0.4645\n",
      "123/500 [======>.......................] - ETA: 6:30 - loss: 1.8369 - regression_loss: 1.3721 - classification_loss: 0.4648\n",
      "124/500 [======>.......................] - ETA: 6:29 - loss: 1.8336 - regression_loss: 1.3704 - classification_loss: 0.4632\n",
      "125/500 [======>.......................] - ETA: 6:28 - loss: 1.8423 - regression_loss: 1.3757 - classification_loss: 0.4666\n",
      "126/500 [======>.......................] - ETA: 6:27 - loss: 1.8363 - regression_loss: 1.3715 - classification_loss: 0.4648\n",
      "127/500 [======>.......................] - ETA: 6:26 - loss: 1.8313 - regression_loss: 1.3677 - classification_loss: 0.4636\n",
      "128/500 [======>.......................] - ETA: 6:25 - loss: 1.8319 - regression_loss: 1.3665 - classification_loss: 0.4654\n",
      "129/500 [======>.......................] - ETA: 6:24 - loss: 1.8288 - regression_loss: 1.3652 - classification_loss: 0.4636\n",
      "130/500 [======>.......................] - ETA: 6:22 - loss: 1.8255 - regression_loss: 1.3623 - classification_loss: 0.4631\n",
      "131/500 [======>.......................] - ETA: 6:22 - loss: 1.8269 - regression_loss: 1.3647 - classification_loss: 0.4622\n",
      "132/500 [======>.......................] - ETA: 6:21 - loss: 1.8362 - regression_loss: 1.3701 - classification_loss: 0.4661\n",
      "133/500 [======>.......................] - ETA: 6:19 - loss: 1.8342 - regression_loss: 1.3691 - classification_loss: 0.4651\n",
      "134/500 [=======>......................] - ETA: 6:19 - loss: 1.8337 - regression_loss: 1.3695 - classification_loss: 0.4642\n",
      "135/500 [=======>......................] - ETA: 6:17 - loss: 1.8391 - regression_loss: 1.3734 - classification_loss: 0.4657\n",
      "136/500 [=======>......................] - ETA: 6:16 - loss: 1.8376 - regression_loss: 1.3724 - classification_loss: 0.4652\n",
      "137/500 [=======>......................] - ETA: 6:15 - loss: 1.8384 - regression_loss: 1.3730 - classification_loss: 0.4654\n",
      "138/500 [=======>......................] - ETA: 6:14 - loss: 1.8385 - regression_loss: 1.3736 - classification_loss: 0.4649\n",
      "139/500 [=======>......................] - ETA: 6:13 - loss: 1.8383 - regression_loss: 1.3729 - classification_loss: 0.4654\n",
      "140/500 [=======>......................] - ETA: 6:12 - loss: 1.8395 - regression_loss: 1.3745 - classification_loss: 0.4650\n",
      "141/500 [=======>......................] - ETA: 6:11 - loss: 1.8339 - regression_loss: 1.3704 - classification_loss: 0.4635\n",
      "142/500 [=======>......................] - ETA: 6:10 - loss: 1.8304 - regression_loss: 1.3679 - classification_loss: 0.4625\n",
      "143/500 [=======>......................] - ETA: 6:09 - loss: 1.8315 - regression_loss: 1.3683 - classification_loss: 0.4633\n",
      "144/500 [=======>......................] - ETA: 6:08 - loss: 1.8356 - regression_loss: 1.3713 - classification_loss: 0.4643\n",
      "145/500 [=======>......................] - ETA: 6:07 - loss: 1.8404 - regression_loss: 1.3744 - classification_loss: 0.4660\n",
      "146/500 [=======>......................] - ETA: 6:05 - loss: 1.8475 - regression_loss: 1.3792 - classification_loss: 0.4683\n",
      "147/500 [=======>......................] - ETA: 6:04 - loss: 1.8520 - regression_loss: 1.3834 - classification_loss: 0.4686\n",
      "148/500 [=======>......................] - ETA: 6:03 - loss: 1.8591 - regression_loss: 1.3892 - classification_loss: 0.4699\n",
      "149/500 [=======>......................] - ETA: 6:03 - loss: 1.8625 - regression_loss: 1.3920 - classification_loss: 0.4705\n",
      "150/500 [========>.....................] - ETA: 6:02 - loss: 1.8704 - regression_loss: 1.3985 - classification_loss: 0.4718\n",
      "151/500 [========>.....................] - ETA: 6:01 - loss: 1.8692 - regression_loss: 1.3976 - classification_loss: 0.4716\n",
      "152/500 [========>.....................] - ETA: 5:59 - loss: 1.8674 - regression_loss: 1.3970 - classification_loss: 0.4705\n",
      "153/500 [========>.....................] - ETA: 5:59 - loss: 1.8730 - regression_loss: 1.4003 - classification_loss: 0.4726\n",
      "154/500 [========>.....................] - ETA: 5:57 - loss: 1.8685 - regression_loss: 1.3973 - classification_loss: 0.4712\n",
      "155/500 [========>.....................] - ETA: 5:56 - loss: 1.8625 - regression_loss: 1.3931 - classification_loss: 0.4694\n",
      "156/500 [========>.....................] - ETA: 5:55 - loss: 1.8625 - regression_loss: 1.3940 - classification_loss: 0.4685\n",
      "157/500 [========>.....................] - ETA: 5:54 - loss: 1.8619 - regression_loss: 1.3947 - classification_loss: 0.4672\n",
      "158/500 [========>.....................] - ETA: 5:53 - loss: 1.8608 - regression_loss: 1.3942 - classification_loss: 0.4667\n",
      "159/500 [========>.....................] - ETA: 5:52 - loss: 1.8566 - regression_loss: 1.3905 - classification_loss: 0.4661\n",
      "160/500 [========>.....................] - ETA: 5:51 - loss: 1.8531 - regression_loss: 1.3879 - classification_loss: 0.4652\n",
      "161/500 [========>.....................] - ETA: 5:50 - loss: 1.8562 - regression_loss: 1.3904 - classification_loss: 0.4657\n",
      "162/500 [========>.....................] - ETA: 5:49 - loss: 1.8545 - regression_loss: 1.3895 - classification_loss: 0.4650\n",
      "163/500 [========>.....................] - ETA: 5:48 - loss: 1.8588 - regression_loss: 1.3922 - classification_loss: 0.4666\n",
      "164/500 [========>.....................] - ETA: 5:47 - loss: 1.8629 - regression_loss: 1.3946 - classification_loss: 0.4683\n",
      "165/500 [========>.....................] - ETA: 5:46 - loss: 1.8619 - regression_loss: 1.3938 - classification_loss: 0.4681\n",
      "166/500 [========>.....................] - ETA: 5:45 - loss: 1.8594 - regression_loss: 1.3918 - classification_loss: 0.4676\n",
      "167/500 [=========>....................] - ETA: 5:44 - loss: 1.8601 - regression_loss: 1.3931 - classification_loss: 0.4669\n",
      "168/500 [=========>....................] - ETA: 5:42 - loss: 1.8568 - regression_loss: 1.3910 - classification_loss: 0.4657\n",
      "169/500 [=========>....................] - ETA: 5:41 - loss: 1.8541 - regression_loss: 1.3895 - classification_loss: 0.4646\n",
      "170/500 [=========>....................] - ETA: 5:40 - loss: 1.8557 - regression_loss: 1.3910 - classification_loss: 0.4647\n",
      "171/500 [=========>....................] - ETA: 5:39 - loss: 1.8596 - regression_loss: 1.3932 - classification_loss: 0.4664\n",
      "172/500 [=========>....................] - ETA: 5:38 - loss: 1.8574 - regression_loss: 1.3920 - classification_loss: 0.4654\n",
      "173/500 [=========>....................] - ETA: 5:37 - loss: 1.8579 - regression_loss: 1.3921 - classification_loss: 0.4659\n",
      "174/500 [=========>....................] - ETA: 5:36 - loss: 1.8617 - regression_loss: 1.3954 - classification_loss: 0.4663\n",
      "175/500 [=========>....................] - ETA: 5:35 - loss: 1.8621 - regression_loss: 1.3956 - classification_loss: 0.4665\n",
      "176/500 [=========>....................] - ETA: 5:34 - loss: 1.8583 - regression_loss: 1.3932 - classification_loss: 0.4651\n",
      "177/500 [=========>....................] - ETA: 5:33 - loss: 1.8562 - regression_loss: 1.3916 - classification_loss: 0.4646\n",
      "178/500 [=========>....................] - ETA: 5:32 - loss: 1.8564 - regression_loss: 1.3915 - classification_loss: 0.4649\n",
      "179/500 [=========>....................] - ETA: 5:31 - loss: 1.8527 - regression_loss: 1.3886 - classification_loss: 0.4641\n",
      "180/500 [=========>....................] - ETA: 5:30 - loss: 1.8541 - regression_loss: 1.3893 - classification_loss: 0.4649\n",
      "181/500 [=========>....................] - ETA: 5:29 - loss: 1.8490 - regression_loss: 1.3856 - classification_loss: 0.4635\n",
      "182/500 [=========>....................] - ETA: 5:28 - loss: 1.8536 - regression_loss: 1.3877 - classification_loss: 0.4659\n",
      "183/500 [=========>....................] - ETA: 5:27 - loss: 1.8501 - regression_loss: 1.3845 - classification_loss: 0.4655\n",
      "184/500 [==========>...................] - ETA: 5:26 - loss: 1.8491 - regression_loss: 1.3839 - classification_loss: 0.4652\n",
      "185/500 [==========>...................] - ETA: 5:24 - loss: 1.8501 - regression_loss: 1.3848 - classification_loss: 0.4653\n",
      "186/500 [==========>...................] - ETA: 5:23 - loss: 1.8509 - regression_loss: 1.3856 - classification_loss: 0.4653\n",
      "187/500 [==========>...................] - ETA: 5:22 - loss: 1.8486 - regression_loss: 1.3837 - classification_loss: 0.4648\n",
      "188/500 [==========>...................] - ETA: 5:21 - loss: 1.8478 - regression_loss: 1.3836 - classification_loss: 0.4642\n",
      "189/500 [==========>...................] - ETA: 5:20 - loss: 1.8491 - regression_loss: 1.3847 - classification_loss: 0.4644\n",
      "190/500 [==========>...................] - ETA: 5:19 - loss: 1.8487 - regression_loss: 1.3845 - classification_loss: 0.4642\n",
      "191/500 [==========>...................] - ETA: 5:18 - loss: 1.8453 - regression_loss: 1.3824 - classification_loss: 0.4629\n",
      "192/500 [==========>...................] - ETA: 5:17 - loss: 1.8432 - regression_loss: 1.3803 - classification_loss: 0.4629\n",
      "193/500 [==========>...................] - ETA: 5:16 - loss: 1.8512 - regression_loss: 1.3857 - classification_loss: 0.4655\n",
      "194/500 [==========>...................] - ETA: 5:15 - loss: 1.8505 - regression_loss: 1.3850 - classification_loss: 0.4656\n",
      "195/500 [==========>...................] - ETA: 5:14 - loss: 1.8476 - regression_loss: 1.3832 - classification_loss: 0.4644\n",
      "196/500 [==========>...................] - ETA: 5:13 - loss: 1.8467 - regression_loss: 1.3826 - classification_loss: 0.4641\n",
      "197/500 [==========>...................] - ETA: 5:12 - loss: 1.8495 - regression_loss: 1.3838 - classification_loss: 0.4657\n",
      "198/500 [==========>...................] - ETA: 5:11 - loss: 1.8482 - regression_loss: 1.3832 - classification_loss: 0.4650\n",
      "199/500 [==========>...................] - ETA: 5:10 - loss: 1.8527 - regression_loss: 1.3873 - classification_loss: 0.4655\n",
      "200/500 [===========>..................] - ETA: 5:09 - loss: 1.8561 - regression_loss: 1.3897 - classification_loss: 0.4664\n",
      "201/500 [===========>..................] - ETA: 5:08 - loss: 1.8556 - regression_loss: 1.3891 - classification_loss: 0.4665\n",
      "202/500 [===========>..................] - ETA: 5:07 - loss: 1.8564 - regression_loss: 1.3887 - classification_loss: 0.4677\n",
      "203/500 [===========>..................] - ETA: 5:06 - loss: 1.8530 - regression_loss: 1.3856 - classification_loss: 0.4673\n",
      "204/500 [===========>..................] - ETA: 5:05 - loss: 1.8569 - regression_loss: 1.3891 - classification_loss: 0.4678\n",
      "205/500 [===========>..................] - ETA: 5:04 - loss: 1.8556 - regression_loss: 1.3882 - classification_loss: 0.4674\n",
      "206/500 [===========>..................] - ETA: 5:03 - loss: 1.8539 - regression_loss: 1.3871 - classification_loss: 0.4668\n",
      "207/500 [===========>..................] - ETA: 5:02 - loss: 1.8542 - regression_loss: 1.3866 - classification_loss: 0.4676\n",
      "208/500 [===========>..................] - ETA: 5:01 - loss: 1.8605 - regression_loss: 1.3904 - classification_loss: 0.4701\n",
      "209/500 [===========>..................] - ETA: 5:00 - loss: 1.8584 - regression_loss: 1.3882 - classification_loss: 0.4702\n",
      "210/500 [===========>..................] - ETA: 4:59 - loss: 1.8598 - regression_loss: 1.3895 - classification_loss: 0.4703\n",
      "211/500 [===========>..................] - ETA: 4:58 - loss: 1.8613 - regression_loss: 1.3914 - classification_loss: 0.4699\n",
      "212/500 [===========>..................] - ETA: 4:57 - loss: 1.8602 - regression_loss: 1.3909 - classification_loss: 0.4692\n",
      "213/500 [===========>..................] - ETA: 4:56 - loss: 1.8616 - regression_loss: 1.3924 - classification_loss: 0.4692\n",
      "214/500 [===========>..................] - ETA: 4:54 - loss: 1.8616 - regression_loss: 1.3929 - classification_loss: 0.4687\n",
      "215/500 [===========>..................] - ETA: 4:53 - loss: 1.8634 - regression_loss: 1.3939 - classification_loss: 0.4695\n",
      "216/500 [===========>..................] - ETA: 4:52 - loss: 1.8613 - regression_loss: 1.3925 - classification_loss: 0.4689\n",
      "217/500 [============>.................] - ETA: 4:52 - loss: 1.8628 - regression_loss: 1.3937 - classification_loss: 0.4690\n",
      "218/500 [============>.................] - ETA: 4:50 - loss: 1.8627 - regression_loss: 1.3942 - classification_loss: 0.4685\n",
      "219/500 [============>.................] - ETA: 4:49 - loss: 1.8603 - regression_loss: 1.3925 - classification_loss: 0.4678\n",
      "220/500 [============>.................] - ETA: 4:48 - loss: 1.8616 - regression_loss: 1.3934 - classification_loss: 0.4682\n",
      "221/500 [============>.................] - ETA: 4:47 - loss: 1.8594 - regression_loss: 1.3912 - classification_loss: 0.4681\n",
      "222/500 [============>.................] - ETA: 4:46 - loss: 1.8620 - regression_loss: 1.3936 - classification_loss: 0.4684\n",
      "223/500 [============>.................] - ETA: 4:45 - loss: 1.8602 - regression_loss: 1.3928 - classification_loss: 0.4674\n",
      "224/500 [============>.................] - ETA: 4:44 - loss: 1.8569 - regression_loss: 1.3901 - classification_loss: 0.4668\n",
      "225/500 [============>.................] - ETA: 4:43 - loss: 1.8552 - regression_loss: 1.3890 - classification_loss: 0.4663\n",
      "226/500 [============>.................] - ETA: 4:42 - loss: 1.8543 - regression_loss: 1.3882 - classification_loss: 0.4661\n",
      "227/500 [============>.................] - ETA: 4:41 - loss: 1.8524 - regression_loss: 1.3867 - classification_loss: 0.4657\n",
      "228/500 [============>.................] - ETA: 4:40 - loss: 1.8544 - regression_loss: 1.3877 - classification_loss: 0.4667\n",
      "229/500 [============>.................] - ETA: 4:39 - loss: 1.8521 - regression_loss: 1.3864 - classification_loss: 0.4658\n",
      "230/500 [============>.................] - ETA: 4:38 - loss: 1.8501 - regression_loss: 1.3838 - classification_loss: 0.4663\n",
      "231/500 [============>.................] - ETA: 4:37 - loss: 1.8515 - regression_loss: 1.3848 - classification_loss: 0.4667\n",
      "232/500 [============>.................] - ETA: 4:36 - loss: 1.8501 - regression_loss: 1.3840 - classification_loss: 0.4661\n",
      "233/500 [============>.................] - ETA: 4:35 - loss: 1.8492 - regression_loss: 1.3835 - classification_loss: 0.4657\n",
      "234/500 [=============>................] - ETA: 4:34 - loss: 1.8474 - regression_loss: 1.3821 - classification_loss: 0.4652\n",
      "235/500 [=============>................] - ETA: 4:33 - loss: 1.8475 - regression_loss: 1.3830 - classification_loss: 0.4645\n",
      "236/500 [=============>................] - ETA: 4:32 - loss: 1.8493 - regression_loss: 1.3832 - classification_loss: 0.4661\n",
      "237/500 [=============>................] - ETA: 4:31 - loss: 1.8487 - regression_loss: 1.3831 - classification_loss: 0.4655\n",
      "238/500 [=============>................] - ETA: 4:30 - loss: 1.8489 - regression_loss: 1.3835 - classification_loss: 0.4655\n",
      "239/500 [=============>................] - ETA: 4:29 - loss: 1.8501 - regression_loss: 1.3842 - classification_loss: 0.4659\n",
      "240/500 [=============>................] - ETA: 4:28 - loss: 1.8508 - regression_loss: 1.3844 - classification_loss: 0.4664\n",
      "241/500 [=============>................] - ETA: 4:27 - loss: 1.8509 - regression_loss: 1.3849 - classification_loss: 0.4661\n",
      "242/500 [=============>................] - ETA: 4:26 - loss: 1.8515 - regression_loss: 1.3852 - classification_loss: 0.4663\n",
      "243/500 [=============>................] - ETA: 4:25 - loss: 1.8511 - regression_loss: 1.3848 - classification_loss: 0.4663\n",
      "244/500 [=============>................] - ETA: 4:24 - loss: 1.8559 - regression_loss: 1.3878 - classification_loss: 0.4681\n",
      "245/500 [=============>................] - ETA: 4:23 - loss: 1.8573 - regression_loss: 1.3889 - classification_loss: 0.4684\n",
      "246/500 [=============>................] - ETA: 4:21 - loss: 1.8548 - regression_loss: 1.3868 - classification_loss: 0.4680\n",
      "247/500 [=============>................] - ETA: 4:20 - loss: 1.8533 - regression_loss: 1.3857 - classification_loss: 0.4676\n",
      "248/500 [=============>................] - ETA: 4:19 - loss: 1.8547 - regression_loss: 1.3870 - classification_loss: 0.4677\n",
      "249/500 [=============>................] - ETA: 4:18 - loss: 1.8550 - regression_loss: 1.3870 - classification_loss: 0.4680\n",
      "250/500 [==============>...............] - ETA: 4:17 - loss: 1.8548 - regression_loss: 1.3869 - classification_loss: 0.4679\n",
      "251/500 [==============>...............] - ETA: 4:16 - loss: 1.8541 - regression_loss: 1.3861 - classification_loss: 0.4681\n",
      "252/500 [==============>...............] - ETA: 4:15 - loss: 1.8517 - regression_loss: 1.3842 - classification_loss: 0.4675\n",
      "253/500 [==============>...............] - ETA: 4:14 - loss: 1.8497 - regression_loss: 1.3824 - classification_loss: 0.4674\n",
      "254/500 [==============>...............] - ETA: 4:13 - loss: 1.8481 - regression_loss: 1.3810 - classification_loss: 0.4671\n",
      "255/500 [==============>...............] - ETA: 4:12 - loss: 1.8488 - regression_loss: 1.3818 - classification_loss: 0.4670\n",
      "256/500 [==============>...............] - ETA: 4:11 - loss: 1.8483 - regression_loss: 1.3813 - classification_loss: 0.4670\n",
      "257/500 [==============>...............] - ETA: 4:10 - loss: 1.8541 - regression_loss: 1.3851 - classification_loss: 0.4690\n",
      "258/500 [==============>...............] - ETA: 4:09 - loss: 1.8535 - regression_loss: 1.3848 - classification_loss: 0.4688\n",
      "259/500 [==============>...............] - ETA: 4:08 - loss: 1.8535 - regression_loss: 1.3840 - classification_loss: 0.4695\n",
      "260/500 [==============>...............] - ETA: 4:07 - loss: 1.8531 - regression_loss: 1.3834 - classification_loss: 0.4698\n",
      "261/500 [==============>...............] - ETA: 4:06 - loss: 1.8549 - regression_loss: 1.3847 - classification_loss: 0.4702\n",
      "262/500 [==============>...............] - ETA: 4:05 - loss: 1.8556 - regression_loss: 1.3851 - classification_loss: 0.4705\n",
      "263/500 [==============>...............] - ETA: 4:04 - loss: 1.8539 - regression_loss: 1.3840 - classification_loss: 0.4699\n",
      "264/500 [==============>...............] - ETA: 4:03 - loss: 1.8572 - regression_loss: 1.3858 - classification_loss: 0.4714\n",
      "265/500 [==============>...............] - ETA: 4:02 - loss: 1.8588 - regression_loss: 1.3866 - classification_loss: 0.4722\n",
      "266/500 [==============>...............] - ETA: 4:01 - loss: 1.8574 - regression_loss: 1.3856 - classification_loss: 0.4718\n",
      "267/500 [===============>..............] - ETA: 4:00 - loss: 1.8564 - regression_loss: 1.3853 - classification_loss: 0.4712\n",
      "268/500 [===============>..............] - ETA: 3:59 - loss: 1.8562 - regression_loss: 1.3841 - classification_loss: 0.4721\n",
      "269/500 [===============>..............] - ETA: 3:58 - loss: 1.8593 - regression_loss: 1.3859 - classification_loss: 0.4734\n",
      "270/500 [===============>..............] - ETA: 3:57 - loss: 1.8591 - regression_loss: 1.3864 - classification_loss: 0.4727\n",
      "271/500 [===============>..............] - ETA: 3:56 - loss: 1.8617 - regression_loss: 1.3879 - classification_loss: 0.4738\n",
      "272/500 [===============>..............] - ETA: 3:55 - loss: 1.8641 - regression_loss: 1.3893 - classification_loss: 0.4749\n",
      "273/500 [===============>..............] - ETA: 3:54 - loss: 1.8619 - regression_loss: 1.3869 - classification_loss: 0.4749\n",
      "274/500 [===============>..............] - ETA: 3:53 - loss: 1.8613 - regression_loss: 1.3860 - classification_loss: 0.4754\n",
      "275/500 [===============>..............] - ETA: 3:52 - loss: 1.8628 - regression_loss: 1.3866 - classification_loss: 0.4762\n",
      "276/500 [===============>..............] - ETA: 3:51 - loss: 1.8629 - regression_loss: 1.3868 - classification_loss: 0.4761\n",
      "277/500 [===============>..............] - ETA: 3:49 - loss: 1.8635 - regression_loss: 1.3869 - classification_loss: 0.4766\n",
      "278/500 [===============>..............] - ETA: 3:48 - loss: 1.8639 - regression_loss: 1.3874 - classification_loss: 0.4765\n",
      "279/500 [===============>..............] - ETA: 3:47 - loss: 1.8675 - regression_loss: 1.3897 - classification_loss: 0.4778\n",
      "280/500 [===============>..............] - ETA: 3:46 - loss: 1.8707 - regression_loss: 1.3917 - classification_loss: 0.4790\n",
      "281/500 [===============>..............] - ETA: 3:45 - loss: 1.8690 - regression_loss: 1.3907 - classification_loss: 0.4783\n",
      "282/500 [===============>..............] - ETA: 3:44 - loss: 1.8695 - regression_loss: 1.3908 - classification_loss: 0.4787\n",
      "283/500 [===============>..............] - ETA: 3:43 - loss: 1.8699 - regression_loss: 1.3914 - classification_loss: 0.4784\n",
      "284/500 [================>.............] - ETA: 3:42 - loss: 1.8689 - regression_loss: 1.3908 - classification_loss: 0.4780\n",
      "285/500 [================>.............] - ETA: 3:41 - loss: 1.8666 - regression_loss: 1.3893 - classification_loss: 0.4773\n",
      "286/500 [================>.............] - ETA: 3:40 - loss: 1.8678 - regression_loss: 1.3905 - classification_loss: 0.4773\n",
      "287/500 [================>.............] - ETA: 3:39 - loss: 1.8659 - regression_loss: 1.3894 - classification_loss: 0.4765\n",
      "288/500 [================>.............] - ETA: 3:38 - loss: 1.8641 - regression_loss: 1.3880 - classification_loss: 0.4761\n",
      "289/500 [================>.............] - ETA: 3:37 - loss: 1.8635 - regression_loss: 1.3877 - classification_loss: 0.4758\n",
      "290/500 [================>.............] - ETA: 3:36 - loss: 1.8644 - regression_loss: 1.3879 - classification_loss: 0.4765\n",
      "291/500 [================>.............] - ETA: 3:35 - loss: 1.8639 - regression_loss: 1.3874 - classification_loss: 0.4765\n",
      "292/500 [================>.............] - ETA: 3:34 - loss: 1.8623 - regression_loss: 1.3861 - classification_loss: 0.4762\n",
      "293/500 [================>.............] - ETA: 3:33 - loss: 1.8605 - regression_loss: 1.3852 - classification_loss: 0.4753\n",
      "294/500 [================>.............] - ETA: 3:32 - loss: 1.8601 - regression_loss: 1.3847 - classification_loss: 0.4754\n",
      "295/500 [================>.............] - ETA: 3:31 - loss: 1.8627 - regression_loss: 1.3870 - classification_loss: 0.4757\n",
      "296/500 [================>.............] - ETA: 3:30 - loss: 1.8623 - regression_loss: 1.3864 - classification_loss: 0.4760\n",
      "297/500 [================>.............] - ETA: 3:29 - loss: 1.8623 - regression_loss: 1.3863 - classification_loss: 0.4760\n",
      "298/500 [================>.............] - ETA: 3:28 - loss: 1.8617 - regression_loss: 1.3862 - classification_loss: 0.4755\n",
      "299/500 [================>.............] - ETA: 3:27 - loss: 1.8619 - regression_loss: 1.3866 - classification_loss: 0.4753\n",
      "300/500 [=================>............] - ETA: 3:26 - loss: 1.8636 - regression_loss: 1.3879 - classification_loss: 0.4758\n",
      "301/500 [=================>............] - ETA: 3:25 - loss: 1.8624 - regression_loss: 1.3870 - classification_loss: 0.4753\n",
      "302/500 [=================>............] - ETA: 3:24 - loss: 1.8647 - regression_loss: 1.3888 - classification_loss: 0.4758\n",
      "303/500 [=================>............] - ETA: 3:23 - loss: 1.8653 - regression_loss: 1.3894 - classification_loss: 0.4759\n",
      "304/500 [=================>............] - ETA: 3:21 - loss: 1.8634 - regression_loss: 1.3883 - classification_loss: 0.4750\n",
      "305/500 [=================>............] - ETA: 3:20 - loss: 1.8661 - regression_loss: 1.3910 - classification_loss: 0.4751\n",
      "306/500 [=================>............] - ETA: 3:20 - loss: 1.8673 - regression_loss: 1.3914 - classification_loss: 0.4759\n",
      "307/500 [=================>............] - ETA: 3:18 - loss: 1.8651 - regression_loss: 1.3899 - classification_loss: 0.4751\n",
      "308/500 [=================>............] - ETA: 3:17 - loss: 1.8670 - regression_loss: 1.3909 - classification_loss: 0.4760\n",
      "309/500 [=================>............] - ETA: 3:16 - loss: 1.8659 - regression_loss: 1.3899 - classification_loss: 0.4760\n",
      "310/500 [=================>............] - ETA: 3:15 - loss: 1.8661 - regression_loss: 1.3903 - classification_loss: 0.4758\n",
      "311/500 [=================>............] - ETA: 3:14 - loss: 1.8666 - regression_loss: 1.3911 - classification_loss: 0.4755\n",
      "312/500 [=================>............] - ETA: 3:13 - loss: 1.8683 - regression_loss: 1.3918 - classification_loss: 0.4764\n",
      "313/500 [=================>............] - ETA: 3:12 - loss: 1.8663 - regression_loss: 1.3904 - classification_loss: 0.4759\n",
      "314/500 [=================>............] - ETA: 3:11 - loss: 1.8703 - regression_loss: 1.3928 - classification_loss: 0.4774\n",
      "315/500 [=================>............] - ETA: 3:10 - loss: 1.8684 - regression_loss: 1.3916 - classification_loss: 0.4768\n",
      "316/500 [=================>............] - ETA: 3:09 - loss: 1.8684 - regression_loss: 1.3918 - classification_loss: 0.4766\n",
      "317/500 [==================>...........] - ETA: 3:08 - loss: 1.8682 - regression_loss: 1.3918 - classification_loss: 0.4764\n",
      "318/500 [==================>...........] - ETA: 3:07 - loss: 1.8687 - regression_loss: 1.3925 - classification_loss: 0.4762\n",
      "319/500 [==================>...........] - ETA: 3:06 - loss: 1.8694 - regression_loss: 1.3926 - classification_loss: 0.4768\n",
      "320/500 [==================>...........] - ETA: 3:05 - loss: 1.8687 - regression_loss: 1.3918 - classification_loss: 0.4769\n",
      "321/500 [==================>...........] - ETA: 3:04 - loss: 1.8709 - regression_loss: 1.3933 - classification_loss: 0.4776\n",
      "322/500 [==================>...........] - ETA: 3:03 - loss: 1.8706 - regression_loss: 1.3927 - classification_loss: 0.4778\n",
      "323/500 [==================>...........] - ETA: 3:02 - loss: 1.8701 - regression_loss: 1.3929 - classification_loss: 0.4773\n",
      "324/500 [==================>...........] - ETA: 3:01 - loss: 1.8684 - regression_loss: 1.3918 - classification_loss: 0.4766\n",
      "325/500 [==================>...........] - ETA: 3:00 - loss: 1.8700 - regression_loss: 1.3927 - classification_loss: 0.4773\n",
      "326/500 [==================>...........] - ETA: 2:59 - loss: 1.8686 - regression_loss: 1.3913 - classification_loss: 0.4773\n",
      "327/500 [==================>...........] - ETA: 2:58 - loss: 1.8674 - regression_loss: 1.3906 - classification_loss: 0.4768\n",
      "328/500 [==================>...........] - ETA: 2:57 - loss: 1.8661 - regression_loss: 1.3899 - classification_loss: 0.4763\n",
      "329/500 [==================>...........] - ETA: 2:55 - loss: 1.8673 - regression_loss: 1.3908 - classification_loss: 0.4765\n",
      "330/500 [==================>...........] - ETA: 2:54 - loss: 1.8659 - regression_loss: 1.3899 - classification_loss: 0.4759\n",
      "331/500 [==================>...........] - ETA: 2:53 - loss: 1.8676 - regression_loss: 1.3912 - classification_loss: 0.4764\n",
      "332/500 [==================>...........] - ETA: 2:52 - loss: 1.8683 - regression_loss: 1.3915 - classification_loss: 0.4767\n",
      "333/500 [==================>...........] - ETA: 2:51 - loss: 1.8700 - regression_loss: 1.3930 - classification_loss: 0.4769\n",
      "334/500 [===================>..........] - ETA: 2:50 - loss: 1.8730 - regression_loss: 1.3947 - classification_loss: 0.4783\n",
      "335/500 [===================>..........] - ETA: 2:49 - loss: 1.8745 - regression_loss: 1.3958 - classification_loss: 0.4787\n",
      "336/500 [===================>..........] - ETA: 2:48 - loss: 1.8761 - regression_loss: 1.3967 - classification_loss: 0.4794\n",
      "337/500 [===================>..........] - ETA: 2:47 - loss: 1.8756 - regression_loss: 1.3962 - classification_loss: 0.4794\n",
      "338/500 [===================>..........] - ETA: 2:46 - loss: 1.8781 - regression_loss: 1.3985 - classification_loss: 0.4796\n",
      "339/500 [===================>..........] - ETA: 2:45 - loss: 1.8803 - regression_loss: 1.3992 - classification_loss: 0.4811\n",
      "340/500 [===================>..........] - ETA: 2:44 - loss: 1.8813 - regression_loss: 1.3994 - classification_loss: 0.4819\n",
      "341/500 [===================>..........] - ETA: 2:43 - loss: 1.8810 - regression_loss: 1.3991 - classification_loss: 0.4819\n",
      "342/500 [===================>..........] - ETA: 2:42 - loss: 1.8785 - regression_loss: 1.3974 - classification_loss: 0.4811\n",
      "343/500 [===================>..........] - ETA: 2:41 - loss: 1.8788 - regression_loss: 1.3977 - classification_loss: 0.4810\n",
      "344/500 [===================>..........] - ETA: 2:40 - loss: 1.8772 - regression_loss: 1.3965 - classification_loss: 0.4807\n",
      "345/500 [===================>..........] - ETA: 2:39 - loss: 1.8776 - regression_loss: 1.3968 - classification_loss: 0.4807\n",
      "346/500 [===================>..........] - ETA: 2:38 - loss: 1.8776 - regression_loss: 1.3973 - classification_loss: 0.4804\n",
      "347/500 [===================>..........] - ETA: 2:37 - loss: 1.8769 - regression_loss: 1.3969 - classification_loss: 0.4800\n",
      "348/500 [===================>..........] - ETA: 2:36 - loss: 1.8759 - regression_loss: 1.3962 - classification_loss: 0.4797\n",
      "349/500 [===================>..........] - ETA: 2:35 - loss: 1.8743 - regression_loss: 1.3947 - classification_loss: 0.4796\n",
      "350/500 [====================>.........] - ETA: 2:34 - loss: 1.8728 - regression_loss: 1.3938 - classification_loss: 0.4790\n",
      "351/500 [====================>.........] - ETA: 2:33 - loss: 1.8714 - regression_loss: 1.3927 - classification_loss: 0.4786\n",
      "352/500 [====================>.........] - ETA: 2:32 - loss: 1.8716 - regression_loss: 1.3928 - classification_loss: 0.4788\n",
      "353/500 [====================>.........] - ETA: 2:31 - loss: 1.8718 - regression_loss: 1.3930 - classification_loss: 0.4788\n",
      "354/500 [====================>.........] - ETA: 2:30 - loss: 1.8713 - regression_loss: 1.3929 - classification_loss: 0.4784\n",
      "355/500 [====================>.........] - ETA: 2:29 - loss: 1.8712 - regression_loss: 1.3935 - classification_loss: 0.4777\n",
      "356/500 [====================>.........] - ETA: 2:28 - loss: 1.8716 - regression_loss: 1.3937 - classification_loss: 0.4779\n",
      "357/500 [====================>.........] - ETA: 2:27 - loss: 1.8716 - regression_loss: 1.3938 - classification_loss: 0.4778\n",
      "358/500 [====================>.........] - ETA: 2:26 - loss: 1.8713 - regression_loss: 1.3938 - classification_loss: 0.4774\n",
      "359/500 [====================>.........] - ETA: 2:25 - loss: 1.8708 - regression_loss: 1.3934 - classification_loss: 0.4774\n",
      "360/500 [====================>.........] - ETA: 2:24 - loss: 1.8712 - regression_loss: 1.3940 - classification_loss: 0.4772\n",
      "361/500 [====================>.........] - ETA: 2:23 - loss: 1.8694 - regression_loss: 1.3928 - classification_loss: 0.4766\n",
      "362/500 [====================>.........] - ETA: 2:22 - loss: 1.8686 - regression_loss: 1.3925 - classification_loss: 0.4762\n",
      "363/500 [====================>.........] - ETA: 2:20 - loss: 1.8679 - regression_loss: 1.3918 - classification_loss: 0.4761\n",
      "364/500 [====================>.........] - ETA: 2:19 - loss: 1.8692 - regression_loss: 1.3919 - classification_loss: 0.4773\n",
      "365/500 [====================>.........] - ETA: 2:18 - loss: 1.8701 - regression_loss: 1.3924 - classification_loss: 0.4777\n",
      "366/500 [====================>.........] - ETA: 2:17 - loss: 1.8686 - regression_loss: 1.3913 - classification_loss: 0.4773\n",
      "367/500 [=====================>........] - ETA: 2:16 - loss: 1.8692 - regression_loss: 1.3920 - classification_loss: 0.4772\n",
      "368/500 [=====================>........] - ETA: 2:15 - loss: 1.8714 - regression_loss: 1.3933 - classification_loss: 0.4780\n",
      "369/500 [=====================>........] - ETA: 2:14 - loss: 1.8708 - regression_loss: 1.3932 - classification_loss: 0.4777\n",
      "370/500 [=====================>........] - ETA: 2:13 - loss: 1.8708 - regression_loss: 1.3934 - classification_loss: 0.4774\n",
      "371/500 [=====================>........] - ETA: 2:12 - loss: 1.8699 - regression_loss: 1.3928 - classification_loss: 0.4771\n",
      "372/500 [=====================>........] - ETA: 2:11 - loss: 1.8692 - regression_loss: 1.3924 - classification_loss: 0.4767\n",
      "373/500 [=====================>........] - ETA: 2:10 - loss: 1.8709 - regression_loss: 1.3934 - classification_loss: 0.4776\n",
      "374/500 [=====================>........] - ETA: 2:09 - loss: 1.8708 - regression_loss: 1.3926 - classification_loss: 0.4781\n",
      "375/500 [=====================>........] - ETA: 2:08 - loss: 1.8703 - regression_loss: 1.3921 - classification_loss: 0.4782\n",
      "376/500 [=====================>........] - ETA: 2:07 - loss: 1.8727 - regression_loss: 1.3931 - classification_loss: 0.4796\n",
      "377/500 [=====================>........] - ETA: 2:06 - loss: 1.8737 - regression_loss: 1.3940 - classification_loss: 0.4797\n",
      "378/500 [=====================>........] - ETA: 2:05 - loss: 1.8729 - regression_loss: 1.3934 - classification_loss: 0.4795\n",
      "379/500 [=====================>........] - ETA: 2:04 - loss: 1.8723 - regression_loss: 1.3930 - classification_loss: 0.4792\n",
      "380/500 [=====================>........] - ETA: 2:03 - loss: 1.8737 - regression_loss: 1.3939 - classification_loss: 0.4798\n",
      "381/500 [=====================>........] - ETA: 2:02 - loss: 1.8724 - regression_loss: 1.3930 - classification_loss: 0.4794\n",
      "382/500 [=====================>........] - ETA: 2:01 - loss: 1.8724 - regression_loss: 1.3929 - classification_loss: 0.4795\n",
      "383/500 [=====================>........] - ETA: 2:00 - loss: 1.8737 - regression_loss: 1.3936 - classification_loss: 0.4801\n",
      "384/500 [======================>.......] - ETA: 1:59 - loss: 1.8733 - regression_loss: 1.3935 - classification_loss: 0.4799\n",
      "385/500 [======================>.......] - ETA: 1:58 - loss: 1.8723 - regression_loss: 1.3927 - classification_loss: 0.4796\n",
      "386/500 [======================>.......] - ETA: 1:57 - loss: 1.8737 - regression_loss: 1.3937 - classification_loss: 0.4800\n",
      "387/500 [======================>.......] - ETA: 1:56 - loss: 1.8724 - regression_loss: 1.3929 - classification_loss: 0.4795\n",
      "388/500 [======================>.......] - ETA: 1:55 - loss: 1.8704 - regression_loss: 1.3916 - classification_loss: 0.4787\n",
      "389/500 [======================>.......] - ETA: 1:54 - loss: 1.8686 - regression_loss: 1.3905 - classification_loss: 0.4781\n",
      "390/500 [======================>.......] - ETA: 1:53 - loss: 1.8667 - regression_loss: 1.3891 - classification_loss: 0.4776\n",
      "391/500 [======================>.......] - ETA: 1:52 - loss: 1.8684 - regression_loss: 1.3904 - classification_loss: 0.4780\n",
      "392/500 [======================>.......] - ETA: 1:51 - loss: 1.8692 - regression_loss: 1.3904 - classification_loss: 0.4788\n",
      "393/500 [======================>.......] - ETA: 1:49 - loss: 1.8694 - regression_loss: 1.3905 - classification_loss: 0.4790\n",
      "394/500 [======================>.......] - ETA: 1:48 - loss: 1.8707 - regression_loss: 1.3916 - classification_loss: 0.4791\n",
      "395/500 [======================>.......] - ETA: 1:47 - loss: 1.8726 - regression_loss: 1.3926 - classification_loss: 0.4800\n",
      "396/500 [======================>.......] - ETA: 1:46 - loss: 1.8755 - regression_loss: 1.3945 - classification_loss: 0.4810\n",
      "397/500 [======================>.......] - ETA: 1:45 - loss: 1.8735 - regression_loss: 1.3931 - classification_loss: 0.4804\n",
      "398/500 [======================>.......] - ETA: 1:44 - loss: 1.8723 - regression_loss: 1.3921 - classification_loss: 0.4802\n",
      "399/500 [======================>.......] - ETA: 1:43 - loss: 1.8712 - regression_loss: 1.3912 - classification_loss: 0.4801\n",
      "400/500 [=======================>......] - ETA: 1:42 - loss: 1.8723 - regression_loss: 1.3922 - classification_loss: 0.4801\n",
      "401/500 [=======================>......] - ETA: 1:41 - loss: 1.8705 - regression_loss: 1.3909 - classification_loss: 0.4797\n",
      "402/500 [=======================>......] - ETA: 1:40 - loss: 1.8716 - regression_loss: 1.3921 - classification_loss: 0.4795\n",
      "403/500 [=======================>......] - ETA: 1:39 - loss: 1.8699 - regression_loss: 1.3909 - classification_loss: 0.4790\n",
      "404/500 [=======================>......] - ETA: 1:38 - loss: 1.8684 - regression_loss: 1.3898 - classification_loss: 0.4786\n",
      "405/500 [=======================>......] - ETA: 1:37 - loss: 1.8676 - regression_loss: 1.3889 - classification_loss: 0.4787\n",
      "406/500 [=======================>......] - ETA: 1:36 - loss: 1.8679 - regression_loss: 1.3892 - classification_loss: 0.4787\n",
      "407/500 [=======================>......] - ETA: 1:35 - loss: 1.8661 - regression_loss: 1.3880 - classification_loss: 0.4781\n",
      "408/500 [=======================>......] - ETA: 1:34 - loss: 1.8649 - regression_loss: 1.3870 - classification_loss: 0.4778\n",
      "409/500 [=======================>......] - ETA: 1:33 - loss: 1.8657 - regression_loss: 1.3879 - classification_loss: 0.4778\n",
      "410/500 [=======================>......] - ETA: 1:32 - loss: 1.8640 - regression_loss: 1.3864 - classification_loss: 0.4776\n",
      "411/500 [=======================>......] - ETA: 1:31 - loss: 1.8648 - regression_loss: 1.3865 - classification_loss: 0.4783\n",
      "412/500 [=======================>......] - ETA: 1:30 - loss: 1.8663 - regression_loss: 1.3876 - classification_loss: 0.4787\n",
      "413/500 [=======================>......] - ETA: 1:29 - loss: 1.8686 - regression_loss: 1.3897 - classification_loss: 0.4790\n",
      "414/500 [=======================>......] - ETA: 1:28 - loss: 1.8671 - regression_loss: 1.3886 - classification_loss: 0.4785\n",
      "415/500 [=======================>......] - ETA: 1:27 - loss: 1.8677 - regression_loss: 1.3887 - classification_loss: 0.4790\n",
      "416/500 [=======================>......] - ETA: 1:26 - loss: 1.8670 - regression_loss: 1.3881 - classification_loss: 0.4789\n",
      "417/500 [========================>.....] - ETA: 1:25 - loss: 1.8691 - regression_loss: 1.3896 - classification_loss: 0.4795\n",
      "418/500 [========================>.....] - ETA: 1:24 - loss: 1.8697 - regression_loss: 1.3901 - classification_loss: 0.4796\n",
      "419/500 [========================>.....] - ETA: 1:23 - loss: 1.8704 - regression_loss: 1.3909 - classification_loss: 0.4795\n",
      "420/500 [========================>.....] - ETA: 1:22 - loss: 1.8695 - regression_loss: 1.3902 - classification_loss: 0.4793\n",
      "421/500 [========================>.....] - ETA: 1:21 - loss: 1.8672 - regression_loss: 1.3886 - classification_loss: 0.4786\n",
      "422/500 [========================>.....] - ETA: 1:20 - loss: 1.8657 - regression_loss: 1.3875 - classification_loss: 0.4782\n",
      "423/500 [========================>.....] - ETA: 1:19 - loss: 1.8657 - regression_loss: 1.3876 - classification_loss: 0.4781\n",
      "424/500 [========================>.....] - ETA: 1:18 - loss: 1.8665 - regression_loss: 1.3884 - classification_loss: 0.4781\n",
      "425/500 [========================>.....] - ETA: 1:17 - loss: 1.8670 - regression_loss: 1.3886 - classification_loss: 0.4784\n",
      "426/500 [========================>.....] - ETA: 1:16 - loss: 1.8658 - regression_loss: 1.3877 - classification_loss: 0.4781\n",
      "427/500 [========================>.....] - ETA: 1:15 - loss: 1.8657 - regression_loss: 1.3877 - classification_loss: 0.4781\n",
      "428/500 [========================>.....] - ETA: 1:14 - loss: 1.8638 - regression_loss: 1.3864 - classification_loss: 0.4774\n",
      "429/500 [========================>.....] - ETA: 1:13 - loss: 1.8658 - regression_loss: 1.3882 - classification_loss: 0.4777\n",
      "430/500 [========================>.....] - ETA: 1:11 - loss: 1.8664 - regression_loss: 1.3886 - classification_loss: 0.4778\n",
      "431/500 [========================>.....] - ETA: 1:10 - loss: 1.8651 - regression_loss: 1.3876 - classification_loss: 0.4775\n",
      "432/500 [========================>.....] - ETA: 1:09 - loss: 1.8650 - regression_loss: 1.3877 - classification_loss: 0.4773\n",
      "433/500 [========================>.....] - ETA: 1:08 - loss: 1.8647 - regression_loss: 1.3871 - classification_loss: 0.4775\n",
      "434/500 [=========================>....] - ETA: 1:07 - loss: 1.8667 - regression_loss: 1.3886 - classification_loss: 0.4781\n",
      "435/500 [=========================>....] - ETA: 1:06 - loss: 1.8661 - regression_loss: 1.3883 - classification_loss: 0.4777\n",
      "436/500 [=========================>....] - ETA: 1:05 - loss: 1.8652 - regression_loss: 1.3879 - classification_loss: 0.4773\n",
      "437/500 [=========================>....] - ETA: 1:04 - loss: 1.8657 - regression_loss: 1.3885 - classification_loss: 0.4771\n",
      "438/500 [=========================>....] - ETA: 1:03 - loss: 1.8653 - regression_loss: 1.3883 - classification_loss: 0.4770\n",
      "439/500 [=========================>....] - ETA: 1:02 - loss: 1.8657 - regression_loss: 1.3886 - classification_loss: 0.4771\n",
      "440/500 [=========================>....] - ETA: 1:01 - loss: 1.8647 - regression_loss: 1.3878 - classification_loss: 0.4769\n",
      "441/500 [=========================>....] - ETA: 1:00 - loss: 1.8667 - regression_loss: 1.3894 - classification_loss: 0.4773\n",
      "442/500 [=========================>....] - ETA: 59s - loss: 1.8687 - regression_loss: 1.3908 - classification_loss: 0.4779 \n",
      "443/500 [=========================>....] - ETA: 58s - loss: 1.8689 - regression_loss: 1.3914 - classification_loss: 0.4776\n",
      "444/500 [=========================>....] - ETA: 57s - loss: 1.8687 - regression_loss: 1.3911 - classification_loss: 0.4775\n",
      "445/500 [=========================>....] - ETA: 56s - loss: 1.8686 - regression_loss: 1.3909 - classification_loss: 0.4777\n",
      "446/500 [=========================>....] - ETA: 55s - loss: 1.8686 - regression_loss: 1.3910 - classification_loss: 0.4776\n",
      "447/500 [=========================>....] - ETA: 54s - loss: 1.8691 - regression_loss: 1.3915 - classification_loss: 0.4775\n",
      "448/500 [=========================>....] - ETA: 53s - loss: 1.8684 - regression_loss: 1.3913 - classification_loss: 0.4771\n",
      "449/500 [=========================>....] - ETA: 52s - loss: 1.8684 - regression_loss: 1.3914 - classification_loss: 0.4769\n",
      "450/500 [==========================>...] - ETA: 51s - loss: 1.8698 - regression_loss: 1.3924 - classification_loss: 0.4774\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 1.8701 - regression_loss: 1.3926 - classification_loss: 0.4775\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 1.8702 - regression_loss: 1.3930 - classification_loss: 0.4772\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.8689 - regression_loss: 1.3922 - classification_loss: 0.4768\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.8686 - regression_loss: 1.3918 - classification_loss: 0.4767\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.8676 - regression_loss: 1.3911 - classification_loss: 0.4766\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.8680 - regression_loss: 1.3915 - classification_loss: 0.4764\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.8701 - regression_loss: 1.3928 - classification_loss: 0.4774\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.8693 - regression_loss: 1.3923 - classification_loss: 0.4770\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.8711 - regression_loss: 1.3935 - classification_loss: 0.4776\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.8710 - regression_loss: 1.3935 - classification_loss: 0.4775\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 1.8699 - regression_loss: 1.3928 - classification_loss: 0.4770\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 1.8684 - regression_loss: 1.3915 - classification_loss: 0.4769\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 1.8677 - regression_loss: 1.3912 - classification_loss: 0.4764\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 1.8683 - regression_loss: 1.3921 - classification_loss: 0.4762\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 1.8667 - regression_loss: 1.3909 - classification_loss: 0.4758\n",
      "466/500 [==========================>...] - ETA: 34s - loss: 1.8678 - regression_loss: 1.3917 - classification_loss: 0.4762\n",
      "467/500 [===========================>..] - ETA: 34s - loss: 1.8659 - regression_loss: 1.3902 - classification_loss: 0.4757\n",
      "468/500 [===========================>..] - ETA: 33s - loss: 1.8653 - regression_loss: 1.3898 - classification_loss: 0.4755\n",
      "469/500 [===========================>..] - ETA: 32s - loss: 1.8683 - regression_loss: 1.3907 - classification_loss: 0.4775\n",
      "470/500 [===========================>..] - ETA: 31s - loss: 1.8688 - regression_loss: 1.3908 - classification_loss: 0.4780\n",
      "471/500 [===========================>..] - ETA: 29s - loss: 1.8713 - regression_loss: 1.3924 - classification_loss: 0.4789\n",
      "472/500 [===========================>..] - ETA: 28s - loss: 1.8725 - regression_loss: 1.3933 - classification_loss: 0.4792\n",
      "473/500 [===========================>..] - ETA: 27s - loss: 1.8733 - regression_loss: 1.3940 - classification_loss: 0.4793\n",
      "474/500 [===========================>..] - ETA: 26s - loss: 1.8723 - regression_loss: 1.3934 - classification_loss: 0.4789\n",
      "475/500 [===========================>..] - ETA: 25s - loss: 1.8753 - regression_loss: 1.3956 - classification_loss: 0.4797\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 1.8753 - regression_loss: 1.3956 - classification_loss: 0.4797\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.8745 - regression_loss: 1.3950 - classification_loss: 0.4796\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.8754 - regression_loss: 1.3957 - classification_loss: 0.4797\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.8751 - regression_loss: 1.3955 - classification_loss: 0.4797\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.8769 - regression_loss: 1.3970 - classification_loss: 0.4799\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.8788 - regression_loss: 1.3981 - classification_loss: 0.4807\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.8791 - regression_loss: 1.3985 - classification_loss: 0.4807\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.8799 - regression_loss: 1.3993 - classification_loss: 0.4806\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.8804 - regression_loss: 1.3998 - classification_loss: 0.4806\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.8800 - regression_loss: 1.3995 - classification_loss: 0.4805\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.8796 - regression_loss: 1.3992 - classification_loss: 0.4804\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.8798 - regression_loss: 1.3994 - classification_loss: 0.4804\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.8811 - regression_loss: 1.3998 - classification_loss: 0.4813\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.8811 - regression_loss: 1.3999 - classification_loss: 0.4812\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.8827 - regression_loss: 1.4006 - classification_loss: 0.4820\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.8831 - regression_loss: 1.4011 - classification_loss: 0.4821 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.8833 - regression_loss: 1.4012 - classification_loss: 0.4821\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.8835 - regression_loss: 1.4015 - classification_loss: 0.4820\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.8851 - regression_loss: 1.4029 - classification_loss: 0.4822\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.8849 - regression_loss: 1.4028 - classification_loss: 0.4820\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.8850 - regression_loss: 1.4031 - classification_loss: 0.4819\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.8848 - regression_loss: 1.4028 - classification_loss: 0.4820\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.8841 - regression_loss: 1.4024 - classification_loss: 0.4817\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.8842 - regression_loss: 1.4021 - classification_loss: 0.4821\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.8840 - regression_loss: 1.4021 - classification_loss: 0.4819\n",
      "Epoch 00032: saving model to ./snapshots\\resnet50_csv_32.h5\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 9.999999092680235e-13.\n",
      "\n",
      "500/500 [==============================] - 521s 1s/step - loss: 1.8840 - regression_loss: 1.4021 - classification_loss: 0.4819\n",
      "Epoch 33/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 1.9691 - regression_loss: 1.4434 - classification_loss: 0.5257\n",
      "  2/500 [..............................] - ETA: 4:06 - loss: 1.8205 - regression_loss: 1.3579 - classification_loss: 0.4627\n",
      "  3/500 [..............................] - ETA: 5:26 - loss: 1.5967 - regression_loss: 1.1925 - classification_loss: 0.4042\n",
      "  4/500 [..............................] - ETA: 6:18 - loss: 1.7408 - regression_loss: 1.2584 - classification_loss: 0.4824\n",
      "  5/500 [..............................] - ETA: 6:42 - loss: 1.7728 - regression_loss: 1.2929 - classification_loss: 0.4799\n",
      "  6/500 [..............................] - ETA: 7:03 - loss: 1.9596 - regression_loss: 1.4076 - classification_loss: 0.5520\n",
      "  7/500 [..............................] - ETA: 7:12 - loss: 2.0487 - regression_loss: 1.4457 - classification_loss: 0.6029\n",
      "  8/500 [..............................] - ETA: 7:17 - loss: 2.0799 - regression_loss: 1.5060 - classification_loss: 0.5739\n",
      "  9/500 [..............................] - ETA: 7:21 - loss: 2.0569 - regression_loss: 1.5076 - classification_loss: 0.5493\n",
      " 10/500 [..............................] - ETA: 7:26 - loss: 2.1400 - regression_loss: 1.5642 - classification_loss: 0.5758\n",
      " 11/500 [..............................] - ETA: 7:28 - loss: 2.1585 - regression_loss: 1.5658 - classification_loss: 0.5927\n",
      " 12/500 [..............................] - ETA: 7:30 - loss: 2.0704 - regression_loss: 1.5058 - classification_loss: 0.5645\n",
      " 13/500 [..............................] - ETA: 7:35 - loss: 2.0377 - regression_loss: 1.4867 - classification_loss: 0.5510\n",
      " 14/500 [..............................] - ETA: 7:39 - loss: 2.0418 - regression_loss: 1.4885 - classification_loss: 0.5534\n",
      " 15/500 [..............................] - ETA: 7:39 - loss: 2.0957 - regression_loss: 1.5284 - classification_loss: 0.5674\n",
      " 16/500 [..............................] - ETA: 7:42 - loss: 2.1067 - regression_loss: 1.5322 - classification_loss: 0.5745\n",
      " 17/500 [>.............................] - ETA: 7:35 - loss: 2.0521 - regression_loss: 1.4939 - classification_loss: 0.5582\n",
      " 18/500 [>.............................] - ETA: 7:40 - loss: 2.0849 - regression_loss: 1.5211 - classification_loss: 0.5638\n",
      " 19/500 [>.............................] - ETA: 7:40 - loss: 2.0466 - regression_loss: 1.4949 - classification_loss: 0.5517\n",
      " 20/500 [>.............................] - ETA: 7:40 - loss: 2.0394 - regression_loss: 1.4847 - classification_loss: 0.5547\n",
      " 21/500 [>.............................] - ETA: 7:41 - loss: 2.0546 - regression_loss: 1.4982 - classification_loss: 0.5563\n",
      " 22/500 [>.............................] - ETA: 7:41 - loss: 2.0340 - regression_loss: 1.4845 - classification_loss: 0.5495\n",
      " 23/500 [>.............................] - ETA: 7:40 - loss: 2.0468 - regression_loss: 1.4970 - classification_loss: 0.5498\n",
      " 24/500 [>.............................] - ETA: 7:39 - loss: 2.0103 - regression_loss: 1.4764 - classification_loss: 0.5340\n",
      " 25/500 [>.............................] - ETA: 7:39 - loss: 1.9936 - regression_loss: 1.4624 - classification_loss: 0.5312\n",
      " 26/500 [>.............................] - ETA: 7:40 - loss: 2.0263 - regression_loss: 1.4856 - classification_loss: 0.5408\n",
      " 27/500 [>.............................] - ETA: 7:39 - loss: 2.0136 - regression_loss: 1.4794 - classification_loss: 0.5342\n",
      " 28/500 [>.............................] - ETA: 7:40 - loss: 2.0013 - regression_loss: 1.4709 - classification_loss: 0.5304\n",
      " 29/500 [>.............................] - ETA: 7:41 - loss: 2.0256 - regression_loss: 1.4862 - classification_loss: 0.5394\n",
      " 30/500 [>.............................] - ETA: 7:42 - loss: 2.0252 - regression_loss: 1.4887 - classification_loss: 0.5365\n",
      " 31/500 [>.............................] - ETA: 7:42 - loss: 2.0333 - regression_loss: 1.4981 - classification_loss: 0.5352\n",
      " 32/500 [>.............................] - ETA: 7:43 - loss: 2.0381 - regression_loss: 1.5068 - classification_loss: 0.5313\n",
      " 33/500 [>.............................] - ETA: 7:42 - loss: 2.0340 - regression_loss: 1.5026 - classification_loss: 0.5314\n",
      " 34/500 [=>............................] - ETA: 7:42 - loss: 2.0018 - regression_loss: 1.4772 - classification_loss: 0.5246\n",
      " 35/500 [=>............................] - ETA: 7:41 - loss: 1.9949 - regression_loss: 1.4709 - classification_loss: 0.5240\n",
      " 36/500 [=>............................] - ETA: 7:40 - loss: 1.9926 - regression_loss: 1.4711 - classification_loss: 0.5215\n",
      " 37/500 [=>............................] - ETA: 7:39 - loss: 2.0120 - regression_loss: 1.4851 - classification_loss: 0.5269\n",
      " 38/500 [=>............................] - ETA: 7:38 - loss: 2.0017 - regression_loss: 1.4781 - classification_loss: 0.5236\n",
      " 39/500 [=>............................] - ETA: 7:36 - loss: 1.9878 - regression_loss: 1.4675 - classification_loss: 0.5203\n",
      " 40/500 [=>............................] - ETA: 7:36 - loss: 1.9955 - regression_loss: 1.4745 - classification_loss: 0.5210\n",
      " 41/500 [=>............................] - ETA: 7:35 - loss: 1.9748 - regression_loss: 1.4610 - classification_loss: 0.5138\n",
      " 42/500 [=>............................] - ETA: 7:35 - loss: 1.9842 - regression_loss: 1.4673 - classification_loss: 0.5169\n",
      " 43/500 [=>............................] - ETA: 7:34 - loss: 1.9679 - regression_loss: 1.4516 - classification_loss: 0.5163\n",
      " 44/500 [=>............................] - ETA: 7:34 - loss: 1.9574 - regression_loss: 1.4467 - classification_loss: 0.5108\n",
      " 45/500 [=>............................] - ETA: 7:34 - loss: 1.9550 - regression_loss: 1.4425 - classification_loss: 0.5125\n",
      " 46/500 [=>............................] - ETA: 7:30 - loss: 1.9619 - regression_loss: 1.4484 - classification_loss: 0.5135\n",
      " 47/500 [=>............................] - ETA: 7:32 - loss: 1.9635 - regression_loss: 1.4506 - classification_loss: 0.5129\n",
      " 48/500 [=>............................] - ETA: 7:31 - loss: 1.9700 - regression_loss: 1.4515 - classification_loss: 0.5185\n",
      " 49/500 [=>............................] - ETA: 7:30 - loss: 1.9610 - regression_loss: 1.4431 - classification_loss: 0.5179\n",
      " 50/500 [==>...........................] - ETA: 7:30 - loss: 1.9502 - regression_loss: 1.4353 - classification_loss: 0.5149\n",
      " 51/500 [==>...........................] - ETA: 7:29 - loss: 1.9433 - regression_loss: 1.4293 - classification_loss: 0.5140\n",
      " 52/500 [==>...........................] - ETA: 7:28 - loss: 1.9358 - regression_loss: 1.4269 - classification_loss: 0.5088\n",
      " 53/500 [==>...........................] - ETA: 7:27 - loss: 1.9543 - regression_loss: 1.4371 - classification_loss: 0.5172\n",
      " 54/500 [==>...........................] - ETA: 7:26 - loss: 1.9573 - regression_loss: 1.4399 - classification_loss: 0.5174\n",
      " 55/500 [==>...........................] - ETA: 7:26 - loss: 1.9679 - regression_loss: 1.4468 - classification_loss: 0.5211\n",
      " 56/500 [==>...........................] - ETA: 7:26 - loss: 1.9675 - regression_loss: 1.4466 - classification_loss: 0.5209\n",
      " 57/500 [==>...........................] - ETA: 7:24 - loss: 1.9523 - regression_loss: 1.4333 - classification_loss: 0.5190\n",
      " 58/500 [==>...........................] - ETA: 7:23 - loss: 1.9560 - regression_loss: 1.4360 - classification_loss: 0.5200\n",
      " 59/500 [==>...........................] - ETA: 7:23 - loss: 1.9567 - regression_loss: 1.4350 - classification_loss: 0.5217\n",
      " 60/500 [==>...........................] - ETA: 7:20 - loss: 1.9486 - regression_loss: 1.4298 - classification_loss: 0.5188\n",
      " 61/500 [==>...........................] - ETA: 7:20 - loss: 1.9358 - regression_loss: 1.4217 - classification_loss: 0.5141\n",
      " 62/500 [==>...........................] - ETA: 7:19 - loss: 1.9290 - regression_loss: 1.4177 - classification_loss: 0.5113\n",
      " 63/500 [==>...........................] - ETA: 7:19 - loss: 1.9184 - regression_loss: 1.4105 - classification_loss: 0.5080\n",
      " 64/500 [==>...........................] - ETA: 7:18 - loss: 1.9176 - regression_loss: 1.4117 - classification_loss: 0.5058\n",
      " 65/500 [==>...........................] - ETA: 7:17 - loss: 1.9220 - regression_loss: 1.4159 - classification_loss: 0.5061\n",
      " 66/500 [==>...........................] - ETA: 7:16 - loss: 1.9335 - regression_loss: 1.4257 - classification_loss: 0.5077\n",
      " 67/500 [===>..........................] - ETA: 7:14 - loss: 1.9329 - regression_loss: 1.4255 - classification_loss: 0.5074\n",
      " 68/500 [===>..........................] - ETA: 7:13 - loss: 1.9254 - regression_loss: 1.4200 - classification_loss: 0.5053\n",
      " 69/500 [===>..........................] - ETA: 7:13 - loss: 1.9385 - regression_loss: 1.4259 - classification_loss: 0.5125\n",
      " 70/500 [===>..........................] - ETA: 7:12 - loss: 1.9368 - regression_loss: 1.4254 - classification_loss: 0.5114\n",
      " 71/500 [===>..........................] - ETA: 7:10 - loss: 1.9425 - regression_loss: 1.4316 - classification_loss: 0.5109\n",
      " 72/500 [===>..........................] - ETA: 7:09 - loss: 1.9399 - regression_loss: 1.4304 - classification_loss: 0.5095\n",
      " 73/500 [===>..........................] - ETA: 7:09 - loss: 1.9480 - regression_loss: 1.4362 - classification_loss: 0.5117\n",
      " 74/500 [===>..........................] - ETA: 7:08 - loss: 1.9391 - regression_loss: 1.4296 - classification_loss: 0.5095\n",
      " 75/500 [===>..........................] - ETA: 7:07 - loss: 1.9466 - regression_loss: 1.4373 - classification_loss: 0.5093\n",
      " 76/500 [===>..........................] - ETA: 7:06 - loss: 1.9460 - regression_loss: 1.4369 - classification_loss: 0.5091\n",
      " 77/500 [===>..........................] - ETA: 7:04 - loss: 1.9544 - regression_loss: 1.4443 - classification_loss: 0.5101\n",
      " 78/500 [===>..........................] - ETA: 7:04 - loss: 1.9586 - regression_loss: 1.4486 - classification_loss: 0.5100\n",
      " 79/500 [===>..........................] - ETA: 7:03 - loss: 1.9592 - regression_loss: 1.4495 - classification_loss: 0.5096\n",
      " 80/500 [===>..........................] - ETA: 7:03 - loss: 1.9550 - regression_loss: 1.4471 - classification_loss: 0.5079\n",
      " 81/500 [===>..........................] - ETA: 7:02 - loss: 1.9612 - regression_loss: 1.4516 - classification_loss: 0.5096\n",
      " 82/500 [===>..........................] - ETA: 7:01 - loss: 1.9790 - regression_loss: 1.4642 - classification_loss: 0.5148\n",
      " 83/500 [===>..........................] - ETA: 7:00 - loss: 1.9820 - regression_loss: 1.4672 - classification_loss: 0.5148\n",
      " 84/500 [====>.........................] - ETA: 6:59 - loss: 1.9844 - regression_loss: 1.4700 - classification_loss: 0.5144\n",
      " 85/500 [====>.........................] - ETA: 6:58 - loss: 1.9809 - regression_loss: 1.4683 - classification_loss: 0.5125\n",
      " 86/500 [====>.........................] - ETA: 6:57 - loss: 1.9760 - regression_loss: 1.4659 - classification_loss: 0.5101\n",
      " 87/500 [====>.........................] - ETA: 6:56 - loss: 1.9723 - regression_loss: 1.4646 - classification_loss: 0.5077\n",
      " 88/500 [====>.........................] - ETA: 6:55 - loss: 1.9778 - regression_loss: 1.4693 - classification_loss: 0.5085\n",
      " 89/500 [====>.........................] - ETA: 6:54 - loss: 1.9852 - regression_loss: 1.4750 - classification_loss: 0.5102\n",
      " 90/500 [====>.........................] - ETA: 6:53 - loss: 1.9907 - regression_loss: 1.4796 - classification_loss: 0.5111\n",
      " 91/500 [====>.........................] - ETA: 6:52 - loss: 2.0023 - regression_loss: 1.4870 - classification_loss: 0.5153\n",
      " 92/500 [====>.........................] - ETA: 6:51 - loss: 2.0042 - regression_loss: 1.4886 - classification_loss: 0.5155\n",
      " 93/500 [====>.........................] - ETA: 6:50 - loss: 1.9985 - regression_loss: 1.4826 - classification_loss: 0.5159\n",
      " 94/500 [====>.........................] - ETA: 6:49 - loss: 2.0000 - regression_loss: 1.4851 - classification_loss: 0.5149\n",
      " 95/500 [====>.........................] - ETA: 6:48 - loss: 2.0014 - regression_loss: 1.4854 - classification_loss: 0.5160\n",
      " 96/500 [====>.........................] - ETA: 6:47 - loss: 1.9995 - regression_loss: 1.4839 - classification_loss: 0.5156\n",
      " 97/500 [====>.........................] - ETA: 6:46 - loss: 1.9921 - regression_loss: 1.4775 - classification_loss: 0.5146\n",
      " 98/500 [====>.........................] - ETA: 6:46 - loss: 1.9887 - regression_loss: 1.4743 - classification_loss: 0.5144\n",
      " 99/500 [====>.........................] - ETA: 6:44 - loss: 1.9842 - regression_loss: 1.4684 - classification_loss: 0.5157\n",
      "100/500 [=====>........................] - ETA: 6:43 - loss: 1.9920 - regression_loss: 1.4713 - classification_loss: 0.5208\n",
      "101/500 [=====>........................] - ETA: 6:42 - loss: 1.9872 - regression_loss: 1.4678 - classification_loss: 0.5195\n",
      "102/500 [=====>........................] - ETA: 6:41 - loss: 1.9866 - regression_loss: 1.4643 - classification_loss: 0.5222\n",
      "103/500 [=====>........................] - ETA: 6:40 - loss: 1.9898 - regression_loss: 1.4660 - classification_loss: 0.5239\n",
      "104/500 [=====>........................] - ETA: 6:39 - loss: 1.9864 - regression_loss: 1.4636 - classification_loss: 0.5228\n",
      "105/500 [=====>........................] - ETA: 6:38 - loss: 1.9902 - regression_loss: 1.4662 - classification_loss: 0.5240\n",
      "106/500 [=====>........................] - ETA: 6:37 - loss: 1.9858 - regression_loss: 1.4629 - classification_loss: 0.5229\n",
      "107/500 [=====>........................] - ETA: 6:36 - loss: 1.9925 - regression_loss: 1.4690 - classification_loss: 0.5235\n",
      "108/500 [=====>........................] - ETA: 6:34 - loss: 1.9994 - regression_loss: 1.4728 - classification_loss: 0.5266\n",
      "109/500 [=====>........................] - ETA: 6:34 - loss: 1.9982 - regression_loss: 1.4725 - classification_loss: 0.5257\n",
      "110/500 [=====>........................] - ETA: 6:33 - loss: 1.9964 - regression_loss: 1.4711 - classification_loss: 0.5253\n",
      "111/500 [=====>........................] - ETA: 6:32 - loss: 2.0031 - regression_loss: 1.4782 - classification_loss: 0.5249\n",
      "112/500 [=====>........................] - ETA: 6:31 - loss: 2.0054 - regression_loss: 1.4791 - classification_loss: 0.5263\n",
      "113/500 [=====>........................] - ETA: 6:30 - loss: 2.0083 - regression_loss: 1.4827 - classification_loss: 0.5255\n",
      "114/500 [=====>........................] - ETA: 6:28 - loss: 2.0027 - regression_loss: 1.4784 - classification_loss: 0.5243\n",
      "115/500 [=====>........................] - ETA: 6:27 - loss: 2.0006 - regression_loss: 1.4764 - classification_loss: 0.5242\n",
      "116/500 [=====>........................] - ETA: 6:26 - loss: 2.0011 - regression_loss: 1.4773 - classification_loss: 0.5237\n",
      "117/500 [======>.......................] - ETA: 6:25 - loss: 1.9995 - regression_loss: 1.4753 - classification_loss: 0.5242\n",
      "118/500 [======>.......................] - ETA: 6:24 - loss: 2.0014 - regression_loss: 1.4786 - classification_loss: 0.5228\n",
      "119/500 [======>.......................] - ETA: 6:23 - loss: 2.0001 - regression_loss: 1.4782 - classification_loss: 0.5219\n",
      "120/500 [======>.......................] - ETA: 6:22 - loss: 1.9974 - regression_loss: 1.4765 - classification_loss: 0.5210\n",
      "121/500 [======>.......................] - ETA: 6:21 - loss: 2.0039 - regression_loss: 1.4809 - classification_loss: 0.5230\n",
      "122/500 [======>.......................] - ETA: 6:20 - loss: 1.9989 - regression_loss: 1.4777 - classification_loss: 0.5212\n",
      "123/500 [======>.......................] - ETA: 6:19 - loss: 1.9986 - regression_loss: 1.4767 - classification_loss: 0.5219\n",
      "124/500 [======>.......................] - ETA: 6:18 - loss: 1.9987 - regression_loss: 1.4762 - classification_loss: 0.5225\n",
      "125/500 [======>.......................] - ETA: 6:17 - loss: 1.9990 - regression_loss: 1.4754 - classification_loss: 0.5237\n",
      "126/500 [======>.......................] - ETA: 6:16 - loss: 2.0020 - regression_loss: 1.4786 - classification_loss: 0.5234\n",
      "127/500 [======>.......................] - ETA: 6:15 - loss: 1.9986 - regression_loss: 1.4768 - classification_loss: 0.5217\n",
      "128/500 [======>.......................] - ETA: 6:14 - loss: 1.9992 - regression_loss: 1.4777 - classification_loss: 0.5215\n",
      "129/500 [======>.......................] - ETA: 6:14 - loss: 1.9965 - regression_loss: 1.4764 - classification_loss: 0.5202\n",
      "130/500 [======>.......................] - ETA: 6:13 - loss: 1.9942 - regression_loss: 1.4748 - classification_loss: 0.5194\n",
      "131/500 [======>.......................] - ETA: 6:12 - loss: 1.9948 - regression_loss: 1.4741 - classification_loss: 0.5207\n",
      "132/500 [======>.......................] - ETA: 6:11 - loss: 1.9927 - regression_loss: 1.4730 - classification_loss: 0.5196\n",
      "133/500 [======>.......................] - ETA: 6:10 - loss: 1.9858 - regression_loss: 1.4676 - classification_loss: 0.5182\n",
      "134/500 [=======>......................] - ETA: 6:09 - loss: 1.9879 - regression_loss: 1.4701 - classification_loss: 0.5178\n",
      "135/500 [=======>......................] - ETA: 6:08 - loss: 1.9895 - regression_loss: 1.4723 - classification_loss: 0.5172\n",
      "136/500 [=======>......................] - ETA: 6:06 - loss: 1.9919 - regression_loss: 1.4731 - classification_loss: 0.5188\n",
      "137/500 [=======>......................] - ETA: 6:06 - loss: 1.9913 - regression_loss: 1.4728 - classification_loss: 0.5185\n",
      "138/500 [=======>......................] - ETA: 6:04 - loss: 1.9862 - regression_loss: 1.4689 - classification_loss: 0.5172\n",
      "139/500 [=======>......................] - ETA: 6:04 - loss: 1.9793 - regression_loss: 1.4643 - classification_loss: 0.5149\n",
      "140/500 [=======>......................] - ETA: 6:03 - loss: 1.9817 - regression_loss: 1.4663 - classification_loss: 0.5155\n",
      "141/500 [=======>......................] - ETA: 6:02 - loss: 1.9790 - regression_loss: 1.4644 - classification_loss: 0.5147\n",
      "142/500 [=======>......................] - ETA: 6:00 - loss: 1.9804 - regression_loss: 1.4658 - classification_loss: 0.5146\n",
      "143/500 [=======>......................] - ETA: 6:00 - loss: 1.9770 - regression_loss: 1.4644 - classification_loss: 0.5126\n",
      "144/500 [=======>......................] - ETA: 5:58 - loss: 1.9709 - regression_loss: 1.4597 - classification_loss: 0.5111\n",
      "145/500 [=======>......................] - ETA: 5:57 - loss: 1.9729 - regression_loss: 1.4608 - classification_loss: 0.5122\n",
      "146/500 [=======>......................] - ETA: 5:56 - loss: 1.9799 - regression_loss: 1.4653 - classification_loss: 0.5146\n",
      "147/500 [=======>......................] - ETA: 5:55 - loss: 1.9731 - regression_loss: 1.4600 - classification_loss: 0.5130\n",
      "148/500 [=======>......................] - ETA: 5:54 - loss: 1.9782 - regression_loss: 1.4632 - classification_loss: 0.5150\n",
      "149/500 [=======>......................] - ETA: 5:53 - loss: 1.9811 - regression_loss: 1.4657 - classification_loss: 0.5154\n",
      "150/500 [========>.....................] - ETA: 5:53 - loss: 1.9794 - regression_loss: 1.4647 - classification_loss: 0.5147\n",
      "151/500 [========>.....................] - ETA: 5:51 - loss: 1.9827 - regression_loss: 1.4670 - classification_loss: 0.5157\n",
      "152/500 [========>.....................] - ETA: 5:50 - loss: 1.9860 - regression_loss: 1.4697 - classification_loss: 0.5163\n",
      "153/500 [========>.....................] - ETA: 5:49 - loss: 1.9877 - regression_loss: 1.4709 - classification_loss: 0.5168\n",
      "154/500 [========>.....................] - ETA: 5:48 - loss: 1.9855 - regression_loss: 1.4693 - classification_loss: 0.5163\n",
      "155/500 [========>.....................] - ETA: 5:48 - loss: 1.9908 - regression_loss: 1.4723 - classification_loss: 0.5184\n",
      "156/500 [========>.....................] - ETA: 5:47 - loss: 1.9947 - regression_loss: 1.4757 - classification_loss: 0.5191\n",
      "157/500 [========>.....................] - ETA: 5:46 - loss: 1.9946 - regression_loss: 1.4761 - classification_loss: 0.5185\n",
      "158/500 [========>.....................] - ETA: 5:45 - loss: 1.9997 - regression_loss: 1.4788 - classification_loss: 0.5209\n",
      "159/500 [========>.....................] - ETA: 5:44 - loss: 1.9985 - regression_loss: 1.4784 - classification_loss: 0.5201\n",
      "160/500 [========>.....................] - ETA: 5:43 - loss: 1.9982 - regression_loss: 1.4785 - classification_loss: 0.5196\n",
      "161/500 [========>.....................] - ETA: 5:42 - loss: 1.9947 - regression_loss: 1.4764 - classification_loss: 0.5183\n",
      "162/500 [========>.....................] - ETA: 5:41 - loss: 1.9983 - regression_loss: 1.4801 - classification_loss: 0.5182\n",
      "163/500 [========>.....................] - ETA: 5:40 - loss: 1.9941 - regression_loss: 1.4771 - classification_loss: 0.5170\n",
      "164/500 [========>.....................] - ETA: 5:39 - loss: 1.9900 - regression_loss: 1.4743 - classification_loss: 0.5157\n",
      "165/500 [========>.....................] - ETA: 5:38 - loss: 1.9882 - regression_loss: 1.4737 - classification_loss: 0.5145\n",
      "166/500 [========>.....................] - ETA: 5:37 - loss: 1.9971 - regression_loss: 1.4792 - classification_loss: 0.5179\n",
      "167/500 [=========>....................] - ETA: 5:36 - loss: 1.9933 - regression_loss: 1.4752 - classification_loss: 0.5180\n",
      "168/500 [=========>....................] - ETA: 5:35 - loss: 1.9896 - regression_loss: 1.4727 - classification_loss: 0.5169\n",
      "169/500 [=========>....................] - ETA: 5:34 - loss: 1.9884 - regression_loss: 1.4717 - classification_loss: 0.5167\n",
      "170/500 [=========>....................] - ETA: 5:34 - loss: 1.9917 - regression_loss: 1.4726 - classification_loss: 0.5191\n",
      "171/500 [=========>....................] - ETA: 5:33 - loss: 1.9883 - regression_loss: 1.4707 - classification_loss: 0.5176\n",
      "172/500 [=========>....................] - ETA: 5:32 - loss: 1.9853 - regression_loss: 1.4684 - classification_loss: 0.5169\n",
      "173/500 [=========>....................] - ETA: 5:31 - loss: 1.9863 - regression_loss: 1.4681 - classification_loss: 0.5182\n",
      "174/500 [=========>....................] - ETA: 5:30 - loss: 1.9852 - regression_loss: 1.4673 - classification_loss: 0.5179\n",
      "175/500 [=========>....................] - ETA: 5:29 - loss: 1.9811 - regression_loss: 1.4646 - classification_loss: 0.5165\n",
      "176/500 [=========>....................] - ETA: 5:28 - loss: 1.9818 - regression_loss: 1.4650 - classification_loss: 0.5168\n",
      "177/500 [=========>....................] - ETA: 5:27 - loss: 1.9869 - regression_loss: 1.4678 - classification_loss: 0.5191\n",
      "178/500 [=========>....................] - ETA: 5:26 - loss: 1.9907 - regression_loss: 1.4707 - classification_loss: 0.5200\n",
      "179/500 [=========>....................] - ETA: 5:24 - loss: 1.9957 - regression_loss: 1.4743 - classification_loss: 0.5214\n",
      "180/500 [=========>....................] - ETA: 5:24 - loss: 1.9911 - regression_loss: 1.4703 - classification_loss: 0.5208\n",
      "181/500 [=========>....................] - ETA: 5:23 - loss: 1.9911 - regression_loss: 1.4711 - classification_loss: 0.5200\n",
      "182/500 [=========>....................] - ETA: 5:22 - loss: 1.9934 - regression_loss: 1.4727 - classification_loss: 0.5207\n",
      "183/500 [=========>....................] - ETA: 5:21 - loss: 1.9966 - regression_loss: 1.4761 - classification_loss: 0.5205\n",
      "184/500 [==========>...................] - ETA: 5:20 - loss: 1.9933 - regression_loss: 1.4737 - classification_loss: 0.5197\n",
      "185/500 [==========>...................] - ETA: 5:19 - loss: 1.9955 - regression_loss: 1.4747 - classification_loss: 0.5208\n",
      "186/500 [==========>...................] - ETA: 5:18 - loss: 1.9975 - regression_loss: 1.4757 - classification_loss: 0.5218\n",
      "187/500 [==========>...................] - ETA: 5:17 - loss: 1.9929 - regression_loss: 1.4727 - classification_loss: 0.5202\n",
      "188/500 [==========>...................] - ETA: 5:16 - loss: 1.9904 - regression_loss: 1.4708 - classification_loss: 0.5196\n",
      "189/500 [==========>...................] - ETA: 5:15 - loss: 1.9889 - regression_loss: 1.4693 - classification_loss: 0.5196\n",
      "190/500 [==========>...................] - ETA: 5:14 - loss: 1.9881 - regression_loss: 1.4688 - classification_loss: 0.5193\n",
      "191/500 [==========>...................] - ETA: 5:13 - loss: 1.9922 - regression_loss: 1.4711 - classification_loss: 0.5211\n",
      "192/500 [==========>...................] - ETA: 5:12 - loss: 1.9933 - regression_loss: 1.4725 - classification_loss: 0.5208\n",
      "193/500 [==========>...................] - ETA: 5:11 - loss: 1.9910 - regression_loss: 1.4715 - classification_loss: 0.5196\n",
      "194/500 [==========>...................] - ETA: 5:10 - loss: 1.9921 - regression_loss: 1.4731 - classification_loss: 0.5190\n",
      "195/500 [==========>...................] - ETA: 5:09 - loss: 1.9892 - regression_loss: 1.4712 - classification_loss: 0.5180\n",
      "196/500 [==========>...................] - ETA: 5:08 - loss: 1.9896 - regression_loss: 1.4715 - classification_loss: 0.5182\n",
      "197/500 [==========>...................] - ETA: 5:07 - loss: 1.9865 - regression_loss: 1.4692 - classification_loss: 0.5173\n",
      "198/500 [==========>...................] - ETA: 5:07 - loss: 1.9858 - regression_loss: 1.4685 - classification_loss: 0.5173\n",
      "199/500 [==========>...................] - ETA: 5:06 - loss: 1.9906 - regression_loss: 1.4715 - classification_loss: 0.5191\n",
      "200/500 [===========>..................] - ETA: 5:05 - loss: 1.9891 - regression_loss: 1.4709 - classification_loss: 0.5183\n",
      "201/500 [===========>..................] - ETA: 5:04 - loss: 1.9868 - regression_loss: 1.4695 - classification_loss: 0.5173\n",
      "202/500 [===========>..................] - ETA: 5:03 - loss: 1.9899 - regression_loss: 1.4722 - classification_loss: 0.5177\n",
      "203/500 [===========>..................] - ETA: 5:02 - loss: 1.9889 - regression_loss: 1.4719 - classification_loss: 0.5170\n",
      "204/500 [===========>..................] - ETA: 5:01 - loss: 1.9878 - regression_loss: 1.4708 - classification_loss: 0.5170\n",
      "205/500 [===========>..................] - ETA: 5:00 - loss: 1.9875 - regression_loss: 1.4705 - classification_loss: 0.5170\n",
      "206/500 [===========>..................] - ETA: 4:59 - loss: 1.9858 - regression_loss: 1.4694 - classification_loss: 0.5164\n",
      "207/500 [===========>..................] - ETA: 4:58 - loss: 1.9842 - regression_loss: 1.4690 - classification_loss: 0.5153\n",
      "208/500 [===========>..................] - ETA: 4:57 - loss: 1.9808 - regression_loss: 1.4670 - classification_loss: 0.5138\n",
      "209/500 [===========>..................] - ETA: 4:56 - loss: 1.9823 - regression_loss: 1.4673 - classification_loss: 0.5150\n",
      "210/500 [===========>..................] - ETA: 4:55 - loss: 1.9851 - regression_loss: 1.4702 - classification_loss: 0.5148\n",
      "211/500 [===========>..................] - ETA: 4:54 - loss: 1.9905 - regression_loss: 1.4737 - classification_loss: 0.5168\n",
      "212/500 [===========>..................] - ETA: 4:53 - loss: 1.9922 - regression_loss: 1.4750 - classification_loss: 0.5171\n",
      "213/500 [===========>..................] - ETA: 4:52 - loss: 1.9882 - regression_loss: 1.4726 - classification_loss: 0.5156\n",
      "214/500 [===========>..................] - ETA: 4:50 - loss: 1.9843 - regression_loss: 1.4697 - classification_loss: 0.5146\n",
      "215/500 [===========>..................] - ETA: 4:50 - loss: 1.9855 - regression_loss: 1.4693 - classification_loss: 0.5162\n",
      "216/500 [===========>..................] - ETA: 4:49 - loss: 1.9839 - regression_loss: 1.4681 - classification_loss: 0.5158\n",
      "217/500 [============>.................] - ETA: 4:48 - loss: 1.9835 - regression_loss: 1.4685 - classification_loss: 0.5151\n",
      "218/500 [============>.................] - ETA: 4:47 - loss: 1.9858 - regression_loss: 1.4706 - classification_loss: 0.5152\n",
      "219/500 [============>.................] - ETA: 4:45 - loss: 1.9856 - regression_loss: 1.4707 - classification_loss: 0.5148\n",
      "220/500 [============>.................] - ETA: 4:44 - loss: 1.9847 - regression_loss: 1.4702 - classification_loss: 0.5146\n",
      "221/500 [============>.................] - ETA: 4:43 - loss: 1.9845 - regression_loss: 1.4704 - classification_loss: 0.5142\n",
      "222/500 [============>.................] - ETA: 4:42 - loss: 1.9838 - regression_loss: 1.4697 - classification_loss: 0.5141\n",
      "223/500 [============>.................] - ETA: 4:42 - loss: 1.9823 - regression_loss: 1.4682 - classification_loss: 0.5142\n",
      "224/500 [============>.................] - ETA: 4:41 - loss: 1.9828 - regression_loss: 1.4686 - classification_loss: 0.5142\n",
      "225/500 [============>.................] - ETA: 4:39 - loss: 1.9812 - regression_loss: 1.4668 - classification_loss: 0.5145\n",
      "226/500 [============>.................] - ETA: 4:38 - loss: 1.9777 - regression_loss: 1.4641 - classification_loss: 0.5136\n",
      "227/500 [============>.................] - ETA: 4:37 - loss: 1.9783 - regression_loss: 1.4650 - classification_loss: 0.5133\n",
      "228/500 [============>.................] - ETA: 4:36 - loss: 1.9739 - regression_loss: 1.4612 - classification_loss: 0.5126\n",
      "229/500 [============>.................] - ETA: 4:35 - loss: 1.9753 - regression_loss: 1.4626 - classification_loss: 0.5127\n",
      "230/500 [============>.................] - ETA: 4:34 - loss: 1.9772 - regression_loss: 1.4653 - classification_loss: 0.5119\n",
      "231/500 [============>.................] - ETA: 4:33 - loss: 1.9762 - regression_loss: 1.4644 - classification_loss: 0.5118\n",
      "232/500 [============>.................] - ETA: 4:32 - loss: 1.9736 - regression_loss: 1.4629 - classification_loss: 0.5107\n",
      "233/500 [============>.................] - ETA: 4:31 - loss: 1.9749 - regression_loss: 1.4638 - classification_loss: 0.5111\n",
      "234/500 [=============>................] - ETA: 4:30 - loss: 1.9714 - regression_loss: 1.4612 - classification_loss: 0.5102\n",
      "235/500 [=============>................] - ETA: 4:29 - loss: 1.9701 - regression_loss: 1.4604 - classification_loss: 0.5097\n",
      "236/500 [=============>................] - ETA: 4:28 - loss: 1.9682 - regression_loss: 1.4586 - classification_loss: 0.5096\n",
      "237/500 [=============>................] - ETA: 4:27 - loss: 1.9666 - regression_loss: 1.4573 - classification_loss: 0.5093\n",
      "238/500 [=============>................] - ETA: 4:26 - loss: 1.9667 - regression_loss: 1.4574 - classification_loss: 0.5093\n",
      "239/500 [=============>................] - ETA: 4:25 - loss: 1.9636 - regression_loss: 1.4556 - classification_loss: 0.5080\n",
      "240/500 [=============>................] - ETA: 4:24 - loss: 1.9673 - regression_loss: 1.4578 - classification_loss: 0.5095\n",
      "241/500 [=============>................] - ETA: 4:23 - loss: 1.9656 - regression_loss: 1.4567 - classification_loss: 0.5089\n",
      "242/500 [=============>................] - ETA: 4:22 - loss: 1.9654 - regression_loss: 1.4570 - classification_loss: 0.5085\n",
      "243/500 [=============>................] - ETA: 4:21 - loss: 1.9637 - regression_loss: 1.4557 - classification_loss: 0.5080\n",
      "244/500 [=============>................] - ETA: 4:20 - loss: 1.9651 - regression_loss: 1.4571 - classification_loss: 0.5079\n",
      "245/500 [=============>................] - ETA: 4:19 - loss: 1.9633 - regression_loss: 1.4551 - classification_loss: 0.5082\n",
      "246/500 [=============>................] - ETA: 4:20 - loss: 1.9610 - regression_loss: 1.4534 - classification_loss: 0.5076\n",
      "247/500 [=============>................] - ETA: 4:19 - loss: 1.9589 - regression_loss: 1.4516 - classification_loss: 0.5073\n",
      "248/500 [=============>................] - ETA: 4:18 - loss: 1.9594 - regression_loss: 1.4516 - classification_loss: 0.5078\n",
      "249/500 [=============>................] - ETA: 4:17 - loss: 1.9579 - regression_loss: 1.4507 - classification_loss: 0.5073\n",
      "250/500 [==============>...............] - ETA: 4:16 - loss: 1.9591 - regression_loss: 1.4517 - classification_loss: 0.5074\n",
      "251/500 [==============>...............] - ETA: 4:15 - loss: 1.9560 - regression_loss: 1.4497 - classification_loss: 0.5063\n",
      "252/500 [==============>...............] - ETA: 4:14 - loss: 1.9543 - regression_loss: 1.4489 - classification_loss: 0.5054\n",
      "253/500 [==============>...............] - ETA: 4:13 - loss: 1.9545 - regression_loss: 1.4494 - classification_loss: 0.5051\n",
      "254/500 [==============>...............] - ETA: 4:12 - loss: 1.9582 - regression_loss: 1.4522 - classification_loss: 0.5060\n",
      "255/500 [==============>...............] - ETA: 4:11 - loss: 1.9585 - regression_loss: 1.4524 - classification_loss: 0.5060\n",
      "256/500 [==============>...............] - ETA: 4:10 - loss: 1.9575 - regression_loss: 1.4518 - classification_loss: 0.5057\n",
      "257/500 [==============>...............] - ETA: 4:09 - loss: 1.9552 - regression_loss: 1.4504 - classification_loss: 0.5048\n",
      "258/500 [==============>...............] - ETA: 4:08 - loss: 1.9562 - regression_loss: 1.4508 - classification_loss: 0.5053\n",
      "259/500 [==============>...............] - ETA: 4:07 - loss: 1.9535 - regression_loss: 1.4489 - classification_loss: 0.5047\n",
      "260/500 [==============>...............] - ETA: 4:06 - loss: 1.9528 - regression_loss: 1.4483 - classification_loss: 0.5045\n",
      "261/500 [==============>...............] - ETA: 4:05 - loss: 1.9545 - regression_loss: 1.4486 - classification_loss: 0.5058\n",
      "262/500 [==============>...............] - ETA: 4:04 - loss: 1.9502 - regression_loss: 1.4456 - classification_loss: 0.5047\n",
      "263/500 [==============>...............] - ETA: 4:03 - loss: 1.9524 - regression_loss: 1.4478 - classification_loss: 0.5046\n",
      "264/500 [==============>...............] - ETA: 4:02 - loss: 1.9524 - regression_loss: 1.4481 - classification_loss: 0.5042\n",
      "265/500 [==============>...............] - ETA: 4:01 - loss: 1.9538 - regression_loss: 1.4484 - classification_loss: 0.5053\n",
      "266/500 [==============>...............] - ETA: 4:00 - loss: 1.9558 - regression_loss: 1.4496 - classification_loss: 0.5062\n",
      "267/500 [===============>..............] - ETA: 3:58 - loss: 1.9566 - regression_loss: 1.4509 - classification_loss: 0.5057\n",
      "268/500 [===============>..............] - ETA: 3:58 - loss: 1.9574 - regression_loss: 1.4515 - classification_loss: 0.5058\n",
      "269/500 [===============>..............] - ETA: 3:56 - loss: 1.9572 - regression_loss: 1.4514 - classification_loss: 0.5059\n",
      "270/500 [===============>..............] - ETA: 3:55 - loss: 1.9558 - regression_loss: 1.4505 - classification_loss: 0.5053\n",
      "271/500 [===============>..............] - ETA: 3:54 - loss: 1.9579 - regression_loss: 1.4508 - classification_loss: 0.5070\n",
      "272/500 [===============>..............] - ETA: 3:54 - loss: 1.9577 - regression_loss: 1.4506 - classification_loss: 0.5071\n",
      "273/500 [===============>..............] - ETA: 3:53 - loss: 1.9575 - regression_loss: 1.4507 - classification_loss: 0.5069\n",
      "274/500 [===============>..............] - ETA: 3:51 - loss: 1.9599 - regression_loss: 1.4516 - classification_loss: 0.5083\n",
      "275/500 [===============>..............] - ETA: 3:51 - loss: 1.9591 - regression_loss: 1.4514 - classification_loss: 0.5077\n",
      "276/500 [===============>..............] - ETA: 3:50 - loss: 1.9590 - regression_loss: 1.4517 - classification_loss: 0.5073\n",
      "277/500 [===============>..............] - ETA: 3:48 - loss: 1.9567 - regression_loss: 1.4501 - classification_loss: 0.5066\n",
      "278/500 [===============>..............] - ETA: 3:47 - loss: 1.9548 - regression_loss: 1.4486 - classification_loss: 0.5061\n",
      "279/500 [===============>..............] - ETA: 3:46 - loss: 1.9542 - regression_loss: 1.4484 - classification_loss: 0.5058\n",
      "280/500 [===============>..............] - ETA: 3:45 - loss: 1.9547 - regression_loss: 1.4481 - classification_loss: 0.5066\n",
      "281/500 [===============>..............] - ETA: 3:44 - loss: 1.9545 - regression_loss: 1.4473 - classification_loss: 0.5072\n",
      "282/500 [===============>..............] - ETA: 3:43 - loss: 1.9559 - regression_loss: 1.4483 - classification_loss: 0.5077\n",
      "283/500 [===============>..............] - ETA: 3:42 - loss: 1.9581 - regression_loss: 1.4497 - classification_loss: 0.5084\n",
      "284/500 [================>.............] - ETA: 3:41 - loss: 1.9554 - regression_loss: 1.4478 - classification_loss: 0.5076\n",
      "285/500 [================>.............] - ETA: 3:40 - loss: 1.9571 - regression_loss: 1.4499 - classification_loss: 0.5072\n",
      "286/500 [================>.............] - ETA: 3:39 - loss: 1.9546 - regression_loss: 1.4483 - classification_loss: 0.5062\n",
      "287/500 [================>.............] - ETA: 3:38 - loss: 1.9529 - regression_loss: 1.4476 - classification_loss: 0.5053\n",
      "288/500 [================>.............] - ETA: 3:37 - loss: 1.9523 - regression_loss: 1.4475 - classification_loss: 0.5048\n",
      "289/500 [================>.............] - ETA: 3:36 - loss: 1.9522 - regression_loss: 1.4473 - classification_loss: 0.5049\n",
      "290/500 [================>.............] - ETA: 3:35 - loss: 1.9534 - regression_loss: 1.4483 - classification_loss: 0.5051\n",
      "291/500 [================>.............] - ETA: 3:34 - loss: 1.9540 - regression_loss: 1.4491 - classification_loss: 0.5049\n",
      "292/500 [================>.............] - ETA: 3:33 - loss: 1.9554 - regression_loss: 1.4502 - classification_loss: 0.5052\n",
      "293/500 [================>.............] - ETA: 3:32 - loss: 1.9583 - regression_loss: 1.4528 - classification_loss: 0.5055\n",
      "294/500 [================>.............] - ETA: 3:31 - loss: 1.9583 - regression_loss: 1.4525 - classification_loss: 0.5058\n",
      "295/500 [================>.............] - ETA: 3:30 - loss: 1.9571 - regression_loss: 1.4516 - classification_loss: 0.5055\n",
      "296/500 [================>.............] - ETA: 3:29 - loss: 1.9555 - regression_loss: 1.4509 - classification_loss: 0.5047\n",
      "297/500 [================>.............] - ETA: 3:28 - loss: 1.9546 - regression_loss: 1.4503 - classification_loss: 0.5042\n",
      "298/500 [================>.............] - ETA: 3:27 - loss: 1.9574 - regression_loss: 1.4515 - classification_loss: 0.5059\n",
      "299/500 [================>.............] - ETA: 3:26 - loss: 1.9579 - regression_loss: 1.4524 - classification_loss: 0.5055\n",
      "300/500 [=================>............] - ETA: 3:25 - loss: 1.9598 - regression_loss: 1.4539 - classification_loss: 0.5059\n",
      "301/500 [=================>............] - ETA: 3:24 - loss: 1.9600 - regression_loss: 1.4537 - classification_loss: 0.5063\n",
      "302/500 [=================>............] - ETA: 3:23 - loss: 1.9568 - regression_loss: 1.4512 - classification_loss: 0.5056\n",
      "303/500 [=================>............] - ETA: 3:22 - loss: 1.9568 - regression_loss: 1.4514 - classification_loss: 0.5054\n",
      "304/500 [=================>............] - ETA: 3:21 - loss: 1.9547 - regression_loss: 1.4495 - classification_loss: 0.5052\n",
      "305/500 [=================>............] - ETA: 3:20 - loss: 1.9575 - regression_loss: 1.4517 - classification_loss: 0.5058\n",
      "306/500 [=================>............] - ETA: 3:19 - loss: 1.9554 - regression_loss: 1.4500 - classification_loss: 0.5054\n",
      "307/500 [=================>............] - ETA: 3:18 - loss: 1.9549 - regression_loss: 1.4497 - classification_loss: 0.5052\n",
      "308/500 [=================>............] - ETA: 3:17 - loss: 1.9544 - regression_loss: 1.4494 - classification_loss: 0.5050\n",
      "309/500 [=================>............] - ETA: 3:16 - loss: 1.9559 - regression_loss: 1.4504 - classification_loss: 0.5055\n",
      "310/500 [=================>............] - ETA: 3:15 - loss: 1.9542 - regression_loss: 1.4488 - classification_loss: 0.5054\n",
      "311/500 [=================>............] - ETA: 3:14 - loss: 1.9505 - regression_loss: 1.4462 - classification_loss: 0.5043\n",
      "312/500 [=================>............] - ETA: 3:13 - loss: 1.9477 - regression_loss: 1.4440 - classification_loss: 0.5037\n",
      "313/500 [=================>............] - ETA: 3:12 - loss: 1.9509 - regression_loss: 1.4458 - classification_loss: 0.5051\n",
      "314/500 [=================>............] - ETA: 3:10 - loss: 1.9513 - regression_loss: 1.4455 - classification_loss: 0.5059\n",
      "315/500 [=================>............] - ETA: 3:09 - loss: 1.9504 - regression_loss: 1.4454 - classification_loss: 0.5050\n",
      "316/500 [=================>............] - ETA: 3:08 - loss: 1.9504 - regression_loss: 1.4457 - classification_loss: 0.5047\n",
      "317/500 [==================>...........] - ETA: 3:07 - loss: 1.9529 - regression_loss: 1.4477 - classification_loss: 0.5052\n",
      "318/500 [==================>...........] - ETA: 3:06 - loss: 1.9523 - regression_loss: 1.4472 - classification_loss: 0.5051\n",
      "319/500 [==================>...........] - ETA: 3:05 - loss: 1.9501 - regression_loss: 1.4456 - classification_loss: 0.5044\n",
      "320/500 [==================>...........] - ETA: 3:04 - loss: 1.9509 - regression_loss: 1.4465 - classification_loss: 0.5044\n",
      "321/500 [==================>...........] - ETA: 3:03 - loss: 1.9490 - regression_loss: 1.4451 - classification_loss: 0.5039\n",
      "322/500 [==================>...........] - ETA: 3:02 - loss: 1.9502 - regression_loss: 1.4458 - classification_loss: 0.5044\n",
      "323/500 [==================>...........] - ETA: 3:01 - loss: 1.9531 - regression_loss: 1.4479 - classification_loss: 0.5052\n",
      "324/500 [==================>...........] - ETA: 3:00 - loss: 1.9520 - regression_loss: 1.4474 - classification_loss: 0.5046\n",
      "325/500 [==================>...........] - ETA: 2:59 - loss: 1.9499 - regression_loss: 1.4458 - classification_loss: 0.5041\n",
      "326/500 [==================>...........] - ETA: 2:58 - loss: 1.9474 - regression_loss: 1.4442 - classification_loss: 0.5032\n",
      "327/500 [==================>...........] - ETA: 2:57 - loss: 1.9481 - regression_loss: 1.4451 - classification_loss: 0.5030\n",
      "328/500 [==================>...........] - ETA: 2:56 - loss: 1.9468 - regression_loss: 1.4442 - classification_loss: 0.5026\n",
      "329/500 [==================>...........] - ETA: 2:55 - loss: 1.9444 - regression_loss: 1.4425 - classification_loss: 0.5019\n",
      "330/500 [==================>...........] - ETA: 2:54 - loss: 1.9489 - regression_loss: 1.4454 - classification_loss: 0.5035\n",
      "331/500 [==================>...........] - ETA: 2:53 - loss: 1.9481 - regression_loss: 1.4452 - classification_loss: 0.5029\n",
      "332/500 [==================>...........] - ETA: 2:52 - loss: 1.9481 - regression_loss: 1.4449 - classification_loss: 0.5031\n",
      "333/500 [==================>...........] - ETA: 2:51 - loss: 1.9472 - regression_loss: 1.4446 - classification_loss: 0.5026\n",
      "334/500 [===================>..........] - ETA: 2:50 - loss: 1.9488 - regression_loss: 1.4462 - classification_loss: 0.5026\n",
      "335/500 [===================>..........] - ETA: 2:49 - loss: 1.9493 - regression_loss: 1.4468 - classification_loss: 0.5025\n",
      "336/500 [===================>..........] - ETA: 2:48 - loss: 1.9475 - regression_loss: 1.4455 - classification_loss: 0.5020\n",
      "337/500 [===================>..........] - ETA: 2:47 - loss: 1.9508 - regression_loss: 1.4473 - classification_loss: 0.5035\n",
      "338/500 [===================>..........] - ETA: 2:46 - loss: 1.9511 - regression_loss: 1.4475 - classification_loss: 0.5036\n",
      "339/500 [===================>..........] - ETA: 2:45 - loss: 1.9521 - regression_loss: 1.4486 - classification_loss: 0.5034\n",
      "340/500 [===================>..........] - ETA: 2:44 - loss: 1.9508 - regression_loss: 1.4477 - classification_loss: 0.5032\n",
      "341/500 [===================>..........] - ETA: 2:43 - loss: 1.9514 - regression_loss: 1.4479 - classification_loss: 0.5035\n",
      "342/500 [===================>..........] - ETA: 2:42 - loss: 1.9508 - regression_loss: 1.4474 - classification_loss: 0.5034\n",
      "343/500 [===================>..........] - ETA: 2:41 - loss: 1.9496 - regression_loss: 1.4469 - classification_loss: 0.5027\n",
      "344/500 [===================>..........] - ETA: 2:40 - loss: 1.9493 - regression_loss: 1.4468 - classification_loss: 0.5025\n",
      "345/500 [===================>..........] - ETA: 2:39 - loss: 1.9497 - regression_loss: 1.4473 - classification_loss: 0.5024\n",
      "346/500 [===================>..........] - ETA: 2:38 - loss: 1.9509 - regression_loss: 1.4482 - classification_loss: 0.5026\n",
      "347/500 [===================>..........] - ETA: 2:37 - loss: 1.9511 - regression_loss: 1.4485 - classification_loss: 0.5026\n",
      "348/500 [===================>..........] - ETA: 2:36 - loss: 1.9501 - regression_loss: 1.4478 - classification_loss: 0.5022\n",
      "349/500 [===================>..........] - ETA: 2:35 - loss: 1.9508 - regression_loss: 1.4488 - classification_loss: 0.5020\n",
      "350/500 [====================>.........] - ETA: 2:34 - loss: 1.9513 - regression_loss: 1.4496 - classification_loss: 0.5017\n",
      "351/500 [====================>.........] - ETA: 2:33 - loss: 1.9496 - regression_loss: 1.4485 - classification_loss: 0.5011\n",
      "352/500 [====================>.........] - ETA: 2:32 - loss: 1.9500 - regression_loss: 1.4484 - classification_loss: 0.5016\n",
      "353/500 [====================>.........] - ETA: 2:31 - loss: 1.9484 - regression_loss: 1.4472 - classification_loss: 0.5012\n",
      "354/500 [====================>.........] - ETA: 2:30 - loss: 1.9479 - regression_loss: 1.4470 - classification_loss: 0.5009\n",
      "355/500 [====================>.........] - ETA: 2:29 - loss: 1.9469 - regression_loss: 1.4466 - classification_loss: 0.5003\n",
      "356/500 [====================>.........] - ETA: 2:28 - loss: 1.9449 - regression_loss: 1.4452 - classification_loss: 0.4997\n",
      "357/500 [====================>.........] - ETA: 2:27 - loss: 1.9430 - regression_loss: 1.4440 - classification_loss: 0.4990\n",
      "358/500 [====================>.........] - ETA: 2:26 - loss: 1.9409 - regression_loss: 1.4423 - classification_loss: 0.4987\n",
      "359/500 [====================>.........] - ETA: 2:25 - loss: 1.9413 - regression_loss: 1.4426 - classification_loss: 0.4987\n",
      "360/500 [====================>.........] - ETA: 2:24 - loss: 1.9399 - regression_loss: 1.4417 - classification_loss: 0.4982\n",
      "361/500 [====================>.........] - ETA: 2:23 - loss: 1.9399 - regression_loss: 1.4421 - classification_loss: 0.4978\n",
      "362/500 [====================>.........] - ETA: 2:22 - loss: 1.9428 - regression_loss: 1.4441 - classification_loss: 0.4987\n",
      "363/500 [====================>.........] - ETA: 2:21 - loss: 1.9416 - regression_loss: 1.4434 - classification_loss: 0.4982\n",
      "364/500 [====================>.........] - ETA: 2:20 - loss: 1.9412 - regression_loss: 1.4427 - classification_loss: 0.4985\n",
      "365/500 [====================>.........] - ETA: 2:19 - loss: 1.9404 - regression_loss: 1.4424 - classification_loss: 0.4980\n",
      "366/500 [====================>.........] - ETA: 2:18 - loss: 1.9387 - regression_loss: 1.4412 - classification_loss: 0.4975\n",
      "367/500 [=====================>........] - ETA: 2:16 - loss: 1.9374 - regression_loss: 1.4402 - classification_loss: 0.4971\n",
      "368/500 [=====================>........] - ETA: 2:15 - loss: 1.9376 - regression_loss: 1.4406 - classification_loss: 0.4970\n",
      "369/500 [=====================>........] - ETA: 2:14 - loss: 1.9401 - regression_loss: 1.4425 - classification_loss: 0.4976\n",
      "370/500 [=====================>........] - ETA: 2:13 - loss: 1.9386 - regression_loss: 1.4411 - classification_loss: 0.4974\n",
      "371/500 [=====================>........] - ETA: 2:12 - loss: 1.9374 - regression_loss: 1.4404 - classification_loss: 0.4971\n",
      "372/500 [=====================>........] - ETA: 2:11 - loss: 1.9398 - regression_loss: 1.4425 - classification_loss: 0.4972\n",
      "373/500 [=====================>........] - ETA: 2:10 - loss: 1.9394 - regression_loss: 1.4421 - classification_loss: 0.4972\n",
      "374/500 [=====================>........] - ETA: 2:09 - loss: 1.9384 - regression_loss: 1.4416 - classification_loss: 0.4968\n",
      "375/500 [=====================>........] - ETA: 2:08 - loss: 1.9378 - regression_loss: 1.4415 - classification_loss: 0.4963\n",
      "376/500 [=====================>........] - ETA: 2:07 - loss: 1.9374 - regression_loss: 1.4416 - classification_loss: 0.4958\n",
      "377/500 [=====================>........] - ETA: 2:06 - loss: 1.9378 - regression_loss: 1.4421 - classification_loss: 0.4957\n",
      "378/500 [=====================>........] - ETA: 2:05 - loss: 1.9394 - regression_loss: 1.4436 - classification_loss: 0.4957\n",
      "379/500 [=====================>........] - ETA: 2:04 - loss: 1.9416 - regression_loss: 1.4450 - classification_loss: 0.4966\n",
      "380/500 [=====================>........] - ETA: 2:03 - loss: 1.9429 - regression_loss: 1.4458 - classification_loss: 0.4971\n",
      "381/500 [=====================>........] - ETA: 2:02 - loss: 1.9440 - regression_loss: 1.4468 - classification_loss: 0.4973\n",
      "382/500 [=====================>........] - ETA: 2:01 - loss: 1.9464 - regression_loss: 1.4482 - classification_loss: 0.4982\n",
      "383/500 [=====================>........] - ETA: 2:00 - loss: 1.9482 - regression_loss: 1.4497 - classification_loss: 0.4985\n",
      "384/500 [======================>.......] - ETA: 1:59 - loss: 1.9475 - regression_loss: 1.4493 - classification_loss: 0.4982\n",
      "385/500 [======================>.......] - ETA: 1:58 - loss: 1.9487 - regression_loss: 1.4500 - classification_loss: 0.4987\n",
      "386/500 [======================>.......] - ETA: 1:57 - loss: 1.9469 - regression_loss: 1.4486 - classification_loss: 0.4983\n",
      "387/500 [======================>.......] - ETA: 1:56 - loss: 1.9467 - regression_loss: 1.4489 - classification_loss: 0.4978\n",
      "388/500 [======================>.......] - ETA: 1:55 - loss: 1.9444 - regression_loss: 1.4473 - classification_loss: 0.4971\n",
      "389/500 [======================>.......] - ETA: 1:54 - loss: 1.9433 - regression_loss: 1.4467 - classification_loss: 0.4967\n",
      "390/500 [======================>.......] - ETA: 1:53 - loss: 1.9415 - regression_loss: 1.4455 - classification_loss: 0.4960\n",
      "391/500 [======================>.......] - ETA: 1:52 - loss: 1.9395 - regression_loss: 1.4440 - classification_loss: 0.4955\n",
      "392/500 [======================>.......] - ETA: 1:51 - loss: 1.9407 - regression_loss: 1.4448 - classification_loss: 0.4959\n",
      "393/500 [======================>.......] - ETA: 1:50 - loss: 1.9398 - regression_loss: 1.4442 - classification_loss: 0.4957\n",
      "394/500 [======================>.......] - ETA: 1:49 - loss: 1.9401 - regression_loss: 1.4446 - classification_loss: 0.4955\n",
      "395/500 [======================>.......] - ETA: 1:48 - loss: 1.9374 - regression_loss: 1.4428 - classification_loss: 0.4946\n",
      "396/500 [======================>.......] - ETA: 1:47 - loss: 1.9375 - regression_loss: 1.4431 - classification_loss: 0.4944\n",
      "397/500 [======================>.......] - ETA: 1:46 - loss: 1.9396 - regression_loss: 1.4445 - classification_loss: 0.4951\n",
      "398/500 [======================>.......] - ETA: 1:45 - loss: 1.9388 - regression_loss: 1.4440 - classification_loss: 0.4947\n",
      "399/500 [======================>.......] - ETA: 1:44 - loss: 1.9402 - regression_loss: 1.4456 - classification_loss: 0.4946\n",
      "400/500 [=======================>......] - ETA: 1:42 - loss: 1.9390 - regression_loss: 1.4447 - classification_loss: 0.4942\n",
      "401/500 [=======================>......] - ETA: 1:41 - loss: 1.9396 - regression_loss: 1.4452 - classification_loss: 0.4944\n",
      "402/500 [=======================>......] - ETA: 1:40 - loss: 1.9392 - regression_loss: 1.4451 - classification_loss: 0.4941\n",
      "403/500 [=======================>......] - ETA: 1:39 - loss: 1.9388 - regression_loss: 1.4452 - classification_loss: 0.4936\n",
      "404/500 [=======================>......] - ETA: 1:38 - loss: 1.9378 - regression_loss: 1.4446 - classification_loss: 0.4931\n",
      "405/500 [=======================>......] - ETA: 1:37 - loss: 1.9370 - regression_loss: 1.4438 - classification_loss: 0.4932\n",
      "406/500 [=======================>......] - ETA: 1:36 - loss: 1.9358 - regression_loss: 1.4428 - classification_loss: 0.4930\n",
      "407/500 [=======================>......] - ETA: 1:35 - loss: 1.9336 - regression_loss: 1.4411 - classification_loss: 0.4925\n",
      "408/500 [=======================>......] - ETA: 1:34 - loss: 1.9333 - regression_loss: 1.4410 - classification_loss: 0.4923\n",
      "409/500 [=======================>......] - ETA: 1:33 - loss: 1.9324 - regression_loss: 1.4402 - classification_loss: 0.4921\n",
      "410/500 [=======================>......] - ETA: 1:32 - loss: 1.9324 - regression_loss: 1.4406 - classification_loss: 0.4918\n",
      "411/500 [=======================>......] - ETA: 1:31 - loss: 1.9300 - regression_loss: 1.4390 - classification_loss: 0.4911\n",
      "412/500 [=======================>......] - ETA: 1:30 - loss: 1.9289 - regression_loss: 1.4383 - classification_loss: 0.4906\n",
      "413/500 [=======================>......] - ETA: 1:29 - loss: 1.9279 - regression_loss: 1.4378 - classification_loss: 0.4902\n",
      "414/500 [=======================>......] - ETA: 1:28 - loss: 1.9256 - regression_loss: 1.4359 - classification_loss: 0.4897\n",
      "415/500 [=======================>......] - ETA: 1:27 - loss: 1.9259 - regression_loss: 1.4362 - classification_loss: 0.4897\n",
      "416/500 [=======================>......] - ETA: 1:26 - loss: 1.9252 - regression_loss: 1.4356 - classification_loss: 0.4896\n",
      "417/500 [========================>.....] - ETA: 1:25 - loss: 1.9239 - regression_loss: 1.4347 - classification_loss: 0.4892\n",
      "418/500 [========================>.....] - ETA: 1:24 - loss: 1.9241 - regression_loss: 1.4350 - classification_loss: 0.4891\n",
      "419/500 [========================>.....] - ETA: 1:23 - loss: 1.9233 - regression_loss: 1.4345 - classification_loss: 0.4888\n",
      "420/500 [========================>.....] - ETA: 1:22 - loss: 1.9249 - regression_loss: 1.4355 - classification_loss: 0.4894\n",
      "421/500 [========================>.....] - ETA: 1:21 - loss: 1.9255 - regression_loss: 1.4364 - classification_loss: 0.4891\n",
      "422/500 [========================>.....] - ETA: 1:20 - loss: 1.9263 - regression_loss: 1.4371 - classification_loss: 0.4892\n",
      "423/500 [========================>.....] - ETA: 1:19 - loss: 1.9265 - regression_loss: 1.4373 - classification_loss: 0.4891\n",
      "424/500 [========================>.....] - ETA: 1:18 - loss: 1.9254 - regression_loss: 1.4366 - classification_loss: 0.4888\n",
      "425/500 [========================>.....] - ETA: 1:17 - loss: 1.9262 - regression_loss: 1.4371 - classification_loss: 0.4891\n",
      "426/500 [========================>.....] - ETA: 1:16 - loss: 1.9259 - regression_loss: 1.4369 - classification_loss: 0.4890\n",
      "427/500 [========================>.....] - ETA: 1:15 - loss: 1.9263 - regression_loss: 1.4372 - classification_loss: 0.4891\n",
      "428/500 [========================>.....] - ETA: 1:14 - loss: 1.9251 - regression_loss: 1.4365 - classification_loss: 0.4886\n",
      "429/500 [========================>.....] - ETA: 1:13 - loss: 1.9261 - regression_loss: 1.4372 - classification_loss: 0.4889\n",
      "430/500 [========================>.....] - ETA: 1:12 - loss: 1.9288 - regression_loss: 1.4395 - classification_loss: 0.4893\n",
      "431/500 [========================>.....] - ETA: 1:11 - loss: 1.9292 - regression_loss: 1.4398 - classification_loss: 0.4895\n",
      "432/500 [========================>.....] - ETA: 1:10 - loss: 1.9300 - regression_loss: 1.4395 - classification_loss: 0.4905\n",
      "433/500 [========================>.....] - ETA: 1:09 - loss: 1.9289 - regression_loss: 1.4386 - classification_loss: 0.4903\n",
      "434/500 [=========================>....] - ETA: 1:07 - loss: 1.9295 - regression_loss: 1.4391 - classification_loss: 0.4904\n",
      "435/500 [=========================>....] - ETA: 1:06 - loss: 1.9297 - regression_loss: 1.4393 - classification_loss: 0.4904\n",
      "436/500 [=========================>....] - ETA: 1:05 - loss: 1.9287 - regression_loss: 1.4385 - classification_loss: 0.4902\n",
      "437/500 [=========================>....] - ETA: 1:04 - loss: 1.9273 - regression_loss: 1.4374 - classification_loss: 0.4899\n",
      "438/500 [=========================>....] - ETA: 1:03 - loss: 1.9263 - regression_loss: 1.4369 - classification_loss: 0.4894\n",
      "439/500 [=========================>....] - ETA: 1:02 - loss: 1.9279 - regression_loss: 1.4378 - classification_loss: 0.4901\n",
      "440/500 [=========================>....] - ETA: 1:01 - loss: 1.9264 - regression_loss: 1.4367 - classification_loss: 0.4897\n",
      "441/500 [=========================>....] - ETA: 1:00 - loss: 1.9248 - regression_loss: 1.4357 - classification_loss: 0.4891\n",
      "442/500 [=========================>....] - ETA: 59s - loss: 1.9241 - regression_loss: 1.4352 - classification_loss: 0.4889 \n",
      "443/500 [=========================>....] - ETA: 58s - loss: 1.9247 - regression_loss: 1.4359 - classification_loss: 0.4888\n",
      "444/500 [=========================>....] - ETA: 57s - loss: 1.9240 - regression_loss: 1.4352 - classification_loss: 0.4887\n",
      "445/500 [=========================>....] - ETA: 56s - loss: 1.9228 - regression_loss: 1.4346 - classification_loss: 0.4882\n",
      "446/500 [=========================>....] - ETA: 55s - loss: 1.9219 - regression_loss: 1.4339 - classification_loss: 0.4880\n",
      "447/500 [=========================>....] - ETA: 54s - loss: 1.9208 - regression_loss: 1.4333 - classification_loss: 0.4875\n",
      "448/500 [=========================>....] - ETA: 53s - loss: 1.9193 - regression_loss: 1.4323 - classification_loss: 0.4870\n",
      "449/500 [=========================>....] - ETA: 52s - loss: 1.9193 - regression_loss: 1.4323 - classification_loss: 0.4870\n",
      "450/500 [==========================>...] - ETA: 51s - loss: 1.9204 - regression_loss: 1.4328 - classification_loss: 0.4875\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 1.9200 - regression_loss: 1.4324 - classification_loss: 0.4876\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 1.9188 - regression_loss: 1.4315 - classification_loss: 0.4873\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.9193 - regression_loss: 1.4315 - classification_loss: 0.4879\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.9208 - regression_loss: 1.4329 - classification_loss: 0.4879\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.9220 - regression_loss: 1.4338 - classification_loss: 0.4882\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.9201 - regression_loss: 1.4325 - classification_loss: 0.4876\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.9221 - regression_loss: 1.4325 - classification_loss: 0.4896\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.9204 - regression_loss: 1.4310 - classification_loss: 0.4894\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.9192 - regression_loss: 1.4302 - classification_loss: 0.4890\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.9195 - regression_loss: 1.4305 - classification_loss: 0.4890\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 1.9185 - regression_loss: 1.4298 - classification_loss: 0.4887\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 1.9196 - regression_loss: 1.4306 - classification_loss: 0.4890\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 1.9186 - regression_loss: 1.4299 - classification_loss: 0.4887\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 1.9205 - regression_loss: 1.4311 - classification_loss: 0.4894\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 1.9193 - regression_loss: 1.4302 - classification_loss: 0.4891\n",
      "466/500 [==========================>...] - ETA: 35s - loss: 1.9196 - regression_loss: 1.4306 - classification_loss: 0.4890\n",
      "467/500 [===========================>..] - ETA: 33s - loss: 1.9195 - regression_loss: 1.4301 - classification_loss: 0.4894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/500 [===========================>..] - ETA: 32s - loss: 1.9179 - regression_loss: 1.4289 - classification_loss: 0.4890\n",
      "469/500 [===========================>..] - ETA: 31s - loss: 1.9174 - regression_loss: 1.4287 - classification_loss: 0.4887\n",
      "470/500 [===========================>..] - ETA: 30s - loss: 1.9194 - regression_loss: 1.4299 - classification_loss: 0.4895\n",
      "471/500 [===========================>..] - ETA: 29s - loss: 1.9189 - regression_loss: 1.4294 - classification_loss: 0.4896\n",
      "472/500 [===========================>..] - ETA: 28s - loss: 1.9178 - regression_loss: 1.4285 - classification_loss: 0.4893\n",
      "473/500 [===========================>..] - ETA: 27s - loss: 1.9169 - regression_loss: 1.4274 - classification_loss: 0.4895\n",
      "474/500 [===========================>..] - ETA: 26s - loss: 1.9169 - regression_loss: 1.4277 - classification_loss: 0.4892\n",
      "475/500 [===========================>..] - ETA: 25s - loss: 1.9171 - regression_loss: 1.4280 - classification_loss: 0.4891\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 1.9156 - regression_loss: 1.4268 - classification_loss: 0.4888\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.9149 - regression_loss: 1.4263 - classification_loss: 0.4886\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.9151 - regression_loss: 1.4264 - classification_loss: 0.4886\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.9153 - regression_loss: 1.4265 - classification_loss: 0.4888\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.9142 - regression_loss: 1.4258 - classification_loss: 0.4885\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.9129 - regression_loss: 1.4247 - classification_loss: 0.4882\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.9132 - regression_loss: 1.4247 - classification_loss: 0.4884\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.9140 - regression_loss: 1.4252 - classification_loss: 0.4888\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.9139 - regression_loss: 1.4252 - classification_loss: 0.4887\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.9143 - regression_loss: 1.4255 - classification_loss: 0.4888\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.9142 - regression_loss: 1.4255 - classification_loss: 0.4887\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.9128 - regression_loss: 1.4244 - classification_loss: 0.4885\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.9128 - regression_loss: 1.4246 - classification_loss: 0.4882\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.9124 - regression_loss: 1.4244 - classification_loss: 0.4880\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.9103 - regression_loss: 1.4228 - classification_loss: 0.4875\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.9109 - regression_loss: 1.4231 - classification_loss: 0.4878 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.9132 - regression_loss: 1.4243 - classification_loss: 0.4889\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.9135 - regression_loss: 1.4247 - classification_loss: 0.4888\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.9134 - regression_loss: 1.4247 - classification_loss: 0.4886\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.9130 - regression_loss: 1.4245 - classification_loss: 0.4886\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.9124 - regression_loss: 1.4242 - classification_loss: 0.4883\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.9112 - regression_loss: 1.4235 - classification_loss: 0.4877\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.9092 - regression_loss: 1.4219 - classification_loss: 0.4873\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.9077 - regression_loss: 1.4209 - classification_loss: 0.4868\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9071 - regression_loss: 1.4207 - classification_loss: 0.4864\n",
      "Epoch 00033: saving model to ./snapshots\\resnet50_csv_33.h5\n",
      "\n",
      "500/500 [==============================] - 515s 1s/step - loss: 1.9071 - regression_loss: 1.4207 - classification_loss: 0.4864\n",
      "Epoch 34/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 3.0772 - regression_loss: 1.9895 - classification_loss: 1.0877\n",
      "  2/500 [..............................] - ETA: 4:04 - loss: 2.6389 - regression_loss: 1.8385 - classification_loss: 0.8004\n",
      "  3/500 [..............................] - ETA: 5:26 - loss: 2.3052 - regression_loss: 1.6645 - classification_loss: 0.6408\n",
      "  4/500 [..............................] - ETA: 6:18 - loss: 2.4578 - regression_loss: 1.7566 - classification_loss: 0.7013\n",
      "  5/500 [..............................] - ETA: 6:41 - loss: 2.3779 - regression_loss: 1.7410 - classification_loss: 0.6369\n",
      "  6/500 [..............................] - ETA: 6:42 - loss: 2.3350 - regression_loss: 1.7276 - classification_loss: 0.6074\n",
      "  7/500 [..............................] - ETA: 7:04 - loss: 2.2271 - regression_loss: 1.6508 - classification_loss: 0.5763\n",
      "  8/500 [..............................] - ETA: 7:11 - loss: 2.1313 - regression_loss: 1.5749 - classification_loss: 0.5564\n",
      "  9/500 [..............................] - ETA: 7:16 - loss: 2.1703 - regression_loss: 1.6109 - classification_loss: 0.5595\n",
      " 10/500 [..............................] - ETA: 7:20 - loss: 2.0833 - regression_loss: 1.5580 - classification_loss: 0.5253\n",
      " 11/500 [..............................] - ETA: 7:27 - loss: 2.0156 - regression_loss: 1.5090 - classification_loss: 0.5066\n",
      " 12/500 [..............................] - ETA: 7:33 - loss: 2.0347 - regression_loss: 1.5213 - classification_loss: 0.5134\n",
      " 13/500 [..............................] - ETA: 7:38 - loss: 2.0688 - regression_loss: 1.5477 - classification_loss: 0.5210\n",
      " 14/500 [..............................] - ETA: 7:39 - loss: 2.0386 - regression_loss: 1.5187 - classification_loss: 0.5199\n",
      " 15/500 [..............................] - ETA: 7:43 - loss: 2.0052 - regression_loss: 1.4941 - classification_loss: 0.5111\n",
      " 16/500 [..............................] - ETA: 7:46 - loss: 2.0451 - regression_loss: 1.5263 - classification_loss: 0.5188\n",
      " 17/500 [>.............................] - ETA: 7:49 - loss: 2.0399 - regression_loss: 1.5185 - classification_loss: 0.5214\n",
      " 18/500 [>.............................] - ETA: 7:43 - loss: 2.0565 - regression_loss: 1.5266 - classification_loss: 0.5299\n",
      " 19/500 [>.............................] - ETA: 7:46 - loss: 2.0605 - regression_loss: 1.5395 - classification_loss: 0.5210\n",
      " 20/500 [>.............................] - ETA: 7:42 - loss: 2.0198 - regression_loss: 1.5057 - classification_loss: 0.5141\n",
      " 21/500 [>.............................] - ETA: 7:46 - loss: 1.9932 - regression_loss: 1.4838 - classification_loss: 0.5094\n",
      " 22/500 [>.............................] - ETA: 7:47 - loss: 1.9950 - regression_loss: 1.4886 - classification_loss: 0.5064\n",
      " 23/500 [>.............................] - ETA: 7:49 - loss: 2.0131 - regression_loss: 1.4971 - classification_loss: 0.5160\n",
      " 24/500 [>.............................] - ETA: 7:48 - loss: 1.9730 - regression_loss: 1.4683 - classification_loss: 0.5046\n",
      " 25/500 [>.............................] - ETA: 7:49 - loss: 1.9346 - regression_loss: 1.4378 - classification_loss: 0.4968\n",
      " 26/500 [>.............................] - ETA: 7:51 - loss: 1.9521 - regression_loss: 1.4524 - classification_loss: 0.4997\n",
      " 27/500 [>.............................] - ETA: 7:51 - loss: 1.9362 - regression_loss: 1.4408 - classification_loss: 0.4954\n",
      " 28/500 [>.............................] - ETA: 7:50 - loss: 1.9236 - regression_loss: 1.4295 - classification_loss: 0.4941\n",
      " 29/500 [>.............................] - ETA: 7:49 - loss: 1.9005 - regression_loss: 1.4161 - classification_loss: 0.4844\n",
      " 30/500 [>.............................] - ETA: 7:48 - loss: 1.8940 - regression_loss: 1.4141 - classification_loss: 0.4798\n",
      " 31/500 [>.............................] - ETA: 7:48 - loss: 1.8982 - regression_loss: 1.4197 - classification_loss: 0.4785\n",
      " 32/500 [>.............................] - ETA: 7:47 - loss: 1.8960 - regression_loss: 1.4188 - classification_loss: 0.4772\n",
      " 33/500 [>.............................] - ETA: 7:46 - loss: 1.9021 - regression_loss: 1.4230 - classification_loss: 0.4792\n",
      " 34/500 [=>............................] - ETA: 7:45 - loss: 1.9034 - regression_loss: 1.4234 - classification_loss: 0.4800\n",
      " 35/500 [=>............................] - ETA: 7:45 - loss: 1.8822 - regression_loss: 1.4080 - classification_loss: 0.4742\n",
      " 36/500 [=>............................] - ETA: 7:45 - loss: 1.8719 - regression_loss: 1.4021 - classification_loss: 0.4698\n",
      " 37/500 [=>............................] - ETA: 7:44 - loss: 1.8817 - regression_loss: 1.4134 - classification_loss: 0.4683\n",
      " 38/500 [=>............................] - ETA: 7:41 - loss: 1.8609 - regression_loss: 1.3995 - classification_loss: 0.4614\n",
      " 39/500 [=>............................] - ETA: 7:42 - loss: 1.8764 - regression_loss: 1.4092 - classification_loss: 0.4672\n",
      " 40/500 [=>............................] - ETA: 7:40 - loss: 1.8793 - regression_loss: 1.4113 - classification_loss: 0.4680\n",
      " 41/500 [=>............................] - ETA: 7:41 - loss: 1.8811 - regression_loss: 1.4180 - classification_loss: 0.4631\n",
      " 42/500 [=>............................] - ETA: 7:39 - loss: 1.8860 - regression_loss: 1.4264 - classification_loss: 0.4596\n",
      " 43/500 [=>............................] - ETA: 7:39 - loss: 1.8903 - regression_loss: 1.4299 - classification_loss: 0.4604\n",
      " 44/500 [=>............................] - ETA: 7:39 - loss: 1.8786 - regression_loss: 1.4210 - classification_loss: 0.4576\n",
      " 45/500 [=>............................] - ETA: 7:38 - loss: 1.8775 - regression_loss: 1.4197 - classification_loss: 0.4578\n",
      " 46/500 [=>............................] - ETA: 7:37 - loss: 1.8824 - regression_loss: 1.4247 - classification_loss: 0.4577\n",
      " 47/500 [=>............................] - ETA: 7:34 - loss: 1.8845 - regression_loss: 1.4261 - classification_loss: 0.4584\n",
      " 48/500 [=>............................] - ETA: 7:35 - loss: 1.9070 - regression_loss: 1.4433 - classification_loss: 0.4637\n",
      " 49/500 [=>............................] - ETA: 7:35 - loss: 1.9063 - regression_loss: 1.4433 - classification_loss: 0.4630\n",
      " 50/500 [==>...........................] - ETA: 7:32 - loss: 1.8944 - regression_loss: 1.4340 - classification_loss: 0.4604\n",
      " 51/500 [==>...........................] - ETA: 7:32 - loss: 1.9021 - regression_loss: 1.4386 - classification_loss: 0.4635\n",
      " 52/500 [==>...........................] - ETA: 7:31 - loss: 1.8989 - regression_loss: 1.4360 - classification_loss: 0.4628\n",
      " 53/500 [==>...........................] - ETA: 7:31 - loss: 1.8995 - regression_loss: 1.4362 - classification_loss: 0.4632\n",
      " 54/500 [==>...........................] - ETA: 7:30 - loss: 1.8972 - regression_loss: 1.4349 - classification_loss: 0.4623\n",
      " 55/500 [==>...........................] - ETA: 7:29 - loss: 1.8871 - regression_loss: 1.4269 - classification_loss: 0.4602\n",
      " 56/500 [==>...........................] - ETA: 7:29 - loss: 1.8964 - regression_loss: 1.4379 - classification_loss: 0.4585\n",
      " 57/500 [==>...........................] - ETA: 7:28 - loss: 1.8999 - regression_loss: 1.4384 - classification_loss: 0.4614\n",
      " 58/500 [==>...........................] - ETA: 7:27 - loss: 1.9009 - regression_loss: 1.4390 - classification_loss: 0.4619\n",
      " 59/500 [==>...........................] - ETA: 7:26 - loss: 1.8901 - regression_loss: 1.4278 - classification_loss: 0.4623\n",
      " 60/500 [==>...........................] - ETA: 7:24 - loss: 1.9012 - regression_loss: 1.4350 - classification_loss: 0.4662\n",
      " 61/500 [==>...........................] - ETA: 7:24 - loss: 1.8912 - regression_loss: 1.4270 - classification_loss: 0.4643\n",
      " 62/500 [==>...........................] - ETA: 7:23 - loss: 1.8889 - regression_loss: 1.4262 - classification_loss: 0.4627\n",
      " 63/500 [==>...........................] - ETA: 7:21 - loss: 1.8815 - regression_loss: 1.4207 - classification_loss: 0.4608\n",
      " 64/500 [==>...........................] - ETA: 7:21 - loss: 1.8943 - regression_loss: 1.4293 - classification_loss: 0.4650\n",
      " 65/500 [==>...........................] - ETA: 7:20 - loss: 1.8980 - regression_loss: 1.4335 - classification_loss: 0.4644\n",
      " 66/500 [==>...........................] - ETA: 7:19 - loss: 1.8906 - regression_loss: 1.4277 - classification_loss: 0.4630\n",
      " 67/500 [===>..........................] - ETA: 7:17 - loss: 1.8812 - regression_loss: 1.4219 - classification_loss: 0.4592\n",
      " 68/500 [===>..........................] - ETA: 7:17 - loss: 1.8758 - regression_loss: 1.4170 - classification_loss: 0.4588\n",
      " 69/500 [===>..........................] - ETA: 7:16 - loss: 1.8815 - regression_loss: 1.4183 - classification_loss: 0.4632\n",
      " 70/500 [===>..........................] - ETA: 7:16 - loss: 1.8788 - regression_loss: 1.4162 - classification_loss: 0.4626\n",
      " 71/500 [===>..........................] - ETA: 7:15 - loss: 1.8824 - regression_loss: 1.4199 - classification_loss: 0.4624\n",
      " 72/500 [===>..........................] - ETA: 7:13 - loss: 1.8816 - regression_loss: 1.4179 - classification_loss: 0.4637\n",
      " 73/500 [===>..........................] - ETA: 7:12 - loss: 1.8885 - regression_loss: 1.4232 - classification_loss: 0.4653\n",
      " 74/500 [===>..........................] - ETA: 7:12 - loss: 1.8916 - regression_loss: 1.4257 - classification_loss: 0.4659\n",
      " 75/500 [===>..........................] - ETA: 7:11 - loss: 1.8886 - regression_loss: 1.4245 - classification_loss: 0.4641\n",
      " 76/500 [===>..........................] - ETA: 7:10 - loss: 1.8823 - regression_loss: 1.4194 - classification_loss: 0.4629\n",
      " 77/500 [===>..........................] - ETA: 7:08 - loss: 1.8748 - regression_loss: 1.4135 - classification_loss: 0.4613\n",
      " 78/500 [===>..........................] - ETA: 7:06 - loss: 1.8845 - regression_loss: 1.4217 - classification_loss: 0.4627\n",
      " 79/500 [===>..........................] - ETA: 7:06 - loss: 1.8891 - regression_loss: 1.4253 - classification_loss: 0.4639\n",
      " 80/500 [===>..........................] - ETA: 7:05 - loss: 1.8915 - regression_loss: 1.4288 - classification_loss: 0.4627\n",
      " 81/500 [===>..........................] - ETA: 7:04 - loss: 1.8971 - regression_loss: 1.4322 - classification_loss: 0.4649\n",
      " 82/500 [===>..........................] - ETA: 7:03 - loss: 1.9100 - regression_loss: 1.4375 - classification_loss: 0.4725\n",
      " 83/500 [===>..........................] - ETA: 7:01 - loss: 1.9174 - regression_loss: 1.4429 - classification_loss: 0.4745\n",
      " 84/500 [====>.........................] - ETA: 7:01 - loss: 1.9165 - regression_loss: 1.4426 - classification_loss: 0.4738\n",
      " 85/500 [====>.........................] - ETA: 7:01 - loss: 1.9256 - regression_loss: 1.4470 - classification_loss: 0.4787\n",
      " 86/500 [====>.........................] - ETA: 6:58 - loss: 1.9232 - regression_loss: 1.4467 - classification_loss: 0.4765\n",
      " 87/500 [====>.........................] - ETA: 6:58 - loss: 1.9147 - regression_loss: 1.4414 - classification_loss: 0.4733\n",
      " 88/500 [====>.........................] - ETA: 6:57 - loss: 1.9120 - regression_loss: 1.4400 - classification_loss: 0.4720\n",
      " 89/500 [====>.........................] - ETA: 6:56 - loss: 1.9052 - regression_loss: 1.4344 - classification_loss: 0.4708\n",
      " 90/500 [====>.........................] - ETA: 6:55 - loss: 1.8999 - regression_loss: 1.4312 - classification_loss: 0.4686\n",
      " 91/500 [====>.........................] - ETA: 6:54 - loss: 1.9094 - regression_loss: 1.4390 - classification_loss: 0.4704\n",
      " 92/500 [====>.........................] - ETA: 6:53 - loss: 1.9056 - regression_loss: 1.4366 - classification_loss: 0.4691\n",
      " 93/500 [====>.........................] - ETA: 6:51 - loss: 1.9079 - regression_loss: 1.4365 - classification_loss: 0.4715\n",
      " 94/500 [====>.........................] - ETA: 6:51 - loss: 1.9104 - regression_loss: 1.4392 - classification_loss: 0.4712\n",
      " 95/500 [====>.........................] - ETA: 6:50 - loss: 1.9040 - regression_loss: 1.4338 - classification_loss: 0.4702\n",
      " 96/500 [====>.........................] - ETA: 6:49 - loss: 1.8996 - regression_loss: 1.4315 - classification_loss: 0.4681\n",
      " 97/500 [====>.........................] - ETA: 6:48 - loss: 1.8970 - regression_loss: 1.4295 - classification_loss: 0.4675\n",
      " 98/500 [====>.........................] - ETA: 6:46 - loss: 1.9052 - regression_loss: 1.4375 - classification_loss: 0.4677\n",
      " 99/500 [====>.........................] - ETA: 6:46 - loss: 1.8976 - regression_loss: 1.4312 - classification_loss: 0.4664\n",
      "100/500 [=====>........................] - ETA: 6:45 - loss: 1.8953 - regression_loss: 1.4298 - classification_loss: 0.4655\n",
      "101/500 [=====>........................] - ETA: 6:44 - loss: 1.8945 - regression_loss: 1.4287 - classification_loss: 0.4658\n",
      "102/500 [=====>........................] - ETA: 6:43 - loss: 1.8959 - regression_loss: 1.4309 - classification_loss: 0.4650\n",
      "103/500 [=====>........................] - ETA: 6:42 - loss: 1.8980 - regression_loss: 1.4334 - classification_loss: 0.4646\n",
      "104/500 [=====>........................] - ETA: 6:41 - loss: 1.8972 - regression_loss: 1.4343 - classification_loss: 0.4628\n",
      "105/500 [=====>........................] - ETA: 6:40 - loss: 1.8925 - regression_loss: 1.4318 - classification_loss: 0.4607\n",
      "106/500 [=====>........................] - ETA: 6:39 - loss: 1.8917 - regression_loss: 1.4313 - classification_loss: 0.4605\n",
      "107/500 [=====>........................] - ETA: 6:38 - loss: 1.8893 - regression_loss: 1.4295 - classification_loss: 0.4597\n",
      "108/500 [=====>........................] - ETA: 6:37 - loss: 1.8818 - regression_loss: 1.4248 - classification_loss: 0.4571\n",
      "109/500 [=====>........................] - ETA: 6:36 - loss: 1.8861 - regression_loss: 1.4277 - classification_loss: 0.4584\n",
      "110/500 [=====>........................] - ETA: 6:35 - loss: 1.8882 - regression_loss: 1.4276 - classification_loss: 0.4605\n",
      "111/500 [=====>........................] - ETA: 6:33 - loss: 1.8946 - regression_loss: 1.4290 - classification_loss: 0.4656\n",
      "112/500 [=====>........................] - ETA: 6:33 - loss: 1.8933 - regression_loss: 1.4269 - classification_loss: 0.4664\n",
      "113/500 [=====>........................] - ETA: 6:32 - loss: 1.8943 - regression_loss: 1.4267 - classification_loss: 0.4676\n",
      "114/500 [=====>........................] - ETA: 6:31 - loss: 1.8966 - regression_loss: 1.4292 - classification_loss: 0.4673\n",
      "115/500 [=====>........................] - ETA: 6:30 - loss: 1.8966 - regression_loss: 1.4265 - classification_loss: 0.4701\n",
      "116/500 [=====>........................] - ETA: 6:29 - loss: 1.8907 - regression_loss: 1.4220 - classification_loss: 0.4688\n",
      "117/500 [======>.......................] - ETA: 6:28 - loss: 1.8858 - regression_loss: 1.4179 - classification_loss: 0.4678\n",
      "118/500 [======>.......................] - ETA: 6:27 - loss: 1.8827 - regression_loss: 1.4160 - classification_loss: 0.4667\n",
      "119/500 [======>.......................] - ETA: 6:26 - loss: 1.8818 - regression_loss: 1.4147 - classification_loss: 0.4671\n",
      "120/500 [======>.......................] - ETA: 6:25 - loss: 1.8908 - regression_loss: 1.4205 - classification_loss: 0.4703\n",
      "121/500 [======>.......................] - ETA: 6:25 - loss: 1.8894 - regression_loss: 1.4174 - classification_loss: 0.4720\n",
      "122/500 [======>.......................] - ETA: 6:23 - loss: 1.8853 - regression_loss: 1.4129 - classification_loss: 0.4724\n",
      "123/500 [======>.......................] - ETA: 6:22 - loss: 1.8827 - regression_loss: 1.4101 - classification_loss: 0.4726\n",
      "124/500 [======>.......................] - ETA: 6:21 - loss: 1.8810 - regression_loss: 1.4092 - classification_loss: 0.4718\n",
      "125/500 [======>.......................] - ETA: 6:19 - loss: 1.8869 - regression_loss: 1.4113 - classification_loss: 0.4756\n",
      "126/500 [======>.......................] - ETA: 6:19 - loss: 1.8927 - regression_loss: 1.4153 - classification_loss: 0.4774\n",
      "127/500 [======>.......................] - ETA: 6:17 - loss: 1.8926 - regression_loss: 1.4145 - classification_loss: 0.4782\n",
      "128/500 [======>.......................] - ETA: 6:17 - loss: 1.8951 - regression_loss: 1.4167 - classification_loss: 0.4784\n",
      "129/500 [======>.......................] - ETA: 6:16 - loss: 1.8891 - regression_loss: 1.4115 - classification_loss: 0.4775\n",
      "130/500 [======>.......................] - ETA: 6:15 - loss: 1.8846 - regression_loss: 1.4081 - classification_loss: 0.4765\n",
      "131/500 [======>.......................] - ETA: 6:14 - loss: 1.8861 - regression_loss: 1.4072 - classification_loss: 0.4789\n",
      "132/500 [======>.......................] - ETA: 6:13 - loss: 1.8817 - regression_loss: 1.4040 - classification_loss: 0.4777\n",
      "133/500 [======>.......................] - ETA: 6:12 - loss: 1.8876 - regression_loss: 1.4075 - classification_loss: 0.4801\n",
      "134/500 [=======>......................] - ETA: 6:10 - loss: 1.8880 - regression_loss: 1.4082 - classification_loss: 0.4798\n",
      "135/500 [=======>......................] - ETA: 6:10 - loss: 1.8941 - regression_loss: 1.4102 - classification_loss: 0.4838\n",
      "136/500 [=======>......................] - ETA: 6:09 - loss: 1.8977 - regression_loss: 1.4134 - classification_loss: 0.4843\n",
      "137/500 [=======>......................] - ETA: 6:08 - loss: 1.8980 - regression_loss: 1.4141 - classification_loss: 0.4840\n",
      "138/500 [=======>......................] - ETA: 6:06 - loss: 1.8946 - regression_loss: 1.4123 - classification_loss: 0.4824\n",
      "139/500 [=======>......................] - ETA: 6:06 - loss: 1.8931 - regression_loss: 1.4119 - classification_loss: 0.4812\n",
      "140/500 [=======>......................] - ETA: 6:05 - loss: 1.8994 - regression_loss: 1.4164 - classification_loss: 0.4830\n",
      "141/500 [=======>......................] - ETA: 6:04 - loss: 1.9008 - regression_loss: 1.4183 - classification_loss: 0.4825\n",
      "142/500 [=======>......................] - ETA: 6:03 - loss: 1.8997 - regression_loss: 1.4181 - classification_loss: 0.4816\n",
      "143/500 [=======>......................] - ETA: 6:01 - loss: 1.9047 - regression_loss: 1.4222 - classification_loss: 0.4825\n",
      "144/500 [=======>......................] - ETA: 6:01 - loss: 1.8984 - regression_loss: 1.4177 - classification_loss: 0.4807\n",
      "145/500 [=======>......................] - ETA: 6:00 - loss: 1.9010 - regression_loss: 1.4197 - classification_loss: 0.4813\n",
      "146/500 [=======>......................] - ETA: 5:59 - loss: 1.8999 - regression_loss: 1.4196 - classification_loss: 0.4803\n",
      "147/500 [=======>......................] - ETA: 5:58 - loss: 1.8964 - regression_loss: 1.4172 - classification_loss: 0.4792\n",
      "148/500 [=======>......................] - ETA: 5:57 - loss: 1.8926 - regression_loss: 1.4145 - classification_loss: 0.4781\n",
      "149/500 [=======>......................] - ETA: 5:56 - loss: 1.8897 - regression_loss: 1.4132 - classification_loss: 0.4765\n",
      "150/500 [========>.....................] - ETA: 5:55 - loss: 1.8843 - regression_loss: 1.4095 - classification_loss: 0.4748\n",
      "151/500 [========>.....................] - ETA: 5:54 - loss: 1.8879 - regression_loss: 1.4128 - classification_loss: 0.4751\n",
      "152/500 [========>.....................] - ETA: 5:53 - loss: 1.8896 - regression_loss: 1.4138 - classification_loss: 0.4758\n",
      "153/500 [========>.....................] - ETA: 5:52 - loss: 1.8902 - regression_loss: 1.4145 - classification_loss: 0.4757\n",
      "154/500 [========>.....................] - ETA: 5:51 - loss: 1.8922 - regression_loss: 1.4146 - classification_loss: 0.4775\n",
      "155/500 [========>.....................] - ETA: 5:50 - loss: 1.8878 - regression_loss: 1.4114 - classification_loss: 0.4764\n",
      "156/500 [========>.....................] - ETA: 5:49 - loss: 1.8908 - regression_loss: 1.4137 - classification_loss: 0.4771\n",
      "157/500 [========>.....................] - ETA: 5:48 - loss: 1.8921 - regression_loss: 1.4150 - classification_loss: 0.4771\n",
      "158/500 [========>.....................] - ETA: 5:47 - loss: 1.8900 - regression_loss: 1.4141 - classification_loss: 0.4759\n",
      "159/500 [========>.....................] - ETA: 5:46 - loss: 1.8892 - regression_loss: 1.4131 - classification_loss: 0.4761\n",
      "160/500 [========>.....................] - ETA: 5:45 - loss: 1.8878 - regression_loss: 1.4120 - classification_loss: 0.4758\n",
      "161/500 [========>.....................] - ETA: 5:44 - loss: 1.8888 - regression_loss: 1.4126 - classification_loss: 0.4761\n",
      "162/500 [========>.....................] - ETA: 5:43 - loss: 1.8894 - regression_loss: 1.4133 - classification_loss: 0.4762\n",
      "163/500 [========>.....................] - ETA: 5:41 - loss: 1.8860 - regression_loss: 1.4102 - classification_loss: 0.4758\n",
      "164/500 [========>.....................] - ETA: 5:41 - loss: 1.8808 - regression_loss: 1.4059 - classification_loss: 0.4750\n",
      "165/500 [========>.....................] - ETA: 5:40 - loss: 1.8855 - regression_loss: 1.4094 - classification_loss: 0.4761\n",
      "166/500 [========>.....................] - ETA: 5:39 - loss: 1.8868 - regression_loss: 1.4100 - classification_loss: 0.4767\n",
      "167/500 [=========>....................] - ETA: 5:38 - loss: 1.8926 - regression_loss: 1.4137 - classification_loss: 0.4789\n",
      "168/500 [=========>....................] - ETA: 5:37 - loss: 1.8896 - regression_loss: 1.4115 - classification_loss: 0.4781\n",
      "169/500 [=========>....................] - ETA: 5:36 - loss: 1.8927 - regression_loss: 1.4128 - classification_loss: 0.4800\n",
      "170/500 [=========>....................] - ETA: 5:35 - loss: 1.8878 - regression_loss: 1.4083 - classification_loss: 0.4795\n",
      "171/500 [=========>....................] - ETA: 5:34 - loss: 1.8877 - regression_loss: 1.4080 - classification_loss: 0.4797\n",
      "172/500 [=========>....................] - ETA: 5:33 - loss: 1.8855 - regression_loss: 1.4064 - classification_loss: 0.4791\n",
      "173/500 [=========>....................] - ETA: 5:32 - loss: 1.8875 - regression_loss: 1.4076 - classification_loss: 0.4799\n",
      "174/500 [=========>....................] - ETA: 5:31 - loss: 1.8859 - regression_loss: 1.4061 - classification_loss: 0.4798\n",
      "175/500 [=========>....................] - ETA: 5:30 - loss: 1.8867 - regression_loss: 1.4070 - classification_loss: 0.4797\n",
      "176/500 [=========>....................] - ETA: 5:29 - loss: 1.8855 - regression_loss: 1.4067 - classification_loss: 0.4788\n",
      "177/500 [=========>....................] - ETA: 5:28 - loss: 1.8823 - regression_loss: 1.4049 - classification_loss: 0.4774\n",
      "178/500 [=========>....................] - ETA: 5:27 - loss: 1.8814 - regression_loss: 1.4041 - classification_loss: 0.4773\n",
      "179/500 [=========>....................] - ETA: 5:26 - loss: 1.8804 - regression_loss: 1.4034 - classification_loss: 0.4770\n",
      "180/500 [=========>....................] - ETA: 5:25 - loss: 1.8812 - regression_loss: 1.4046 - classification_loss: 0.4766\n",
      "181/500 [=========>....................] - ETA: 5:24 - loss: 1.8803 - regression_loss: 1.4044 - classification_loss: 0.4759\n",
      "182/500 [=========>....................] - ETA: 5:23 - loss: 1.8841 - regression_loss: 1.4073 - classification_loss: 0.4768\n",
      "183/500 [=========>....................] - ETA: 5:22 - loss: 1.8847 - regression_loss: 1.4071 - classification_loss: 0.4776\n",
      "184/500 [==========>...................] - ETA: 5:22 - loss: 1.8843 - regression_loss: 1.4066 - classification_loss: 0.4776\n",
      "185/500 [==========>...................] - ETA: 5:21 - loss: 1.8869 - regression_loss: 1.4078 - classification_loss: 0.4791\n",
      "186/500 [==========>...................] - ETA: 5:20 - loss: 1.8849 - regression_loss: 1.4067 - classification_loss: 0.4783\n",
      "187/500 [==========>...................] - ETA: 5:19 - loss: 1.8822 - regression_loss: 1.4053 - classification_loss: 0.4769\n",
      "188/500 [==========>...................] - ETA: 5:17 - loss: 1.8799 - regression_loss: 1.4037 - classification_loss: 0.4762\n",
      "189/500 [==========>...................] - ETA: 5:17 - loss: 1.8776 - regression_loss: 1.4012 - classification_loss: 0.4764\n",
      "190/500 [==========>...................] - ETA: 5:15 - loss: 1.8762 - regression_loss: 1.4008 - classification_loss: 0.4754\n",
      "191/500 [==========>...................] - ETA: 5:15 - loss: 1.8787 - regression_loss: 1.4020 - classification_loss: 0.4767\n",
      "192/500 [==========>...................] - ETA: 5:14 - loss: 1.8756 - regression_loss: 1.3999 - classification_loss: 0.4757\n",
      "193/500 [==========>...................] - ETA: 5:13 - loss: 1.8715 - regression_loss: 1.3966 - classification_loss: 0.4749\n",
      "194/500 [==========>...................] - ETA: 5:12 - loss: 1.8779 - regression_loss: 1.4011 - classification_loss: 0.4768\n",
      "195/500 [==========>...................] - ETA: 5:11 - loss: 1.8763 - regression_loss: 1.3991 - classification_loss: 0.4772\n",
      "196/500 [==========>...................] - ETA: 5:10 - loss: 1.8746 - regression_loss: 1.3977 - classification_loss: 0.4770\n",
      "197/500 [==========>...................] - ETA: 5:09 - loss: 1.8775 - regression_loss: 1.3992 - classification_loss: 0.4784\n",
      "198/500 [==========>...................] - ETA: 5:08 - loss: 1.8741 - regression_loss: 1.3966 - classification_loss: 0.4775\n",
      "199/500 [==========>...................] - ETA: 5:07 - loss: 1.8749 - regression_loss: 1.3968 - classification_loss: 0.4781\n",
      "200/500 [===========>..................] - ETA: 5:05 - loss: 1.8716 - regression_loss: 1.3942 - classification_loss: 0.4774\n",
      "201/500 [===========>..................] - ETA: 5:05 - loss: 1.8780 - regression_loss: 1.3981 - classification_loss: 0.4799\n",
      "202/500 [===========>..................] - ETA: 5:04 - loss: 1.8734 - regression_loss: 1.3948 - classification_loss: 0.4786\n",
      "203/500 [===========>..................] - ETA: 5:03 - loss: 1.8739 - regression_loss: 1.3949 - classification_loss: 0.4789\n",
      "204/500 [===========>..................] - ETA: 5:02 - loss: 1.8800 - regression_loss: 1.3989 - classification_loss: 0.4812\n",
      "205/500 [===========>..................] - ETA: 5:01 - loss: 1.8824 - regression_loss: 1.4003 - classification_loss: 0.4821\n",
      "206/500 [===========>..................] - ETA: 5:00 - loss: 1.8793 - regression_loss: 1.3984 - classification_loss: 0.4810\n",
      "207/500 [===========>..................] - ETA: 4:59 - loss: 1.8785 - regression_loss: 1.3980 - classification_loss: 0.4806\n",
      "208/500 [===========>..................] - ETA: 4:58 - loss: 1.8765 - regression_loss: 1.3964 - classification_loss: 0.4801\n",
      "209/500 [===========>..................] - ETA: 4:57 - loss: 1.8780 - regression_loss: 1.3968 - classification_loss: 0.4812\n",
      "210/500 [===========>..................] - ETA: 4:56 - loss: 1.8784 - regression_loss: 1.3973 - classification_loss: 0.4811\n",
      "211/500 [===========>..................] - ETA: 4:55 - loss: 1.8784 - regression_loss: 1.3974 - classification_loss: 0.4810\n",
      "212/500 [===========>..................] - ETA: 4:54 - loss: 1.8768 - regression_loss: 1.3958 - classification_loss: 0.4810\n",
      "213/500 [===========>..................] - ETA: 4:53 - loss: 1.8759 - regression_loss: 1.3954 - classification_loss: 0.4804\n",
      "214/500 [===========>..................] - ETA: 4:52 - loss: 1.8796 - regression_loss: 1.3991 - classification_loss: 0.4805\n",
      "215/500 [===========>..................] - ETA: 4:51 - loss: 1.8815 - regression_loss: 1.4014 - classification_loss: 0.4800\n",
      "216/500 [===========>..................] - ETA: 4:50 - loss: 1.8798 - regression_loss: 1.4005 - classification_loss: 0.4793\n",
      "217/500 [============>.................] - ETA: 4:49 - loss: 1.8785 - regression_loss: 1.3994 - classification_loss: 0.4791\n",
      "218/500 [============>.................] - ETA: 4:48 - loss: 1.8791 - regression_loss: 1.3998 - classification_loss: 0.4792\n",
      "219/500 [============>.................] - ETA: 4:47 - loss: 1.8812 - regression_loss: 1.4014 - classification_loss: 0.4798\n",
      "220/500 [============>.................] - ETA: 4:46 - loss: 1.8829 - regression_loss: 1.4027 - classification_loss: 0.4802\n",
      "221/500 [============>.................] - ETA: 4:45 - loss: 1.8774 - regression_loss: 1.3987 - classification_loss: 0.4788\n",
      "222/500 [============>.................] - ETA: 4:44 - loss: 1.8749 - regression_loss: 1.3966 - classification_loss: 0.4783\n",
      "223/500 [============>.................] - ETA: 4:43 - loss: 1.8735 - regression_loss: 1.3950 - classification_loss: 0.4784\n",
      "224/500 [============>.................] - ETA: 4:42 - loss: 1.8744 - regression_loss: 1.3954 - classification_loss: 0.4790\n",
      "225/500 [============>.................] - ETA: 4:41 - loss: 1.8773 - regression_loss: 1.3977 - classification_loss: 0.4796\n",
      "226/500 [============>.................] - ETA: 4:40 - loss: 1.8768 - regression_loss: 1.3971 - classification_loss: 0.4797\n",
      "227/500 [============>.................] - ETA: 4:38 - loss: 1.8760 - regression_loss: 1.3966 - classification_loss: 0.4794\n",
      "228/500 [============>.................] - ETA: 4:38 - loss: 1.8766 - regression_loss: 1.3969 - classification_loss: 0.4796\n",
      "229/500 [============>.................] - ETA: 4:37 - loss: 1.8752 - regression_loss: 1.3960 - classification_loss: 0.4792\n",
      "230/500 [============>.................] - ETA: 4:36 - loss: 1.8752 - regression_loss: 1.3958 - classification_loss: 0.4794\n",
      "231/500 [============>.................] - ETA: 4:35 - loss: 1.8747 - regression_loss: 1.3956 - classification_loss: 0.4791\n",
      "232/500 [============>.................] - ETA: 4:34 - loss: 1.8770 - regression_loss: 1.3976 - classification_loss: 0.4795\n",
      "233/500 [============>.................] - ETA: 4:33 - loss: 1.8789 - regression_loss: 1.3979 - classification_loss: 0.4811\n",
      "234/500 [=============>................] - ETA: 4:32 - loss: 1.8840 - regression_loss: 1.4021 - classification_loss: 0.4819\n",
      "235/500 [=============>................] - ETA: 4:31 - loss: 1.8826 - regression_loss: 1.4015 - classification_loss: 0.4811\n",
      "236/500 [=============>................] - ETA: 4:30 - loss: 1.8844 - regression_loss: 1.4027 - classification_loss: 0.4817\n",
      "237/500 [=============>................] - ETA: 4:29 - loss: 1.8850 - regression_loss: 1.4031 - classification_loss: 0.4819\n",
      "238/500 [=============>................] - ETA: 4:28 - loss: 1.8840 - regression_loss: 1.4015 - classification_loss: 0.4825\n",
      "239/500 [=============>................] - ETA: 4:27 - loss: 1.8832 - regression_loss: 1.4008 - classification_loss: 0.4824\n",
      "240/500 [=============>................] - ETA: 4:26 - loss: 1.8833 - regression_loss: 1.4008 - classification_loss: 0.4825\n",
      "241/500 [=============>................] - ETA: 4:25 - loss: 1.8812 - regression_loss: 1.3998 - classification_loss: 0.4813\n",
      "242/500 [=============>................] - ETA: 4:24 - loss: 1.8845 - regression_loss: 1.4026 - classification_loss: 0.4820\n",
      "243/500 [=============>................] - ETA: 4:23 - loss: 1.8837 - regression_loss: 1.4024 - classification_loss: 0.4813\n",
      "244/500 [=============>................] - ETA: 4:22 - loss: 1.8850 - regression_loss: 1.4035 - classification_loss: 0.4815\n",
      "245/500 [=============>................] - ETA: 4:20 - loss: 1.8841 - regression_loss: 1.4028 - classification_loss: 0.4814\n",
      "246/500 [=============>................] - ETA: 4:19 - loss: 1.8827 - regression_loss: 1.4011 - classification_loss: 0.4816\n",
      "247/500 [=============>................] - ETA: 4:18 - loss: 1.8836 - regression_loss: 1.4016 - classification_loss: 0.4819\n",
      "248/500 [=============>................] - ETA: 4:17 - loss: 1.8809 - regression_loss: 1.4001 - classification_loss: 0.4809\n",
      "249/500 [=============>................] - ETA: 4:16 - loss: 1.8797 - regression_loss: 1.3992 - classification_loss: 0.4805\n",
      "250/500 [==============>...............] - ETA: 4:15 - loss: 1.8797 - regression_loss: 1.3997 - classification_loss: 0.4800\n",
      "251/500 [==============>...............] - ETA: 4:14 - loss: 1.8813 - regression_loss: 1.4009 - classification_loss: 0.4804\n",
      "252/500 [==============>...............] - ETA: 4:13 - loss: 1.8847 - regression_loss: 1.4036 - classification_loss: 0.4811\n",
      "253/500 [==============>...............] - ETA: 4:12 - loss: 1.8841 - regression_loss: 1.4033 - classification_loss: 0.4808\n",
      "254/500 [==============>...............] - ETA: 4:11 - loss: 1.8831 - regression_loss: 1.4025 - classification_loss: 0.4805\n",
      "255/500 [==============>...............] - ETA: 4:10 - loss: 1.8802 - regression_loss: 1.4006 - classification_loss: 0.4796\n",
      "256/500 [==============>...............] - ETA: 4:09 - loss: 1.8834 - regression_loss: 1.4038 - classification_loss: 0.4796\n",
      "257/500 [==============>...............] - ETA: 4:08 - loss: 1.8857 - regression_loss: 1.4058 - classification_loss: 0.4799\n",
      "258/500 [==============>...............] - ETA: 4:07 - loss: 1.8860 - regression_loss: 1.4067 - classification_loss: 0.4793\n",
      "259/500 [==============>...............] - ETA: 4:06 - loss: 1.8858 - regression_loss: 1.4061 - classification_loss: 0.4796\n",
      "260/500 [==============>...............] - ETA: 4:05 - loss: 1.8857 - regression_loss: 1.4058 - classification_loss: 0.4798\n",
      "261/500 [==============>...............] - ETA: 4:04 - loss: 1.8829 - regression_loss: 1.4037 - classification_loss: 0.4792\n",
      "262/500 [==============>...............] - ETA: 4:03 - loss: 1.8805 - regression_loss: 1.4014 - classification_loss: 0.4791\n",
      "263/500 [==============>...............] - ETA: 4:02 - loss: 1.8793 - regression_loss: 1.4004 - classification_loss: 0.4789\n",
      "264/500 [==============>...............] - ETA: 4:01 - loss: 1.8834 - regression_loss: 1.4036 - classification_loss: 0.4797\n",
      "265/500 [==============>...............] - ETA: 4:00 - loss: 1.8834 - regression_loss: 1.4037 - classification_loss: 0.4797\n",
      "266/500 [==============>...............] - ETA: 3:59 - loss: 1.8835 - regression_loss: 1.4042 - classification_loss: 0.4794\n",
      "267/500 [===============>..............] - ETA: 3:58 - loss: 1.8873 - regression_loss: 1.4065 - classification_loss: 0.4808\n",
      "268/500 [===============>..............] - ETA: 3:57 - loss: 1.8867 - regression_loss: 1.4061 - classification_loss: 0.4806\n",
      "269/500 [===============>..............] - ETA: 3:56 - loss: 1.8842 - regression_loss: 1.4045 - classification_loss: 0.4798\n",
      "270/500 [===============>..............] - ETA: 3:55 - loss: 1.8825 - regression_loss: 1.4033 - classification_loss: 0.4792\n",
      "271/500 [===============>..............] - ETA: 3:54 - loss: 1.8809 - regression_loss: 1.4020 - classification_loss: 0.4789\n",
      "272/500 [===============>..............] - ETA: 3:53 - loss: 1.8802 - regression_loss: 1.4017 - classification_loss: 0.4785\n",
      "273/500 [===============>..............] - ETA: 3:52 - loss: 1.8796 - regression_loss: 1.4010 - classification_loss: 0.4786\n",
      "274/500 [===============>..............] - ETA: 3:51 - loss: 1.8778 - regression_loss: 1.3997 - classification_loss: 0.4781\n",
      "275/500 [===============>..............] - ETA: 3:50 - loss: 1.8787 - regression_loss: 1.4000 - classification_loss: 0.4787\n",
      "276/500 [===============>..............] - ETA: 3:49 - loss: 1.8813 - regression_loss: 1.4015 - classification_loss: 0.4798\n",
      "277/500 [===============>..............] - ETA: 3:48 - loss: 1.8801 - regression_loss: 1.4004 - classification_loss: 0.4797\n",
      "278/500 [===============>..............] - ETA: 3:47 - loss: 1.8786 - regression_loss: 1.3995 - classification_loss: 0.4791\n",
      "279/500 [===============>..............] - ETA: 3:46 - loss: 1.8787 - regression_loss: 1.3995 - classification_loss: 0.4791\n",
      "280/500 [===============>..............] - ETA: 3:45 - loss: 1.8774 - regression_loss: 1.3985 - classification_loss: 0.4789\n",
      "281/500 [===============>..............] - ETA: 3:44 - loss: 1.8795 - regression_loss: 1.4000 - classification_loss: 0.4795\n",
      "282/500 [===============>..............] - ETA: 3:43 - loss: 1.8800 - regression_loss: 1.4002 - classification_loss: 0.4798\n",
      "283/500 [===============>..............] - ETA: 3:42 - loss: 1.8839 - regression_loss: 1.4031 - classification_loss: 0.4808\n",
      "284/500 [================>.............] - ETA: 3:41 - loss: 1.8837 - regression_loss: 1.4031 - classification_loss: 0.4807\n",
      "285/500 [================>.............] - ETA: 3:40 - loss: 1.8835 - regression_loss: 1.4033 - classification_loss: 0.4802\n",
      "286/500 [================>.............] - ETA: 3:39 - loss: 1.8844 - regression_loss: 1.4035 - classification_loss: 0.4809\n",
      "287/500 [================>.............] - ETA: 3:38 - loss: 1.8848 - regression_loss: 1.4033 - classification_loss: 0.4815\n",
      "288/500 [================>.............] - ETA: 3:37 - loss: 1.8872 - regression_loss: 1.4052 - classification_loss: 0.4820\n",
      "289/500 [================>.............] - ETA: 3:36 - loss: 1.8871 - regression_loss: 1.4052 - classification_loss: 0.4819\n",
      "290/500 [================>.............] - ETA: 3:35 - loss: 1.8870 - regression_loss: 1.4054 - classification_loss: 0.4816\n",
      "291/500 [================>.............] - ETA: 3:34 - loss: 1.8891 - regression_loss: 1.4070 - classification_loss: 0.4821\n",
      "292/500 [================>.............] - ETA: 3:33 - loss: 1.8931 - regression_loss: 1.4078 - classification_loss: 0.4853\n",
      "293/500 [================>.............] - ETA: 3:32 - loss: 1.8925 - regression_loss: 1.4075 - classification_loss: 0.4849\n",
      "294/500 [================>.............] - ETA: 3:31 - loss: 1.8895 - regression_loss: 1.4055 - classification_loss: 0.4840\n",
      "295/500 [================>.............] - ETA: 3:30 - loss: 1.8876 - regression_loss: 1.4045 - classification_loss: 0.4831\n",
      "296/500 [================>.............] - ETA: 3:29 - loss: 1.8885 - regression_loss: 1.4055 - classification_loss: 0.4830\n",
      "297/500 [================>.............] - ETA: 3:28 - loss: 1.8897 - regression_loss: 1.4068 - classification_loss: 0.4829\n",
      "298/500 [================>.............] - ETA: 3:27 - loss: 1.8894 - regression_loss: 1.4068 - classification_loss: 0.4826\n",
      "299/500 [================>.............] - ETA: 3:26 - loss: 1.8900 - regression_loss: 1.4077 - classification_loss: 0.4823\n",
      "300/500 [=================>............] - ETA: 3:26 - loss: 1.8885 - regression_loss: 1.4066 - classification_loss: 0.4819\n",
      "301/500 [=================>............] - ETA: 3:25 - loss: 1.8851 - regression_loss: 1.4040 - classification_loss: 0.4811\n",
      "302/500 [=================>............] - ETA: 3:24 - loss: 1.8831 - regression_loss: 1.4026 - classification_loss: 0.4805\n",
      "303/500 [=================>............] - ETA: 3:23 - loss: 1.8865 - regression_loss: 1.4053 - classification_loss: 0.4813\n",
      "304/500 [=================>............] - ETA: 3:22 - loss: 1.8846 - regression_loss: 1.4038 - classification_loss: 0.4807\n",
      "305/500 [=================>............] - ETA: 3:21 - loss: 1.8861 - regression_loss: 1.4048 - classification_loss: 0.4813\n",
      "306/500 [=================>............] - ETA: 3:20 - loss: 1.8849 - regression_loss: 1.4040 - classification_loss: 0.4809\n",
      "307/500 [=================>............] - ETA: 3:19 - loss: 1.8847 - regression_loss: 1.4037 - classification_loss: 0.4810\n",
      "308/500 [=================>............] - ETA: 3:18 - loss: 1.8821 - regression_loss: 1.4014 - classification_loss: 0.4806\n",
      "309/500 [=================>............] - ETA: 3:17 - loss: 1.8811 - regression_loss: 1.4006 - classification_loss: 0.4804\n",
      "310/500 [=================>............] - ETA: 3:15 - loss: 1.8810 - regression_loss: 1.4005 - classification_loss: 0.4805\n",
      "311/500 [=================>............] - ETA: 3:14 - loss: 1.8792 - regression_loss: 1.3991 - classification_loss: 0.4801\n",
      "312/500 [=================>............] - ETA: 3:13 - loss: 1.8783 - regression_loss: 1.3987 - classification_loss: 0.4796\n",
      "313/500 [=================>............] - ETA: 3:12 - loss: 1.8785 - regression_loss: 1.3987 - classification_loss: 0.4798\n",
      "314/500 [=================>............] - ETA: 3:11 - loss: 1.8772 - regression_loss: 1.3971 - classification_loss: 0.4801\n",
      "315/500 [=================>............] - ETA: 3:10 - loss: 1.8794 - regression_loss: 1.3981 - classification_loss: 0.4813\n",
      "316/500 [=================>............] - ETA: 3:09 - loss: 1.8803 - regression_loss: 1.3987 - classification_loss: 0.4815\n",
      "317/500 [==================>...........] - ETA: 3:08 - loss: 1.8815 - regression_loss: 1.3998 - classification_loss: 0.4817\n",
      "318/500 [==================>...........] - ETA: 3:07 - loss: 1.8821 - regression_loss: 1.4005 - classification_loss: 0.4816\n",
      "319/500 [==================>...........] - ETA: 3:06 - loss: 1.8823 - regression_loss: 1.4006 - classification_loss: 0.4816\n",
      "320/500 [==================>...........] - ETA: 3:05 - loss: 1.8818 - regression_loss: 1.4002 - classification_loss: 0.4816\n",
      "321/500 [==================>...........] - ETA: 3:04 - loss: 1.8854 - regression_loss: 1.4022 - classification_loss: 0.4832\n",
      "322/500 [==================>...........] - ETA: 3:03 - loss: 1.8863 - regression_loss: 1.4031 - classification_loss: 0.4832\n",
      "323/500 [==================>...........] - ETA: 3:02 - loss: 1.8884 - regression_loss: 1.4049 - classification_loss: 0.4835\n",
      "324/500 [==================>...........] - ETA: 3:01 - loss: 1.8872 - regression_loss: 1.4042 - classification_loss: 0.4830\n",
      "325/500 [==================>...........] - ETA: 3:00 - loss: 1.8894 - regression_loss: 1.4056 - classification_loss: 0.4838\n",
      "326/500 [==================>...........] - ETA: 2:59 - loss: 1.8886 - regression_loss: 1.4050 - classification_loss: 0.4836\n",
      "327/500 [==================>...........] - ETA: 2:58 - loss: 1.8893 - regression_loss: 1.4050 - classification_loss: 0.4842\n",
      "328/500 [==================>...........] - ETA: 2:57 - loss: 1.8882 - regression_loss: 1.4043 - classification_loss: 0.4838\n",
      "329/500 [==================>...........] - ETA: 2:56 - loss: 1.8888 - regression_loss: 1.4046 - classification_loss: 0.4842\n",
      "330/500 [==================>...........] - ETA: 2:55 - loss: 1.8897 - regression_loss: 1.4053 - classification_loss: 0.4844\n",
      "331/500 [==================>...........] - ETA: 2:54 - loss: 1.8889 - regression_loss: 1.4045 - classification_loss: 0.4844\n",
      "332/500 [==================>...........] - ETA: 2:53 - loss: 1.8887 - regression_loss: 1.4044 - classification_loss: 0.4843\n",
      "333/500 [==================>...........] - ETA: 2:52 - loss: 1.8897 - regression_loss: 1.4055 - classification_loss: 0.4842\n",
      "334/500 [===================>..........] - ETA: 2:51 - loss: 1.8886 - regression_loss: 1.4046 - classification_loss: 0.4839\n",
      "335/500 [===================>..........] - ETA: 2:50 - loss: 1.8880 - regression_loss: 1.4042 - classification_loss: 0.4838\n",
      "336/500 [===================>..........] - ETA: 2:49 - loss: 1.8896 - regression_loss: 1.4058 - classification_loss: 0.4838\n",
      "337/500 [===================>..........] - ETA: 2:48 - loss: 1.8883 - regression_loss: 1.4048 - classification_loss: 0.4835\n",
      "338/500 [===================>..........] - ETA: 2:47 - loss: 1.8909 - regression_loss: 1.4072 - classification_loss: 0.4837\n",
      "339/500 [===================>..........] - ETA: 2:46 - loss: 1.8929 - regression_loss: 1.4088 - classification_loss: 0.4841\n",
      "340/500 [===================>..........] - ETA: 2:45 - loss: 1.8931 - regression_loss: 1.4092 - classification_loss: 0.4840\n",
      "341/500 [===================>..........] - ETA: 2:44 - loss: 1.8908 - regression_loss: 1.4075 - classification_loss: 0.4833\n",
      "342/500 [===================>..........] - ETA: 2:43 - loss: 1.8896 - regression_loss: 1.4071 - classification_loss: 0.4826\n",
      "343/500 [===================>..........] - ETA: 2:42 - loss: 1.8886 - regression_loss: 1.4065 - classification_loss: 0.4821\n",
      "344/500 [===================>..........] - ETA: 2:41 - loss: 1.8880 - regression_loss: 1.4061 - classification_loss: 0.4820\n",
      "345/500 [===================>..........] - ETA: 2:40 - loss: 1.8882 - regression_loss: 1.4064 - classification_loss: 0.4818\n",
      "346/500 [===================>..........] - ETA: 2:39 - loss: 1.8903 - regression_loss: 1.4079 - classification_loss: 0.4824\n",
      "347/500 [===================>..........] - ETA: 2:38 - loss: 1.8880 - regression_loss: 1.4062 - classification_loss: 0.4818\n",
      "348/500 [===================>..........] - ETA: 2:37 - loss: 1.8865 - regression_loss: 1.4052 - classification_loss: 0.4813\n",
      "349/500 [===================>..........] - ETA: 2:36 - loss: 1.8875 - regression_loss: 1.4059 - classification_loss: 0.4815\n",
      "350/500 [====================>.........] - ETA: 2:35 - loss: 1.8874 - regression_loss: 1.4054 - classification_loss: 0.4819\n",
      "351/500 [====================>.........] - ETA: 2:34 - loss: 1.8869 - regression_loss: 1.4055 - classification_loss: 0.4814\n",
      "352/500 [====================>.........] - ETA: 2:33 - loss: 1.8899 - regression_loss: 1.4071 - classification_loss: 0.4829\n",
      "353/500 [====================>.........] - ETA: 2:31 - loss: 1.8886 - regression_loss: 1.4062 - classification_loss: 0.4823\n",
      "354/500 [====================>.........] - ETA: 2:30 - loss: 1.8891 - regression_loss: 1.4065 - classification_loss: 0.4825\n",
      "355/500 [====================>.........] - ETA: 2:29 - loss: 1.8868 - regression_loss: 1.4050 - classification_loss: 0.4818\n",
      "356/500 [====================>.........] - ETA: 2:28 - loss: 1.8874 - regression_loss: 1.4055 - classification_loss: 0.4819\n",
      "357/500 [====================>.........] - ETA: 2:27 - loss: 1.8888 - regression_loss: 1.4064 - classification_loss: 0.4824\n",
      "358/500 [====================>.........] - ETA: 2:26 - loss: 1.8872 - regression_loss: 1.4052 - classification_loss: 0.4820\n",
      "359/500 [====================>.........] - ETA: 2:25 - loss: 1.8870 - regression_loss: 1.4054 - classification_loss: 0.4816\n",
      "360/500 [====================>.........] - ETA: 2:24 - loss: 1.8842 - regression_loss: 1.4033 - classification_loss: 0.4809\n",
      "361/500 [====================>.........] - ETA: 2:23 - loss: 1.8850 - regression_loss: 1.4040 - classification_loss: 0.4811\n",
      "362/500 [====================>.........] - ETA: 2:22 - loss: 1.8870 - regression_loss: 1.4053 - classification_loss: 0.4817\n",
      "363/500 [====================>.........] - ETA: 2:21 - loss: 1.8867 - regression_loss: 1.4055 - classification_loss: 0.4812\n",
      "364/500 [====================>.........] - ETA: 2:20 - loss: 1.8870 - regression_loss: 1.4062 - classification_loss: 0.4808\n",
      "365/500 [====================>.........] - ETA: 2:19 - loss: 1.8871 - regression_loss: 1.4068 - classification_loss: 0.4803\n",
      "366/500 [====================>.........] - ETA: 2:18 - loss: 1.8871 - regression_loss: 1.4064 - classification_loss: 0.4807\n",
      "367/500 [=====================>........] - ETA: 2:17 - loss: 1.8870 - regression_loss: 1.4064 - classification_loss: 0.4806\n",
      "368/500 [=====================>........] - ETA: 2:16 - loss: 1.8903 - regression_loss: 1.4086 - classification_loss: 0.4817\n",
      "369/500 [=====================>........] - ETA: 2:15 - loss: 1.8901 - regression_loss: 1.4087 - classification_loss: 0.4814\n",
      "370/500 [=====================>........] - ETA: 2:14 - loss: 1.8898 - regression_loss: 1.4085 - classification_loss: 0.4813\n",
      "371/500 [=====================>........] - ETA: 2:13 - loss: 1.8894 - regression_loss: 1.4084 - classification_loss: 0.4810\n",
      "372/500 [=====================>........] - ETA: 2:12 - loss: 1.8891 - regression_loss: 1.4078 - classification_loss: 0.4813\n",
      "373/500 [=====================>........] - ETA: 2:11 - loss: 1.8903 - regression_loss: 1.4086 - classification_loss: 0.4818\n",
      "374/500 [=====================>........] - ETA: 2:10 - loss: 1.8928 - regression_loss: 1.4105 - classification_loss: 0.4823\n",
      "375/500 [=====================>........] - ETA: 2:09 - loss: 1.8924 - regression_loss: 1.4105 - classification_loss: 0.4819\n",
      "376/500 [=====================>........] - ETA: 2:09 - loss: 1.8943 - regression_loss: 1.4111 - classification_loss: 0.4832\n",
      "377/500 [=====================>........] - ETA: 2:08 - loss: 1.8923 - regression_loss: 1.4099 - classification_loss: 0.4824\n",
      "378/500 [=====================>........] - ETA: 2:07 - loss: 1.8921 - regression_loss: 1.4100 - classification_loss: 0.4821\n",
      "379/500 [=====================>........] - ETA: 2:06 - loss: 1.8919 - regression_loss: 1.4102 - classification_loss: 0.4817\n",
      "380/500 [=====================>........] - ETA: 2:05 - loss: 1.8909 - regression_loss: 1.4093 - classification_loss: 0.4815\n",
      "381/500 [=====================>........] - ETA: 2:04 - loss: 1.8899 - regression_loss: 1.4086 - classification_loss: 0.4813\n",
      "382/500 [=====================>........] - ETA: 2:03 - loss: 1.8899 - regression_loss: 1.4087 - classification_loss: 0.4811\n",
      "383/500 [=====================>........] - ETA: 2:02 - loss: 1.8893 - regression_loss: 1.4081 - classification_loss: 0.4812\n",
      "384/500 [======================>.......] - ETA: 2:01 - loss: 1.8899 - regression_loss: 1.4087 - classification_loss: 0.4812\n",
      "385/500 [======================>.......] - ETA: 2:00 - loss: 1.8921 - regression_loss: 1.4098 - classification_loss: 0.4824\n",
      "386/500 [======================>.......] - ETA: 1:58 - loss: 1.8904 - regression_loss: 1.4084 - classification_loss: 0.4820\n",
      "387/500 [======================>.......] - ETA: 1:57 - loss: 1.8897 - regression_loss: 1.4083 - classification_loss: 0.4813\n",
      "388/500 [======================>.......] - ETA: 1:56 - loss: 1.8890 - regression_loss: 1.4077 - classification_loss: 0.4813\n",
      "389/500 [======================>.......] - ETA: 1:55 - loss: 1.8885 - regression_loss: 1.4078 - classification_loss: 0.4807\n",
      "390/500 [======================>.......] - ETA: 1:54 - loss: 1.8895 - regression_loss: 1.4085 - classification_loss: 0.4810\n",
      "391/500 [======================>.......] - ETA: 1:53 - loss: 1.8913 - regression_loss: 1.4094 - classification_loss: 0.4819\n",
      "392/500 [======================>.......] - ETA: 1:52 - loss: 1.8901 - regression_loss: 1.4086 - classification_loss: 0.4816\n",
      "393/500 [======================>.......] - ETA: 1:51 - loss: 1.8907 - regression_loss: 1.4092 - classification_loss: 0.4815\n",
      "394/500 [======================>.......] - ETA: 1:50 - loss: 1.8902 - regression_loss: 1.4089 - classification_loss: 0.4813\n",
      "395/500 [======================>.......] - ETA: 1:49 - loss: 1.8889 - regression_loss: 1.4079 - classification_loss: 0.4810\n",
      "396/500 [======================>.......] - ETA: 1:48 - loss: 1.8874 - regression_loss: 1.4069 - classification_loss: 0.4805\n",
      "397/500 [======================>.......] - ETA: 1:47 - loss: 1.8881 - regression_loss: 1.4075 - classification_loss: 0.4806\n",
      "398/500 [======================>.......] - ETA: 1:46 - loss: 1.8876 - regression_loss: 1.4073 - classification_loss: 0.4802\n",
      "399/500 [======================>.......] - ETA: 1:45 - loss: 1.8912 - regression_loss: 1.4097 - classification_loss: 0.4815\n",
      "400/500 [=======================>......] - ETA: 1:44 - loss: 1.8927 - regression_loss: 1.4110 - classification_loss: 0.4817\n",
      "401/500 [=======================>......] - ETA: 1:43 - loss: 1.8942 - regression_loss: 1.4121 - classification_loss: 0.4821\n",
      "402/500 [=======================>......] - ETA: 1:42 - loss: 1.8947 - regression_loss: 1.4127 - classification_loss: 0.4820\n",
      "403/500 [=======================>......] - ETA: 1:41 - loss: 1.8932 - regression_loss: 1.4118 - classification_loss: 0.4814\n",
      "404/500 [=======================>......] - ETA: 1:40 - loss: 1.8933 - regression_loss: 1.4118 - classification_loss: 0.4815\n",
      "405/500 [=======================>......] - ETA: 1:39 - loss: 1.8941 - regression_loss: 1.4127 - classification_loss: 0.4814\n",
      "406/500 [=======================>......] - ETA: 1:38 - loss: 1.8935 - regression_loss: 1.4122 - classification_loss: 0.4813\n",
      "407/500 [=======================>......] - ETA: 1:37 - loss: 1.8937 - regression_loss: 1.4121 - classification_loss: 0.4816\n",
      "408/500 [=======================>......] - ETA: 1:35 - loss: 1.8937 - regression_loss: 1.4121 - classification_loss: 0.4816\n",
      "409/500 [=======================>......] - ETA: 1:34 - loss: 1.8964 - regression_loss: 1.4139 - classification_loss: 0.4824\n",
      "410/500 [=======================>......] - ETA: 1:33 - loss: 1.8956 - regression_loss: 1.4133 - classification_loss: 0.4823\n",
      "411/500 [=======================>......] - ETA: 1:32 - loss: 1.8948 - regression_loss: 1.4126 - classification_loss: 0.4822\n",
      "412/500 [=======================>......] - ETA: 1:31 - loss: 1.8960 - regression_loss: 1.4134 - classification_loss: 0.4827\n",
      "413/500 [=======================>......] - ETA: 1:30 - loss: 1.8968 - regression_loss: 1.4144 - classification_loss: 0.4823\n",
      "414/500 [=======================>......] - ETA: 1:29 - loss: 1.8969 - regression_loss: 1.4150 - classification_loss: 0.4819\n",
      "415/500 [=======================>......] - ETA: 1:28 - loss: 1.8961 - regression_loss: 1.4147 - classification_loss: 0.4813\n",
      "416/500 [=======================>......] - ETA: 1:27 - loss: 1.8951 - regression_loss: 1.4137 - classification_loss: 0.4814\n",
      "417/500 [========================>.....] - ETA: 1:26 - loss: 1.8955 - regression_loss: 1.4140 - classification_loss: 0.4815\n",
      "418/500 [========================>.....] - ETA: 1:25 - loss: 1.8949 - regression_loss: 1.4133 - classification_loss: 0.4815\n",
      "419/500 [========================>.....] - ETA: 1:24 - loss: 1.8939 - regression_loss: 1.4129 - classification_loss: 0.4810\n",
      "420/500 [========================>.....] - ETA: 1:23 - loss: 1.8921 - regression_loss: 1.4117 - classification_loss: 0.4804\n",
      "421/500 [========================>.....] - ETA: 1:22 - loss: 1.8933 - regression_loss: 1.4130 - classification_loss: 0.4803\n",
      "422/500 [========================>.....] - ETA: 1:21 - loss: 1.8936 - regression_loss: 1.4131 - classification_loss: 0.4805\n",
      "423/500 [========================>.....] - ETA: 1:20 - loss: 1.8933 - regression_loss: 1.4129 - classification_loss: 0.4804\n",
      "424/500 [========================>.....] - ETA: 1:19 - loss: 1.8923 - regression_loss: 1.4121 - classification_loss: 0.4802\n",
      "425/500 [========================>.....] - ETA: 1:18 - loss: 1.8922 - regression_loss: 1.4122 - classification_loss: 0.4801\n",
      "426/500 [========================>.....] - ETA: 1:17 - loss: 1.8927 - regression_loss: 1.4128 - classification_loss: 0.4799\n",
      "427/500 [========================>.....] - ETA: 1:16 - loss: 1.8932 - regression_loss: 1.4131 - classification_loss: 0.4801\n",
      "428/500 [========================>.....] - ETA: 1:15 - loss: 1.8930 - regression_loss: 1.4129 - classification_loss: 0.4800\n",
      "429/500 [========================>.....] - ETA: 1:14 - loss: 1.8925 - regression_loss: 1.4126 - classification_loss: 0.4799\n",
      "430/500 [========================>.....] - ETA: 1:12 - loss: 1.8922 - regression_loss: 1.4125 - classification_loss: 0.4797\n",
      "431/500 [========================>.....] - ETA: 1:11 - loss: 1.8906 - regression_loss: 1.4112 - classification_loss: 0.4795\n",
      "432/500 [========================>.....] - ETA: 1:10 - loss: 1.8900 - regression_loss: 1.4109 - classification_loss: 0.4791\n",
      "433/500 [========================>.....] - ETA: 1:09 - loss: 1.8912 - regression_loss: 1.4117 - classification_loss: 0.4794\n",
      "434/500 [=========================>....] - ETA: 1:08 - loss: 1.8917 - regression_loss: 1.4122 - classification_loss: 0.4795\n",
      "435/500 [=========================>....] - ETA: 1:07 - loss: 1.8920 - regression_loss: 1.4125 - classification_loss: 0.4794\n",
      "436/500 [=========================>....] - ETA: 1:06 - loss: 1.8910 - regression_loss: 1.4120 - classification_loss: 0.4790\n",
      "437/500 [=========================>....] - ETA: 1:05 - loss: 1.8913 - regression_loss: 1.4121 - classification_loss: 0.4792\n",
      "438/500 [=========================>....] - ETA: 1:04 - loss: 1.8914 - regression_loss: 1.4124 - classification_loss: 0.4790\n",
      "439/500 [=========================>....] - ETA: 1:03 - loss: 1.8901 - regression_loss: 1.4115 - classification_loss: 0.4787\n",
      "440/500 [=========================>....] - ETA: 1:02 - loss: 1.8910 - regression_loss: 1.4124 - classification_loss: 0.4786\n",
      "441/500 [=========================>....] - ETA: 1:01 - loss: 1.8909 - regression_loss: 1.4121 - classification_loss: 0.4789\n",
      "442/500 [=========================>....] - ETA: 1:00 - loss: 1.8897 - regression_loss: 1.4112 - classification_loss: 0.4785\n",
      "443/500 [=========================>....] - ETA: 59s - loss: 1.8898 - regression_loss: 1.4112 - classification_loss: 0.4786 \n",
      "444/500 [=========================>....] - ETA: 58s - loss: 1.8891 - regression_loss: 1.4101 - classification_loss: 0.4790\n",
      "445/500 [=========================>....] - ETA: 57s - loss: 1.8884 - regression_loss: 1.4095 - classification_loss: 0.4789\n",
      "446/500 [=========================>....] - ETA: 56s - loss: 1.8867 - regression_loss: 1.4083 - classification_loss: 0.4784\n",
      "447/500 [=========================>....] - ETA: 55s - loss: 1.8867 - regression_loss: 1.4077 - classification_loss: 0.4790\n",
      "448/500 [=========================>....] - ETA: 54s - loss: 1.8861 - regression_loss: 1.4072 - classification_loss: 0.4789\n",
      "449/500 [=========================>....] - ETA: 53s - loss: 1.8875 - regression_loss: 1.4084 - classification_loss: 0.4791\n",
      "450/500 [==========================>...] - ETA: 52s - loss: 1.8854 - regression_loss: 1.4068 - classification_loss: 0.4786\n",
      "451/500 [==========================>...] - ETA: 51s - loss: 1.8852 - regression_loss: 1.4065 - classification_loss: 0.4786\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 1.8849 - regression_loss: 1.4060 - classification_loss: 0.4789\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.8850 - regression_loss: 1.4064 - classification_loss: 0.4786\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.8833 - regression_loss: 1.4051 - classification_loss: 0.4783\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.8849 - regression_loss: 1.4063 - classification_loss: 0.4786\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.8844 - regression_loss: 1.4061 - classification_loss: 0.4784\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.8831 - regression_loss: 1.4052 - classification_loss: 0.4778\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.8827 - regression_loss: 1.4049 - classification_loss: 0.4778\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.8806 - regression_loss: 1.4035 - classification_loss: 0.4771\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.8820 - regression_loss: 1.4041 - classification_loss: 0.4779\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 1.8837 - regression_loss: 1.4053 - classification_loss: 0.4784\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 1.8837 - regression_loss: 1.4054 - classification_loss: 0.4783\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 1.8834 - regression_loss: 1.4050 - classification_loss: 0.4784\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 1.8839 - regression_loss: 1.4055 - classification_loss: 0.4784\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 1.8844 - regression_loss: 1.4061 - classification_loss: 0.4783\n",
      "466/500 [==========================>...] - ETA: 35s - loss: 1.8826 - regression_loss: 1.4049 - classification_loss: 0.4777\n",
      "467/500 [===========================>..] - ETA: 34s - loss: 1.8842 - regression_loss: 1.4062 - classification_loss: 0.4780\n",
      "468/500 [===========================>..] - ETA: 33s - loss: 1.8828 - regression_loss: 1.4052 - classification_loss: 0.4776\n",
      "469/500 [===========================>..] - ETA: 32s - loss: 1.8829 - regression_loss: 1.4050 - classification_loss: 0.4779\n",
      "470/500 [===========================>..] - ETA: 31s - loss: 1.8856 - regression_loss: 1.4073 - classification_loss: 0.4783\n",
      "471/500 [===========================>..] - ETA: 30s - loss: 1.8849 - regression_loss: 1.4069 - classification_loss: 0.4780\n",
      "472/500 [===========================>..] - ETA: 29s - loss: 1.8845 - regression_loss: 1.4065 - classification_loss: 0.4780\n",
      "473/500 [===========================>..] - ETA: 28s - loss: 1.8847 - regression_loss: 1.4069 - classification_loss: 0.4778\n",
      "474/500 [===========================>..] - ETA: 27s - loss: 1.8857 - regression_loss: 1.4075 - classification_loss: 0.4781\n",
      "475/500 [===========================>..] - ETA: 26s - loss: 1.8858 - regression_loss: 1.4076 - classification_loss: 0.4782\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 1.8850 - regression_loss: 1.4069 - classification_loss: 0.4781\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.8862 - regression_loss: 1.4079 - classification_loss: 0.4783\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.8852 - regression_loss: 1.4071 - classification_loss: 0.4781\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.8867 - regression_loss: 1.4083 - classification_loss: 0.4784\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.8855 - regression_loss: 1.4075 - classification_loss: 0.4780\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.8856 - regression_loss: 1.4075 - classification_loss: 0.4780\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.8863 - regression_loss: 1.4080 - classification_loss: 0.4782\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.8874 - regression_loss: 1.4090 - classification_loss: 0.4783\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.8878 - regression_loss: 1.4092 - classification_loss: 0.4786\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.8897 - regression_loss: 1.4103 - classification_loss: 0.4793\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.8898 - regression_loss: 1.4104 - classification_loss: 0.4795\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.8886 - regression_loss: 1.4094 - classification_loss: 0.4791\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.8873 - regression_loss: 1.4086 - classification_loss: 0.4787\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.8860 - regression_loss: 1.4079 - classification_loss: 0.4781\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.8857 - regression_loss: 1.4079 - classification_loss: 0.4778\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.8860 - regression_loss: 1.4083 - classification_loss: 0.4777 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.8855 - regression_loss: 1.4079 - classification_loss: 0.4776\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.8837 - regression_loss: 1.4063 - classification_loss: 0.4773\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.8852 - regression_loss: 1.4070 - classification_loss: 0.4782\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.8869 - regression_loss: 1.4077 - classification_loss: 0.4792\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.8873 - regression_loss: 1.4076 - classification_loss: 0.4797\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.8891 - regression_loss: 1.4087 - classification_loss: 0.4805\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.8887 - regression_loss: 1.4084 - classification_loss: 0.4804\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.8906 - regression_loss: 1.4100 - classification_loss: 0.4806\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.8890 - regression_loss: 1.4088 - classification_loss: 0.4802\n",
      "Epoch 00034: saving model to ./snapshots\\resnet50_csv_34.h5\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 9.9999988758398e-14.\n",
      "\n",
      "500/500 [==============================] - 525s 1s/step - loss: 1.8890 - regression_loss: 1.4088 - classification_loss: 0.4802\n",
      "Epoch 35/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 2.0840 - regression_loss: 1.4275 - classification_loss: 0.6564\n",
      "  2/500 [..............................] - ETA: 4:07 - loss: 1.6763 - regression_loss: 1.1750 - classification_loss: 0.5013\n",
      "  3/500 [..............................] - ETA: 4:54 - loss: 1.9460 - regression_loss: 1.4223 - classification_loss: 0.5237\n",
      "  4/500 [..............................] - ETA: 6:07 - loss: 1.9523 - regression_loss: 1.4206 - classification_loss: 0.5317\n",
      "  5/500 [..............................] - ETA: 6:42 - loss: 1.9856 - regression_loss: 1.4568 - classification_loss: 0.5288\n",
      "  6/500 [..............................] - ETA: 6:55 - loss: 1.9159 - regression_loss: 1.4368 - classification_loss: 0.4791\n",
      "  7/500 [..............................] - ETA: 7:04 - loss: 1.9808 - regression_loss: 1.4943 - classification_loss: 0.4865\n",
      "  8/500 [..............................] - ETA: 7:12 - loss: 2.0132 - regression_loss: 1.5060 - classification_loss: 0.5072\n",
      "  9/500 [..............................] - ETA: 7:16 - loss: 1.9968 - regression_loss: 1.4911 - classification_loss: 0.5057\n",
      " 10/500 [..............................] - ETA: 7:26 - loss: 2.0421 - regression_loss: 1.5323 - classification_loss: 0.5098\n",
      " 11/500 [..............................] - ETA: 7:32 - loss: 2.0961 - regression_loss: 1.5760 - classification_loss: 0.5201\n",
      " 12/500 [..............................] - ETA: 7:34 - loss: 2.1175 - regression_loss: 1.5940 - classification_loss: 0.5235\n",
      " 13/500 [..............................] - ETA: 7:35 - loss: 2.1106 - regression_loss: 1.5877 - classification_loss: 0.5229\n",
      " 14/500 [..............................] - ETA: 7:36 - loss: 2.0978 - regression_loss: 1.5851 - classification_loss: 0.5127\n",
      " 15/500 [..............................] - ETA: 7:31 - loss: 2.0996 - regression_loss: 1.5827 - classification_loss: 0.5169\n",
      " 16/500 [..............................] - ETA: 7:33 - loss: 2.0562 - regression_loss: 1.5472 - classification_loss: 0.5090\n",
      " 17/500 [>.............................] - ETA: 7:32 - loss: 2.0218 - regression_loss: 1.5118 - classification_loss: 0.5100\n",
      " 18/500 [>.............................] - ETA: 7:38 - loss: 2.0381 - regression_loss: 1.5151 - classification_loss: 0.5231\n",
      " 19/500 [>.............................] - ETA: 7:32 - loss: 2.0493 - regression_loss: 1.5221 - classification_loss: 0.5272\n",
      " 20/500 [>.............................] - ETA: 7:36 - loss: 2.0479 - regression_loss: 1.5291 - classification_loss: 0.5188\n",
      " 21/500 [>.............................] - ETA: 7:36 - loss: 2.0818 - regression_loss: 1.5467 - classification_loss: 0.5351\n",
      " 22/500 [>.............................] - ETA: 7:33 - loss: 2.0523 - regression_loss: 1.5260 - classification_loss: 0.5263\n",
      " 23/500 [>.............................] - ETA: 7:37 - loss: 2.0332 - regression_loss: 1.5147 - classification_loss: 0.5185\n",
      " 24/500 [>.............................] - ETA: 7:39 - loss: 2.0275 - regression_loss: 1.5099 - classification_loss: 0.5176\n",
      " 25/500 [>.............................] - ETA: 7:43 - loss: 2.0074 - regression_loss: 1.4928 - classification_loss: 0.5146\n",
      " 26/500 [>.............................] - ETA: 7:38 - loss: 1.9773 - regression_loss: 1.4677 - classification_loss: 0.5096\n",
      " 27/500 [>.............................] - ETA: 7:41 - loss: 1.9661 - regression_loss: 1.4591 - classification_loss: 0.5070\n",
      " 28/500 [>.............................] - ETA: 7:42 - loss: 1.9780 - regression_loss: 1.4702 - classification_loss: 0.5078\n",
      " 29/500 [>.............................] - ETA: 7:41 - loss: 1.9797 - regression_loss: 1.4704 - classification_loss: 0.5092\n",
      " 30/500 [>.............................] - ETA: 7:42 - loss: 2.0012 - regression_loss: 1.4834 - classification_loss: 0.5178\n",
      " 31/500 [>.............................] - ETA: 7:41 - loss: 2.0058 - regression_loss: 1.4885 - classification_loss: 0.5173\n",
      " 32/500 [>.............................] - ETA: 7:40 - loss: 1.9935 - regression_loss: 1.4792 - classification_loss: 0.5143\n",
      " 33/500 [>.............................] - ETA: 7:41 - loss: 1.9867 - regression_loss: 1.4695 - classification_loss: 0.5173\n",
      " 34/500 [=>............................] - ETA: 7:40 - loss: 1.9883 - regression_loss: 1.4747 - classification_loss: 0.5137\n",
      " 35/500 [=>............................] - ETA: 7:39 - loss: 2.0232 - regression_loss: 1.5012 - classification_loss: 0.5220\n",
      " 36/500 [=>............................] - ETA: 7:38 - loss: 2.0211 - regression_loss: 1.5015 - classification_loss: 0.5196\n",
      " 37/500 [=>............................] - ETA: 7:37 - loss: 2.0290 - regression_loss: 1.5130 - classification_loss: 0.5161\n",
      " 38/500 [=>............................] - ETA: 7:33 - loss: 2.0469 - regression_loss: 1.5201 - classification_loss: 0.5268\n",
      " 39/500 [=>............................] - ETA: 7:35 - loss: 2.0611 - regression_loss: 1.5310 - classification_loss: 0.5301\n",
      " 40/500 [=>............................] - ETA: 7:34 - loss: 2.0636 - regression_loss: 1.5360 - classification_loss: 0.5276\n",
      " 41/500 [=>............................] - ETA: 7:33 - loss: 2.0464 - regression_loss: 1.5243 - classification_loss: 0.5221\n",
      " 42/500 [=>............................] - ETA: 7:32 - loss: 2.0288 - regression_loss: 1.5111 - classification_loss: 0.5177\n",
      " 43/500 [=>............................] - ETA: 7:32 - loss: 2.0214 - regression_loss: 1.5062 - classification_loss: 0.5152\n",
      " 44/500 [=>............................] - ETA: 7:31 - loss: 2.0050 - regression_loss: 1.4944 - classification_loss: 0.5107\n",
      " 45/500 [=>............................] - ETA: 7:30 - loss: 1.9902 - regression_loss: 1.4849 - classification_loss: 0.5053\n",
      " 46/500 [=>............................] - ETA: 7:29 - loss: 1.9931 - regression_loss: 1.4864 - classification_loss: 0.5067\n",
      " 47/500 [=>............................] - ETA: 7:28 - loss: 1.9955 - regression_loss: 1.4878 - classification_loss: 0.5077\n",
      " 48/500 [=>............................] - ETA: 7:28 - loss: 2.0077 - regression_loss: 1.4929 - classification_loss: 0.5148\n",
      " 49/500 [=>............................] - ETA: 7:27 - loss: 2.0179 - regression_loss: 1.4943 - classification_loss: 0.5235\n",
      " 50/500 [==>...........................] - ETA: 7:27 - loss: 2.0139 - regression_loss: 1.4940 - classification_loss: 0.5200\n",
      " 51/500 [==>...........................] - ETA: 7:26 - loss: 2.0160 - regression_loss: 1.4937 - classification_loss: 0.5223\n",
      " 52/500 [==>...........................] - ETA: 7:26 - loss: 2.0181 - regression_loss: 1.4942 - classification_loss: 0.5239\n",
      " 53/500 [==>...........................] - ETA: 7:25 - loss: 2.0128 - regression_loss: 1.4905 - classification_loss: 0.5223\n",
      " 54/500 [==>...........................] - ETA: 7:24 - loss: 2.0201 - regression_loss: 1.4941 - classification_loss: 0.5260\n",
      " 55/500 [==>...........................] - ETA: 7:24 - loss: 2.0076 - regression_loss: 1.4861 - classification_loss: 0.5215\n",
      " 56/500 [==>...........................] - ETA: 7:23 - loss: 2.0054 - regression_loss: 1.4849 - classification_loss: 0.5205\n",
      " 57/500 [==>...........................] - ETA: 7:23 - loss: 2.0267 - regression_loss: 1.4970 - classification_loss: 0.5296\n",
      " 58/500 [==>...........................] - ETA: 7:20 - loss: 2.0078 - regression_loss: 1.4830 - classification_loss: 0.5248\n",
      " 59/500 [==>...........................] - ETA: 7:19 - loss: 2.0043 - regression_loss: 1.4812 - classification_loss: 0.5231\n",
      " 60/500 [==>...........................] - ETA: 7:19 - loss: 2.0061 - regression_loss: 1.4836 - classification_loss: 0.5226\n",
      " 61/500 [==>...........................] - ETA: 7:17 - loss: 1.9987 - regression_loss: 1.4787 - classification_loss: 0.5200\n",
      " 62/500 [==>...........................] - ETA: 7:17 - loss: 1.9991 - regression_loss: 1.4787 - classification_loss: 0.5203\n",
      " 63/500 [==>...........................] - ETA: 7:17 - loss: 2.0038 - regression_loss: 1.4837 - classification_loss: 0.5202\n",
      " 64/500 [==>...........................] - ETA: 7:14 - loss: 2.0052 - regression_loss: 1.4850 - classification_loss: 0.5202\n",
      " 65/500 [==>...........................] - ETA: 7:14 - loss: 1.9957 - regression_loss: 1.4760 - classification_loss: 0.5198\n",
      " 66/500 [==>...........................] - ETA: 7:14 - loss: 1.9959 - regression_loss: 1.4765 - classification_loss: 0.5193\n",
      " 67/500 [===>..........................] - ETA: 7:13 - loss: 2.0022 - regression_loss: 1.4813 - classification_loss: 0.5208\n",
      " 68/500 [===>..........................] - ETA: 7:13 - loss: 2.0017 - regression_loss: 1.4831 - classification_loss: 0.5185\n",
      " 69/500 [===>..........................] - ETA: 7:12 - loss: 2.0046 - regression_loss: 1.4864 - classification_loss: 0.5182\n",
      " 70/500 [===>..........................] - ETA: 7:17 - loss: 2.0219 - regression_loss: 1.4980 - classification_loss: 0.5239\n",
      " 71/500 [===>..........................] - ETA: 7:17 - loss: 2.0154 - regression_loss: 1.4938 - classification_loss: 0.5216\n",
      " 72/500 [===>..........................] - ETA: 7:16 - loss: 2.0127 - regression_loss: 1.4933 - classification_loss: 0.5195\n",
      " 73/500 [===>..........................] - ETA: 7:13 - loss: 2.0046 - regression_loss: 1.4881 - classification_loss: 0.5166\n",
      " 74/500 [===>..........................] - ETA: 7:14 - loss: 2.0146 - regression_loss: 1.4945 - classification_loss: 0.5201\n",
      " 75/500 [===>..........................] - ETA: 7:12 - loss: 2.0077 - regression_loss: 1.4905 - classification_loss: 0.5172\n",
      " 76/500 [===>..........................] - ETA: 7:11 - loss: 1.9993 - regression_loss: 1.4848 - classification_loss: 0.5145\n",
      " 77/500 [===>..........................] - ETA: 7:11 - loss: 1.9980 - regression_loss: 1.4850 - classification_loss: 0.5130\n",
      " 78/500 [===>..........................] - ETA: 7:09 - loss: 1.9934 - regression_loss: 1.4818 - classification_loss: 0.5115\n",
      " 79/500 [===>..........................] - ETA: 7:08 - loss: 2.0019 - regression_loss: 1.4864 - classification_loss: 0.5155\n",
      " 80/500 [===>..........................] - ETA: 7:07 - loss: 2.0066 - regression_loss: 1.4901 - classification_loss: 0.5166\n",
      " 81/500 [===>..........................] - ETA: 7:06 - loss: 1.9980 - regression_loss: 1.4830 - classification_loss: 0.5150\n",
      " 82/500 [===>..........................] - ETA: 7:05 - loss: 1.9967 - regression_loss: 1.4806 - classification_loss: 0.5161\n",
      " 83/500 [===>..........................] - ETA: 7:03 - loss: 1.9971 - regression_loss: 1.4815 - classification_loss: 0.5156\n",
      " 84/500 [====>.........................] - ETA: 7:03 - loss: 1.9992 - regression_loss: 1.4842 - classification_loss: 0.5150\n",
      " 85/500 [====>.........................] - ETA: 7:02 - loss: 2.0101 - regression_loss: 1.4887 - classification_loss: 0.5214\n",
      " 86/500 [====>.........................] - ETA: 7:01 - loss: 2.0185 - regression_loss: 1.4925 - classification_loss: 0.5260\n",
      " 87/500 [====>.........................] - ETA: 7:01 - loss: 2.0201 - regression_loss: 1.4944 - classification_loss: 0.5257\n",
      " 88/500 [====>.........................] - ETA: 7:00 - loss: 2.0168 - regression_loss: 1.4915 - classification_loss: 0.5254\n",
      " 89/500 [====>.........................] - ETA: 6:59 - loss: 2.0165 - regression_loss: 1.4913 - classification_loss: 0.5252\n",
      " 90/500 [====>.........................] - ETA: 6:59 - loss: 2.0178 - regression_loss: 1.4940 - classification_loss: 0.5239\n",
      " 91/500 [====>.........................] - ETA: 6:58 - loss: 2.0163 - regression_loss: 1.4950 - classification_loss: 0.5214\n",
      " 92/500 [====>.........................] - ETA: 6:57 - loss: 2.0201 - regression_loss: 1.4977 - classification_loss: 0.5224\n",
      " 93/500 [====>.........................] - ETA: 6:55 - loss: 2.0101 - regression_loss: 1.4910 - classification_loss: 0.5191\n",
      " 94/500 [====>.........................] - ETA: 6:55 - loss: 2.0181 - regression_loss: 1.4966 - classification_loss: 0.5215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 95/500 [====>.........................] - ETA: 6:53 - loss: 2.0180 - regression_loss: 1.4945 - classification_loss: 0.5235\n",
      " 96/500 [====>.........................] - ETA: 6:52 - loss: 2.0055 - regression_loss: 1.4851 - classification_loss: 0.5203\n",
      " 97/500 [====>.........................] - ETA: 6:52 - loss: 2.0065 - regression_loss: 1.4856 - classification_loss: 0.5209\n",
      " 98/500 [====>.........................] - ETA: 6:49 - loss: 2.0051 - regression_loss: 1.4838 - classification_loss: 0.5213\n",
      " 99/500 [====>.........................] - ETA: 6:49 - loss: 2.0019 - regression_loss: 1.4820 - classification_loss: 0.5199\n",
      "100/500 [=====>........................] - ETA: 6:48 - loss: 2.0068 - regression_loss: 1.4861 - classification_loss: 0.5207\n",
      "101/500 [=====>........................] - ETA: 6:47 - loss: 1.9991 - regression_loss: 1.4805 - classification_loss: 0.5186\n",
      "102/500 [=====>........................] - ETA: 6:46 - loss: 1.9897 - regression_loss: 1.4728 - classification_loss: 0.5169\n",
      "103/500 [=====>........................] - ETA: 6:45 - loss: 1.9873 - regression_loss: 1.4686 - classification_loss: 0.5187\n",
      "104/500 [=====>........................] - ETA: 6:45 - loss: 1.9859 - regression_loss: 1.4674 - classification_loss: 0.5186\n",
      "105/500 [=====>........................] - ETA: 6:44 - loss: 1.9927 - regression_loss: 1.4721 - classification_loss: 0.5206\n",
      "106/500 [=====>........................] - ETA: 6:43 - loss: 1.9976 - regression_loss: 1.4751 - classification_loss: 0.5225\n",
      "107/500 [=====>........................] - ETA: 6:42 - loss: 1.9896 - regression_loss: 1.4693 - classification_loss: 0.5202\n",
      "108/500 [=====>........................] - ETA: 6:41 - loss: 1.9895 - regression_loss: 1.4685 - classification_loss: 0.5211\n",
      "109/500 [=====>........................] - ETA: 6:40 - loss: 1.9944 - regression_loss: 1.4714 - classification_loss: 0.5230\n",
      "110/500 [=====>........................] - ETA: 6:38 - loss: 1.9930 - regression_loss: 1.4684 - classification_loss: 0.5246\n",
      "111/500 [=====>........................] - ETA: 6:38 - loss: 1.9920 - regression_loss: 1.4660 - classification_loss: 0.5260\n",
      "112/500 [=====>........................] - ETA: 6:37 - loss: 1.9935 - regression_loss: 1.4673 - classification_loss: 0.5262\n",
      "113/500 [=====>........................] - ETA: 6:36 - loss: 1.9855 - regression_loss: 1.4618 - classification_loss: 0.5237\n",
      "114/500 [=====>........................] - ETA: 6:34 - loss: 1.9827 - regression_loss: 1.4590 - classification_loss: 0.5236\n",
      "115/500 [=====>........................] - ETA: 6:34 - loss: 1.9804 - regression_loss: 1.4577 - classification_loss: 0.5227\n",
      "116/500 [=====>........................] - ETA: 6:33 - loss: 1.9845 - regression_loss: 1.4620 - classification_loss: 0.5225\n",
      "117/500 [======>.......................] - ETA: 6:32 - loss: 1.9797 - regression_loss: 1.4593 - classification_loss: 0.5204\n",
      "118/500 [======>.......................] - ETA: 6:31 - loss: 1.9764 - regression_loss: 1.4574 - classification_loss: 0.5191\n",
      "119/500 [======>.......................] - ETA: 6:30 - loss: 1.9816 - regression_loss: 1.4626 - classification_loss: 0.5190\n",
      "120/500 [======>.......................] - ETA: 6:29 - loss: 1.9875 - regression_loss: 1.4661 - classification_loss: 0.5214\n",
      "121/500 [======>.......................] - ETA: 6:28 - loss: 1.9869 - regression_loss: 1.4637 - classification_loss: 0.5232\n",
      "122/500 [======>.......................] - ETA: 6:28 - loss: 1.9926 - regression_loss: 1.4684 - classification_loss: 0.5243\n",
      "123/500 [======>.......................] - ETA: 6:27 - loss: 1.9898 - regression_loss: 1.4671 - classification_loss: 0.5227\n",
      "124/500 [======>.......................] - ETA: 6:26 - loss: 1.9944 - regression_loss: 1.4701 - classification_loss: 0.5243\n",
      "125/500 [======>.......................] - ETA: 6:25 - loss: 1.9962 - regression_loss: 1.4726 - classification_loss: 0.5236\n",
      "126/500 [======>.......................] - ETA: 6:24 - loss: 1.9941 - regression_loss: 1.4706 - classification_loss: 0.5236\n",
      "127/500 [======>.......................] - ETA: 6:23 - loss: 1.9910 - regression_loss: 1.4687 - classification_loss: 0.5223\n",
      "128/500 [======>.......................] - ETA: 6:22 - loss: 1.9888 - regression_loss: 1.4668 - classification_loss: 0.5220\n",
      "129/500 [======>.......................] - ETA: 6:21 - loss: 1.9842 - regression_loss: 1.4629 - classification_loss: 0.5213\n",
      "130/500 [======>.......................] - ETA: 6:20 - loss: 1.9795 - regression_loss: 1.4595 - classification_loss: 0.5200\n",
      "131/500 [======>.......................] - ETA: 6:19 - loss: 1.9745 - regression_loss: 1.4561 - classification_loss: 0.5184\n",
      "132/500 [======>.......................] - ETA: 6:18 - loss: 1.9675 - regression_loss: 1.4512 - classification_loss: 0.5163\n",
      "133/500 [======>.......................] - ETA: 6:17 - loss: 1.9746 - regression_loss: 1.4559 - classification_loss: 0.5187\n",
      "134/500 [=======>......................] - ETA: 6:16 - loss: 1.9778 - regression_loss: 1.4588 - classification_loss: 0.5190\n",
      "135/500 [=======>......................] - ETA: 6:15 - loss: 1.9743 - regression_loss: 1.4556 - classification_loss: 0.5187\n",
      "136/500 [=======>......................] - ETA: 6:14 - loss: 1.9752 - regression_loss: 1.4571 - classification_loss: 0.5181\n",
      "137/500 [=======>......................] - ETA: 6:13 - loss: 1.9686 - regression_loss: 1.4528 - classification_loss: 0.5159\n",
      "138/500 [=======>......................] - ETA: 6:12 - loss: 1.9722 - regression_loss: 1.4566 - classification_loss: 0.5155\n",
      "139/500 [=======>......................] - ETA: 6:11 - loss: 1.9719 - regression_loss: 1.4571 - classification_loss: 0.5147\n",
      "140/500 [=======>......................] - ETA: 6:10 - loss: 1.9659 - regression_loss: 1.4521 - classification_loss: 0.5138\n",
      "141/500 [=======>......................] - ETA: 6:09 - loss: 1.9689 - regression_loss: 1.4540 - classification_loss: 0.5149\n",
      "142/500 [=======>......................] - ETA: 6:08 - loss: 1.9703 - regression_loss: 1.4554 - classification_loss: 0.5149\n",
      "143/500 [=======>......................] - ETA: 6:07 - loss: 1.9687 - regression_loss: 1.4544 - classification_loss: 0.5143\n",
      "144/500 [=======>......................] - ETA: 6:05 - loss: 1.9683 - regression_loss: 1.4535 - classification_loss: 0.5148\n",
      "145/500 [=======>......................] - ETA: 6:05 - loss: 1.9688 - regression_loss: 1.4546 - classification_loss: 0.5141\n",
      "146/500 [=======>......................] - ETA: 6:04 - loss: 1.9667 - regression_loss: 1.4528 - classification_loss: 0.5139\n",
      "147/500 [=======>......................] - ETA: 6:03 - loss: 1.9661 - regression_loss: 1.4527 - classification_loss: 0.5134\n",
      "148/500 [=======>......................] - ETA: 6:02 - loss: 1.9722 - regression_loss: 1.4570 - classification_loss: 0.5152\n",
      "149/500 [=======>......................] - ETA: 6:01 - loss: 1.9693 - regression_loss: 1.4538 - classification_loss: 0.5155\n",
      "150/500 [========>.....................] - ETA: 6:00 - loss: 1.9778 - regression_loss: 1.4581 - classification_loss: 0.5197\n",
      "151/500 [========>.....................] - ETA: 5:59 - loss: 1.9756 - regression_loss: 1.4566 - classification_loss: 0.5190\n",
      "152/500 [========>.....................] - ETA: 5:58 - loss: 1.9780 - regression_loss: 1.4581 - classification_loss: 0.5200\n",
      "153/500 [========>.....................] - ETA: 6:06 - loss: 1.9842 - regression_loss: 1.4608 - classification_loss: 0.5234\n",
      "154/500 [========>.....................] - ETA: 6:05 - loss: 1.9793 - regression_loss: 1.4577 - classification_loss: 0.5216\n",
      "155/500 [========>.....................] - ETA: 6:04 - loss: 1.9844 - regression_loss: 1.4613 - classification_loss: 0.5231\n",
      "156/500 [========>.....................] - ETA: 6:03 - loss: 1.9849 - regression_loss: 1.4621 - classification_loss: 0.5227\n",
      "157/500 [========>.....................] - ETA: 6:02 - loss: 1.9826 - regression_loss: 1.4615 - classification_loss: 0.5211\n",
      "158/500 [========>.....................] - ETA: 6:00 - loss: 1.9788 - regression_loss: 1.4596 - classification_loss: 0.5192\n",
      "159/500 [========>.....................] - ETA: 5:59 - loss: 1.9785 - regression_loss: 1.4598 - classification_loss: 0.5187\n",
      "160/500 [========>.....................] - ETA: 5:58 - loss: 1.9765 - regression_loss: 1.4587 - classification_loss: 0.5177\n",
      "161/500 [========>.....................] - ETA: 5:57 - loss: 1.9781 - regression_loss: 1.4592 - classification_loss: 0.5189\n",
      "162/500 [========>.....................] - ETA: 5:56 - loss: 1.9797 - regression_loss: 1.4584 - classification_loss: 0.5213\n",
      "163/500 [========>.....................] - ETA: 5:55 - loss: 1.9836 - regression_loss: 1.4616 - classification_loss: 0.5220\n",
      "164/500 [========>.....................] - ETA: 5:54 - loss: 1.9868 - regression_loss: 1.4637 - classification_loss: 0.5231\n",
      "165/500 [========>.....................] - ETA: 5:53 - loss: 1.9867 - regression_loss: 1.4641 - classification_loss: 0.5226\n",
      "166/500 [========>.....................] - ETA: 5:51 - loss: 1.9878 - regression_loss: 1.4653 - classification_loss: 0.5225\n",
      "167/500 [=========>....................] - ETA: 5:50 - loss: 1.9854 - regression_loss: 1.4638 - classification_loss: 0.5216\n",
      "168/500 [=========>....................] - ETA: 5:49 - loss: 1.9855 - regression_loss: 1.4644 - classification_loss: 0.5211\n",
      "169/500 [=========>....................] - ETA: 5:48 - loss: 1.9853 - regression_loss: 1.4643 - classification_loss: 0.5210\n",
      "170/500 [=========>....................] - ETA: 5:47 - loss: 1.9844 - regression_loss: 1.4636 - classification_loss: 0.5208\n",
      "171/500 [=========>....................] - ETA: 5:46 - loss: 1.9826 - regression_loss: 1.4631 - classification_loss: 0.5195\n",
      "172/500 [=========>....................] - ETA: 5:45 - loss: 1.9802 - regression_loss: 1.4619 - classification_loss: 0.5182\n",
      "173/500 [=========>....................] - ETA: 5:43 - loss: 1.9810 - regression_loss: 1.4630 - classification_loss: 0.5180\n",
      "174/500 [=========>....................] - ETA: 5:42 - loss: 1.9795 - regression_loss: 1.4624 - classification_loss: 0.5171\n",
      "175/500 [=========>....................] - ETA: 5:41 - loss: 1.9818 - regression_loss: 1.4635 - classification_loss: 0.5183\n",
      "176/500 [=========>....................] - ETA: 5:40 - loss: 1.9795 - regression_loss: 1.4616 - classification_loss: 0.5179\n",
      "177/500 [=========>....................] - ETA: 5:38 - loss: 1.9810 - regression_loss: 1.4632 - classification_loss: 0.5177\n",
      "178/500 [=========>....................] - ETA: 5:37 - loss: 1.9849 - regression_loss: 1.4658 - classification_loss: 0.5190\n",
      "179/500 [=========>....................] - ETA: 5:36 - loss: 1.9895 - regression_loss: 1.4698 - classification_loss: 0.5197\n",
      "180/500 [=========>....................] - ETA: 5:35 - loss: 1.9924 - regression_loss: 1.4718 - classification_loss: 0.5206\n",
      "181/500 [=========>....................] - ETA: 5:34 - loss: 1.9922 - regression_loss: 1.4716 - classification_loss: 0.5206\n",
      "182/500 [=========>....................] - ETA: 5:33 - loss: 1.9887 - regression_loss: 1.4691 - classification_loss: 0.5196\n",
      "183/500 [=========>....................] - ETA: 5:31 - loss: 1.9879 - regression_loss: 1.4692 - classification_loss: 0.5187\n",
      "184/500 [==========>...................] - ETA: 5:31 - loss: 1.9837 - regression_loss: 1.4667 - classification_loss: 0.5170\n",
      "185/500 [==========>...................] - ETA: 5:30 - loss: 1.9827 - regression_loss: 1.4659 - classification_loss: 0.5168\n",
      "186/500 [==========>...................] - ETA: 5:28 - loss: 1.9811 - regression_loss: 1.4655 - classification_loss: 0.5156\n",
      "187/500 [==========>...................] - ETA: 5:27 - loss: 1.9809 - regression_loss: 1.4651 - classification_loss: 0.5158\n",
      "188/500 [==========>...................] - ETA: 5:26 - loss: 1.9806 - regression_loss: 1.4637 - classification_loss: 0.5168\n",
      "189/500 [==========>...................] - ETA: 5:25 - loss: 1.9807 - regression_loss: 1.4640 - classification_loss: 0.5167\n",
      "190/500 [==========>...................] - ETA: 5:24 - loss: 1.9800 - regression_loss: 1.4643 - classification_loss: 0.5157\n",
      "191/500 [==========>...................] - ETA: 5:23 - loss: 1.9788 - regression_loss: 1.4632 - classification_loss: 0.5156\n",
      "192/500 [==========>...................] - ETA: 5:22 - loss: 1.9742 - regression_loss: 1.4603 - classification_loss: 0.5139\n",
      "193/500 [==========>...................] - ETA: 5:21 - loss: 1.9763 - regression_loss: 1.4609 - classification_loss: 0.5154\n",
      "194/500 [==========>...................] - ETA: 5:20 - loss: 1.9750 - regression_loss: 1.4607 - classification_loss: 0.5142\n",
      "195/500 [==========>...................] - ETA: 5:19 - loss: 1.9746 - regression_loss: 1.4606 - classification_loss: 0.5140\n",
      "196/500 [==========>...................] - ETA: 5:18 - loss: 1.9731 - regression_loss: 1.4600 - classification_loss: 0.5131\n",
      "197/500 [==========>...................] - ETA: 5:17 - loss: 1.9720 - regression_loss: 1.4601 - classification_loss: 0.5119\n",
      "198/500 [==========>...................] - ETA: 5:16 - loss: 1.9690 - regression_loss: 1.4579 - classification_loss: 0.5111\n",
      "199/500 [==========>...................] - ETA: 5:15 - loss: 1.9692 - regression_loss: 1.4581 - classification_loss: 0.5111\n",
      "200/500 [===========>..................] - ETA: 5:14 - loss: 1.9672 - regression_loss: 1.4568 - classification_loss: 0.5104\n",
      "201/500 [===========>..................] - ETA: 5:13 - loss: 1.9659 - regression_loss: 1.4558 - classification_loss: 0.5101\n",
      "202/500 [===========>..................] - ETA: 5:12 - loss: 1.9666 - regression_loss: 1.4565 - classification_loss: 0.5100\n",
      "203/500 [===========>..................] - ETA: 5:11 - loss: 1.9703 - regression_loss: 1.4591 - classification_loss: 0.5113\n",
      "204/500 [===========>..................] - ETA: 5:10 - loss: 1.9654 - regression_loss: 1.4557 - classification_loss: 0.5097\n",
      "205/500 [===========>..................] - ETA: 5:09 - loss: 1.9678 - regression_loss: 1.4575 - classification_loss: 0.5103\n",
      "206/500 [===========>..................] - ETA: 5:07 - loss: 1.9701 - regression_loss: 1.4591 - classification_loss: 0.5110\n",
      "207/500 [===========>..................] - ETA: 5:06 - loss: 1.9757 - regression_loss: 1.4631 - classification_loss: 0.5126\n",
      "208/500 [===========>..................] - ETA: 5:05 - loss: 1.9768 - regression_loss: 1.4642 - classification_loss: 0.5126\n",
      "209/500 [===========>..................] - ETA: 5:04 - loss: 1.9818 - regression_loss: 1.4670 - classification_loss: 0.5149\n",
      "210/500 [===========>..................] - ETA: 5:03 - loss: 1.9819 - regression_loss: 1.4671 - classification_loss: 0.5148\n",
      "211/500 [===========>..................] - ETA: 5:02 - loss: 1.9828 - regression_loss: 1.4676 - classification_loss: 0.5152\n",
      "212/500 [===========>..................] - ETA: 5:01 - loss: 1.9781 - regression_loss: 1.4645 - classification_loss: 0.5136\n",
      "213/500 [===========>..................] - ETA: 5:00 - loss: 1.9750 - regression_loss: 1.4619 - classification_loss: 0.5131\n",
      "214/500 [===========>..................] - ETA: 4:59 - loss: 1.9767 - regression_loss: 1.4630 - classification_loss: 0.5137\n",
      "215/500 [===========>..................] - ETA: 4:58 - loss: 1.9741 - regression_loss: 1.4614 - classification_loss: 0.5126\n",
      "216/500 [===========>..................] - ETA: 4:57 - loss: 1.9763 - regression_loss: 1.4629 - classification_loss: 0.5135\n",
      "217/500 [============>.................] - ETA: 4:56 - loss: 1.9756 - regression_loss: 1.4627 - classification_loss: 0.5129\n",
      "218/500 [============>.................] - ETA: 4:55 - loss: 1.9723 - regression_loss: 1.4604 - classification_loss: 0.5119\n",
      "219/500 [============>.................] - ETA: 4:54 - loss: 1.9728 - regression_loss: 1.4608 - classification_loss: 0.5120\n",
      "220/500 [============>.................] - ETA: 4:53 - loss: 1.9740 - regression_loss: 1.4618 - classification_loss: 0.5122\n",
      "221/500 [============>.................] - ETA: 4:52 - loss: 1.9766 - regression_loss: 1.4641 - classification_loss: 0.5125\n",
      "222/500 [============>.................] - ETA: 4:51 - loss: 1.9749 - regression_loss: 1.4625 - classification_loss: 0.5123\n",
      "223/500 [============>.................] - ETA: 4:50 - loss: 1.9714 - regression_loss: 1.4595 - classification_loss: 0.5119\n",
      "224/500 [============>.................] - ETA: 4:49 - loss: 1.9681 - regression_loss: 1.4573 - classification_loss: 0.5108\n",
      "225/500 [============>.................] - ETA: 4:47 - loss: 1.9641 - regression_loss: 1.4548 - classification_loss: 0.5093\n",
      "226/500 [============>.................] - ETA: 4:47 - loss: 1.9628 - regression_loss: 1.4529 - classification_loss: 0.5100\n",
      "227/500 [============>.................] - ETA: 4:46 - loss: 1.9604 - regression_loss: 1.4511 - classification_loss: 0.5093\n",
      "228/500 [============>.................] - ETA: 4:44 - loss: 1.9594 - regression_loss: 1.4505 - classification_loss: 0.5090\n",
      "229/500 [============>.................] - ETA: 4:43 - loss: 1.9600 - regression_loss: 1.4511 - classification_loss: 0.5089\n",
      "230/500 [============>.................] - ETA: 4:42 - loss: 1.9569 - regression_loss: 1.4492 - classification_loss: 0.5077\n",
      "231/500 [============>.................] - ETA: 4:41 - loss: 1.9535 - regression_loss: 1.4470 - classification_loss: 0.5065\n",
      "232/500 [============>.................] - ETA: 4:40 - loss: 1.9536 - regression_loss: 1.4467 - classification_loss: 0.5070\n",
      "233/500 [============>.................] - ETA: 4:39 - loss: 1.9534 - regression_loss: 1.4464 - classification_loss: 0.5070\n",
      "234/500 [=============>................] - ETA: 4:38 - loss: 1.9559 - regression_loss: 1.4479 - classification_loss: 0.5079\n",
      "235/500 [=============>................] - ETA: 4:37 - loss: 1.9541 - regression_loss: 1.4470 - classification_loss: 0.5071\n",
      "236/500 [=============>................] - ETA: 4:36 - loss: 1.9520 - regression_loss: 1.4458 - classification_loss: 0.5061\n",
      "237/500 [=============>................] - ETA: 4:35 - loss: 1.9547 - regression_loss: 1.4484 - classification_loss: 0.5063\n",
      "238/500 [=============>................] - ETA: 4:34 - loss: 1.9536 - regression_loss: 1.4472 - classification_loss: 0.5064\n",
      "239/500 [=============>................] - ETA: 4:33 - loss: 1.9533 - regression_loss: 1.4472 - classification_loss: 0.5062\n",
      "240/500 [=============>................] - ETA: 4:31 - loss: 1.9540 - regression_loss: 1.4485 - classification_loss: 0.5055\n",
      "241/500 [=============>................] - ETA: 4:30 - loss: 1.9547 - regression_loss: 1.4494 - classification_loss: 0.5053\n",
      "242/500 [=============>................] - ETA: 4:29 - loss: 1.9520 - regression_loss: 1.4477 - classification_loss: 0.5043\n",
      "243/500 [=============>................] - ETA: 4:28 - loss: 1.9521 - regression_loss: 1.4481 - classification_loss: 0.5039\n",
      "244/500 [=============>................] - ETA: 4:27 - loss: 1.9509 - regression_loss: 1.4478 - classification_loss: 0.5031\n",
      "245/500 [=============>................] - ETA: 4:26 - loss: 1.9513 - regression_loss: 1.4478 - classification_loss: 0.5035\n",
      "246/500 [=============>................] - ETA: 4:25 - loss: 1.9484 - regression_loss: 1.4461 - classification_loss: 0.5024\n",
      "247/500 [=============>................] - ETA: 4:24 - loss: 1.9476 - regression_loss: 1.4452 - classification_loss: 0.5024\n",
      "248/500 [=============>................] - ETA: 4:23 - loss: 1.9516 - regression_loss: 1.4479 - classification_loss: 0.5037\n",
      "249/500 [=============>................] - ETA: 4:22 - loss: 1.9551 - regression_loss: 1.4500 - classification_loss: 0.5051\n",
      "250/500 [==============>...............] - ETA: 4:21 - loss: 1.9510 - regression_loss: 1.4472 - classification_loss: 0.5038\n",
      "251/500 [==============>...............] - ETA: 4:20 - loss: 1.9505 - regression_loss: 1.4465 - classification_loss: 0.5040\n",
      "252/500 [==============>...............] - ETA: 4:18 - loss: 1.9523 - regression_loss: 1.4479 - classification_loss: 0.5043\n",
      "253/500 [==============>...............] - ETA: 4:17 - loss: 1.9510 - regression_loss: 1.4470 - classification_loss: 0.5040\n",
      "254/500 [==============>...............] - ETA: 4:16 - loss: 1.9531 - regression_loss: 1.4481 - classification_loss: 0.5050\n",
      "255/500 [==============>...............] - ETA: 4:15 - loss: 1.9517 - regression_loss: 1.4472 - classification_loss: 0.5045\n",
      "256/500 [==============>...............] - ETA: 4:14 - loss: 1.9505 - regression_loss: 1.4465 - classification_loss: 0.5040\n",
      "257/500 [==============>...............] - ETA: 4:13 - loss: 1.9528 - regression_loss: 1.4479 - classification_loss: 0.5048\n",
      "258/500 [==============>...............] - ETA: 4:12 - loss: 1.9559 - regression_loss: 1.4498 - classification_loss: 0.5061\n",
      "259/500 [==============>...............] - ETA: 4:11 - loss: 1.9570 - regression_loss: 1.4507 - classification_loss: 0.5063\n",
      "260/500 [==============>...............] - ETA: 4:10 - loss: 1.9530 - regression_loss: 1.4478 - classification_loss: 0.5052\n",
      "261/500 [==============>...............] - ETA: 4:09 - loss: 1.9524 - regression_loss: 1.4479 - classification_loss: 0.5045\n",
      "262/500 [==============>...............] - ETA: 4:08 - loss: 1.9509 - regression_loss: 1.4463 - classification_loss: 0.5046\n",
      "263/500 [==============>...............] - ETA: 4:07 - loss: 1.9527 - regression_loss: 1.4483 - classification_loss: 0.5044\n",
      "264/500 [==============>...............] - ETA: 4:06 - loss: 1.9540 - regression_loss: 1.4499 - classification_loss: 0.5040\n",
      "265/500 [==============>...............] - ETA: 4:05 - loss: 1.9543 - regression_loss: 1.4501 - classification_loss: 0.5042\n",
      "266/500 [==============>...............] - ETA: 4:04 - loss: 1.9528 - regression_loss: 1.4489 - classification_loss: 0.5039\n",
      "267/500 [===============>..............] - ETA: 4:03 - loss: 1.9495 - regression_loss: 1.4466 - classification_loss: 0.5028\n",
      "268/500 [===============>..............] - ETA: 4:02 - loss: 1.9484 - regression_loss: 1.4463 - classification_loss: 0.5021\n",
      "269/500 [===============>..............] - ETA: 4:01 - loss: 1.9471 - regression_loss: 1.4455 - classification_loss: 0.5016\n",
      "270/500 [===============>..............] - ETA: 3:59 - loss: 1.9434 - regression_loss: 1.4429 - classification_loss: 0.5005\n",
      "271/500 [===============>..............] - ETA: 3:58 - loss: 1.9418 - regression_loss: 1.4414 - classification_loss: 0.5004\n",
      "272/500 [===============>..............] - ETA: 3:57 - loss: 1.9401 - regression_loss: 1.4404 - classification_loss: 0.4997\n",
      "273/500 [===============>..............] - ETA: 3:56 - loss: 1.9414 - regression_loss: 1.4406 - classification_loss: 0.5008\n",
      "274/500 [===============>..............] - ETA: 3:55 - loss: 1.9399 - regression_loss: 1.4399 - classification_loss: 0.5000\n",
      "275/500 [===============>..............] - ETA: 3:54 - loss: 1.9398 - regression_loss: 1.4396 - classification_loss: 0.5002\n",
      "276/500 [===============>..............] - ETA: 3:53 - loss: 1.9426 - regression_loss: 1.4415 - classification_loss: 0.5011\n",
      "277/500 [===============>..............] - ETA: 3:52 - loss: 1.9420 - regression_loss: 1.4408 - classification_loss: 0.5012\n",
      "278/500 [===============>..............] - ETA: 3:51 - loss: 1.9432 - regression_loss: 1.4413 - classification_loss: 0.5019\n",
      "279/500 [===============>..............] - ETA: 3:50 - loss: 1.9407 - regression_loss: 1.4395 - classification_loss: 0.5012\n",
      "280/500 [===============>..............] - ETA: 3:49 - loss: 1.9408 - regression_loss: 1.4395 - classification_loss: 0.5012\n",
      "281/500 [===============>..............] - ETA: 3:48 - loss: 1.9428 - regression_loss: 1.4402 - classification_loss: 0.5026\n",
      "282/500 [===============>..............] - ETA: 3:47 - loss: 1.9431 - regression_loss: 1.4407 - classification_loss: 0.5023\n",
      "283/500 [===============>..............] - ETA: 3:45 - loss: 1.9400 - regression_loss: 1.4384 - classification_loss: 0.5016\n",
      "284/500 [================>.............] - ETA: 3:44 - loss: 1.9395 - regression_loss: 1.4380 - classification_loss: 0.5015\n",
      "285/500 [================>.............] - ETA: 3:43 - loss: 1.9395 - regression_loss: 1.4381 - classification_loss: 0.5014\n",
      "286/500 [================>.............] - ETA: 3:42 - loss: 1.9417 - regression_loss: 1.4402 - classification_loss: 0.5015\n",
      "287/500 [================>.............] - ETA: 3:41 - loss: 1.9409 - regression_loss: 1.4399 - classification_loss: 0.5010\n",
      "288/500 [================>.............] - ETA: 3:40 - loss: 1.9413 - regression_loss: 1.4405 - classification_loss: 0.5008\n",
      "289/500 [================>.............] - ETA: 3:39 - loss: 1.9398 - regression_loss: 1.4391 - classification_loss: 0.5007\n",
      "290/500 [================>.............] - ETA: 3:38 - loss: 1.9428 - regression_loss: 1.4412 - classification_loss: 0.5016\n",
      "291/500 [================>.............] - ETA: 3:37 - loss: 1.9404 - regression_loss: 1.4396 - classification_loss: 0.5008\n",
      "292/500 [================>.............] - ETA: 3:36 - loss: 1.9390 - regression_loss: 1.4387 - classification_loss: 0.5003\n",
      "293/500 [================>.............] - ETA: 3:35 - loss: 1.9398 - regression_loss: 1.4400 - classification_loss: 0.4998\n",
      "294/500 [================>.............] - ETA: 3:34 - loss: 1.9387 - regression_loss: 1.4391 - classification_loss: 0.4996\n",
      "295/500 [================>.............] - ETA: 3:33 - loss: 1.9422 - regression_loss: 1.4422 - classification_loss: 0.5000\n",
      "296/500 [================>.............] - ETA: 3:32 - loss: 1.9421 - regression_loss: 1.4424 - classification_loss: 0.4996\n",
      "297/500 [================>.............] - ETA: 3:31 - loss: 1.9435 - regression_loss: 1.4440 - classification_loss: 0.4995\n",
      "298/500 [================>.............] - ETA: 3:30 - loss: 1.9407 - regression_loss: 1.4418 - classification_loss: 0.4988\n",
      "299/500 [================>.............] - ETA: 3:29 - loss: 1.9378 - regression_loss: 1.4397 - classification_loss: 0.4981\n",
      "300/500 [=================>............] - ETA: 3:28 - loss: 1.9403 - regression_loss: 1.4410 - classification_loss: 0.4993\n",
      "301/500 [=================>............] - ETA: 3:27 - loss: 1.9421 - regression_loss: 1.4419 - classification_loss: 0.5002\n",
      "302/500 [=================>............] - ETA: 3:26 - loss: 1.9447 - regression_loss: 1.4438 - classification_loss: 0.5009\n",
      "303/500 [=================>............] - ETA: 3:25 - loss: 1.9481 - regression_loss: 1.4463 - classification_loss: 0.5018\n",
      "304/500 [=================>............] - ETA: 3:23 - loss: 1.9504 - regression_loss: 1.4480 - classification_loss: 0.5024\n",
      "305/500 [=================>............] - ETA: 3:22 - loss: 1.9515 - regression_loss: 1.4488 - classification_loss: 0.5028\n",
      "306/500 [=================>............] - ETA: 3:21 - loss: 1.9500 - regression_loss: 1.4477 - classification_loss: 0.5023\n",
      "307/500 [=================>............] - ETA: 3:20 - loss: 1.9511 - regression_loss: 1.4488 - classification_loss: 0.5023\n",
      "308/500 [=================>............] - ETA: 3:19 - loss: 1.9496 - regression_loss: 1.4478 - classification_loss: 0.5018\n",
      "309/500 [=================>............] - ETA: 3:18 - loss: 1.9495 - regression_loss: 1.4471 - classification_loss: 0.5024\n",
      "310/500 [=================>............] - ETA: 3:17 - loss: 1.9483 - regression_loss: 1.4463 - classification_loss: 0.5020\n",
      "311/500 [=================>............] - ETA: 3:16 - loss: 1.9517 - regression_loss: 1.4483 - classification_loss: 0.5034\n",
      "312/500 [=================>............] - ETA: 3:15 - loss: 1.9510 - regression_loss: 1.4479 - classification_loss: 0.5030\n",
      "313/500 [=================>............] - ETA: 3:14 - loss: 1.9520 - regression_loss: 1.4486 - classification_loss: 0.5034\n",
      "314/500 [=================>............] - ETA: 3:13 - loss: 1.9524 - regression_loss: 1.4480 - classification_loss: 0.5043\n",
      "315/500 [=================>............] - ETA: 3:12 - loss: 1.9543 - regression_loss: 1.4493 - classification_loss: 0.5049\n",
      "316/500 [=================>............] - ETA: 3:11 - loss: 1.9556 - regression_loss: 1.4499 - classification_loss: 0.5057\n",
      "317/500 [==================>...........] - ETA: 3:10 - loss: 1.9551 - regression_loss: 1.4500 - classification_loss: 0.5051\n",
      "318/500 [==================>...........] - ETA: 3:09 - loss: 1.9546 - regression_loss: 1.4493 - classification_loss: 0.5052\n",
      "319/500 [==================>...........] - ETA: 3:08 - loss: 1.9530 - regression_loss: 1.4483 - classification_loss: 0.5047\n",
      "320/500 [==================>...........] - ETA: 3:07 - loss: 1.9508 - regression_loss: 1.4467 - classification_loss: 0.5042\n",
      "321/500 [==================>...........] - ETA: 3:06 - loss: 1.9518 - regression_loss: 1.4477 - classification_loss: 0.5040\n",
      "322/500 [==================>...........] - ETA: 3:05 - loss: 1.9510 - regression_loss: 1.4476 - classification_loss: 0.5034\n",
      "323/500 [==================>...........] - ETA: 3:04 - loss: 1.9505 - regression_loss: 1.4474 - classification_loss: 0.5031\n",
      "324/500 [==================>...........] - ETA: 3:02 - loss: 1.9496 - regression_loss: 1.4466 - classification_loss: 0.5030\n",
      "325/500 [==================>...........] - ETA: 3:01 - loss: 1.9484 - regression_loss: 1.4460 - classification_loss: 0.5024\n",
      "326/500 [==================>...........] - ETA: 3:00 - loss: 1.9471 - regression_loss: 1.4456 - classification_loss: 0.5015\n",
      "327/500 [==================>...........] - ETA: 2:59 - loss: 1.9516 - regression_loss: 1.4483 - classification_loss: 0.5033\n",
      "328/500 [==================>...........] - ETA: 2:58 - loss: 1.9511 - regression_loss: 1.4481 - classification_loss: 0.5030\n",
      "329/500 [==================>...........] - ETA: 2:57 - loss: 1.9500 - regression_loss: 1.4474 - classification_loss: 0.5026\n",
      "330/500 [==================>...........] - ETA: 2:56 - loss: 1.9477 - regression_loss: 1.4460 - classification_loss: 0.5017\n",
      "331/500 [==================>...........] - ETA: 2:55 - loss: 1.9456 - regression_loss: 1.4446 - classification_loss: 0.5010\n",
      "332/500 [==================>...........] - ETA: 2:54 - loss: 1.9446 - regression_loss: 1.4438 - classification_loss: 0.5008\n",
      "333/500 [==================>...........] - ETA: 2:53 - loss: 1.9456 - regression_loss: 1.4445 - classification_loss: 0.5011\n",
      "334/500 [===================>..........] - ETA: 2:52 - loss: 1.9471 - regression_loss: 1.4458 - classification_loss: 0.5013\n",
      "335/500 [===================>..........] - ETA: 2:51 - loss: 1.9463 - regression_loss: 1.4454 - classification_loss: 0.5009\n",
      "336/500 [===================>..........] - ETA: 2:50 - loss: 1.9478 - regression_loss: 1.4465 - classification_loss: 0.5013\n",
      "337/500 [===================>..........] - ETA: 2:49 - loss: 1.9503 - regression_loss: 1.4482 - classification_loss: 0.5021\n",
      "338/500 [===================>..........] - ETA: 2:48 - loss: 1.9496 - regression_loss: 1.4479 - classification_loss: 0.5016\n",
      "339/500 [===================>..........] - ETA: 2:47 - loss: 1.9508 - regression_loss: 1.4487 - classification_loss: 0.5021\n",
      "340/500 [===================>..........] - ETA: 2:46 - loss: 1.9505 - regression_loss: 1.4475 - classification_loss: 0.5030\n",
      "341/500 [===================>..........] - ETA: 2:45 - loss: 1.9498 - regression_loss: 1.4475 - classification_loss: 0.5022\n",
      "342/500 [===================>..........] - ETA: 2:44 - loss: 1.9524 - regression_loss: 1.4486 - classification_loss: 0.5038\n",
      "343/500 [===================>..........] - ETA: 2:43 - loss: 1.9520 - regression_loss: 1.4487 - classification_loss: 0.5033\n",
      "344/500 [===================>..........] - ETA: 2:42 - loss: 1.9521 - regression_loss: 1.4486 - classification_loss: 0.5036\n",
      "345/500 [===================>..........] - ETA: 2:41 - loss: 1.9529 - regression_loss: 1.4493 - classification_loss: 0.5036\n",
      "346/500 [===================>..........] - ETA: 2:39 - loss: 1.9538 - regression_loss: 1.4497 - classification_loss: 0.5041\n",
      "347/500 [===================>..........] - ETA: 2:38 - loss: 1.9526 - regression_loss: 1.4489 - classification_loss: 0.5038\n",
      "348/500 [===================>..........] - ETA: 2:37 - loss: 1.9507 - regression_loss: 1.4478 - classification_loss: 0.5029\n",
      "349/500 [===================>..........] - ETA: 2:36 - loss: 1.9488 - regression_loss: 1.4461 - classification_loss: 0.5027\n",
      "350/500 [====================>.........] - ETA: 2:35 - loss: 1.9498 - regression_loss: 1.4470 - classification_loss: 0.5028\n",
      "351/500 [====================>.........] - ETA: 2:34 - loss: 1.9487 - regression_loss: 1.4466 - classification_loss: 0.5022\n",
      "352/500 [====================>.........] - ETA: 2:33 - loss: 1.9489 - regression_loss: 1.4469 - classification_loss: 0.5021\n",
      "353/500 [====================>.........] - ETA: 2:32 - loss: 1.9491 - regression_loss: 1.4466 - classification_loss: 0.5025\n",
      "354/500 [====================>.........] - ETA: 2:31 - loss: 1.9498 - regression_loss: 1.4472 - classification_loss: 0.5026\n",
      "355/500 [====================>.........] - ETA: 2:30 - loss: 1.9477 - regression_loss: 1.4456 - classification_loss: 0.5021\n",
      "356/500 [====================>.........] - ETA: 2:29 - loss: 1.9469 - regression_loss: 1.4452 - classification_loss: 0.5017\n",
      "357/500 [====================>.........] - ETA: 2:28 - loss: 1.9462 - regression_loss: 1.4447 - classification_loss: 0.5015\n",
      "358/500 [====================>.........] - ETA: 2:27 - loss: 1.9459 - regression_loss: 1.4445 - classification_loss: 0.5014\n",
      "359/500 [====================>.........] - ETA: 2:26 - loss: 1.9464 - regression_loss: 1.4451 - classification_loss: 0.5012\n",
      "360/500 [====================>.........] - ETA: 2:25 - loss: 1.9440 - regression_loss: 1.4436 - classification_loss: 0.5005\n",
      "361/500 [====================>.........] - ETA: 2:24 - loss: 1.9454 - regression_loss: 1.4450 - classification_loss: 0.5004\n",
      "362/500 [====================>.........] - ETA: 2:23 - loss: 1.9452 - regression_loss: 1.4450 - classification_loss: 0.5002\n",
      "363/500 [====================>.........] - ETA: 2:22 - loss: 1.9436 - regression_loss: 1.4436 - classification_loss: 0.5000\n",
      "364/500 [====================>.........] - ETA: 2:21 - loss: 1.9431 - regression_loss: 1.4437 - classification_loss: 0.4995\n",
      "365/500 [====================>.........] - ETA: 2:20 - loss: 1.9427 - regression_loss: 1.4435 - classification_loss: 0.4992\n",
      "366/500 [====================>.........] - ETA: 2:18 - loss: 1.9448 - regression_loss: 1.4449 - classification_loss: 0.4999\n",
      "367/500 [=====================>........] - ETA: 2:17 - loss: 1.9436 - regression_loss: 1.4443 - classification_loss: 0.4993\n",
      "368/500 [=====================>........] - ETA: 2:16 - loss: 1.9445 - regression_loss: 1.4448 - classification_loss: 0.4996\n",
      "369/500 [=====================>........] - ETA: 2:15 - loss: 1.9446 - regression_loss: 1.4450 - classification_loss: 0.4996\n",
      "370/500 [=====================>........] - ETA: 2:14 - loss: 1.9433 - regression_loss: 1.4442 - classification_loss: 0.4991\n",
      "371/500 [=====================>........] - ETA: 2:13 - loss: 1.9436 - regression_loss: 1.4448 - classification_loss: 0.4988\n",
      "372/500 [=====================>........] - ETA: 2:12 - loss: 1.9418 - regression_loss: 1.4434 - classification_loss: 0.4984\n",
      "373/500 [=====================>........] - ETA: 2:11 - loss: 1.9409 - regression_loss: 1.4430 - classification_loss: 0.4979\n",
      "374/500 [=====================>........] - ETA: 2:10 - loss: 1.9396 - regression_loss: 1.4422 - classification_loss: 0.4974\n",
      "375/500 [=====================>........] - ETA: 2:09 - loss: 1.9400 - regression_loss: 1.4423 - classification_loss: 0.4977\n",
      "376/500 [=====================>........] - ETA: 2:08 - loss: 1.9402 - regression_loss: 1.4422 - classification_loss: 0.4981\n",
      "377/500 [=====================>........] - ETA: 2:07 - loss: 1.9401 - regression_loss: 1.4423 - classification_loss: 0.4979\n",
      "378/500 [=====================>........] - ETA: 2:06 - loss: 1.9417 - regression_loss: 1.4440 - classification_loss: 0.4977\n",
      "379/500 [=====================>........] - ETA: 2:05 - loss: 1.9414 - regression_loss: 1.4436 - classification_loss: 0.4978\n",
      "380/500 [=====================>........] - ETA: 2:04 - loss: 1.9397 - regression_loss: 1.4425 - classification_loss: 0.4972\n",
      "381/500 [=====================>........] - ETA: 2:03 - loss: 1.9384 - regression_loss: 1.4417 - classification_loss: 0.4966\n",
      "382/500 [=====================>........] - ETA: 2:02 - loss: 1.9389 - regression_loss: 1.4419 - classification_loss: 0.4969\n",
      "383/500 [=====================>........] - ETA: 2:01 - loss: 1.9360 - regression_loss: 1.4399 - classification_loss: 0.4961\n",
      "384/500 [======================>.......] - ETA: 2:00 - loss: 1.9350 - regression_loss: 1.4393 - classification_loss: 0.4957\n",
      "385/500 [======================>.......] - ETA: 1:59 - loss: 1.9341 - regression_loss: 1.4388 - classification_loss: 0.4954\n",
      "386/500 [======================>.......] - ETA: 1:58 - loss: 1.9374 - regression_loss: 1.4418 - classification_loss: 0.4956\n",
      "387/500 [======================>.......] - ETA: 1:57 - loss: 1.9385 - regression_loss: 1.4428 - classification_loss: 0.4957\n",
      "388/500 [======================>.......] - ETA: 1:56 - loss: 1.9374 - regression_loss: 1.4420 - classification_loss: 0.4954\n",
      "389/500 [======================>.......] - ETA: 1:55 - loss: 1.9358 - regression_loss: 1.4410 - classification_loss: 0.4948\n",
      "390/500 [======================>.......] - ETA: 1:54 - loss: 1.9385 - regression_loss: 1.4425 - classification_loss: 0.4960\n",
      "391/500 [======================>.......] - ETA: 1:53 - loss: 1.9377 - regression_loss: 1.4421 - classification_loss: 0.4957\n",
      "392/500 [======================>.......] - ETA: 1:51 - loss: 1.9380 - regression_loss: 1.4423 - classification_loss: 0.4957\n",
      "393/500 [======================>.......] - ETA: 1:50 - loss: 1.9403 - regression_loss: 1.4437 - classification_loss: 0.4966\n",
      "394/500 [======================>.......] - ETA: 1:49 - loss: 1.9379 - regression_loss: 1.4420 - classification_loss: 0.4959\n",
      "395/500 [======================>.......] - ETA: 1:48 - loss: 1.9380 - regression_loss: 1.4423 - classification_loss: 0.4957\n",
      "396/500 [======================>.......] - ETA: 1:47 - loss: 1.9379 - regression_loss: 1.4421 - classification_loss: 0.4958\n",
      "397/500 [======================>.......] - ETA: 1:46 - loss: 1.9377 - regression_loss: 1.4419 - classification_loss: 0.4958\n",
      "398/500 [======================>.......] - ETA: 1:45 - loss: 1.9369 - regression_loss: 1.4412 - classification_loss: 0.4957\n",
      "399/500 [======================>.......] - ETA: 1:44 - loss: 1.9386 - regression_loss: 1.4426 - classification_loss: 0.4959\n",
      "400/500 [=======================>......] - ETA: 1:43 - loss: 1.9381 - regression_loss: 1.4422 - classification_loss: 0.4959\n",
      "401/500 [=======================>......] - ETA: 1:42 - loss: 1.9386 - regression_loss: 1.4428 - classification_loss: 0.4958\n",
      "402/500 [=======================>......] - ETA: 1:41 - loss: 1.9373 - regression_loss: 1.4419 - classification_loss: 0.4954\n",
      "403/500 [=======================>......] - ETA: 1:40 - loss: 1.9380 - regression_loss: 1.4424 - classification_loss: 0.4955\n",
      "404/500 [=======================>......] - ETA: 1:39 - loss: 1.9364 - regression_loss: 1.4416 - classification_loss: 0.4948\n",
      "405/500 [=======================>......] - ETA: 1:38 - loss: 1.9339 - regression_loss: 1.4396 - classification_loss: 0.4944\n",
      "406/500 [=======================>......] - ETA: 1:37 - loss: 1.9355 - regression_loss: 1.4410 - classification_loss: 0.4945\n",
      "407/500 [=======================>......] - ETA: 1:36 - loss: 1.9347 - regression_loss: 1.4405 - classification_loss: 0.4941\n",
      "408/500 [=======================>......] - ETA: 1:35 - loss: 1.9366 - regression_loss: 1.4415 - classification_loss: 0.4951\n",
      "409/500 [=======================>......] - ETA: 1:34 - loss: 1.9355 - regression_loss: 1.4408 - classification_loss: 0.4947\n",
      "410/500 [=======================>......] - ETA: 1:33 - loss: 1.9360 - regression_loss: 1.4412 - classification_loss: 0.4948\n",
      "411/500 [=======================>......] - ETA: 1:32 - loss: 1.9346 - regression_loss: 1.4401 - classification_loss: 0.4946\n",
      "412/500 [=======================>......] - ETA: 1:31 - loss: 1.9348 - regression_loss: 1.4405 - classification_loss: 0.4943\n",
      "413/500 [=======================>......] - ETA: 1:30 - loss: 1.9334 - regression_loss: 1.4393 - classification_loss: 0.4941\n",
      "414/500 [=======================>......] - ETA: 1:29 - loss: 1.9322 - regression_loss: 1.4387 - classification_loss: 0.4935\n",
      "415/500 [=======================>......] - ETA: 1:28 - loss: 1.9330 - regression_loss: 1.4394 - classification_loss: 0.4936\n",
      "416/500 [=======================>......] - ETA: 1:27 - loss: 1.9329 - regression_loss: 1.4395 - classification_loss: 0.4934\n",
      "417/500 [========================>.....] - ETA: 1:26 - loss: 1.9315 - regression_loss: 1.4385 - classification_loss: 0.4930\n",
      "418/500 [========================>.....] - ETA: 1:25 - loss: 1.9325 - regression_loss: 1.4390 - classification_loss: 0.4935\n",
      "419/500 [========================>.....] - ETA: 1:23 - loss: 1.9343 - regression_loss: 1.4401 - classification_loss: 0.4942\n",
      "420/500 [========================>.....] - ETA: 1:22 - loss: 1.9331 - regression_loss: 1.4393 - classification_loss: 0.4939\n",
      "421/500 [========================>.....] - ETA: 1:21 - loss: 1.9348 - regression_loss: 1.4402 - classification_loss: 0.4946\n",
      "422/500 [========================>.....] - ETA: 1:20 - loss: 1.9346 - regression_loss: 1.4402 - classification_loss: 0.4943\n",
      "423/500 [========================>.....] - ETA: 1:19 - loss: 1.9340 - regression_loss: 1.4398 - classification_loss: 0.4942\n",
      "424/500 [========================>.....] - ETA: 1:18 - loss: 1.9324 - regression_loss: 1.4387 - classification_loss: 0.4937\n",
      "425/500 [========================>.....] - ETA: 1:17 - loss: 1.9327 - regression_loss: 1.4392 - classification_loss: 0.4936\n",
      "426/500 [========================>.....] - ETA: 1:16 - loss: 1.9338 - regression_loss: 1.4397 - classification_loss: 0.4941\n",
      "427/500 [========================>.....] - ETA: 1:15 - loss: 1.9366 - regression_loss: 1.4414 - classification_loss: 0.4952\n",
      "428/500 [========================>.....] - ETA: 1:14 - loss: 1.9362 - regression_loss: 1.4411 - classification_loss: 0.4951\n",
      "429/500 [========================>.....] - ETA: 1:13 - loss: 1.9357 - regression_loss: 1.4404 - classification_loss: 0.4953\n",
      "430/500 [========================>.....] - ETA: 1:12 - loss: 1.9352 - regression_loss: 1.4399 - classification_loss: 0.4953\n",
      "431/500 [========================>.....] - ETA: 1:11 - loss: 1.9347 - regression_loss: 1.4396 - classification_loss: 0.4951\n",
      "432/500 [========================>.....] - ETA: 1:10 - loss: 1.9352 - regression_loss: 1.4397 - classification_loss: 0.4955\n",
      "433/500 [========================>.....] - ETA: 1:09 - loss: 1.9339 - regression_loss: 1.4388 - classification_loss: 0.4952\n",
      "434/500 [=========================>....] - ETA: 1:08 - loss: 1.9361 - regression_loss: 1.4402 - classification_loss: 0.4959\n",
      "435/500 [=========================>....] - ETA: 1:07 - loss: 1.9374 - regression_loss: 1.4410 - classification_loss: 0.4964\n",
      "436/500 [=========================>....] - ETA: 1:06 - loss: 1.9398 - regression_loss: 1.4426 - classification_loss: 0.4971\n",
      "437/500 [=========================>....] - ETA: 1:05 - loss: 1.9414 - regression_loss: 1.4432 - classification_loss: 0.4982\n",
      "438/500 [=========================>....] - ETA: 1:04 - loss: 1.9399 - regression_loss: 1.4421 - classification_loss: 0.4979\n",
      "439/500 [=========================>....] - ETA: 1:03 - loss: 1.9421 - regression_loss: 1.4438 - classification_loss: 0.4983\n",
      "440/500 [=========================>....] - ETA: 1:02 - loss: 1.9436 - regression_loss: 1.4445 - classification_loss: 0.4991\n",
      "441/500 [=========================>....] - ETA: 1:01 - loss: 1.9440 - regression_loss: 1.4447 - classification_loss: 0.4993\n",
      "442/500 [=========================>....] - ETA: 1:00 - loss: 1.9454 - regression_loss: 1.4453 - classification_loss: 0.5001\n",
      "443/500 [=========================>....] - ETA: 59s - loss: 1.9444 - regression_loss: 1.4445 - classification_loss: 0.4999 \n",
      "444/500 [=========================>....] - ETA: 58s - loss: 1.9448 - regression_loss: 1.4447 - classification_loss: 0.5001\n",
      "445/500 [=========================>....] - ETA: 56s - loss: 1.9457 - regression_loss: 1.4454 - classification_loss: 0.5003\n",
      "446/500 [=========================>....] - ETA: 55s - loss: 1.9448 - regression_loss: 1.4448 - classification_loss: 0.4999\n",
      "447/500 [=========================>....] - ETA: 54s - loss: 1.9445 - regression_loss: 1.4448 - classification_loss: 0.4997\n",
      "448/500 [=========================>....] - ETA: 53s - loss: 1.9437 - regression_loss: 1.4441 - classification_loss: 0.4995\n",
      "449/500 [=========================>....] - ETA: 52s - loss: 1.9425 - regression_loss: 1.4434 - classification_loss: 0.4992\n",
      "450/500 [==========================>...] - ETA: 51s - loss: 1.9423 - regression_loss: 1.4433 - classification_loss: 0.4990\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 1.9427 - regression_loss: 1.4436 - classification_loss: 0.4991\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 1.9430 - regression_loss: 1.4439 - classification_loss: 0.4991\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.9439 - regression_loss: 1.4447 - classification_loss: 0.4992\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.9438 - regression_loss: 1.4447 - classification_loss: 0.4991\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.9435 - regression_loss: 1.4447 - classification_loss: 0.4988\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.9431 - regression_loss: 1.4446 - classification_loss: 0.4985\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.9418 - regression_loss: 1.4437 - classification_loss: 0.4981\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.9407 - regression_loss: 1.4429 - classification_loss: 0.4978\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.9421 - regression_loss: 1.4437 - classification_loss: 0.4984\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.9425 - regression_loss: 1.4442 - classification_loss: 0.4983\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 1.9414 - regression_loss: 1.4435 - classification_loss: 0.4980\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 1.9415 - regression_loss: 1.4434 - classification_loss: 0.4981\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 1.9420 - regression_loss: 1.4440 - classification_loss: 0.4980\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 1.9432 - regression_loss: 1.4449 - classification_loss: 0.4983\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 1.9416 - regression_loss: 1.4436 - classification_loss: 0.4979\n",
      "466/500 [==========================>...] - ETA: 35s - loss: 1.9404 - regression_loss: 1.4424 - classification_loss: 0.4980\n",
      "467/500 [===========================>..] - ETA: 34s - loss: 1.9396 - regression_loss: 1.4419 - classification_loss: 0.4977\n",
      "468/500 [===========================>..] - ETA: 33s - loss: 1.9400 - regression_loss: 1.4426 - classification_loss: 0.4975\n",
      "469/500 [===========================>..] - ETA: 32s - loss: 1.9397 - regression_loss: 1.4423 - classification_loss: 0.4974\n",
      "470/500 [===========================>..] - ETA: 31s - loss: 1.9421 - regression_loss: 1.4443 - classification_loss: 0.4978\n",
      "471/500 [===========================>..] - ETA: 30s - loss: 1.9421 - regression_loss: 1.4443 - classification_loss: 0.4978\n",
      "472/500 [===========================>..] - ETA: 28s - loss: 1.9428 - regression_loss: 1.4447 - classification_loss: 0.4981\n",
      "473/500 [===========================>..] - ETA: 27s - loss: 1.9419 - regression_loss: 1.4440 - classification_loss: 0.4979\n",
      "474/500 [===========================>..] - ETA: 26s - loss: 1.9423 - regression_loss: 1.4442 - classification_loss: 0.4981\n",
      "475/500 [===========================>..] - ETA: 25s - loss: 1.9413 - regression_loss: 1.4434 - classification_loss: 0.4979\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 1.9399 - regression_loss: 1.4425 - classification_loss: 0.4974\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.9381 - regression_loss: 1.4410 - classification_loss: 0.4970\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.9371 - regression_loss: 1.4405 - classification_loss: 0.4966\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.9370 - regression_loss: 1.4402 - classification_loss: 0.4968\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.9378 - regression_loss: 1.4405 - classification_loss: 0.4973\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.9381 - regression_loss: 1.4405 - classification_loss: 0.4976\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.9363 - regression_loss: 1.4392 - classification_loss: 0.4971\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.9342 - regression_loss: 1.4376 - classification_loss: 0.4966\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.9330 - regression_loss: 1.4368 - classification_loss: 0.4962\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.9338 - regression_loss: 1.4375 - classification_loss: 0.4963\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.9327 - regression_loss: 1.4369 - classification_loss: 0.4958\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.9334 - regression_loss: 1.4371 - classification_loss: 0.4963\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.9319 - regression_loss: 1.4360 - classification_loss: 0.4959\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.9319 - regression_loss: 1.4359 - classification_loss: 0.4959\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.9319 - regression_loss: 1.4355 - classification_loss: 0.4963\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.9327 - regression_loss: 1.4365 - classification_loss: 0.4961 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.9306 - regression_loss: 1.4352 - classification_loss: 0.4955\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.9308 - regression_loss: 1.4354 - classification_loss: 0.4954\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.9329 - regression_loss: 1.4366 - classification_loss: 0.4963\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.9323 - regression_loss: 1.4364 - classification_loss: 0.4959\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.9334 - regression_loss: 1.4370 - classification_loss: 0.4963\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.9333 - regression_loss: 1.4369 - classification_loss: 0.4964\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.9338 - regression_loss: 1.4371 - classification_loss: 0.4967\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.9365 - regression_loss: 1.4380 - classification_loss: 0.4985\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9370 - regression_loss: 1.4385 - classification_loss: 0.4985\n",
      "Epoch 00035: saving model to ./snapshots\\resnet50_csv_35.h5\n",
      "\n",
      "500/500 [==============================] - 517s 1s/step - loss: 1.9370 - regression_loss: 1.4385 - classification_loss: 0.4985\n",
      "Epoch 36/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 1.6483 - regression_loss: 1.2546 - classification_loss: 0.3937\n",
      "  2/500 [..............................] - ETA: 4:14 - loss: 1.5672 - regression_loss: 1.2117 - classification_loss: 0.3554\n",
      "  3/500 [..............................] - ETA: 5:14 - loss: 1.9065 - regression_loss: 1.4470 - classification_loss: 0.4595\n",
      "  4/500 [..............................] - ETA: 6:03 - loss: 2.1297 - regression_loss: 1.5728 - classification_loss: 0.5569\n",
      "  5/500 [..............................] - ETA: 6:26 - loss: 2.0198 - regression_loss: 1.4872 - classification_loss: 0.5326\n",
      "  6/500 [..............................] - ETA: 6:42 - loss: 1.8517 - regression_loss: 1.3579 - classification_loss: 0.4938\n",
      "  7/500 [..............................] - ETA: 6:59 - loss: 1.8135 - regression_loss: 1.3369 - classification_loss: 0.4766\n",
      "  8/500 [..............................] - ETA: 7:06 - loss: 1.7884 - regression_loss: 1.3280 - classification_loss: 0.4605\n",
      "  9/500 [..............................] - ETA: 6:59 - loss: 1.8756 - regression_loss: 1.3588 - classification_loss: 0.5168\n",
      " 10/500 [..............................] - ETA: 7:16 - loss: 1.9309 - regression_loss: 1.3840 - classification_loss: 0.5469\n",
      " 11/500 [..............................] - ETA: 7:19 - loss: 1.9133 - regression_loss: 1.3842 - classification_loss: 0.5291\n",
      " 12/500 [..............................] - ETA: 7:21 - loss: 1.9259 - regression_loss: 1.3746 - classification_loss: 0.5513\n",
      " 13/500 [..............................] - ETA: 7:27 - loss: 1.8979 - regression_loss: 1.3607 - classification_loss: 0.5371\n",
      " 14/500 [..............................] - ETA: 7:29 - loss: 1.9477 - regression_loss: 1.3921 - classification_loss: 0.5557\n",
      " 15/500 [..............................] - ETA: 7:26 - loss: 1.9307 - regression_loss: 1.3730 - classification_loss: 0.5576\n",
      " 16/500 [..............................] - ETA: 7:32 - loss: 1.9195 - regression_loss: 1.3744 - classification_loss: 0.5451\n",
      " 17/500 [>.............................] - ETA: 7:32 - loss: 1.9102 - regression_loss: 1.3692 - classification_loss: 0.5410\n",
      " 18/500 [>.............................] - ETA: 7:35 - loss: 1.9312 - regression_loss: 1.3893 - classification_loss: 0.5419\n",
      " 19/500 [>.............................] - ETA: 7:38 - loss: 1.9338 - regression_loss: 1.3989 - classification_loss: 0.5349\n",
      " 20/500 [>.............................] - ETA: 7:40 - loss: 1.9273 - regression_loss: 1.4029 - classification_loss: 0.5244\n",
      " 21/500 [>.............................] - ETA: 7:40 - loss: 1.9539 - regression_loss: 1.4189 - classification_loss: 0.5350\n",
      " 22/500 [>.............................] - ETA: 7:40 - loss: 1.9705 - regression_loss: 1.4367 - classification_loss: 0.5339\n",
      " 23/500 [>.............................] - ETA: 7:42 - loss: 2.0005 - regression_loss: 1.4653 - classification_loss: 0.5352\n",
      " 24/500 [>.............................] - ETA: 7:43 - loss: 2.0343 - regression_loss: 1.4909 - classification_loss: 0.5433\n",
      " 25/500 [>.............................] - ETA: 7:42 - loss: 2.0661 - regression_loss: 1.5130 - classification_loss: 0.5531\n",
      " 26/500 [>.............................] - ETA: 7:41 - loss: 2.0827 - regression_loss: 1.5337 - classification_loss: 0.5491\n",
      " 27/500 [>.............................] - ETA: 7:42 - loss: 2.0917 - regression_loss: 1.5387 - classification_loss: 0.5530\n",
      " 28/500 [>.............................] - ETA: 7:39 - loss: 2.1193 - regression_loss: 1.5660 - classification_loss: 0.5533\n",
      " 29/500 [>.............................] - ETA: 7:37 - loss: 2.1333 - regression_loss: 1.5761 - classification_loss: 0.5571\n",
      " 30/500 [>.............................] - ETA: 7:40 - loss: 2.1362 - regression_loss: 1.5760 - classification_loss: 0.5602\n",
      " 31/500 [>.............................] - ETA: 7:40 - loss: 2.1549 - regression_loss: 1.5916 - classification_loss: 0.5633\n",
      " 32/500 [>.............................] - ETA: 7:39 - loss: 2.1577 - regression_loss: 1.5908 - classification_loss: 0.5669\n",
      " 33/500 [>.............................] - ETA: 7:35 - loss: 2.1742 - regression_loss: 1.5999 - classification_loss: 0.5743\n",
      " 34/500 [=>............................] - ETA: 7:36 - loss: 2.1645 - regression_loss: 1.5906 - classification_loss: 0.5739\n",
      " 35/500 [=>............................] - ETA: 7:37 - loss: 2.1810 - regression_loss: 1.6042 - classification_loss: 0.5768\n",
      " 36/500 [=>............................] - ETA: 7:36 - loss: 2.1791 - regression_loss: 1.6074 - classification_loss: 0.5717\n",
      " 37/500 [=>............................] - ETA: 7:35 - loss: 2.1756 - regression_loss: 1.6062 - classification_loss: 0.5694\n",
      " 38/500 [=>............................] - ETA: 7:34 - loss: 2.1777 - regression_loss: 1.6043 - classification_loss: 0.5733\n",
      " 39/500 [=>............................] - ETA: 7:34 - loss: 2.1860 - regression_loss: 1.6111 - classification_loss: 0.5749\n",
      " 40/500 [=>............................] - ETA: 7:33 - loss: 2.1672 - regression_loss: 1.5991 - classification_loss: 0.5681\n",
      " 41/500 [=>............................] - ETA: 7:30 - loss: 2.1582 - regression_loss: 1.5960 - classification_loss: 0.5622\n",
      " 42/500 [=>............................] - ETA: 7:31 - loss: 2.1465 - regression_loss: 1.5876 - classification_loss: 0.5589\n",
      " 43/500 [=>............................] - ETA: 7:31 - loss: 2.1313 - regression_loss: 1.5777 - classification_loss: 0.5535\n",
      " 44/500 [=>............................] - ETA: 7:30 - loss: 2.1227 - regression_loss: 1.5715 - classification_loss: 0.5512\n",
      " 45/500 [=>............................] - ETA: 7:28 - loss: 2.1259 - regression_loss: 1.5763 - classification_loss: 0.5495\n",
      " 46/500 [=>............................] - ETA: 7:27 - loss: 2.1244 - regression_loss: 1.5767 - classification_loss: 0.5477\n",
      " 47/500 [=>............................] - ETA: 7:28 - loss: 2.1259 - regression_loss: 1.5779 - classification_loss: 0.5480\n",
      " 48/500 [=>............................] - ETA: 7:27 - loss: 2.1299 - regression_loss: 1.5781 - classification_loss: 0.5518\n",
      " 49/500 [=>............................] - ETA: 7:27 - loss: 2.1328 - regression_loss: 1.5806 - classification_loss: 0.5522\n",
      " 50/500 [==>...........................] - ETA: 7:27 - loss: 2.1333 - regression_loss: 1.5836 - classification_loss: 0.5497\n",
      " 51/500 [==>...........................] - ETA: 7:26 - loss: 2.1481 - regression_loss: 1.5944 - classification_loss: 0.5536\n",
      " 52/500 [==>...........................] - ETA: 7:26 - loss: 2.1427 - regression_loss: 1.5912 - classification_loss: 0.5514\n",
      " 53/500 [==>...........................] - ETA: 7:24 - loss: 2.1244 - regression_loss: 1.5775 - classification_loss: 0.5469\n",
      " 54/500 [==>...........................] - ETA: 7:24 - loss: 2.1268 - regression_loss: 1.5802 - classification_loss: 0.5466\n",
      " 55/500 [==>...........................] - ETA: 7:21 - loss: 2.1203 - regression_loss: 1.5796 - classification_loss: 0.5407\n",
      " 56/500 [==>...........................] - ETA: 7:22 - loss: 2.1209 - regression_loss: 1.5823 - classification_loss: 0.5386\n",
      " 57/500 [==>...........................] - ETA: 7:20 - loss: 2.1133 - regression_loss: 1.5782 - classification_loss: 0.5351\n",
      " 58/500 [==>...........................] - ETA: 7:21 - loss: 2.1028 - regression_loss: 1.5682 - classification_loss: 0.5345\n",
      " 59/500 [==>...........................] - ETA: 7:21 - loss: 2.1036 - regression_loss: 1.5659 - classification_loss: 0.5377\n",
      " 60/500 [==>...........................] - ETA: 7:20 - loss: 2.0899 - regression_loss: 1.5568 - classification_loss: 0.5331\n",
      " 61/500 [==>...........................] - ETA: 7:20 - loss: 2.0876 - regression_loss: 1.5540 - classification_loss: 0.5336\n",
      " 62/500 [==>...........................] - ETA: 7:19 - loss: 2.0941 - regression_loss: 1.5607 - classification_loss: 0.5334\n",
      " 63/500 [==>...........................] - ETA: 7:19 - loss: 2.0984 - regression_loss: 1.5626 - classification_loss: 0.5359\n",
      " 64/500 [==>...........................] - ETA: 7:18 - loss: 2.0970 - regression_loss: 1.5625 - classification_loss: 0.5345\n",
      " 65/500 [==>...........................] - ETA: 7:18 - loss: 2.0958 - regression_loss: 1.5629 - classification_loss: 0.5329\n",
      " 66/500 [==>...........................] - ETA: 7:17 - loss: 2.1020 - regression_loss: 1.5672 - classification_loss: 0.5348\n",
      " 67/500 [===>..........................] - ETA: 7:17 - loss: 2.0994 - regression_loss: 1.5640 - classification_loss: 0.5355\n",
      " 68/500 [===>..........................] - ETA: 7:16 - loss: 2.1015 - regression_loss: 1.5662 - classification_loss: 0.5354\n",
      " 69/500 [===>..........................] - ETA: 7:15 - loss: 2.0979 - regression_loss: 1.5639 - classification_loss: 0.5340\n",
      " 70/500 [===>..........................] - ETA: 7:14 - loss: 2.0795 - regression_loss: 1.5500 - classification_loss: 0.5294\n",
      " 71/500 [===>..........................] - ETA: 7:13 - loss: 2.0702 - regression_loss: 1.5431 - classification_loss: 0.5271\n",
      " 72/500 [===>..........................] - ETA: 7:11 - loss: 2.0777 - regression_loss: 1.5473 - classification_loss: 0.5304\n",
      " 73/500 [===>..........................] - ETA: 7:10 - loss: 2.0738 - regression_loss: 1.5452 - classification_loss: 0.5286\n",
      " 74/500 [===>..........................] - ETA: 7:10 - loss: 2.0872 - regression_loss: 1.5557 - classification_loss: 0.5315\n",
      " 75/500 [===>..........................] - ETA: 7:10 - loss: 2.0919 - regression_loss: 1.5585 - classification_loss: 0.5334\n",
      " 76/500 [===>..........................] - ETA: 7:09 - loss: 2.0795 - regression_loss: 1.5491 - classification_loss: 0.5304\n",
      " 77/500 [===>..........................] - ETA: 7:07 - loss: 2.0760 - regression_loss: 1.5473 - classification_loss: 0.5287\n",
      " 78/500 [===>..........................] - ETA: 7:05 - loss: 2.0681 - regression_loss: 1.5409 - classification_loss: 0.5272\n",
      " 79/500 [===>..........................] - ETA: 7:05 - loss: 2.0702 - regression_loss: 1.5437 - classification_loss: 0.5265\n",
      " 80/500 [===>..........................] - ETA: 7:04 - loss: 2.0655 - regression_loss: 1.5410 - classification_loss: 0.5245\n",
      " 81/500 [===>..........................] - ETA: 7:04 - loss: 2.0568 - regression_loss: 1.5351 - classification_loss: 0.5217\n",
      " 82/500 [===>..........................] - ETA: 7:03 - loss: 2.0629 - regression_loss: 1.5404 - classification_loss: 0.5225\n",
      " 83/500 [===>..........................] - ETA: 7:02 - loss: 2.0611 - regression_loss: 1.5390 - classification_loss: 0.5220\n",
      " 84/500 [====>.........................] - ETA: 7:02 - loss: 2.0657 - regression_loss: 1.5413 - classification_loss: 0.5244\n",
      " 85/500 [====>.........................] - ETA: 7:01 - loss: 2.0630 - regression_loss: 1.5398 - classification_loss: 0.5232\n",
      " 86/500 [====>.........................] - ETA: 7:00 - loss: 2.0538 - regression_loss: 1.5330 - classification_loss: 0.5207\n",
      " 87/500 [====>.........................] - ETA: 6:59 - loss: 2.0578 - regression_loss: 1.5351 - classification_loss: 0.5227\n",
      " 88/500 [====>.........................] - ETA: 6:58 - loss: 2.0514 - regression_loss: 1.5277 - classification_loss: 0.5237\n",
      " 89/500 [====>.........................] - ETA: 6:57 - loss: 2.0424 - regression_loss: 1.5205 - classification_loss: 0.5219\n",
      " 90/500 [====>.........................] - ETA: 6:56 - loss: 2.0352 - regression_loss: 1.5158 - classification_loss: 0.5194\n",
      " 91/500 [====>.........................] - ETA: 6:55 - loss: 2.0266 - regression_loss: 1.5086 - classification_loss: 0.5180\n",
      " 92/500 [====>.........................] - ETA: 6:55 - loss: 2.0348 - regression_loss: 1.5109 - classification_loss: 0.5238\n",
      " 93/500 [====>.........................] - ETA: 6:54 - loss: 2.0309 - regression_loss: 1.5086 - classification_loss: 0.5223\n",
      " 94/500 [====>.........................] - ETA: 6:53 - loss: 2.0290 - regression_loss: 1.5064 - classification_loss: 0.5226\n",
      " 95/500 [====>.........................] - ETA: 6:52 - loss: 2.0275 - regression_loss: 1.5061 - classification_loss: 0.5215\n",
      " 96/500 [====>.........................] - ETA: 6:51 - loss: 2.0177 - regression_loss: 1.4990 - classification_loss: 0.5187\n",
      " 97/500 [====>.........................] - ETA: 6:50 - loss: 2.0169 - regression_loss: 1.4982 - classification_loss: 0.5188\n",
      " 98/500 [====>.........................] - ETA: 6:49 - loss: 2.0158 - regression_loss: 1.4974 - classification_loss: 0.5183\n",
      " 99/500 [====>.........................] - ETA: 6:48 - loss: 2.0200 - regression_loss: 1.5001 - classification_loss: 0.5198\n",
      "100/500 [=====>........................] - ETA: 6:48 - loss: 2.0193 - regression_loss: 1.5001 - classification_loss: 0.5192\n",
      "101/500 [=====>........................] - ETA: 6:47 - loss: 2.0110 - regression_loss: 1.4937 - classification_loss: 0.5173\n",
      "102/500 [=====>........................] - ETA: 6:45 - loss: 2.0050 - regression_loss: 1.4903 - classification_loss: 0.5148\n",
      "103/500 [=====>........................] - ETA: 6:44 - loss: 1.9964 - regression_loss: 1.4840 - classification_loss: 0.5125\n",
      "104/500 [=====>........................] - ETA: 6:43 - loss: 1.9891 - regression_loss: 1.4786 - classification_loss: 0.5105\n",
      "105/500 [=====>........................] - ETA: 6:42 - loss: 1.9843 - regression_loss: 1.4743 - classification_loss: 0.5100\n",
      "106/500 [=====>........................] - ETA: 6:40 - loss: 1.9869 - regression_loss: 1.4769 - classification_loss: 0.5100\n",
      "107/500 [=====>........................] - ETA: 6:40 - loss: 1.9817 - regression_loss: 1.4738 - classification_loss: 0.5080\n",
      "108/500 [=====>........................] - ETA: 6:39 - loss: 1.9813 - regression_loss: 1.4746 - classification_loss: 0.5067\n",
      "109/500 [=====>........................] - ETA: 6:38 - loss: 1.9776 - regression_loss: 1.4713 - classification_loss: 0.5062\n",
      "110/500 [=====>........................] - ETA: 6:37 - loss: 1.9747 - regression_loss: 1.4689 - classification_loss: 0.5058\n",
      "111/500 [=====>........................] - ETA: 6:36 - loss: 1.9753 - regression_loss: 1.4700 - classification_loss: 0.5053\n",
      "112/500 [=====>........................] - ETA: 6:35 - loss: 1.9720 - regression_loss: 1.4676 - classification_loss: 0.5045\n",
      "113/500 [=====>........................] - ETA: 6:34 - loss: 1.9706 - regression_loss: 1.4675 - classification_loss: 0.5032\n",
      "114/500 [=====>........................] - ETA: 6:33 - loss: 1.9750 - regression_loss: 1.4701 - classification_loss: 0.5049\n",
      "115/500 [=====>........................] - ETA: 6:32 - loss: 1.9729 - regression_loss: 1.4687 - classification_loss: 0.5042\n",
      "116/500 [=====>........................] - ETA: 6:31 - loss: 1.9774 - regression_loss: 1.4734 - classification_loss: 0.5040\n",
      "117/500 [======>.......................] - ETA: 6:29 - loss: 1.9797 - regression_loss: 1.4740 - classification_loss: 0.5057\n",
      "118/500 [======>.......................] - ETA: 6:28 - loss: 1.9767 - regression_loss: 1.4721 - classification_loss: 0.5046\n",
      "119/500 [======>.......................] - ETA: 6:27 - loss: 1.9753 - regression_loss: 1.4699 - classification_loss: 0.5055\n",
      "120/500 [======>.......................] - ETA: 6:26 - loss: 1.9746 - regression_loss: 1.4695 - classification_loss: 0.5052\n",
      "121/500 [======>.......................] - ETA: 6:25 - loss: 1.9727 - regression_loss: 1.4687 - classification_loss: 0.5040\n",
      "122/500 [======>.......................] - ETA: 6:24 - loss: 1.9675 - regression_loss: 1.4650 - classification_loss: 0.5025\n",
      "123/500 [======>.......................] - ETA: 6:23 - loss: 1.9742 - regression_loss: 1.4710 - classification_loss: 0.5032\n",
      "124/500 [======>.......................] - ETA: 6:22 - loss: 1.9721 - regression_loss: 1.4685 - classification_loss: 0.5036\n",
      "125/500 [======>.......................] - ETA: 6:21 - loss: 1.9726 - regression_loss: 1.4697 - classification_loss: 0.5029\n",
      "126/500 [======>.......................] - ETA: 6:20 - loss: 1.9729 - regression_loss: 1.4696 - classification_loss: 0.5033\n",
      "127/500 [======>.......................] - ETA: 6:19 - loss: 1.9718 - regression_loss: 1.4686 - classification_loss: 0.5032\n",
      "128/500 [======>.......................] - ETA: 6:18 - loss: 1.9684 - regression_loss: 1.4664 - classification_loss: 0.5020\n",
      "129/500 [======>.......................] - ETA: 6:17 - loss: 1.9620 - regression_loss: 1.4618 - classification_loss: 0.5001\n",
      "130/500 [======>.......................] - ETA: 6:16 - loss: 1.9675 - regression_loss: 1.4648 - classification_loss: 0.5027\n",
      "131/500 [======>.......................] - ETA: 6:15 - loss: 1.9642 - regression_loss: 1.4629 - classification_loss: 0.5013\n",
      "132/500 [======>.......................] - ETA: 6:14 - loss: 1.9630 - regression_loss: 1.4624 - classification_loss: 0.5007\n",
      "133/500 [======>.......................] - ETA: 6:13 - loss: 1.9568 - regression_loss: 1.4582 - classification_loss: 0.4985\n",
      "134/500 [=======>......................] - ETA: 6:12 - loss: 1.9599 - regression_loss: 1.4595 - classification_loss: 0.5005\n",
      "135/500 [=======>......................] - ETA: 6:10 - loss: 1.9571 - regression_loss: 1.4577 - classification_loss: 0.4993\n",
      "136/500 [=======>......................] - ETA: 6:09 - loss: 1.9558 - regression_loss: 1.4570 - classification_loss: 0.4988\n",
      "137/500 [=======>......................] - ETA: 6:08 - loss: 1.9503 - regression_loss: 1.4527 - classification_loss: 0.4976\n",
      "138/500 [=======>......................] - ETA: 6:08 - loss: 1.9453 - regression_loss: 1.4491 - classification_loss: 0.4962\n",
      "139/500 [=======>......................] - ETA: 6:07 - loss: 1.9470 - regression_loss: 1.4508 - classification_loss: 0.4962\n",
      "140/500 [=======>......................] - ETA: 6:06 - loss: 1.9430 - regression_loss: 1.4478 - classification_loss: 0.4953\n",
      "141/500 [=======>......................] - ETA: 6:05 - loss: 1.9451 - regression_loss: 1.4482 - classification_loss: 0.4969\n",
      "142/500 [=======>......................] - ETA: 6:04 - loss: 1.9434 - regression_loss: 1.4465 - classification_loss: 0.4969\n",
      "143/500 [=======>......................] - ETA: 6:03 - loss: 1.9412 - regression_loss: 1.4456 - classification_loss: 0.4956\n",
      "144/500 [=======>......................] - ETA: 6:02 - loss: 1.9468 - regression_loss: 1.4483 - classification_loss: 0.4985\n",
      "145/500 [=======>......................] - ETA: 6:01 - loss: 1.9453 - regression_loss: 1.4474 - classification_loss: 0.4979\n",
      "146/500 [=======>......................] - ETA: 6:00 - loss: 1.9469 - regression_loss: 1.4482 - classification_loss: 0.4987\n",
      "147/500 [=======>......................] - ETA: 5:59 - loss: 1.9399 - regression_loss: 1.4428 - classification_loss: 0.4971\n",
      "148/500 [=======>......................] - ETA: 5:58 - loss: 1.9382 - regression_loss: 1.4424 - classification_loss: 0.4959\n",
      "149/500 [=======>......................] - ETA: 5:57 - loss: 1.9390 - regression_loss: 1.4429 - classification_loss: 0.4961\n",
      "150/500 [========>.....................] - ETA: 5:56 - loss: 1.9375 - regression_loss: 1.4421 - classification_loss: 0.4954\n",
      "151/500 [========>.....................] - ETA: 5:55 - loss: 1.9350 - regression_loss: 1.4403 - classification_loss: 0.4946\n",
      "152/500 [========>.....................] - ETA: 5:54 - loss: 1.9383 - regression_loss: 1.4418 - classification_loss: 0.4965\n",
      "153/500 [========>.....................] - ETA: 5:53 - loss: 1.9385 - regression_loss: 1.4415 - classification_loss: 0.4969\n",
      "154/500 [========>.....................] - ETA: 5:52 - loss: 1.9347 - regression_loss: 1.4387 - classification_loss: 0.4960\n",
      "155/500 [========>.....................] - ETA: 5:51 - loss: 1.9322 - regression_loss: 1.4372 - classification_loss: 0.4950\n",
      "156/500 [========>.....................] - ETA: 5:50 - loss: 1.9355 - regression_loss: 1.4387 - classification_loss: 0.4969\n",
      "157/500 [========>.....................] - ETA: 5:49 - loss: 1.9352 - regression_loss: 1.4391 - classification_loss: 0.4961\n",
      "158/500 [========>.....................] - ETA: 5:48 - loss: 1.9383 - regression_loss: 1.4412 - classification_loss: 0.4971\n",
      "159/500 [========>.....................] - ETA: 5:47 - loss: 1.9351 - regression_loss: 1.4393 - classification_loss: 0.4957\n",
      "160/500 [========>.....................] - ETA: 5:46 - loss: 1.9345 - regression_loss: 1.4397 - classification_loss: 0.4948\n",
      "161/500 [========>.....................] - ETA: 5:45 - loss: 1.9358 - regression_loss: 1.4412 - classification_loss: 0.4946\n",
      "162/500 [========>.....................] - ETA: 5:44 - loss: 1.9314 - regression_loss: 1.4382 - classification_loss: 0.4931\n",
      "163/500 [========>.....................] - ETA: 5:43 - loss: 1.9308 - regression_loss: 1.4384 - classification_loss: 0.4923\n",
      "164/500 [========>.....................] - ETA: 5:42 - loss: 1.9281 - regression_loss: 1.4368 - classification_loss: 0.4913\n",
      "165/500 [========>.....................] - ETA: 5:41 - loss: 1.9279 - regression_loss: 1.4369 - classification_loss: 0.4911\n",
      "166/500 [========>.....................] - ETA: 5:40 - loss: 1.9264 - regression_loss: 1.4342 - classification_loss: 0.4922\n",
      "167/500 [=========>....................] - ETA: 5:39 - loss: 1.9304 - regression_loss: 1.4389 - classification_loss: 0.4914\n",
      "168/500 [=========>....................] - ETA: 5:38 - loss: 1.9319 - regression_loss: 1.4406 - classification_loss: 0.4913\n",
      "169/500 [=========>....................] - ETA: 5:37 - loss: 1.9331 - regression_loss: 1.4411 - classification_loss: 0.4919\n",
      "170/500 [=========>....................] - ETA: 5:36 - loss: 1.9305 - regression_loss: 1.4393 - classification_loss: 0.4913\n",
      "171/500 [=========>....................] - ETA: 5:35 - loss: 1.9253 - regression_loss: 1.4353 - classification_loss: 0.4900\n",
      "172/500 [=========>....................] - ETA: 5:34 - loss: 1.9260 - regression_loss: 1.4361 - classification_loss: 0.4898\n",
      "173/500 [=========>....................] - ETA: 5:33 - loss: 1.9212 - regression_loss: 1.4325 - classification_loss: 0.4887\n",
      "174/500 [=========>....................] - ETA: 5:32 - loss: 1.9159 - regression_loss: 1.4285 - classification_loss: 0.4874\n",
      "175/500 [=========>....................] - ETA: 5:31 - loss: 1.9198 - regression_loss: 1.4295 - classification_loss: 0.4903\n",
      "176/500 [=========>....................] - ETA: 5:30 - loss: 1.9218 - regression_loss: 1.4307 - classification_loss: 0.4911\n",
      "177/500 [=========>....................] - ETA: 5:29 - loss: 1.9196 - regression_loss: 1.4290 - classification_loss: 0.4906\n",
      "178/500 [=========>....................] - ETA: 5:28 - loss: 1.9196 - regression_loss: 1.4294 - classification_loss: 0.4902\n",
      "179/500 [=========>....................] - ETA: 5:27 - loss: 1.9186 - regression_loss: 1.4294 - classification_loss: 0.4892\n",
      "180/500 [=========>....................] - ETA: 5:26 - loss: 1.9151 - regression_loss: 1.4274 - classification_loss: 0.4878\n",
      "181/500 [=========>....................] - ETA: 5:25 - loss: 1.9126 - regression_loss: 1.4250 - classification_loss: 0.4876\n",
      "182/500 [=========>....................] - ETA: 5:24 - loss: 1.9142 - regression_loss: 1.4270 - classification_loss: 0.4872\n",
      "183/500 [=========>....................] - ETA: 5:23 - loss: 1.9219 - regression_loss: 1.4319 - classification_loss: 0.4901\n",
      "184/500 [==========>...................] - ETA: 5:22 - loss: 1.9218 - regression_loss: 1.4313 - classification_loss: 0.4905\n",
      "185/500 [==========>...................] - ETA: 5:21 - loss: 1.9225 - regression_loss: 1.4314 - classification_loss: 0.4911\n",
      "186/500 [==========>...................] - ETA: 5:20 - loss: 1.9209 - regression_loss: 1.4308 - classification_loss: 0.4901\n",
      "187/500 [==========>...................] - ETA: 5:19 - loss: 1.9184 - regression_loss: 1.4290 - classification_loss: 0.4895\n",
      "188/500 [==========>...................] - ETA: 5:18 - loss: 1.9170 - regression_loss: 1.4278 - classification_loss: 0.4892\n",
      "189/500 [==========>...................] - ETA: 5:17 - loss: 1.9161 - regression_loss: 1.4276 - classification_loss: 0.4885\n",
      "190/500 [==========>...................] - ETA: 5:16 - loss: 1.9160 - regression_loss: 1.4270 - classification_loss: 0.4890\n",
      "191/500 [==========>...................] - ETA: 5:15 - loss: 1.9168 - regression_loss: 1.4276 - classification_loss: 0.4892\n",
      "192/500 [==========>...................] - ETA: 5:15 - loss: 1.9157 - regression_loss: 1.4264 - classification_loss: 0.4893\n",
      "193/500 [==========>...................] - ETA: 5:13 - loss: 1.9123 - regression_loss: 1.4240 - classification_loss: 0.4884\n",
      "194/500 [==========>...................] - ETA: 5:12 - loss: 1.9125 - regression_loss: 1.4246 - classification_loss: 0.4879\n",
      "195/500 [==========>...................] - ETA: 5:11 - loss: 1.9099 - regression_loss: 1.4224 - classification_loss: 0.4875\n",
      "196/500 [==========>...................] - ETA: 5:10 - loss: 1.9080 - regression_loss: 1.4209 - classification_loss: 0.4871\n",
      "197/500 [==========>...................] - ETA: 5:09 - loss: 1.9050 - regression_loss: 1.4191 - classification_loss: 0.4859\n",
      "198/500 [==========>...................] - ETA: 5:08 - loss: 1.9068 - regression_loss: 1.4206 - classification_loss: 0.4862\n",
      "199/500 [==========>...................] - ETA: 5:07 - loss: 1.9083 - regression_loss: 1.4206 - classification_loss: 0.4877\n",
      "200/500 [===========>..................] - ETA: 5:06 - loss: 1.9085 - regression_loss: 1.4206 - classification_loss: 0.4879\n",
      "201/500 [===========>..................] - ETA: 5:05 - loss: 1.9085 - regression_loss: 1.4211 - classification_loss: 0.4874\n",
      "202/500 [===========>..................] - ETA: 5:04 - loss: 1.9113 - regression_loss: 1.4234 - classification_loss: 0.4879\n",
      "203/500 [===========>..................] - ETA: 5:03 - loss: 1.9108 - regression_loss: 1.4231 - classification_loss: 0.4877\n",
      "204/500 [===========>..................] - ETA: 5:02 - loss: 1.9090 - regression_loss: 1.4219 - classification_loss: 0.4870\n",
      "205/500 [===========>..................] - ETA: 5:01 - loss: 1.9136 - regression_loss: 1.4252 - classification_loss: 0.4883\n",
      "206/500 [===========>..................] - ETA: 5:00 - loss: 1.9140 - regression_loss: 1.4259 - classification_loss: 0.4882\n",
      "207/500 [===========>..................] - ETA: 4:59 - loss: 1.9134 - regression_loss: 1.4253 - classification_loss: 0.4881\n",
      "208/500 [===========>..................] - ETA: 4:58 - loss: 1.9130 - regression_loss: 1.4246 - classification_loss: 0.4884\n",
      "209/500 [===========>..................] - ETA: 4:57 - loss: 1.9156 - regression_loss: 1.4265 - classification_loss: 0.4891\n",
      "210/500 [===========>..................] - ETA: 4:56 - loss: 1.9179 - regression_loss: 1.4283 - classification_loss: 0.4897\n",
      "211/500 [===========>..................] - ETA: 4:55 - loss: 1.9169 - regression_loss: 1.4281 - classification_loss: 0.4888\n",
      "212/500 [===========>..................] - ETA: 4:54 - loss: 1.9172 - regression_loss: 1.4284 - classification_loss: 0.4888\n",
      "213/500 [===========>..................] - ETA: 4:53 - loss: 1.9181 - regression_loss: 1.4294 - classification_loss: 0.4887\n",
      "214/500 [===========>..................] - ETA: 4:52 - loss: 1.9196 - regression_loss: 1.4304 - classification_loss: 0.4892\n",
      "215/500 [===========>..................] - ETA: 4:51 - loss: 1.9191 - regression_loss: 1.4294 - classification_loss: 0.4897\n",
      "216/500 [===========>..................] - ETA: 4:50 - loss: 1.9218 - regression_loss: 1.4318 - classification_loss: 0.4900\n",
      "217/500 [============>.................] - ETA: 4:49 - loss: 1.9203 - regression_loss: 1.4309 - classification_loss: 0.4894\n",
      "218/500 [============>.................] - ETA: 4:48 - loss: 1.9206 - regression_loss: 1.4311 - classification_loss: 0.4895\n",
      "219/500 [============>.................] - ETA: 4:47 - loss: 1.9191 - regression_loss: 1.4301 - classification_loss: 0.4890\n",
      "220/500 [============>.................] - ETA: 4:46 - loss: 1.9193 - regression_loss: 1.4302 - classification_loss: 0.4891\n",
      "221/500 [============>.................] - ETA: 4:45 - loss: 1.9186 - regression_loss: 1.4298 - classification_loss: 0.4888\n",
      "222/500 [============>.................] - ETA: 4:44 - loss: 1.9165 - regression_loss: 1.4286 - classification_loss: 0.4880\n",
      "223/500 [============>.................] - ETA: 4:43 - loss: 1.9148 - regression_loss: 1.4269 - classification_loss: 0.4879\n",
      "224/500 [============>.................] - ETA: 4:42 - loss: 1.9108 - regression_loss: 1.4237 - classification_loss: 0.4871\n",
      "225/500 [============>.................] - ETA: 4:41 - loss: 1.9098 - regression_loss: 1.4234 - classification_loss: 0.4864\n",
      "226/500 [============>.................] - ETA: 4:40 - loss: 1.9115 - regression_loss: 1.4247 - classification_loss: 0.4868\n",
      "227/500 [============>.................] - ETA: 4:39 - loss: 1.9138 - regression_loss: 1.4252 - classification_loss: 0.4886\n",
      "228/500 [============>.................] - ETA: 4:38 - loss: 1.9123 - regression_loss: 1.4238 - classification_loss: 0.4885\n",
      "229/500 [============>.................] - ETA: 4:37 - loss: 1.9105 - regression_loss: 1.4220 - classification_loss: 0.4885\n",
      "230/500 [============>.................] - ETA: 4:36 - loss: 1.9127 - regression_loss: 1.4224 - classification_loss: 0.4902\n",
      "231/500 [============>.................] - ETA: 4:35 - loss: 1.9162 - regression_loss: 1.4251 - classification_loss: 0.4911\n",
      "232/500 [============>.................] - ETA: 4:33 - loss: 1.9200 - regression_loss: 1.4279 - classification_loss: 0.4921\n",
      "233/500 [============>.................] - ETA: 4:33 - loss: 1.9191 - regression_loss: 1.4276 - classification_loss: 0.4915\n",
      "234/500 [=============>................] - ETA: 4:32 - loss: 1.9200 - regression_loss: 1.4286 - classification_loss: 0.4914\n",
      "235/500 [=============>................] - ETA: 4:31 - loss: 1.9199 - regression_loss: 1.4284 - classification_loss: 0.4915\n",
      "236/500 [=============>................] - ETA: 4:30 - loss: 1.9267 - regression_loss: 1.4333 - classification_loss: 0.4935\n",
      "237/500 [=============>................] - ETA: 4:29 - loss: 1.9268 - regression_loss: 1.4334 - classification_loss: 0.4934\n",
      "238/500 [=============>................] - ETA: 4:27 - loss: 1.9229 - regression_loss: 1.4306 - classification_loss: 0.4923\n",
      "239/500 [=============>................] - ETA: 4:27 - loss: 1.9211 - regression_loss: 1.4294 - classification_loss: 0.4917\n",
      "240/500 [=============>................] - ETA: 4:26 - loss: 1.9180 - regression_loss: 1.4268 - classification_loss: 0.4913\n",
      "241/500 [=============>................] - ETA: 4:25 - loss: 1.9178 - regression_loss: 1.4267 - classification_loss: 0.4911\n",
      "242/500 [=============>................] - ETA: 4:23 - loss: 1.9137 - regression_loss: 1.4237 - classification_loss: 0.4899\n",
      "243/500 [=============>................] - ETA: 4:22 - loss: 1.9119 - regression_loss: 1.4229 - classification_loss: 0.4890\n",
      "244/500 [=============>................] - ETA: 4:21 - loss: 1.9105 - regression_loss: 1.4219 - classification_loss: 0.4886\n",
      "245/500 [=============>................] - ETA: 4:20 - loss: 1.9095 - regression_loss: 1.4208 - classification_loss: 0.4887\n",
      "246/500 [=============>................] - ETA: 4:19 - loss: 1.9076 - regression_loss: 1.4198 - classification_loss: 0.4878\n",
      "247/500 [=============>................] - ETA: 4:18 - loss: 1.9091 - regression_loss: 1.4212 - classification_loss: 0.4879\n",
      "248/500 [=============>................] - ETA: 4:17 - loss: 1.9082 - regression_loss: 1.4201 - classification_loss: 0.4881\n",
      "249/500 [=============>................] - ETA: 4:16 - loss: 1.9075 - regression_loss: 1.4199 - classification_loss: 0.4876\n",
      "250/500 [==============>...............] - ETA: 4:15 - loss: 1.9065 - regression_loss: 1.4189 - classification_loss: 0.4876\n",
      "251/500 [==============>...............] - ETA: 4:14 - loss: 1.9071 - regression_loss: 1.4192 - classification_loss: 0.4879\n",
      "252/500 [==============>...............] - ETA: 4:13 - loss: 1.9118 - regression_loss: 1.4234 - classification_loss: 0.4884\n",
      "253/500 [==============>...............] - ETA: 4:12 - loss: 1.9152 - regression_loss: 1.4255 - classification_loss: 0.4896\n",
      "254/500 [==============>...............] - ETA: 4:11 - loss: 1.9156 - regression_loss: 1.4262 - classification_loss: 0.4894\n",
      "255/500 [==============>...............] - ETA: 4:10 - loss: 1.9156 - regression_loss: 1.4268 - classification_loss: 0.4887\n",
      "256/500 [==============>...............] - ETA: 4:09 - loss: 1.9185 - regression_loss: 1.4292 - classification_loss: 0.4893\n",
      "257/500 [==============>...............] - ETA: 4:08 - loss: 1.9166 - regression_loss: 1.4276 - classification_loss: 0.4890\n",
      "258/500 [==============>...............] - ETA: 4:07 - loss: 1.9134 - regression_loss: 1.4256 - classification_loss: 0.4878\n",
      "259/500 [==============>...............] - ETA: 4:06 - loss: 1.9120 - regression_loss: 1.4248 - classification_loss: 0.4872\n",
      "260/500 [==============>...............] - ETA: 4:05 - loss: 1.9136 - regression_loss: 1.4260 - classification_loss: 0.4876\n",
      "261/500 [==============>...............] - ETA: 4:04 - loss: 1.9129 - regression_loss: 1.4253 - classification_loss: 0.4875\n",
      "262/500 [==============>...............] - ETA: 4:03 - loss: 1.9140 - regression_loss: 1.4261 - classification_loss: 0.4879\n",
      "263/500 [==============>...............] - ETA: 4:02 - loss: 1.9143 - regression_loss: 1.4264 - classification_loss: 0.4879\n",
      "264/500 [==============>...............] - ETA: 4:01 - loss: 1.9132 - regression_loss: 1.4257 - classification_loss: 0.4875\n",
      "265/500 [==============>...............] - ETA: 4:00 - loss: 1.9111 - regression_loss: 1.4243 - classification_loss: 0.4868\n",
      "266/500 [==============>...............] - ETA: 3:59 - loss: 1.9107 - regression_loss: 1.4238 - classification_loss: 0.4869\n",
      "267/500 [===============>..............] - ETA: 3:58 - loss: 1.9128 - regression_loss: 1.4255 - classification_loss: 0.4874\n",
      "268/500 [===============>..............] - ETA: 3:57 - loss: 1.9118 - regression_loss: 1.4246 - classification_loss: 0.4872\n",
      "269/500 [===============>..............] - ETA: 3:56 - loss: 1.9123 - regression_loss: 1.4249 - classification_loss: 0.4873\n",
      "270/500 [===============>..............] - ETA: 3:55 - loss: 1.9111 - regression_loss: 1.4239 - classification_loss: 0.4873\n",
      "271/500 [===============>..............] - ETA: 3:54 - loss: 1.9113 - regression_loss: 1.4241 - classification_loss: 0.4872\n",
      "272/500 [===============>..............] - ETA: 3:53 - loss: 1.9129 - regression_loss: 1.4251 - classification_loss: 0.4878\n",
      "273/500 [===============>..............] - ETA: 3:52 - loss: 1.9136 - regression_loss: 1.4253 - classification_loss: 0.4884\n",
      "274/500 [===============>..............] - ETA: 3:51 - loss: 1.9160 - regression_loss: 1.4273 - classification_loss: 0.4887\n",
      "275/500 [===============>..............] - ETA: 3:50 - loss: 1.9164 - regression_loss: 1.4275 - classification_loss: 0.4889\n",
      "276/500 [===============>..............] - ETA: 3:49 - loss: 1.9181 - regression_loss: 1.4288 - classification_loss: 0.4893\n",
      "277/500 [===============>..............] - ETA: 3:48 - loss: 1.9158 - regression_loss: 1.4274 - classification_loss: 0.4885\n",
      "278/500 [===============>..............] - ETA: 3:47 - loss: 1.9174 - regression_loss: 1.4285 - classification_loss: 0.4889\n",
      "279/500 [===============>..............] - ETA: 3:46 - loss: 1.9175 - regression_loss: 1.4284 - classification_loss: 0.4891\n",
      "280/500 [===============>..............] - ETA: 3:45 - loss: 1.9186 - regression_loss: 1.4297 - classification_loss: 0.4889\n",
      "281/500 [===============>..............] - ETA: 3:44 - loss: 1.9193 - regression_loss: 1.4305 - classification_loss: 0.4888\n",
      "282/500 [===============>..............] - ETA: 3:43 - loss: 1.9187 - regression_loss: 1.4304 - classification_loss: 0.4882\n",
      "283/500 [===============>..............] - ETA: 3:42 - loss: 1.9188 - regression_loss: 1.4304 - classification_loss: 0.4883\n",
      "284/500 [================>.............] - ETA: 3:41 - loss: 1.9175 - regression_loss: 1.4292 - classification_loss: 0.4883\n",
      "285/500 [================>.............] - ETA: 3:40 - loss: 1.9203 - regression_loss: 1.4304 - classification_loss: 0.4900\n",
      "286/500 [================>.............] - ETA: 3:39 - loss: 1.9213 - regression_loss: 1.4307 - classification_loss: 0.4906\n",
      "287/500 [================>.............] - ETA: 3:38 - loss: 1.9202 - regression_loss: 1.4301 - classification_loss: 0.4901\n",
      "288/500 [================>.............] - ETA: 3:37 - loss: 1.9194 - regression_loss: 1.4296 - classification_loss: 0.4898\n",
      "289/500 [================>.............] - ETA: 3:35 - loss: 1.9210 - regression_loss: 1.4310 - classification_loss: 0.4900\n",
      "290/500 [================>.............] - ETA: 3:35 - loss: 1.9210 - regression_loss: 1.4314 - classification_loss: 0.4896\n",
      "291/500 [================>.............] - ETA: 3:34 - loss: 1.9229 - regression_loss: 1.4327 - classification_loss: 0.4901\n",
      "292/500 [================>.............] - ETA: 3:33 - loss: 1.9254 - regression_loss: 1.4346 - classification_loss: 0.4908\n",
      "293/500 [================>.............] - ETA: 3:32 - loss: 1.9291 - regression_loss: 1.4372 - classification_loss: 0.4919\n",
      "294/500 [================>.............] - ETA: 3:31 - loss: 1.9272 - regression_loss: 1.4359 - classification_loss: 0.4913\n",
      "295/500 [================>.............] - ETA: 3:29 - loss: 1.9277 - regression_loss: 1.4364 - classification_loss: 0.4913\n",
      "296/500 [================>.............] - ETA: 3:28 - loss: 1.9282 - regression_loss: 1.4369 - classification_loss: 0.4913\n",
      "297/500 [================>.............] - ETA: 3:27 - loss: 1.9250 - regression_loss: 1.4341 - classification_loss: 0.4909\n",
      "298/500 [================>.............] - ETA: 3:26 - loss: 1.9254 - regression_loss: 1.4349 - classification_loss: 0.4905\n",
      "299/500 [================>.............] - ETA: 3:25 - loss: 1.9237 - regression_loss: 1.4339 - classification_loss: 0.4899\n",
      "300/500 [=================>............] - ETA: 3:24 - loss: 1.9238 - regression_loss: 1.4337 - classification_loss: 0.4901\n",
      "301/500 [=================>............] - ETA: 3:23 - loss: 1.9251 - regression_loss: 1.4336 - classification_loss: 0.4916\n",
      "302/500 [=================>............] - ETA: 3:22 - loss: 1.9239 - regression_loss: 1.4328 - classification_loss: 0.4911\n",
      "303/500 [=================>............] - ETA: 3:21 - loss: 1.9244 - regression_loss: 1.4335 - classification_loss: 0.4909\n",
      "304/500 [=================>............] - ETA: 3:20 - loss: 1.9242 - regression_loss: 1.4332 - classification_loss: 0.4910\n",
      "305/500 [=================>............] - ETA: 3:19 - loss: 1.9257 - regression_loss: 1.4337 - classification_loss: 0.4919\n",
      "306/500 [=================>............] - ETA: 3:18 - loss: 1.9253 - regression_loss: 1.4332 - classification_loss: 0.4921\n",
      "307/500 [=================>............] - ETA: 3:17 - loss: 1.9252 - regression_loss: 1.4332 - classification_loss: 0.4920\n",
      "308/500 [=================>............] - ETA: 3:16 - loss: 1.9245 - regression_loss: 1.4329 - classification_loss: 0.4916\n",
      "309/500 [=================>............] - ETA: 3:15 - loss: 1.9239 - regression_loss: 1.4328 - classification_loss: 0.4911\n",
      "310/500 [=================>............] - ETA: 3:14 - loss: 1.9229 - regression_loss: 1.4322 - classification_loss: 0.4907\n",
      "311/500 [=================>............] - ETA: 3:13 - loss: 1.9220 - regression_loss: 1.4317 - classification_loss: 0.4903\n",
      "312/500 [=================>............] - ETA: 3:12 - loss: 1.9206 - regression_loss: 1.4310 - classification_loss: 0.4896\n",
      "313/500 [=================>............] - ETA: 3:11 - loss: 1.9206 - regression_loss: 1.4305 - classification_loss: 0.4901\n",
      "314/500 [=================>............] - ETA: 3:10 - loss: 1.9215 - regression_loss: 1.4314 - classification_loss: 0.4901\n",
      "315/500 [=================>............] - ETA: 3:09 - loss: 1.9213 - regression_loss: 1.4308 - classification_loss: 0.4905\n",
      "316/500 [=================>............] - ETA: 3:08 - loss: 1.9201 - regression_loss: 1.4300 - classification_loss: 0.4901\n",
      "317/500 [==================>...........] - ETA: 3:07 - loss: 1.9206 - regression_loss: 1.4305 - classification_loss: 0.4900\n",
      "318/500 [==================>...........] - ETA: 3:06 - loss: 1.9188 - regression_loss: 1.4294 - classification_loss: 0.4894\n",
      "319/500 [==================>...........] - ETA: 3:05 - loss: 1.9186 - regression_loss: 1.4295 - classification_loss: 0.4891\n",
      "320/500 [==================>...........] - ETA: 3:04 - loss: 1.9174 - regression_loss: 1.4284 - classification_loss: 0.4889\n",
      "321/500 [==================>...........] - ETA: 3:03 - loss: 1.9205 - regression_loss: 1.4299 - classification_loss: 0.4906\n",
      "322/500 [==================>...........] - ETA: 3:02 - loss: 1.9216 - regression_loss: 1.4314 - classification_loss: 0.4902\n",
      "323/500 [==================>...........] - ETA: 3:01 - loss: 1.9190 - regression_loss: 1.4296 - classification_loss: 0.4894\n",
      "324/500 [==================>...........] - ETA: 3:00 - loss: 1.9211 - regression_loss: 1.4312 - classification_loss: 0.4899\n",
      "325/500 [==================>...........] - ETA: 2:59 - loss: 1.9204 - regression_loss: 1.4306 - classification_loss: 0.4898\n",
      "326/500 [==================>...........] - ETA: 2:58 - loss: 1.9206 - regression_loss: 1.4311 - classification_loss: 0.4895\n",
      "327/500 [==================>...........] - ETA: 2:57 - loss: 1.9189 - regression_loss: 1.4301 - classification_loss: 0.4888\n",
      "328/500 [==================>...........] - ETA: 2:56 - loss: 1.9196 - regression_loss: 1.4304 - classification_loss: 0.4891\n",
      "329/500 [==================>...........] - ETA: 2:55 - loss: 1.9207 - regression_loss: 1.4315 - classification_loss: 0.4892\n",
      "330/500 [==================>...........] - ETA: 2:54 - loss: 1.9200 - regression_loss: 1.4310 - classification_loss: 0.4890\n",
      "331/500 [==================>...........] - ETA: 2:53 - loss: 1.9237 - regression_loss: 1.4319 - classification_loss: 0.4918\n",
      "332/500 [==================>...........] - ETA: 2:52 - loss: 1.9241 - regression_loss: 1.4319 - classification_loss: 0.4922\n",
      "333/500 [==================>...........] - ETA: 2:51 - loss: 1.9260 - regression_loss: 1.4335 - classification_loss: 0.4925\n",
      "334/500 [===================>..........] - ETA: 2:50 - loss: 1.9266 - regression_loss: 1.4336 - classification_loss: 0.4929\n",
      "335/500 [===================>..........] - ETA: 2:49 - loss: 1.9276 - regression_loss: 1.4343 - classification_loss: 0.4933\n",
      "336/500 [===================>..........] - ETA: 2:48 - loss: 1.9281 - regression_loss: 1.4347 - classification_loss: 0.4935\n",
      "337/500 [===================>..........] - ETA: 2:47 - loss: 1.9290 - regression_loss: 1.4352 - classification_loss: 0.4937\n",
      "338/500 [===================>..........] - ETA: 2:46 - loss: 1.9271 - regression_loss: 1.4337 - classification_loss: 0.4933\n",
      "339/500 [===================>..........] - ETA: 2:45 - loss: 1.9270 - regression_loss: 1.4338 - classification_loss: 0.4932\n",
      "340/500 [===================>..........] - ETA: 2:44 - loss: 1.9254 - regression_loss: 1.4328 - classification_loss: 0.4927\n",
      "341/500 [===================>..........] - ETA: 2:43 - loss: 1.9264 - regression_loss: 1.4340 - classification_loss: 0.4923\n",
      "342/500 [===================>..........] - ETA: 2:42 - loss: 1.9245 - regression_loss: 1.4329 - classification_loss: 0.4916\n",
      "343/500 [===================>..........] - ETA: 2:41 - loss: 1.9231 - regression_loss: 1.4312 - classification_loss: 0.4919\n",
      "344/500 [===================>..........] - ETA: 2:39 - loss: 1.9234 - regression_loss: 1.4316 - classification_loss: 0.4918\n",
      "345/500 [===================>..........] - ETA: 2:38 - loss: 1.9244 - regression_loss: 1.4324 - classification_loss: 0.4920\n",
      "346/500 [===================>..........] - ETA: 2:37 - loss: 1.9226 - regression_loss: 1.4311 - classification_loss: 0.4915\n",
      "347/500 [===================>..........] - ETA: 2:36 - loss: 1.9226 - regression_loss: 1.4312 - classification_loss: 0.4914\n",
      "348/500 [===================>..........] - ETA: 2:35 - loss: 1.9229 - regression_loss: 1.4313 - classification_loss: 0.4916\n",
      "349/500 [===================>..........] - ETA: 2:34 - loss: 1.9205 - regression_loss: 1.4296 - classification_loss: 0.4909\n",
      "350/500 [====================>.........] - ETA: 2:33 - loss: 1.9197 - regression_loss: 1.4288 - classification_loss: 0.4909\n",
      "351/500 [====================>.........] - ETA: 2:32 - loss: 1.9218 - regression_loss: 1.4302 - classification_loss: 0.4916\n",
      "352/500 [====================>.........] - ETA: 2:31 - loss: 1.9230 - regression_loss: 1.4305 - classification_loss: 0.4925\n",
      "353/500 [====================>.........] - ETA: 2:30 - loss: 1.9210 - regression_loss: 1.4291 - classification_loss: 0.4918\n",
      "354/500 [====================>.........] - ETA: 2:29 - loss: 1.9220 - regression_loss: 1.4298 - classification_loss: 0.4922\n",
      "355/500 [====================>.........] - ETA: 2:28 - loss: 1.9193 - regression_loss: 1.4280 - classification_loss: 0.4913\n",
      "356/500 [====================>.........] - ETA: 2:27 - loss: 1.9193 - regression_loss: 1.4282 - classification_loss: 0.4911\n",
      "357/500 [====================>.........] - ETA: 2:26 - loss: 1.9189 - regression_loss: 1.4279 - classification_loss: 0.4910\n",
      "358/500 [====================>.........] - ETA: 2:25 - loss: 1.9190 - regression_loss: 1.4282 - classification_loss: 0.4908\n",
      "359/500 [====================>.........] - ETA: 2:24 - loss: 1.9197 - regression_loss: 1.4286 - classification_loss: 0.4910\n",
      "360/500 [====================>.........] - ETA: 2:23 - loss: 1.9223 - regression_loss: 1.4300 - classification_loss: 0.4922\n",
      "361/500 [====================>.........] - ETA: 2:22 - loss: 1.9234 - regression_loss: 1.4311 - classification_loss: 0.4924\n",
      "362/500 [====================>.........] - ETA: 2:21 - loss: 1.9248 - regression_loss: 1.4327 - classification_loss: 0.4921\n",
      "363/500 [====================>.........] - ETA: 2:20 - loss: 1.9256 - regression_loss: 1.4335 - classification_loss: 0.4921\n",
      "364/500 [====================>.........] - ETA: 2:19 - loss: 1.9252 - regression_loss: 1.4334 - classification_loss: 0.4917\n",
      "365/500 [====================>.........] - ETA: 2:18 - loss: 1.9235 - regression_loss: 1.4323 - classification_loss: 0.4913\n",
      "366/500 [====================>.........] - ETA: 2:17 - loss: 1.9224 - regression_loss: 1.4313 - classification_loss: 0.4912\n",
      "367/500 [=====================>........] - ETA: 2:16 - loss: 1.9228 - regression_loss: 1.4316 - classification_loss: 0.4912\n",
      "368/500 [=====================>........] - ETA: 2:15 - loss: 1.9226 - regression_loss: 1.4314 - classification_loss: 0.4912\n",
      "369/500 [=====================>........] - ETA: 2:14 - loss: 1.9233 - regression_loss: 1.4317 - classification_loss: 0.4916\n",
      "370/500 [=====================>........] - ETA: 2:13 - loss: 1.9233 - regression_loss: 1.4314 - classification_loss: 0.4919\n",
      "371/500 [=====================>........] - ETA: 2:12 - loss: 1.9242 - regression_loss: 1.4315 - classification_loss: 0.4927\n",
      "372/500 [=====================>........] - ETA: 2:11 - loss: 1.9222 - regression_loss: 1.4300 - classification_loss: 0.4923\n",
      "373/500 [=====================>........] - ETA: 2:10 - loss: 1.9197 - regression_loss: 1.4281 - classification_loss: 0.4916\n",
      "374/500 [=====================>........] - ETA: 2:09 - loss: 1.9183 - regression_loss: 1.4272 - classification_loss: 0.4911\n",
      "375/500 [=====================>........] - ETA: 2:08 - loss: 1.9180 - regression_loss: 1.4269 - classification_loss: 0.4912\n",
      "376/500 [=====================>........] - ETA: 2:07 - loss: 1.9173 - regression_loss: 1.4266 - classification_loss: 0.4907\n",
      "377/500 [=====================>........] - ETA: 2:06 - loss: 1.9167 - regression_loss: 1.4264 - classification_loss: 0.4903\n",
      "378/500 [=====================>........] - ETA: 2:05 - loss: 1.9174 - regression_loss: 1.4270 - classification_loss: 0.4904\n",
      "379/500 [=====================>........] - ETA: 2:04 - loss: 1.9156 - regression_loss: 1.4258 - classification_loss: 0.4898\n",
      "380/500 [=====================>........] - ETA: 2:03 - loss: 1.9145 - regression_loss: 1.4252 - classification_loss: 0.4893\n",
      "381/500 [=====================>........] - ETA: 2:02 - loss: 1.9122 - regression_loss: 1.4234 - classification_loss: 0.4888\n",
      "382/500 [=====================>........] - ETA: 2:01 - loss: 1.9142 - regression_loss: 1.4249 - classification_loss: 0.4893\n",
      "383/500 [=====================>........] - ETA: 2:00 - loss: 1.9141 - regression_loss: 1.4248 - classification_loss: 0.4893\n",
      "384/500 [======================>.......] - ETA: 1:59 - loss: 1.9158 - regression_loss: 1.4261 - classification_loss: 0.4897\n",
      "385/500 [======================>.......] - ETA: 1:58 - loss: 1.9159 - regression_loss: 1.4261 - classification_loss: 0.4897\n",
      "386/500 [======================>.......] - ETA: 1:56 - loss: 1.9162 - regression_loss: 1.4258 - classification_loss: 0.4903\n",
      "387/500 [======================>.......] - ETA: 1:55 - loss: 1.9169 - regression_loss: 1.4263 - classification_loss: 0.4906\n",
      "388/500 [======================>.......] - ETA: 1:54 - loss: 1.9175 - regression_loss: 1.4267 - classification_loss: 0.4908\n",
      "389/500 [======================>.......] - ETA: 1:53 - loss: 1.9183 - regression_loss: 1.4275 - classification_loss: 0.4908\n",
      "390/500 [======================>.......] - ETA: 1:52 - loss: 1.9169 - regression_loss: 1.4264 - classification_loss: 0.4905\n",
      "391/500 [======================>.......] - ETA: 1:51 - loss: 1.9167 - regression_loss: 1.4263 - classification_loss: 0.4904\n",
      "392/500 [======================>.......] - ETA: 1:50 - loss: 1.9161 - regression_loss: 1.4256 - classification_loss: 0.4905\n",
      "393/500 [======================>.......] - ETA: 1:50 - loss: 1.9134 - regression_loss: 1.4235 - classification_loss: 0.4899\n",
      "394/500 [======================>.......] - ETA: 1:49 - loss: 1.9130 - regression_loss: 1.4234 - classification_loss: 0.4897\n",
      "395/500 [======================>.......] - ETA: 1:48 - loss: 1.9121 - regression_loss: 1.4227 - classification_loss: 0.4895\n",
      "396/500 [======================>.......] - ETA: 1:47 - loss: 1.9126 - regression_loss: 1.4227 - classification_loss: 0.4899\n",
      "397/500 [======================>.......] - ETA: 1:46 - loss: 1.9113 - regression_loss: 1.4219 - classification_loss: 0.4893\n",
      "398/500 [======================>.......] - ETA: 1:45 - loss: 1.9112 - regression_loss: 1.4222 - classification_loss: 0.4890\n",
      "399/500 [======================>.......] - ETA: 1:44 - loss: 1.9127 - regression_loss: 1.4229 - classification_loss: 0.4898\n",
      "400/500 [=======================>......] - ETA: 1:43 - loss: 1.9102 - regression_loss: 1.4212 - classification_loss: 0.4890\n",
      "401/500 [=======================>......] - ETA: 1:42 - loss: 1.9081 - regression_loss: 1.4198 - classification_loss: 0.4883\n",
      "402/500 [=======================>......] - ETA: 1:41 - loss: 1.9072 - regression_loss: 1.4191 - classification_loss: 0.4881\n",
      "403/500 [=======================>......] - ETA: 1:40 - loss: 1.9081 - regression_loss: 1.4198 - classification_loss: 0.4883\n",
      "404/500 [=======================>......] - ETA: 1:39 - loss: 1.9082 - regression_loss: 1.4197 - classification_loss: 0.4885\n",
      "405/500 [=======================>......] - ETA: 1:38 - loss: 1.9078 - regression_loss: 1.4195 - classification_loss: 0.4883\n",
      "406/500 [=======================>......] - ETA: 1:37 - loss: 1.9069 - regression_loss: 1.4188 - classification_loss: 0.4880\n",
      "407/500 [=======================>......] - ETA: 1:36 - loss: 1.9055 - regression_loss: 1.4178 - classification_loss: 0.4877\n",
      "408/500 [=======================>......] - ETA: 1:35 - loss: 1.9054 - regression_loss: 1.4179 - classification_loss: 0.4875\n",
      "409/500 [=======================>......] - ETA: 1:34 - loss: 1.9048 - regression_loss: 1.4178 - classification_loss: 0.4870\n",
      "410/500 [=======================>......] - ETA: 1:33 - loss: 1.9061 - regression_loss: 1.4190 - classification_loss: 0.4871\n",
      "411/500 [=======================>......] - ETA: 1:32 - loss: 1.9059 - regression_loss: 1.4189 - classification_loss: 0.4870\n",
      "412/500 [=======================>......] - ETA: 1:31 - loss: 1.9062 - regression_loss: 1.4188 - classification_loss: 0.4874\n",
      "413/500 [=======================>......] - ETA: 1:30 - loss: 1.9054 - regression_loss: 1.4182 - classification_loss: 0.4873\n",
      "414/500 [=======================>......] - ETA: 1:29 - loss: 1.9053 - regression_loss: 1.4184 - classification_loss: 0.4869\n",
      "415/500 [=======================>......] - ETA: 1:28 - loss: 1.9067 - regression_loss: 1.4194 - classification_loss: 0.4872\n",
      "416/500 [=======================>......] - ETA: 1:27 - loss: 1.9070 - regression_loss: 1.4196 - classification_loss: 0.4874\n",
      "417/500 [========================>.....] - ETA: 1:25 - loss: 1.9064 - regression_loss: 1.4195 - classification_loss: 0.4869\n",
      "418/500 [========================>.....] - ETA: 1:24 - loss: 1.9083 - regression_loss: 1.4211 - classification_loss: 0.4873\n",
      "419/500 [========================>.....] - ETA: 1:23 - loss: 1.9063 - regression_loss: 1.4196 - classification_loss: 0.4867\n",
      "420/500 [========================>.....] - ETA: 1:22 - loss: 1.9071 - regression_loss: 1.4203 - classification_loss: 0.4867\n",
      "421/500 [========================>.....] - ETA: 1:21 - loss: 1.9069 - regression_loss: 1.4202 - classification_loss: 0.4867\n",
      "422/500 [========================>.....] - ETA: 1:20 - loss: 1.9088 - regression_loss: 1.4222 - classification_loss: 0.4867\n",
      "423/500 [========================>.....] - ETA: 1:19 - loss: 1.9085 - regression_loss: 1.4220 - classification_loss: 0.4865\n",
      "424/500 [========================>.....] - ETA: 1:18 - loss: 1.9096 - regression_loss: 1.4223 - classification_loss: 0.4872\n",
      "425/500 [========================>.....] - ETA: 1:17 - loss: 1.9084 - regression_loss: 1.4217 - classification_loss: 0.4867\n",
      "426/500 [========================>.....] - ETA: 1:16 - loss: 1.9087 - regression_loss: 1.4217 - classification_loss: 0.4870\n",
      "427/500 [========================>.....] - ETA: 1:15 - loss: 1.9079 - regression_loss: 1.4212 - classification_loss: 0.4868\n",
      "428/500 [========================>.....] - ETA: 1:14 - loss: 1.9070 - regression_loss: 1.4205 - classification_loss: 0.4865\n",
      "429/500 [========================>.....] - ETA: 1:13 - loss: 1.9069 - regression_loss: 1.4205 - classification_loss: 0.4864\n",
      "430/500 [========================>.....] - ETA: 1:12 - loss: 1.9073 - regression_loss: 1.4212 - classification_loss: 0.4862\n",
      "431/500 [========================>.....] - ETA: 1:11 - loss: 1.9062 - regression_loss: 1.4205 - classification_loss: 0.4857\n",
      "432/500 [========================>.....] - ETA: 1:10 - loss: 1.9073 - regression_loss: 1.4216 - classification_loss: 0.4857\n",
      "433/500 [========================>.....] - ETA: 1:09 - loss: 1.9063 - regression_loss: 1.4208 - classification_loss: 0.4855\n",
      "434/500 [=========================>....] - ETA: 1:08 - loss: 1.9055 - regression_loss: 1.4204 - classification_loss: 0.4851\n",
      "435/500 [=========================>....] - ETA: 1:07 - loss: 1.9053 - regression_loss: 1.4202 - classification_loss: 0.4851\n",
      "436/500 [=========================>....] - ETA: 1:06 - loss: 1.9074 - regression_loss: 1.4215 - classification_loss: 0.4859\n",
      "437/500 [=========================>....] - ETA: 1:05 - loss: 1.9069 - regression_loss: 1.4212 - classification_loss: 0.4858\n",
      "438/500 [=========================>....] - ETA: 1:04 - loss: 1.9050 - regression_loss: 1.4199 - classification_loss: 0.4851\n",
      "439/500 [=========================>....] - ETA: 1:03 - loss: 1.9037 - regression_loss: 1.4188 - classification_loss: 0.4849\n",
      "440/500 [=========================>....] - ETA: 1:02 - loss: 1.9028 - regression_loss: 1.4179 - classification_loss: 0.4848\n",
      "441/500 [=========================>....] - ETA: 1:01 - loss: 1.9040 - regression_loss: 1.4193 - classification_loss: 0.4847\n",
      "442/500 [=========================>....] - ETA: 1:00 - loss: 1.9044 - regression_loss: 1.4195 - classification_loss: 0.4849\n",
      "443/500 [=========================>....] - ETA: 59s - loss: 1.9046 - regression_loss: 1.4197 - classification_loss: 0.4848 \n",
      "444/500 [=========================>....] - ETA: 57s - loss: 1.9040 - regression_loss: 1.4191 - classification_loss: 0.4848\n",
      "445/500 [=========================>....] - ETA: 56s - loss: 1.9032 - regression_loss: 1.4188 - classification_loss: 0.4844\n",
      "446/500 [=========================>....] - ETA: 55s - loss: 1.9044 - regression_loss: 1.4199 - classification_loss: 0.4845\n",
      "447/500 [=========================>....] - ETA: 54s - loss: 1.9063 - regression_loss: 1.4218 - classification_loss: 0.4846\n",
      "448/500 [=========================>....] - ETA: 53s - loss: 1.9054 - regression_loss: 1.4211 - classification_loss: 0.4843\n",
      "449/500 [=========================>....] - ETA: 52s - loss: 1.9054 - regression_loss: 1.4214 - classification_loss: 0.4840\n",
      "450/500 [==========================>...] - ETA: 51s - loss: 1.9041 - regression_loss: 1.4204 - classification_loss: 0.4837\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 1.9040 - regression_loss: 1.4204 - classification_loss: 0.4835\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 1.9033 - regression_loss: 1.4200 - classification_loss: 0.4834\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.9054 - regression_loss: 1.4214 - classification_loss: 0.4840\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.9049 - regression_loss: 1.4210 - classification_loss: 0.4839\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.9053 - regression_loss: 1.4215 - classification_loss: 0.4839\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.9054 - regression_loss: 1.4219 - classification_loss: 0.4835\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.9052 - regression_loss: 1.4215 - classification_loss: 0.4836\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.9058 - regression_loss: 1.4223 - classification_loss: 0.4835\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.9071 - regression_loss: 1.4233 - classification_loss: 0.4838\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.9085 - regression_loss: 1.4242 - classification_loss: 0.4844\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 1.9071 - regression_loss: 1.4231 - classification_loss: 0.4840\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 1.9071 - regression_loss: 1.4233 - classification_loss: 0.4838\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 1.9083 - regression_loss: 1.4241 - classification_loss: 0.4842\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 1.9070 - regression_loss: 1.4229 - classification_loss: 0.4840\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 1.9061 - regression_loss: 1.4222 - classification_loss: 0.4839\n",
      "466/500 [==========================>...] - ETA: 35s - loss: 1.9055 - regression_loss: 1.4217 - classification_loss: 0.4838\n",
      "467/500 [===========================>..] - ETA: 34s - loss: 1.9053 - regression_loss: 1.4214 - classification_loss: 0.4839\n",
      "468/500 [===========================>..] - ETA: 33s - loss: 1.9034 - regression_loss: 1.4201 - classification_loss: 0.4833\n",
      "469/500 [===========================>..] - ETA: 32s - loss: 1.9036 - regression_loss: 1.4205 - classification_loss: 0.4830\n",
      "470/500 [===========================>..] - ETA: 31s - loss: 1.9025 - regression_loss: 1.4198 - classification_loss: 0.4827\n",
      "471/500 [===========================>..] - ETA: 30s - loss: 1.9019 - regression_loss: 1.4196 - classification_loss: 0.4823\n",
      "472/500 [===========================>..] - ETA: 28s - loss: 1.9026 - regression_loss: 1.4201 - classification_loss: 0.4825\n",
      "473/500 [===========================>..] - ETA: 27s - loss: 1.9035 - regression_loss: 1.4207 - classification_loss: 0.4827\n",
      "474/500 [===========================>..] - ETA: 26s - loss: 1.9016 - regression_loss: 1.4193 - classification_loss: 0.4823\n",
      "475/500 [===========================>..] - ETA: 25s - loss: 1.9008 - regression_loss: 1.4187 - classification_loss: 0.4821\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 1.9005 - regression_loss: 1.4181 - classification_loss: 0.4824\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.9006 - regression_loss: 1.4183 - classification_loss: 0.4822\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.9011 - regression_loss: 1.4190 - classification_loss: 0.4821\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.9006 - regression_loss: 1.4183 - classification_loss: 0.4823\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.8988 - regression_loss: 1.4168 - classification_loss: 0.4820\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.8977 - regression_loss: 1.4160 - classification_loss: 0.4817\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.8969 - regression_loss: 1.4155 - classification_loss: 0.4814\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.8952 - regression_loss: 1.4142 - classification_loss: 0.4810\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.8962 - regression_loss: 1.4147 - classification_loss: 0.4815\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.8961 - regression_loss: 1.4146 - classification_loss: 0.4814\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.8968 - regression_loss: 1.4153 - classification_loss: 0.4815\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.8968 - regression_loss: 1.4152 - classification_loss: 0.4816\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.8993 - regression_loss: 1.4171 - classification_loss: 0.4822\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.9004 - regression_loss: 1.4179 - classification_loss: 0.4824\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.9011 - regression_loss: 1.4184 - classification_loss: 0.4827\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.9002 - regression_loss: 1.4176 - classification_loss: 0.4826 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.9010 - regression_loss: 1.4175 - classification_loss: 0.4835\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.8997 - regression_loss: 1.4168 - classification_loss: 0.4829\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.9006 - regression_loss: 1.4175 - classification_loss: 0.4831\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.9008 - regression_loss: 1.4177 - classification_loss: 0.4832\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.9006 - regression_loss: 1.4179 - classification_loss: 0.4827\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.8995 - regression_loss: 1.4168 - classification_loss: 0.4827\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.8995 - regression_loss: 1.4166 - classification_loss: 0.4829\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.9004 - regression_loss: 1.4170 - classification_loss: 0.4834\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9006 - regression_loss: 1.4173 - classification_loss: 0.4834\n",
      "Epoch 00036: saving model to ./snapshots\\resnet50_csv_36.h5\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 9.999999146890344e-15.\n",
      "\n",
      "500/500 [==============================] - 518s 1s/step - loss: 1.9006 - regression_loss: 1.4173 - classification_loss: 0.4834\n",
      "Epoch 37/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 1.8461 - regression_loss: 1.4666 - classification_loss: 0.3795\n",
      "  2/500 [..............................] - ETA: 4:34 - loss: 1.5812 - regression_loss: 1.2594 - classification_loss: 0.3218\n",
      "  3/500 [..............................] - ETA: 5:44 - loss: 1.5509 - regression_loss: 1.2079 - classification_loss: 0.3430\n",
      "  4/500 [..............................] - ETA: 6:31 - loss: 1.8136 - regression_loss: 1.3685 - classification_loss: 0.4452\n",
      "  5/500 [..............................] - ETA: 6:49 - loss: 1.6909 - regression_loss: 1.2860 - classification_loss: 0.4049\n",
      "  6/500 [..............................] - ETA: 7:10 - loss: 1.7813 - regression_loss: 1.3594 - classification_loss: 0.4220\n",
      "  7/500 [..............................] - ETA: 7:18 - loss: 1.8281 - regression_loss: 1.3752 - classification_loss: 0.4529\n",
      "  8/500 [..............................] - ETA: 7:23 - loss: 1.8939 - regression_loss: 1.4205 - classification_loss: 0.4734\n",
      "  9/500 [..............................] - ETA: 7:26 - loss: 1.9017 - regression_loss: 1.4285 - classification_loss: 0.4731\n",
      " 10/500 [..............................] - ETA: 7:33 - loss: 1.9611 - regression_loss: 1.4770 - classification_loss: 0.4841\n",
      " 11/500 [..............................] - ETA: 7:39 - loss: 1.9936 - regression_loss: 1.4998 - classification_loss: 0.4938\n",
      " 12/500 [..............................] - ETA: 7:40 - loss: 1.9517 - regression_loss: 1.4716 - classification_loss: 0.4801\n",
      " 13/500 [..............................] - ETA: 7:41 - loss: 1.9633 - regression_loss: 1.4762 - classification_loss: 0.4871\n",
      " 14/500 [..............................] - ETA: 7:44 - loss: 1.9333 - regression_loss: 1.4523 - classification_loss: 0.4810\n",
      " 15/500 [..............................] - ETA: 7:44 - loss: 1.9777 - regression_loss: 1.4715 - classification_loss: 0.5063\n",
      " 16/500 [..............................] - ETA: 7:44 - loss: 1.9585 - regression_loss: 1.4596 - classification_loss: 0.4989\n",
      " 17/500 [>.............................] - ETA: 7:44 - loss: 1.9702 - regression_loss: 1.4708 - classification_loss: 0.4994\n",
      " 18/500 [>.............................] - ETA: 7:43 - loss: 1.9831 - regression_loss: 1.4769 - classification_loss: 0.5061\n",
      " 19/500 [>.............................] - ETA: 7:43 - loss: 2.0125 - regression_loss: 1.4942 - classification_loss: 0.5182\n",
      " 20/500 [>.............................] - ETA: 7:43 - loss: 2.0112 - regression_loss: 1.4942 - classification_loss: 0.5170\n",
      " 21/500 [>.............................] - ETA: 7:42 - loss: 1.9731 - regression_loss: 1.4616 - classification_loss: 0.5115\n",
      " 22/500 [>.............................] - ETA: 7:42 - loss: 1.9634 - regression_loss: 1.4565 - classification_loss: 0.5069\n",
      " 23/500 [>.............................] - ETA: 7:41 - loss: 1.9434 - regression_loss: 1.4430 - classification_loss: 0.5003\n",
      " 24/500 [>.............................] - ETA: 7:41 - loss: 1.9565 - regression_loss: 1.4556 - classification_loss: 0.5008\n",
      " 25/500 [>.............................] - ETA: 7:42 - loss: 2.0157 - regression_loss: 1.4946 - classification_loss: 0.5211\n",
      " 26/500 [>.............................] - ETA: 7:43 - loss: 2.0369 - regression_loss: 1.5081 - classification_loss: 0.5288\n",
      " 27/500 [>.............................] - ETA: 7:43 - loss: 2.0284 - regression_loss: 1.5028 - classification_loss: 0.5255\n",
      " 28/500 [>.............................] - ETA: 7:41 - loss: 2.0420 - regression_loss: 1.5148 - classification_loss: 0.5272\n",
      " 29/500 [>.............................] - ETA: 7:40 - loss: 2.0618 - regression_loss: 1.5304 - classification_loss: 0.5314\n",
      " 30/500 [>.............................] - ETA: 7:39 - loss: 2.0536 - regression_loss: 1.5252 - classification_loss: 0.5284\n",
      " 31/500 [>.............................] - ETA: 7:38 - loss: 2.0187 - regression_loss: 1.5008 - classification_loss: 0.5178\n",
      " 32/500 [>.............................] - ETA: 7:40 - loss: 2.0148 - regression_loss: 1.4986 - classification_loss: 0.5163\n",
      " 33/500 [>.............................] - ETA: 7:41 - loss: 1.9967 - regression_loss: 1.4826 - classification_loss: 0.5141\n",
      " 34/500 [=>............................] - ETA: 7:38 - loss: 1.9798 - regression_loss: 1.4716 - classification_loss: 0.5081\n",
      " 35/500 [=>............................] - ETA: 7:38 - loss: 1.9586 - regression_loss: 1.4554 - classification_loss: 0.5032\n",
      " 36/500 [=>............................] - ETA: 7:38 - loss: 1.9676 - regression_loss: 1.4651 - classification_loss: 0.5025\n",
      " 37/500 [=>............................] - ETA: 7:39 - loss: 1.9735 - regression_loss: 1.4706 - classification_loss: 0.5029\n",
      " 38/500 [=>............................] - ETA: 7:39 - loss: 1.9741 - regression_loss: 1.4713 - classification_loss: 0.5027\n",
      " 39/500 [=>............................] - ETA: 7:39 - loss: 1.9537 - regression_loss: 1.4558 - classification_loss: 0.4979\n",
      " 40/500 [=>............................] - ETA: 7:39 - loss: 1.9624 - regression_loss: 1.4664 - classification_loss: 0.4959\n",
      " 41/500 [=>............................] - ETA: 7:38 - loss: 1.9455 - regression_loss: 1.4538 - classification_loss: 0.4917\n",
      " 42/500 [=>............................] - ETA: 7:37 - loss: 1.9446 - regression_loss: 1.4565 - classification_loss: 0.4881\n",
      " 43/500 [=>............................] - ETA: 7:36 - loss: 1.9462 - regression_loss: 1.4588 - classification_loss: 0.4875\n",
      " 44/500 [=>............................] - ETA: 7:35 - loss: 1.9558 - regression_loss: 1.4610 - classification_loss: 0.4948\n",
      " 45/500 [=>............................] - ETA: 7:33 - loss: 1.9592 - regression_loss: 1.4669 - classification_loss: 0.4923\n",
      " 46/500 [=>............................] - ETA: 7:32 - loss: 1.9464 - regression_loss: 1.4576 - classification_loss: 0.4888\n",
      " 47/500 [=>............................] - ETA: 7:32 - loss: 1.9510 - regression_loss: 1.4632 - classification_loss: 0.4879\n",
      " 48/500 [=>............................] - ETA: 7:30 - loss: 1.9605 - regression_loss: 1.4661 - classification_loss: 0.4944\n",
      " 49/500 [=>............................] - ETA: 7:30 - loss: 1.9727 - regression_loss: 1.4785 - classification_loss: 0.4942\n",
      " 50/500 [==>...........................] - ETA: 7:30 - loss: 1.9854 - regression_loss: 1.4875 - classification_loss: 0.4979\n",
      " 51/500 [==>...........................] - ETA: 7:29 - loss: 1.9901 - regression_loss: 1.4905 - classification_loss: 0.4996\n",
      " 52/500 [==>...........................] - ETA: 7:28 - loss: 1.9862 - regression_loss: 1.4884 - classification_loss: 0.4978\n",
      " 53/500 [==>...........................] - ETA: 7:27 - loss: 1.9692 - regression_loss: 1.4776 - classification_loss: 0.4916\n",
      " 54/500 [==>...........................] - ETA: 7:26 - loss: 1.9545 - regression_loss: 1.4664 - classification_loss: 0.4881\n",
      " 55/500 [==>...........................] - ETA: 7:25 - loss: 1.9602 - regression_loss: 1.4706 - classification_loss: 0.4896\n",
      " 56/500 [==>...........................] - ETA: 7:25 - loss: 1.9540 - regression_loss: 1.4678 - classification_loss: 0.4862\n",
      " 57/500 [==>...........................] - ETA: 7:24 - loss: 1.9542 - regression_loss: 1.4692 - classification_loss: 0.4850\n",
      " 58/500 [==>...........................] - ETA: 7:24 - loss: 1.9470 - regression_loss: 1.4654 - classification_loss: 0.4816\n",
      " 59/500 [==>...........................] - ETA: 7:21 - loss: 1.9368 - regression_loss: 1.4589 - classification_loss: 0.4779\n",
      " 60/500 [==>...........................] - ETA: 7:21 - loss: 1.9316 - regression_loss: 1.4554 - classification_loss: 0.4762\n",
      " 61/500 [==>...........................] - ETA: 7:20 - loss: 1.9262 - regression_loss: 1.4499 - classification_loss: 0.4762\n",
      " 62/500 [==>...........................] - ETA: 7:19 - loss: 1.9240 - regression_loss: 1.4482 - classification_loss: 0.4757\n",
      " 63/500 [==>...........................] - ETA: 7:18 - loss: 1.9303 - regression_loss: 1.4539 - classification_loss: 0.4763\n",
      " 64/500 [==>...........................] - ETA: 7:17 - loss: 1.9238 - regression_loss: 1.4492 - classification_loss: 0.4747\n",
      " 65/500 [==>...........................] - ETA: 7:16 - loss: 1.9299 - regression_loss: 1.4549 - classification_loss: 0.4750\n",
      " 66/500 [==>...........................] - ETA: 7:15 - loss: 1.9210 - regression_loss: 1.4486 - classification_loss: 0.4723\n",
      " 67/500 [===>..........................] - ETA: 7:14 - loss: 1.9315 - regression_loss: 1.4539 - classification_loss: 0.4776\n",
      " 68/500 [===>..........................] - ETA: 7:14 - loss: 1.9331 - regression_loss: 1.4547 - classification_loss: 0.4785\n",
      " 69/500 [===>..........................] - ETA: 7:13 - loss: 1.9354 - regression_loss: 1.4559 - classification_loss: 0.4795\n",
      " 70/500 [===>..........................] - ETA: 7:12 - loss: 1.9367 - regression_loss: 1.4539 - classification_loss: 0.4829\n",
      " 71/500 [===>..........................] - ETA: 7:11 - loss: 1.9402 - regression_loss: 1.4560 - classification_loss: 0.4841\n",
      " 72/500 [===>..........................] - ETA: 7:11 - loss: 1.9413 - regression_loss: 1.4561 - classification_loss: 0.4852\n",
      " 73/500 [===>..........................] - ETA: 7:10 - loss: 1.9534 - regression_loss: 1.4670 - classification_loss: 0.4864\n",
      " 74/500 [===>..........................] - ETA: 7:09 - loss: 1.9432 - regression_loss: 1.4579 - classification_loss: 0.4854\n",
      " 75/500 [===>..........................] - ETA: 7:09 - loss: 1.9326 - regression_loss: 1.4482 - classification_loss: 0.4844\n",
      " 76/500 [===>..........................] - ETA: 7:08 - loss: 1.9347 - regression_loss: 1.4513 - classification_loss: 0.4834\n",
      " 77/500 [===>..........................] - ETA: 7:07 - loss: 1.9347 - regression_loss: 1.4519 - classification_loss: 0.4827\n",
      " 78/500 [===>..........................] - ETA: 7:06 - loss: 1.9390 - regression_loss: 1.4568 - classification_loss: 0.4822\n",
      " 79/500 [===>..........................] - ETA: 7:05 - loss: 1.9307 - regression_loss: 1.4512 - classification_loss: 0.4795\n",
      " 80/500 [===>..........................] - ETA: 7:04 - loss: 1.9215 - regression_loss: 1.4439 - classification_loss: 0.4776\n",
      " 81/500 [===>..........................] - ETA: 7:03 - loss: 1.9233 - regression_loss: 1.4464 - classification_loss: 0.4769\n",
      " 82/500 [===>..........................] - ETA: 7:02 - loss: 1.9366 - regression_loss: 1.4552 - classification_loss: 0.4814\n",
      " 83/500 [===>..........................] - ETA: 7:01 - loss: 1.9346 - regression_loss: 1.4528 - classification_loss: 0.4818\n",
      " 84/500 [====>.........................] - ETA: 7:00 - loss: 1.9417 - regression_loss: 1.4559 - classification_loss: 0.4858\n",
      " 85/500 [====>.........................] - ETA: 6:59 - loss: 1.9422 - regression_loss: 1.4573 - classification_loss: 0.4848\n",
      " 86/500 [====>.........................] - ETA: 6:58 - loss: 1.9552 - regression_loss: 1.4654 - classification_loss: 0.4898\n",
      " 87/500 [====>.........................] - ETA: 6:57 - loss: 1.9531 - regression_loss: 1.4641 - classification_loss: 0.4890\n",
      " 88/500 [====>.........................] - ETA: 6:56 - loss: 1.9521 - regression_loss: 1.4640 - classification_loss: 0.4880\n",
      " 89/500 [====>.........................] - ETA: 6:54 - loss: 1.9555 - regression_loss: 1.4644 - classification_loss: 0.4911\n",
      " 90/500 [====>.........................] - ETA: 6:53 - loss: 1.9579 - regression_loss: 1.4673 - classification_loss: 0.4906\n",
      " 91/500 [====>.........................] - ETA: 6:52 - loss: 1.9527 - regression_loss: 1.4633 - classification_loss: 0.4894\n",
      " 92/500 [====>.........................] - ETA: 6:52 - loss: 1.9471 - regression_loss: 1.4599 - classification_loss: 0.4872\n",
      " 93/500 [====>.........................] - ETA: 6:50 - loss: 1.9598 - regression_loss: 1.4680 - classification_loss: 0.4919\n",
      " 94/500 [====>.........................] - ETA: 6:50 - loss: 1.9565 - regression_loss: 1.4656 - classification_loss: 0.4909\n",
      " 95/500 [====>.........................] - ETA: 6:49 - loss: 1.9669 - regression_loss: 1.4728 - classification_loss: 0.4942\n",
      " 96/500 [====>.........................] - ETA: 6:48 - loss: 1.9662 - regression_loss: 1.4721 - classification_loss: 0.4941\n",
      " 97/500 [====>.........................] - ETA: 6:48 - loss: 1.9647 - regression_loss: 1.4704 - classification_loss: 0.4943\n",
      " 98/500 [====>.........................] - ETA: 6:47 - loss: 1.9550 - regression_loss: 1.4634 - classification_loss: 0.4916\n",
      " 99/500 [====>.........................] - ETA: 6:46 - loss: 1.9508 - regression_loss: 1.4606 - classification_loss: 0.4902\n",
      "100/500 [=====>........................] - ETA: 6:45 - loss: 1.9640 - regression_loss: 1.4696 - classification_loss: 0.4945\n",
      "101/500 [=====>........................] - ETA: 6:43 - loss: 1.9630 - regression_loss: 1.4676 - classification_loss: 0.4954\n",
      "102/500 [=====>........................] - ETA: 6:42 - loss: 1.9539 - regression_loss: 1.4609 - classification_loss: 0.4930\n",
      "103/500 [=====>........................] - ETA: 6:41 - loss: 1.9483 - regression_loss: 1.4572 - classification_loss: 0.4911\n",
      "104/500 [=====>........................] - ETA: 6:40 - loss: 1.9416 - regression_loss: 1.4534 - classification_loss: 0.4882\n",
      "105/500 [=====>........................] - ETA: 6:38 - loss: 1.9399 - regression_loss: 1.4511 - classification_loss: 0.4888\n",
      "106/500 [=====>........................] - ETA: 6:38 - loss: 1.9416 - regression_loss: 1.4502 - classification_loss: 0.4914\n",
      "107/500 [=====>........................] - ETA: 6:37 - loss: 1.9383 - regression_loss: 1.4478 - classification_loss: 0.4906\n",
      "108/500 [=====>........................] - ETA: 6:35 - loss: 1.9374 - regression_loss: 1.4468 - classification_loss: 0.4906\n",
      "109/500 [=====>........................] - ETA: 6:35 - loss: 1.9407 - regression_loss: 1.4499 - classification_loss: 0.4908\n",
      "110/500 [=====>........................] - ETA: 6:34 - loss: 1.9402 - regression_loss: 1.4495 - classification_loss: 0.4907\n",
      "111/500 [=====>........................] - ETA: 6:34 - loss: 1.9395 - regression_loss: 1.4485 - classification_loss: 0.4910\n",
      "112/500 [=====>........................] - ETA: 6:33 - loss: 1.9381 - regression_loss: 1.4453 - classification_loss: 0.4928\n",
      "113/500 [=====>........................] - ETA: 6:32 - loss: 1.9346 - regression_loss: 1.4430 - classification_loss: 0.4916\n",
      "114/500 [=====>........................] - ETA: 6:31 - loss: 1.9300 - regression_loss: 1.4402 - classification_loss: 0.4898\n",
      "115/500 [=====>........................] - ETA: 6:30 - loss: 1.9320 - regression_loss: 1.4396 - classification_loss: 0.4923\n",
      "116/500 [=====>........................] - ETA: 6:29 - loss: 1.9276 - regression_loss: 1.4366 - classification_loss: 0.4910\n",
      "117/500 [======>.......................] - ETA: 6:28 - loss: 1.9272 - regression_loss: 1.4360 - classification_loss: 0.4911\n",
      "118/500 [======>.......................] - ETA: 6:28 - loss: 1.9313 - regression_loss: 1.4390 - classification_loss: 0.4923\n",
      "119/500 [======>.......................] - ETA: 6:27 - loss: 1.9279 - regression_loss: 1.4356 - classification_loss: 0.4923\n",
      "120/500 [======>.......................] - ETA: 6:25 - loss: 1.9244 - regression_loss: 1.4335 - classification_loss: 0.4908\n",
      "121/500 [======>.......................] - ETA: 6:24 - loss: 1.9214 - regression_loss: 1.4316 - classification_loss: 0.4898\n",
      "122/500 [======>.......................] - ETA: 6:23 - loss: 1.9228 - regression_loss: 1.4330 - classification_loss: 0.4898\n",
      "123/500 [======>.......................] - ETA: 6:22 - loss: 1.9208 - regression_loss: 1.4316 - classification_loss: 0.4892\n",
      "124/500 [======>.......................] - ETA: 6:21 - loss: 1.9155 - regression_loss: 1.4279 - classification_loss: 0.4876\n",
      "125/500 [======>.......................] - ETA: 6:20 - loss: 1.9206 - regression_loss: 1.4321 - classification_loss: 0.4885\n",
      "126/500 [======>.......................] - ETA: 6:19 - loss: 1.9243 - regression_loss: 1.4341 - classification_loss: 0.4902\n",
      "127/500 [======>.......................] - ETA: 6:18 - loss: 1.9292 - regression_loss: 1.4383 - classification_loss: 0.4909\n",
      "128/500 [======>.......................] - ETA: 6:17 - loss: 1.9256 - regression_loss: 1.4361 - classification_loss: 0.4895\n",
      "129/500 [======>.......................] - ETA: 6:16 - loss: 1.9185 - regression_loss: 1.4313 - classification_loss: 0.4873\n",
      "130/500 [======>.......................] - ETA: 6:15 - loss: 1.9152 - regression_loss: 1.4296 - classification_loss: 0.4857\n",
      "131/500 [======>.......................] - ETA: 6:14 - loss: 1.9239 - regression_loss: 1.4343 - classification_loss: 0.4896\n",
      "132/500 [======>.......................] - ETA: 6:13 - loss: 1.9236 - regression_loss: 1.4328 - classification_loss: 0.4909\n",
      "133/500 [======>.......................] - ETA: 6:12 - loss: 1.9256 - regression_loss: 1.4337 - classification_loss: 0.4918\n",
      "134/500 [=======>......................] - ETA: 6:11 - loss: 1.9223 - regression_loss: 1.4309 - classification_loss: 0.4915\n",
      "135/500 [=======>......................] - ETA: 6:10 - loss: 1.9231 - regression_loss: 1.4298 - classification_loss: 0.4933\n",
      "136/500 [=======>......................] - ETA: 6:09 - loss: 1.9234 - regression_loss: 1.4308 - classification_loss: 0.4926\n",
      "137/500 [=======>......................] - ETA: 6:08 - loss: 1.9238 - regression_loss: 1.4312 - classification_loss: 0.4926\n",
      "138/500 [=======>......................] - ETA: 6:07 - loss: 1.9237 - regression_loss: 1.4319 - classification_loss: 0.4918\n",
      "139/500 [=======>......................] - ETA: 6:06 - loss: 1.9291 - regression_loss: 1.4357 - classification_loss: 0.4934\n",
      "140/500 [=======>......................] - ETA: 6:05 - loss: 1.9339 - regression_loss: 1.4396 - classification_loss: 0.4944\n",
      "141/500 [=======>......................] - ETA: 6:04 - loss: 1.9347 - regression_loss: 1.4401 - classification_loss: 0.4946\n",
      "142/500 [=======>......................] - ETA: 6:03 - loss: 1.9345 - regression_loss: 1.4402 - classification_loss: 0.4944\n",
      "143/500 [=======>......................] - ETA: 6:03 - loss: 1.9341 - regression_loss: 1.4396 - classification_loss: 0.4945\n",
      "144/500 [=======>......................] - ETA: 6:02 - loss: 1.9308 - regression_loss: 1.4371 - classification_loss: 0.4937\n",
      "145/500 [=======>......................] - ETA: 6:00 - loss: 1.9259 - regression_loss: 1.4338 - classification_loss: 0.4921\n",
      "146/500 [=======>......................] - ETA: 6:00 - loss: 1.9235 - regression_loss: 1.4313 - classification_loss: 0.4922\n",
      "147/500 [=======>......................] - ETA: 5:58 - loss: 1.9261 - regression_loss: 1.4337 - classification_loss: 0.4925\n",
      "148/500 [=======>......................] - ETA: 5:57 - loss: 1.9212 - regression_loss: 1.4307 - classification_loss: 0.4905\n",
      "149/500 [=======>......................] - ETA: 5:56 - loss: 1.9240 - regression_loss: 1.4333 - classification_loss: 0.4907\n",
      "150/500 [========>.....................] - ETA: 5:55 - loss: 1.9273 - regression_loss: 1.4352 - classification_loss: 0.4920\n",
      "151/500 [========>.....................] - ETA: 5:55 - loss: 1.9293 - regression_loss: 1.4378 - classification_loss: 0.4915\n",
      "152/500 [========>.....................] - ETA: 5:53 - loss: 1.9246 - regression_loss: 1.4346 - classification_loss: 0.4900\n",
      "153/500 [========>.....................] - ETA: 5:52 - loss: 1.9258 - regression_loss: 1.4369 - classification_loss: 0.4889\n",
      "154/500 [========>.....................] - ETA: 5:52 - loss: 1.9309 - regression_loss: 1.4407 - classification_loss: 0.4902\n",
      "155/500 [========>.....................] - ETA: 5:50 - loss: 1.9310 - regression_loss: 1.4411 - classification_loss: 0.4899\n",
      "156/500 [========>.....................] - ETA: 5:49 - loss: 1.9268 - regression_loss: 1.4380 - classification_loss: 0.4888\n",
      "157/500 [========>.....................] - ETA: 5:48 - loss: 1.9286 - regression_loss: 1.4390 - classification_loss: 0.4896\n",
      "158/500 [========>.....................] - ETA: 5:48 - loss: 1.9274 - regression_loss: 1.4388 - classification_loss: 0.4887\n",
      "159/500 [========>.....................] - ETA: 5:47 - loss: 1.9293 - regression_loss: 1.4387 - classification_loss: 0.4906\n",
      "160/500 [========>.....................] - ETA: 5:46 - loss: 1.9258 - regression_loss: 1.4354 - classification_loss: 0.4904\n",
      "161/500 [========>.....................] - ETA: 5:45 - loss: 1.9220 - regression_loss: 1.4327 - classification_loss: 0.4893\n",
      "162/500 [========>.....................] - ETA: 5:44 - loss: 1.9251 - regression_loss: 1.4353 - classification_loss: 0.4898\n",
      "163/500 [========>.....................] - ETA: 5:43 - loss: 1.9193 - regression_loss: 1.4311 - classification_loss: 0.4882\n",
      "164/500 [========>.....................] - ETA: 5:42 - loss: 1.9182 - regression_loss: 1.4300 - classification_loss: 0.4883\n",
      "165/500 [========>.....................] - ETA: 5:40 - loss: 1.9208 - regression_loss: 1.4315 - classification_loss: 0.4893\n",
      "166/500 [========>.....................] - ETA: 5:40 - loss: 1.9216 - regression_loss: 1.4327 - classification_loss: 0.4888\n",
      "167/500 [=========>....................] - ETA: 5:38 - loss: 1.9204 - regression_loss: 1.4328 - classification_loss: 0.4876\n",
      "168/500 [=========>....................] - ETA: 5:37 - loss: 1.9272 - regression_loss: 1.4382 - classification_loss: 0.4890\n",
      "169/500 [=========>....................] - ETA: 5:36 - loss: 1.9285 - regression_loss: 1.4392 - classification_loss: 0.4893\n",
      "170/500 [=========>....................] - ETA: 5:35 - loss: 1.9306 - regression_loss: 1.4410 - classification_loss: 0.4896\n",
      "171/500 [=========>....................] - ETA: 5:34 - loss: 1.9308 - regression_loss: 1.4414 - classification_loss: 0.4894\n",
      "172/500 [=========>....................] - ETA: 5:33 - loss: 1.9260 - regression_loss: 1.4382 - classification_loss: 0.4878\n",
      "173/500 [=========>....................] - ETA: 5:32 - loss: 1.9255 - regression_loss: 1.4380 - classification_loss: 0.4875\n",
      "174/500 [=========>....................] - ETA: 5:31 - loss: 1.9258 - regression_loss: 1.4392 - classification_loss: 0.4866\n",
      "175/500 [=========>....................] - ETA: 5:30 - loss: 1.9235 - regression_loss: 1.4382 - classification_loss: 0.4853\n",
      "176/500 [=========>....................] - ETA: 5:29 - loss: 1.9184 - regression_loss: 1.4345 - classification_loss: 0.4839\n",
      "177/500 [=========>....................] - ETA: 5:28 - loss: 1.9127 - regression_loss: 1.4303 - classification_loss: 0.4824\n",
      "178/500 [=========>....................] - ETA: 5:27 - loss: 1.9103 - regression_loss: 1.4288 - classification_loss: 0.4815\n",
      "179/500 [=========>....................] - ETA: 5:26 - loss: 1.9101 - regression_loss: 1.4276 - classification_loss: 0.4825\n",
      "180/500 [=========>....................] - ETA: 5:25 - loss: 1.9097 - regression_loss: 1.4273 - classification_loss: 0.4825\n",
      "181/500 [=========>....................] - ETA: 5:23 - loss: 1.9098 - regression_loss: 1.4271 - classification_loss: 0.4827\n",
      "182/500 [=========>....................] - ETA: 5:23 - loss: 1.9075 - regression_loss: 1.4257 - classification_loss: 0.4818\n",
      "183/500 [=========>....................] - ETA: 5:22 - loss: 1.9061 - regression_loss: 1.4251 - classification_loss: 0.4810\n",
      "184/500 [==========>...................] - ETA: 5:21 - loss: 1.9067 - regression_loss: 1.4257 - classification_loss: 0.4811\n",
      "185/500 [==========>...................] - ETA: 5:20 - loss: 1.9058 - regression_loss: 1.4252 - classification_loss: 0.4806\n",
      "186/500 [==========>...................] - ETA: 5:19 - loss: 1.9021 - regression_loss: 1.4223 - classification_loss: 0.4798\n",
      "187/500 [==========>...................] - ETA: 5:18 - loss: 1.9082 - regression_loss: 1.4262 - classification_loss: 0.4820\n",
      "188/500 [==========>...................] - ETA: 5:17 - loss: 1.9092 - regression_loss: 1.4267 - classification_loss: 0.4826\n",
      "189/500 [==========>...................] - ETA: 5:16 - loss: 1.9144 - regression_loss: 1.4304 - classification_loss: 0.4839\n",
      "190/500 [==========>...................] - ETA: 5:15 - loss: 1.9129 - regression_loss: 1.4295 - classification_loss: 0.4834\n",
      "191/500 [==========>...................] - ETA: 5:14 - loss: 1.9131 - regression_loss: 1.4298 - classification_loss: 0.4833\n",
      "192/500 [==========>...................] - ETA: 5:13 - loss: 1.9151 - regression_loss: 1.4309 - classification_loss: 0.4842\n",
      "193/500 [==========>...................] - ETA: 5:12 - loss: 1.9129 - regression_loss: 1.4291 - classification_loss: 0.4838\n",
      "194/500 [==========>...................] - ETA: 5:11 - loss: 1.9134 - regression_loss: 1.4300 - classification_loss: 0.4834\n",
      "195/500 [==========>...................] - ETA: 5:10 - loss: 1.9114 - regression_loss: 1.4284 - classification_loss: 0.4830\n",
      "196/500 [==========>...................] - ETA: 5:09 - loss: 1.9085 - regression_loss: 1.4266 - classification_loss: 0.4819\n",
      "197/500 [==========>...................] - ETA: 5:08 - loss: 1.9063 - regression_loss: 1.4252 - classification_loss: 0.4811\n",
      "198/500 [==========>...................] - ETA: 5:07 - loss: 1.9048 - regression_loss: 1.4241 - classification_loss: 0.4807\n",
      "199/500 [==========>...................] - ETA: 5:06 - loss: 1.9026 - regression_loss: 1.4228 - classification_loss: 0.4798\n",
      "200/500 [===========>..................] - ETA: 5:05 - loss: 1.9029 - regression_loss: 1.4237 - classification_loss: 0.4792\n",
      "201/500 [===========>..................] - ETA: 5:04 - loss: 1.9041 - regression_loss: 1.4252 - classification_loss: 0.4789\n",
      "202/500 [===========>..................] - ETA: 5:03 - loss: 1.9071 - regression_loss: 1.4272 - classification_loss: 0.4799\n",
      "203/500 [===========>..................] - ETA: 5:02 - loss: 1.9096 - regression_loss: 1.4286 - classification_loss: 0.4810\n",
      "204/500 [===========>..................] - ETA: 5:01 - loss: 1.9114 - regression_loss: 1.4296 - classification_loss: 0.4818\n",
      "205/500 [===========>..................] - ETA: 5:00 - loss: 1.9099 - regression_loss: 1.4289 - classification_loss: 0.4810\n",
      "206/500 [===========>..................] - ETA: 4:59 - loss: 1.9145 - regression_loss: 1.4331 - classification_loss: 0.4815\n",
      "207/500 [===========>..................] - ETA: 4:58 - loss: 1.9146 - regression_loss: 1.4336 - classification_loss: 0.4810\n",
      "208/500 [===========>..................] - ETA: 4:57 - loss: 1.9183 - regression_loss: 1.4362 - classification_loss: 0.4821\n",
      "209/500 [===========>..................] - ETA: 4:56 - loss: 1.9220 - regression_loss: 1.4383 - classification_loss: 0.4837\n",
      "210/500 [===========>..................] - ETA: 4:55 - loss: 1.9269 - regression_loss: 1.4425 - classification_loss: 0.4844\n",
      "211/500 [===========>..................] - ETA: 4:54 - loss: 1.9282 - regression_loss: 1.4439 - classification_loss: 0.4844\n",
      "212/500 [===========>..................] - ETA: 4:53 - loss: 1.9276 - regression_loss: 1.4426 - classification_loss: 0.4849\n",
      "213/500 [===========>..................] - ETA: 4:52 - loss: 1.9292 - regression_loss: 1.4445 - classification_loss: 0.4847\n",
      "214/500 [===========>..................] - ETA: 4:51 - loss: 1.9305 - regression_loss: 1.4446 - classification_loss: 0.4858\n",
      "215/500 [===========>..................] - ETA: 4:50 - loss: 1.9312 - regression_loss: 1.4454 - classification_loss: 0.4858\n",
      "216/500 [===========>..................] - ETA: 4:49 - loss: 1.9321 - regression_loss: 1.4461 - classification_loss: 0.4860\n",
      "217/500 [============>.................] - ETA: 4:48 - loss: 1.9318 - regression_loss: 1.4459 - classification_loss: 0.4859\n",
      "218/500 [============>.................] - ETA: 4:47 - loss: 1.9330 - regression_loss: 1.4476 - classification_loss: 0.4854\n",
      "219/500 [============>.................] - ETA: 4:46 - loss: 1.9321 - regression_loss: 1.4473 - classification_loss: 0.4848\n",
      "220/500 [============>.................] - ETA: 4:45 - loss: 1.9353 - regression_loss: 1.4492 - classification_loss: 0.4861\n",
      "221/500 [============>.................] - ETA: 4:44 - loss: 1.9355 - regression_loss: 1.4494 - classification_loss: 0.4861\n",
      "222/500 [============>.................] - ETA: 4:43 - loss: 1.9371 - regression_loss: 1.4490 - classification_loss: 0.4881\n",
      "223/500 [============>.................] - ETA: 4:42 - loss: 1.9351 - regression_loss: 1.4474 - classification_loss: 0.4876\n",
      "224/500 [============>.................] - ETA: 4:41 - loss: 1.9346 - regression_loss: 1.4477 - classification_loss: 0.4869\n",
      "225/500 [============>.................] - ETA: 4:40 - loss: 1.9336 - regression_loss: 1.4470 - classification_loss: 0.4866\n",
      "226/500 [============>.................] - ETA: 4:39 - loss: 1.9338 - regression_loss: 1.4477 - classification_loss: 0.4860\n",
      "227/500 [============>.................] - ETA: 4:38 - loss: 1.9337 - regression_loss: 1.4474 - classification_loss: 0.4863\n",
      "228/500 [============>.................] - ETA: 4:37 - loss: 1.9326 - regression_loss: 1.4469 - classification_loss: 0.4856\n",
      "229/500 [============>.................] - ETA: 4:36 - loss: 1.9369 - regression_loss: 1.4493 - classification_loss: 0.4876\n",
      "230/500 [============>.................] - ETA: 4:35 - loss: 1.9346 - regression_loss: 1.4480 - classification_loss: 0.4866\n",
      "231/500 [============>.................] - ETA: 4:34 - loss: 1.9317 - regression_loss: 1.4459 - classification_loss: 0.4858\n",
      "232/500 [============>.................] - ETA: 4:33 - loss: 1.9300 - regression_loss: 1.4445 - classification_loss: 0.4855\n",
      "233/500 [============>.................] - ETA: 4:32 - loss: 1.9322 - regression_loss: 1.4465 - classification_loss: 0.4857\n",
      "234/500 [=============>................] - ETA: 4:31 - loss: 1.9323 - regression_loss: 1.4471 - classification_loss: 0.4852\n",
      "235/500 [=============>................] - ETA: 4:30 - loss: 1.9301 - regression_loss: 1.4456 - classification_loss: 0.4845\n",
      "236/500 [=============>................] - ETA: 4:29 - loss: 1.9278 - regression_loss: 1.4440 - classification_loss: 0.4838\n",
      "237/500 [=============>................] - ETA: 4:28 - loss: 1.9260 - regression_loss: 1.4428 - classification_loss: 0.4832\n",
      "238/500 [=============>................] - ETA: 4:27 - loss: 1.9227 - regression_loss: 1.4405 - classification_loss: 0.4822\n",
      "239/500 [=============>................] - ETA: 4:26 - loss: 1.9217 - regression_loss: 1.4401 - classification_loss: 0.4816\n",
      "240/500 [=============>................] - ETA: 4:25 - loss: 1.9227 - regression_loss: 1.4410 - classification_loss: 0.4816\n",
      "241/500 [=============>................] - ETA: 4:24 - loss: 1.9231 - regression_loss: 1.4408 - classification_loss: 0.4824\n",
      "242/500 [=============>................] - ETA: 4:23 - loss: 1.9219 - regression_loss: 1.4402 - classification_loss: 0.4817\n",
      "243/500 [=============>................] - ETA: 4:22 - loss: 1.9197 - regression_loss: 1.4387 - classification_loss: 0.4810\n",
      "244/500 [=============>................] - ETA: 4:21 - loss: 1.9193 - regression_loss: 1.4387 - classification_loss: 0.4806\n",
      "245/500 [=============>................] - ETA: 4:20 - loss: 1.9227 - regression_loss: 1.4411 - classification_loss: 0.4816\n",
      "246/500 [=============>................] - ETA: 4:19 - loss: 1.9244 - regression_loss: 1.4425 - classification_loss: 0.4819\n",
      "247/500 [=============>................] - ETA: 4:18 - loss: 1.9270 - regression_loss: 1.4447 - classification_loss: 0.4823\n",
      "248/500 [=============>................] - ETA: 4:17 - loss: 1.9254 - regression_loss: 1.4432 - classification_loss: 0.4822\n",
      "249/500 [=============>................] - ETA: 4:16 - loss: 1.9220 - regression_loss: 1.4407 - classification_loss: 0.4812\n",
      "250/500 [==============>...............] - ETA: 4:15 - loss: 1.9234 - regression_loss: 1.4410 - classification_loss: 0.4824\n",
      "251/500 [==============>...............] - ETA: 4:14 - loss: 1.9214 - regression_loss: 1.4395 - classification_loss: 0.4819\n",
      "252/500 [==============>...............] - ETA: 4:13 - loss: 1.9208 - regression_loss: 1.4393 - classification_loss: 0.4815\n",
      "253/500 [==============>...............] - ETA: 4:12 - loss: 1.9184 - regression_loss: 1.4379 - classification_loss: 0.4806\n",
      "254/500 [==============>...............] - ETA: 4:11 - loss: 1.9184 - regression_loss: 1.4383 - classification_loss: 0.4801\n",
      "255/500 [==============>...............] - ETA: 4:10 - loss: 1.9190 - regression_loss: 1.4384 - classification_loss: 0.4806\n",
      "256/500 [==============>...............] - ETA: 4:09 - loss: 1.9172 - regression_loss: 1.4372 - classification_loss: 0.4800\n",
      "257/500 [==============>...............] - ETA: 4:08 - loss: 1.9155 - regression_loss: 1.4354 - classification_loss: 0.4800\n",
      "258/500 [==============>...............] - ETA: 4:07 - loss: 1.9191 - regression_loss: 1.4378 - classification_loss: 0.4813\n",
      "259/500 [==============>...............] - ETA: 4:06 - loss: 1.9206 - regression_loss: 1.4389 - classification_loss: 0.4818\n",
      "260/500 [==============>...............] - ETA: 4:05 - loss: 1.9179 - regression_loss: 1.4367 - classification_loss: 0.4812\n",
      "261/500 [==============>...............] - ETA: 4:04 - loss: 1.9206 - regression_loss: 1.4394 - classification_loss: 0.4812\n",
      "262/500 [==============>...............] - ETA: 4:03 - loss: 1.9219 - regression_loss: 1.4389 - classification_loss: 0.4830\n",
      "263/500 [==============>...............] - ETA: 4:02 - loss: 1.9262 - regression_loss: 1.4413 - classification_loss: 0.4849\n",
      "264/500 [==============>...............] - ETA: 4:01 - loss: 1.9283 - regression_loss: 1.4426 - classification_loss: 0.4856\n",
      "265/500 [==============>...............] - ETA: 4:00 - loss: 1.9291 - regression_loss: 1.4435 - classification_loss: 0.4856\n",
      "266/500 [==============>...............] - ETA: 3:58 - loss: 1.9292 - regression_loss: 1.4436 - classification_loss: 0.4855\n",
      "267/500 [===============>..............] - ETA: 3:57 - loss: 1.9314 - regression_loss: 1.4455 - classification_loss: 0.4859\n",
      "268/500 [===============>..............] - ETA: 3:56 - loss: 1.9352 - regression_loss: 1.4487 - classification_loss: 0.4865\n",
      "269/500 [===============>..............] - ETA: 3:55 - loss: 1.9347 - regression_loss: 1.4485 - classification_loss: 0.4861\n",
      "270/500 [===============>..............] - ETA: 3:54 - loss: 1.9352 - regression_loss: 1.4491 - classification_loss: 0.4860\n",
      "271/500 [===============>..............] - ETA: 3:53 - loss: 1.9324 - regression_loss: 1.4471 - classification_loss: 0.4853\n",
      "272/500 [===============>..............] - ETA: 3:52 - loss: 1.9327 - regression_loss: 1.4472 - classification_loss: 0.4855\n",
      "273/500 [===============>..............] - ETA: 3:51 - loss: 1.9336 - regression_loss: 1.4474 - classification_loss: 0.4862\n",
      "274/500 [===============>..............] - ETA: 3:50 - loss: 1.9323 - regression_loss: 1.4467 - classification_loss: 0.4856\n",
      "275/500 [===============>..............] - ETA: 3:49 - loss: 1.9299 - regression_loss: 1.4453 - classification_loss: 0.4847\n",
      "276/500 [===============>..............] - ETA: 3:48 - loss: 1.9299 - regression_loss: 1.4456 - classification_loss: 0.4843\n",
      "277/500 [===============>..............] - ETA: 3:47 - loss: 1.9287 - regression_loss: 1.4450 - classification_loss: 0.4838\n",
      "278/500 [===============>..............] - ETA: 3:46 - loss: 1.9260 - regression_loss: 1.4429 - classification_loss: 0.4831\n",
      "279/500 [===============>..............] - ETA: 3:45 - loss: 1.9268 - regression_loss: 1.4437 - classification_loss: 0.4831\n",
      "280/500 [===============>..............] - ETA: 3:44 - loss: 1.9276 - regression_loss: 1.4435 - classification_loss: 0.4841\n",
      "281/500 [===============>..............] - ETA: 3:43 - loss: 1.9262 - regression_loss: 1.4429 - classification_loss: 0.4833\n",
      "282/500 [===============>..............] - ETA: 3:42 - loss: 1.9295 - regression_loss: 1.4442 - classification_loss: 0.4852\n",
      "283/500 [===============>..............] - ETA: 3:41 - loss: 1.9294 - regression_loss: 1.4441 - classification_loss: 0.4853\n",
      "284/500 [================>.............] - ETA: 3:40 - loss: 1.9264 - regression_loss: 1.4418 - classification_loss: 0.4845\n",
      "285/500 [================>.............] - ETA: 3:39 - loss: 1.9258 - regression_loss: 1.4416 - classification_loss: 0.4842\n",
      "286/500 [================>.............] - ETA: 3:38 - loss: 1.9236 - regression_loss: 1.4400 - classification_loss: 0.4835\n",
      "287/500 [================>.............] - ETA: 3:37 - loss: 1.9217 - regression_loss: 1.4389 - classification_loss: 0.4828\n",
      "288/500 [================>.............] - ETA: 3:36 - loss: 1.9226 - regression_loss: 1.4402 - classification_loss: 0.4825\n",
      "289/500 [================>.............] - ETA: 3:35 - loss: 1.9214 - regression_loss: 1.4391 - classification_loss: 0.4823\n",
      "290/500 [================>.............] - ETA: 3:34 - loss: 1.9200 - regression_loss: 1.4383 - classification_loss: 0.4817\n",
      "291/500 [================>.............] - ETA: 3:33 - loss: 1.9182 - regression_loss: 1.4372 - classification_loss: 0.4810\n",
      "292/500 [================>.............] - ETA: 3:32 - loss: 1.9170 - regression_loss: 1.4367 - classification_loss: 0.4803\n",
      "293/500 [================>.............] - ETA: 3:31 - loss: 1.9169 - regression_loss: 1.4366 - classification_loss: 0.4803\n",
      "294/500 [================>.............] - ETA: 3:30 - loss: 1.9173 - regression_loss: 1.4363 - classification_loss: 0.4810\n",
      "295/500 [================>.............] - ETA: 3:29 - loss: 1.9182 - regression_loss: 1.4375 - classification_loss: 0.4808\n",
      "296/500 [================>.............] - ETA: 3:28 - loss: 1.9190 - regression_loss: 1.4381 - classification_loss: 0.4809\n",
      "297/500 [================>.............] - ETA: 3:27 - loss: 1.9159 - regression_loss: 1.4356 - classification_loss: 0.4803\n",
      "298/500 [================>.............] - ETA: 3:26 - loss: 1.9181 - regression_loss: 1.4377 - classification_loss: 0.4804\n",
      "299/500 [================>.............] - ETA: 3:25 - loss: 1.9162 - regression_loss: 1.4366 - classification_loss: 0.4796\n",
      "300/500 [=================>............] - ETA: 3:24 - loss: 1.9172 - regression_loss: 1.4373 - classification_loss: 0.4799\n",
      "301/500 [=================>............] - ETA: 3:23 - loss: 1.9202 - regression_loss: 1.4395 - classification_loss: 0.4807\n",
      "302/500 [=================>............] - ETA: 3:22 - loss: 1.9179 - regression_loss: 1.4382 - classification_loss: 0.4797\n",
      "303/500 [=================>............] - ETA: 3:21 - loss: 1.9197 - regression_loss: 1.4398 - classification_loss: 0.4800\n",
      "304/500 [=================>............] - ETA: 3:20 - loss: 1.9192 - regression_loss: 1.4395 - classification_loss: 0.4796\n",
      "305/500 [=================>............] - ETA: 3:19 - loss: 1.9183 - regression_loss: 1.4386 - classification_loss: 0.4797\n",
      "306/500 [=================>............] - ETA: 3:18 - loss: 1.9162 - regression_loss: 1.4373 - classification_loss: 0.4789\n",
      "307/500 [=================>............] - ETA: 3:17 - loss: 1.9168 - regression_loss: 1.4380 - classification_loss: 0.4788\n",
      "308/500 [=================>............] - ETA: 3:16 - loss: 1.9212 - regression_loss: 1.4410 - classification_loss: 0.4801\n",
      "309/500 [=================>............] - ETA: 3:15 - loss: 1.9240 - regression_loss: 1.4433 - classification_loss: 0.4807\n",
      "310/500 [=================>............] - ETA: 3:14 - loss: 1.9222 - regression_loss: 1.4422 - classification_loss: 0.4800\n",
      "311/500 [=================>............] - ETA: 3:13 - loss: 1.9225 - regression_loss: 1.4427 - classification_loss: 0.4798\n",
      "312/500 [=================>............] - ETA: 3:12 - loss: 1.9221 - regression_loss: 1.4425 - classification_loss: 0.4796\n",
      "313/500 [=================>............] - ETA: 3:11 - loss: 1.9236 - regression_loss: 1.4437 - classification_loss: 0.4799\n",
      "314/500 [=================>............] - ETA: 3:10 - loss: 1.9226 - regression_loss: 1.4426 - classification_loss: 0.4800\n",
      "315/500 [=================>............] - ETA: 3:09 - loss: 1.9237 - regression_loss: 1.4437 - classification_loss: 0.4800\n",
      "316/500 [=================>............] - ETA: 3:08 - loss: 1.9252 - regression_loss: 1.4450 - classification_loss: 0.4803\n",
      "317/500 [==================>...........] - ETA: 3:07 - loss: 1.9281 - regression_loss: 1.4472 - classification_loss: 0.4809\n",
      "318/500 [==================>...........] - ETA: 3:06 - loss: 1.9280 - regression_loss: 1.4476 - classification_loss: 0.4804\n",
      "319/500 [==================>...........] - ETA: 3:05 - loss: 1.9270 - regression_loss: 1.4469 - classification_loss: 0.4801\n",
      "320/500 [==================>...........] - ETA: 3:04 - loss: 1.9258 - regression_loss: 1.4463 - classification_loss: 0.4795\n",
      "321/500 [==================>...........] - ETA: 3:02 - loss: 1.9236 - regression_loss: 1.4445 - classification_loss: 0.4791\n",
      "322/500 [==================>...........] - ETA: 3:01 - loss: 1.9233 - regression_loss: 1.4442 - classification_loss: 0.4791\n",
      "323/500 [==================>...........] - ETA: 3:00 - loss: 1.9226 - regression_loss: 1.4437 - classification_loss: 0.4789\n",
      "324/500 [==================>...........] - ETA: 2:59 - loss: 1.9217 - regression_loss: 1.4434 - classification_loss: 0.4783\n",
      "325/500 [==================>...........] - ETA: 2:58 - loss: 1.9206 - regression_loss: 1.4425 - classification_loss: 0.4780\n",
      "326/500 [==================>...........] - ETA: 2:57 - loss: 1.9220 - regression_loss: 1.4437 - classification_loss: 0.4783\n",
      "327/500 [==================>...........] - ETA: 2:56 - loss: 1.9216 - regression_loss: 1.4437 - classification_loss: 0.4779\n",
      "328/500 [==================>...........] - ETA: 2:55 - loss: 1.9207 - regression_loss: 1.4431 - classification_loss: 0.4776\n",
      "329/500 [==================>...........] - ETA: 2:54 - loss: 1.9200 - regression_loss: 1.4428 - classification_loss: 0.4772\n",
      "330/500 [==================>...........] - ETA: 2:53 - loss: 1.9213 - regression_loss: 1.4440 - classification_loss: 0.4773\n",
      "331/500 [==================>...........] - ETA: 2:52 - loss: 1.9245 - regression_loss: 1.4457 - classification_loss: 0.4788\n",
      "332/500 [==================>...........] - ETA: 2:51 - loss: 1.9246 - regression_loss: 1.4459 - classification_loss: 0.4787\n",
      "333/500 [==================>...........] - ETA: 2:50 - loss: 1.9251 - regression_loss: 1.4460 - classification_loss: 0.4790\n",
      "334/500 [===================>..........] - ETA: 2:49 - loss: 1.9280 - regression_loss: 1.4483 - classification_loss: 0.4797\n",
      "335/500 [===================>..........] - ETA: 2:48 - loss: 1.9292 - regression_loss: 1.4491 - classification_loss: 0.4800\n",
      "336/500 [===================>..........] - ETA: 2:48 - loss: 1.9293 - regression_loss: 1.4496 - classification_loss: 0.4797\n",
      "337/500 [===================>..........] - ETA: 2:47 - loss: 1.9318 - regression_loss: 1.4514 - classification_loss: 0.4804\n",
      "338/500 [===================>..........] - ETA: 2:46 - loss: 1.9313 - regression_loss: 1.4511 - classification_loss: 0.4803\n",
      "339/500 [===================>..........] - ETA: 2:45 - loss: 1.9309 - regression_loss: 1.4510 - classification_loss: 0.4799\n",
      "340/500 [===================>..........] - ETA: 2:44 - loss: 1.9317 - regression_loss: 1.4520 - classification_loss: 0.4796\n",
      "341/500 [===================>..........] - ETA: 2:43 - loss: 1.9324 - regression_loss: 1.4528 - classification_loss: 0.4796\n",
      "342/500 [===================>..........] - ETA: 2:42 - loss: 1.9344 - regression_loss: 1.4548 - classification_loss: 0.4796\n",
      "343/500 [===================>..........] - ETA: 2:41 - loss: 1.9313 - regression_loss: 1.4526 - classification_loss: 0.4788\n",
      "344/500 [===================>..........] - ETA: 2:40 - loss: 1.9313 - regression_loss: 1.4526 - classification_loss: 0.4787\n",
      "345/500 [===================>..........] - ETA: 2:39 - loss: 1.9327 - regression_loss: 1.4542 - classification_loss: 0.4785\n",
      "346/500 [===================>..........] - ETA: 2:38 - loss: 1.9352 - regression_loss: 1.4558 - classification_loss: 0.4794\n",
      "347/500 [===================>..........] - ETA: 2:37 - loss: 1.9347 - regression_loss: 1.4555 - classification_loss: 0.4792\n",
      "348/500 [===================>..........] - ETA: 2:36 - loss: 1.9361 - regression_loss: 1.4560 - classification_loss: 0.4801\n",
      "349/500 [===================>..........] - ETA: 2:35 - loss: 1.9336 - regression_loss: 1.4543 - classification_loss: 0.4793\n",
      "350/500 [====================>.........] - ETA: 2:34 - loss: 1.9340 - regression_loss: 1.4545 - classification_loss: 0.4796\n",
      "351/500 [====================>.........] - ETA: 2:33 - loss: 1.9334 - regression_loss: 1.4541 - classification_loss: 0.4793\n",
      "352/500 [====================>.........] - ETA: 2:32 - loss: 1.9318 - regression_loss: 1.4529 - classification_loss: 0.4789\n",
      "353/500 [====================>.........] - ETA: 2:31 - loss: 1.9324 - regression_loss: 1.4535 - classification_loss: 0.4789\n",
      "354/500 [====================>.........] - ETA: 2:30 - loss: 1.9328 - regression_loss: 1.4538 - classification_loss: 0.4790\n",
      "355/500 [====================>.........] - ETA: 2:29 - loss: 1.9318 - regression_loss: 1.4530 - classification_loss: 0.4788\n",
      "356/500 [====================>.........] - ETA: 2:28 - loss: 1.9312 - regression_loss: 1.4528 - classification_loss: 0.4785\n",
      "357/500 [====================>.........] - ETA: 2:27 - loss: 1.9314 - regression_loss: 1.4533 - classification_loss: 0.4781\n",
      "358/500 [====================>.........] - ETA: 2:26 - loss: 1.9335 - regression_loss: 1.4545 - classification_loss: 0.4790\n",
      "359/500 [====================>.........] - ETA: 2:25 - loss: 1.9327 - regression_loss: 1.4541 - classification_loss: 0.4786\n",
      "360/500 [====================>.........] - ETA: 2:24 - loss: 1.9324 - regression_loss: 1.4542 - classification_loss: 0.4782\n",
      "361/500 [====================>.........] - ETA: 2:23 - loss: 1.9338 - regression_loss: 1.4552 - classification_loss: 0.4786\n",
      "362/500 [====================>.........] - ETA: 2:21 - loss: 1.9323 - regression_loss: 1.4542 - classification_loss: 0.4782\n",
      "363/500 [====================>.........] - ETA: 2:20 - loss: 1.9321 - regression_loss: 1.4538 - classification_loss: 0.4783\n",
      "364/500 [====================>.........] - ETA: 2:19 - loss: 1.9324 - regression_loss: 1.4539 - classification_loss: 0.4785\n",
      "365/500 [====================>.........] - ETA: 2:18 - loss: 1.9318 - regression_loss: 1.4536 - classification_loss: 0.4782\n",
      "366/500 [====================>.........] - ETA: 2:17 - loss: 1.9301 - regression_loss: 1.4523 - classification_loss: 0.4778\n",
      "367/500 [=====================>........] - ETA: 2:16 - loss: 1.9322 - regression_loss: 1.4538 - classification_loss: 0.4784\n",
      "368/500 [=====================>........] - ETA: 2:15 - loss: 1.9317 - regression_loss: 1.4531 - classification_loss: 0.4785\n",
      "369/500 [=====================>........] - ETA: 2:14 - loss: 1.9297 - regression_loss: 1.4517 - classification_loss: 0.4779\n",
      "370/500 [=====================>........] - ETA: 2:13 - loss: 1.9324 - regression_loss: 1.4529 - classification_loss: 0.4795\n",
      "371/500 [=====================>........] - ETA: 2:12 - loss: 1.9322 - regression_loss: 1.4525 - classification_loss: 0.4797\n",
      "372/500 [=====================>........] - ETA: 2:11 - loss: 1.9332 - regression_loss: 1.4535 - classification_loss: 0.4797\n",
      "373/500 [=====================>........] - ETA: 2:10 - loss: 1.9356 - regression_loss: 1.4547 - classification_loss: 0.4809\n",
      "374/500 [=====================>........] - ETA: 2:09 - loss: 1.9373 - regression_loss: 1.4555 - classification_loss: 0.4818\n",
      "375/500 [=====================>........] - ETA: 2:08 - loss: 1.9359 - regression_loss: 1.4545 - classification_loss: 0.4814\n",
      "376/500 [=====================>........] - ETA: 2:07 - loss: 1.9346 - regression_loss: 1.4529 - classification_loss: 0.4817\n",
      "377/500 [=====================>........] - ETA: 2:06 - loss: 1.9332 - regression_loss: 1.4520 - classification_loss: 0.4812\n",
      "378/500 [=====================>........] - ETA: 2:05 - loss: 1.9361 - regression_loss: 1.4543 - classification_loss: 0.4818\n",
      "379/500 [=====================>........] - ETA: 2:04 - loss: 1.9356 - regression_loss: 1.4539 - classification_loss: 0.4816\n",
      "380/500 [=====================>........] - ETA: 2:03 - loss: 1.9348 - regression_loss: 1.4533 - classification_loss: 0.4815\n",
      "381/500 [=====================>........] - ETA: 2:02 - loss: 1.9336 - regression_loss: 1.4524 - classification_loss: 0.4813\n",
      "382/500 [=====================>........] - ETA: 2:01 - loss: 1.9323 - regression_loss: 1.4509 - classification_loss: 0.4814\n",
      "383/500 [=====================>........] - ETA: 2:00 - loss: 1.9316 - regression_loss: 1.4502 - classification_loss: 0.4814\n",
      "384/500 [======================>.......] - ETA: 1:59 - loss: 1.9317 - regression_loss: 1.4506 - classification_loss: 0.4811\n",
      "385/500 [======================>.......] - ETA: 1:58 - loss: 1.9343 - regression_loss: 1.4521 - classification_loss: 0.4822\n",
      "386/500 [======================>.......] - ETA: 1:57 - loss: 1.9357 - regression_loss: 1.4529 - classification_loss: 0.4828\n",
      "387/500 [======================>.......] - ETA: 1:56 - loss: 1.9378 - regression_loss: 1.4538 - classification_loss: 0.4840\n",
      "388/500 [======================>.......] - ETA: 1:55 - loss: 1.9383 - regression_loss: 1.4542 - classification_loss: 0.4842\n",
      "389/500 [======================>.......] - ETA: 1:54 - loss: 1.9359 - regression_loss: 1.4522 - classification_loss: 0.4837\n",
      "390/500 [======================>.......] - ETA: 1:53 - loss: 1.9352 - regression_loss: 1.4515 - classification_loss: 0.4837\n",
      "391/500 [======================>.......] - ETA: 1:53 - loss: 1.9347 - regression_loss: 1.4513 - classification_loss: 0.4834\n",
      "392/500 [======================>.......] - ETA: 1:52 - loss: 1.9325 - regression_loss: 1.4495 - classification_loss: 0.4830\n",
      "393/500 [======================>.......] - ETA: 1:51 - loss: 1.9314 - regression_loss: 1.4489 - classification_loss: 0.4825\n",
      "394/500 [======================>.......] - ETA: 1:50 - loss: 1.9323 - regression_loss: 1.4498 - classification_loss: 0.4825\n",
      "395/500 [======================>.......] - ETA: 1:49 - loss: 1.9303 - regression_loss: 1.4482 - classification_loss: 0.4821\n",
      "396/500 [======================>.......] - ETA: 1:48 - loss: 1.9325 - regression_loss: 1.4500 - classification_loss: 0.4825\n",
      "397/500 [======================>.......] - ETA: 1:47 - loss: 1.9354 - regression_loss: 1.4523 - classification_loss: 0.4831\n",
      "398/500 [======================>.......] - ETA: 1:45 - loss: 1.9338 - regression_loss: 1.4510 - classification_loss: 0.4828\n",
      "399/500 [======================>.......] - ETA: 1:44 - loss: 1.9325 - regression_loss: 1.4502 - classification_loss: 0.4823\n",
      "400/500 [=======================>......] - ETA: 1:43 - loss: 1.9333 - regression_loss: 1.4506 - classification_loss: 0.4827\n",
      "401/500 [=======================>......] - ETA: 1:42 - loss: 1.9307 - regression_loss: 1.4486 - classification_loss: 0.4821\n",
      "402/500 [=======================>......] - ETA: 1:41 - loss: 1.9303 - regression_loss: 1.4483 - classification_loss: 0.4820\n",
      "403/500 [=======================>......] - ETA: 1:40 - loss: 1.9304 - regression_loss: 1.4485 - classification_loss: 0.4820\n",
      "404/500 [=======================>......] - ETA: 1:39 - loss: 1.9319 - regression_loss: 1.4494 - classification_loss: 0.4825\n",
      "405/500 [=======================>......] - ETA: 1:38 - loss: 1.9324 - regression_loss: 1.4497 - classification_loss: 0.4827\n",
      "406/500 [=======================>......] - ETA: 1:37 - loss: 1.9322 - regression_loss: 1.4494 - classification_loss: 0.4828\n",
      "407/500 [=======================>......] - ETA: 1:36 - loss: 1.9328 - regression_loss: 1.4499 - classification_loss: 0.4829\n",
      "408/500 [=======================>......] - ETA: 1:35 - loss: 1.9332 - regression_loss: 1.4503 - classification_loss: 0.4829\n",
      "409/500 [=======================>......] - ETA: 1:34 - loss: 1.9325 - regression_loss: 1.4497 - classification_loss: 0.4829\n",
      "410/500 [=======================>......] - ETA: 1:33 - loss: 1.9314 - regression_loss: 1.4489 - classification_loss: 0.4824\n",
      "411/500 [=======================>......] - ETA: 1:32 - loss: 1.9332 - regression_loss: 1.4504 - classification_loss: 0.4828\n",
      "412/500 [=======================>......] - ETA: 1:31 - loss: 1.9361 - regression_loss: 1.4523 - classification_loss: 0.4838\n",
      "413/500 [=======================>......] - ETA: 1:30 - loss: 1.9364 - regression_loss: 1.4526 - classification_loss: 0.4838\n",
      "414/500 [=======================>......] - ETA: 1:29 - loss: 1.9353 - regression_loss: 1.4519 - classification_loss: 0.4834\n",
      "415/500 [=======================>......] - ETA: 1:28 - loss: 1.9374 - regression_loss: 1.4532 - classification_loss: 0.4843\n",
      "416/500 [=======================>......] - ETA: 1:27 - loss: 1.9376 - regression_loss: 1.4528 - classification_loss: 0.4848\n",
      "417/500 [========================>.....] - ETA: 1:26 - loss: 1.9366 - regression_loss: 1.4521 - classification_loss: 0.4844\n",
      "418/500 [========================>.....] - ETA: 1:25 - loss: 1.9365 - regression_loss: 1.4524 - classification_loss: 0.4841\n",
      "419/500 [========================>.....] - ETA: 1:24 - loss: 1.9379 - regression_loss: 1.4536 - classification_loss: 0.4843\n",
      "420/500 [========================>.....] - ETA: 1:23 - loss: 1.9386 - regression_loss: 1.4542 - classification_loss: 0.4844\n",
      "421/500 [========================>.....] - ETA: 1:22 - loss: 1.9392 - regression_loss: 1.4548 - classification_loss: 0.4844\n",
      "422/500 [========================>.....] - ETA: 1:21 - loss: 1.9390 - regression_loss: 1.4548 - classification_loss: 0.4842\n",
      "423/500 [========================>.....] - ETA: 1:19 - loss: 1.9382 - regression_loss: 1.4540 - classification_loss: 0.4841\n",
      "424/500 [========================>.....] - ETA: 1:18 - loss: 1.9363 - regression_loss: 1.4527 - classification_loss: 0.4836\n",
      "425/500 [========================>.....] - ETA: 1:17 - loss: 1.9358 - regression_loss: 1.4521 - classification_loss: 0.4838\n",
      "426/500 [========================>.....] - ETA: 1:16 - loss: 1.9345 - regression_loss: 1.4509 - classification_loss: 0.4836\n",
      "427/500 [========================>.....] - ETA: 1:15 - loss: 1.9330 - regression_loss: 1.4496 - classification_loss: 0.4834\n",
      "428/500 [========================>.....] - ETA: 1:14 - loss: 1.9322 - regression_loss: 1.4491 - classification_loss: 0.4831\n",
      "429/500 [========================>.....] - ETA: 1:13 - loss: 1.9315 - regression_loss: 1.4487 - classification_loss: 0.4828\n",
      "430/500 [========================>.....] - ETA: 1:12 - loss: 1.9304 - regression_loss: 1.4475 - classification_loss: 0.4829\n",
      "431/500 [========================>.....] - ETA: 1:11 - loss: 1.9290 - regression_loss: 1.4467 - classification_loss: 0.4823\n",
      "432/500 [========================>.....] - ETA: 1:10 - loss: 1.9292 - regression_loss: 1.4471 - classification_loss: 0.4821\n",
      "433/500 [========================>.....] - ETA: 1:09 - loss: 1.9308 - regression_loss: 1.4486 - classification_loss: 0.4822\n",
      "434/500 [=========================>....] - ETA: 1:08 - loss: 1.9299 - regression_loss: 1.4478 - classification_loss: 0.4821\n",
      "435/500 [=========================>....] - ETA: 1:07 - loss: 1.9312 - regression_loss: 1.4480 - classification_loss: 0.4832\n",
      "436/500 [=========================>....] - ETA: 1:06 - loss: 1.9317 - regression_loss: 1.4482 - classification_loss: 0.4835\n",
      "437/500 [=========================>....] - ETA: 1:05 - loss: 1.9339 - regression_loss: 1.4502 - classification_loss: 0.4837\n",
      "438/500 [=========================>....] - ETA: 1:04 - loss: 1.9345 - regression_loss: 1.4507 - classification_loss: 0.4838\n",
      "439/500 [=========================>....] - ETA: 1:03 - loss: 1.9345 - regression_loss: 1.4507 - classification_loss: 0.4837\n",
      "440/500 [=========================>....] - ETA: 1:02 - loss: 1.9335 - regression_loss: 1.4501 - classification_loss: 0.4834\n",
      "441/500 [=========================>....] - ETA: 1:01 - loss: 1.9350 - regression_loss: 1.4509 - classification_loss: 0.4841\n",
      "442/500 [=========================>....] - ETA: 1:00 - loss: 1.9345 - regression_loss: 1.4506 - classification_loss: 0.4840\n",
      "443/500 [=========================>....] - ETA: 59s - loss: 1.9356 - regression_loss: 1.4512 - classification_loss: 0.4844 \n",
      "444/500 [=========================>....] - ETA: 58s - loss: 1.9361 - regression_loss: 1.4514 - classification_loss: 0.4848\n",
      "445/500 [=========================>....] - ETA: 57s - loss: 1.9342 - regression_loss: 1.4500 - classification_loss: 0.4842\n",
      "446/500 [=========================>....] - ETA: 56s - loss: 1.9335 - regression_loss: 1.4492 - classification_loss: 0.4844\n",
      "447/500 [=========================>....] - ETA: 54s - loss: 1.9336 - regression_loss: 1.4489 - classification_loss: 0.4847\n",
      "448/500 [=========================>....] - ETA: 53s - loss: 1.9330 - regression_loss: 1.4488 - classification_loss: 0.4842\n",
      "449/500 [=========================>....] - ETA: 52s - loss: 1.9354 - regression_loss: 1.4505 - classification_loss: 0.4849\n",
      "450/500 [==========================>...] - ETA: 51s - loss: 1.9356 - regression_loss: 1.4504 - classification_loss: 0.4852\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 1.9342 - regression_loss: 1.4493 - classification_loss: 0.4849\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 1.9328 - regression_loss: 1.4485 - classification_loss: 0.4843\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.9320 - regression_loss: 1.4482 - classification_loss: 0.4838\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.9325 - regression_loss: 1.4488 - classification_loss: 0.4836\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.9321 - regression_loss: 1.4487 - classification_loss: 0.4834\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.9303 - regression_loss: 1.4475 - classification_loss: 0.4828\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.9315 - regression_loss: 1.4482 - classification_loss: 0.4832\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.9315 - regression_loss: 1.4483 - classification_loss: 0.4832\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.9310 - regression_loss: 1.4480 - classification_loss: 0.4830\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.9301 - regression_loss: 1.4472 - classification_loss: 0.4829\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 1.9286 - regression_loss: 1.4458 - classification_loss: 0.4828\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 1.9282 - regression_loss: 1.4455 - classification_loss: 0.4827\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 1.9280 - regression_loss: 1.4451 - classification_loss: 0.4829\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 1.9283 - regression_loss: 1.4452 - classification_loss: 0.4831\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 1.9292 - regression_loss: 1.4458 - classification_loss: 0.4834\n",
      "466/500 [==========================>...] - ETA: 35s - loss: 1.9287 - regression_loss: 1.4455 - classification_loss: 0.4832\n",
      "467/500 [===========================>..] - ETA: 34s - loss: 1.9285 - regression_loss: 1.4454 - classification_loss: 0.4831\n",
      "468/500 [===========================>..] - ETA: 33s - loss: 1.9293 - regression_loss: 1.4462 - classification_loss: 0.4831\n",
      "469/500 [===========================>..] - ETA: 32s - loss: 1.9294 - regression_loss: 1.4464 - classification_loss: 0.4830\n",
      "470/500 [===========================>..] - ETA: 31s - loss: 1.9281 - regression_loss: 1.4454 - classification_loss: 0.4826\n",
      "471/500 [===========================>..] - ETA: 30s - loss: 1.9278 - regression_loss: 1.4452 - classification_loss: 0.4827\n",
      "472/500 [===========================>..] - ETA: 29s - loss: 1.9279 - regression_loss: 1.4451 - classification_loss: 0.4828\n",
      "473/500 [===========================>..] - ETA: 28s - loss: 1.9288 - regression_loss: 1.4458 - classification_loss: 0.4829\n",
      "474/500 [===========================>..] - ETA: 26s - loss: 1.9291 - regression_loss: 1.4461 - classification_loss: 0.4830\n",
      "475/500 [===========================>..] - ETA: 25s - loss: 1.9304 - regression_loss: 1.4469 - classification_loss: 0.4835\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 1.9307 - regression_loss: 1.4472 - classification_loss: 0.4834\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.9311 - regression_loss: 1.4473 - classification_loss: 0.4838\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.9303 - regression_loss: 1.4469 - classification_loss: 0.4834\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.9297 - regression_loss: 1.4465 - classification_loss: 0.4831\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.9316 - regression_loss: 1.4478 - classification_loss: 0.4838\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.9321 - regression_loss: 1.4483 - classification_loss: 0.4838\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.9309 - regression_loss: 1.4471 - classification_loss: 0.4838\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.9316 - regression_loss: 1.4478 - classification_loss: 0.4838\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.9300 - regression_loss: 1.4467 - classification_loss: 0.4833\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.9301 - regression_loss: 1.4467 - classification_loss: 0.4833\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.9331 - regression_loss: 1.4485 - classification_loss: 0.4846\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.9319 - regression_loss: 1.4477 - classification_loss: 0.4842\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.9300 - regression_loss: 1.4464 - classification_loss: 0.4836\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.9288 - regression_loss: 1.4454 - classification_loss: 0.4834\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.9289 - regression_loss: 1.4454 - classification_loss: 0.4836\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.9288 - regression_loss: 1.4449 - classification_loss: 0.4839 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.9279 - regression_loss: 1.4442 - classification_loss: 0.4837\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.9266 - regression_loss: 1.4433 - classification_loss: 0.4834\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.9259 - regression_loss: 1.4425 - classification_loss: 0.4834\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.9264 - regression_loss: 1.4433 - classification_loss: 0.4831\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.9248 - regression_loss: 1.4420 - classification_loss: 0.4827\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.9250 - regression_loss: 1.4423 - classification_loss: 0.4827\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.9245 - regression_loss: 1.4421 - classification_loss: 0.4824\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.9270 - regression_loss: 1.4434 - classification_loss: 0.4836\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9255 - regression_loss: 1.4425 - classification_loss: 0.4831\n",
      "Epoch 00037: saving model to ./snapshots\\resnet50_csv_37.h5\n",
      "\n",
      "500/500 [==============================] - 519s 1s/step - loss: 1.9255 - regression_loss: 1.4425 - classification_loss: 0.4831\n",
      "Epoch 38/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 1.3435 - regression_loss: 1.0039 - classification_loss: 0.3396\n",
      "  2/500 [..............................] - ETA: 5:07 - loss: 1.4306 - regression_loss: 1.0852 - classification_loss: 0.3455\n",
      "  3/500 [..............................] - ETA: 5:29 - loss: 1.4959 - regression_loss: 1.1008 - classification_loss: 0.3951\n",
      "  4/500 [..............................] - ETA: 6:29 - loss: 1.4187 - regression_loss: 1.0543 - classification_loss: 0.3643\n",
      "  5/500 [..............................] - ETA: 6:56 - loss: 1.6156 - regression_loss: 1.1652 - classification_loss: 0.4504\n",
      "  6/500 [..............................] - ETA: 7:16 - loss: 1.7652 - regression_loss: 1.2791 - classification_loss: 0.4861\n",
      "  7/500 [..............................] - ETA: 7:24 - loss: 1.7500 - regression_loss: 1.2732 - classification_loss: 0.4768\n",
      "  8/500 [..............................] - ETA: 7:34 - loss: 1.6622 - regression_loss: 1.2112 - classification_loss: 0.4511\n",
      "  9/500 [..............................] - ETA: 7:36 - loss: 1.6762 - regression_loss: 1.2280 - classification_loss: 0.4482\n",
      " 10/500 [..............................] - ETA: 7:30 - loss: 1.6170 - regression_loss: 1.1776 - classification_loss: 0.4394\n",
      " 11/500 [..............................] - ETA: 7:42 - loss: 1.6307 - regression_loss: 1.2031 - classification_loss: 0.4275\n",
      " 12/500 [..............................] - ETA: 7:42 - loss: 1.5810 - regression_loss: 1.1695 - classification_loss: 0.4114\n",
      " 13/500 [..............................] - ETA: 7:43 - loss: 1.5992 - regression_loss: 1.1884 - classification_loss: 0.4108\n",
      " 14/500 [..............................] - ETA: 7:47 - loss: 1.6061 - regression_loss: 1.1993 - classification_loss: 0.4068\n",
      " 15/500 [..............................] - ETA: 7:47 - loss: 1.5977 - regression_loss: 1.1954 - classification_loss: 0.4022\n",
      " 16/500 [..............................] - ETA: 7:51 - loss: 1.6135 - regression_loss: 1.2093 - classification_loss: 0.4041\n",
      " 17/500 [>.............................] - ETA: 7:53 - loss: 1.6792 - regression_loss: 1.2519 - classification_loss: 0.4273\n",
      " 18/500 [>.............................] - ETA: 7:55 - loss: 1.6622 - regression_loss: 1.2332 - classification_loss: 0.4291\n",
      " 19/500 [>.............................] - ETA: 7:56 - loss: 1.6597 - regression_loss: 1.2393 - classification_loss: 0.4204\n",
      " 20/500 [>.............................] - ETA: 7:55 - loss: 1.6677 - regression_loss: 1.2506 - classification_loss: 0.4171\n",
      " 21/500 [>.............................] - ETA: 7:56 - loss: 1.6681 - regression_loss: 1.2514 - classification_loss: 0.4167\n",
      " 22/500 [>.............................] - ETA: 7:55 - loss: 1.6591 - regression_loss: 1.2431 - classification_loss: 0.4159\n",
      " 23/500 [>.............................] - ETA: 7:54 - loss: 1.6473 - regression_loss: 1.2358 - classification_loss: 0.4116\n",
      " 24/500 [>.............................] - ETA: 7:53 - loss: 1.6118 - regression_loss: 1.2101 - classification_loss: 0.4017\n",
      " 25/500 [>.............................] - ETA: 7:52 - loss: 1.6442 - regression_loss: 1.2326 - classification_loss: 0.4115\n",
      " 26/500 [>.............................] - ETA: 7:53 - loss: 1.6636 - regression_loss: 1.2481 - classification_loss: 0.4155\n",
      " 27/500 [>.............................] - ETA: 7:52 - loss: 1.6586 - regression_loss: 1.2435 - classification_loss: 0.4151\n",
      " 28/500 [>.............................] - ETA: 7:53 - loss: 1.6568 - regression_loss: 1.2466 - classification_loss: 0.4102\n",
      " 29/500 [>.............................] - ETA: 7:53 - loss: 1.6680 - regression_loss: 1.2528 - classification_loss: 0.4152\n",
      " 30/500 [>.............................] - ETA: 7:52 - loss: 1.6792 - regression_loss: 1.2649 - classification_loss: 0.4143\n",
      " 31/500 [>.............................] - ETA: 7:52 - loss: 1.7040 - regression_loss: 1.2837 - classification_loss: 0.4203\n",
      " 32/500 [>.............................] - ETA: 7:52 - loss: 1.7279 - regression_loss: 1.2979 - classification_loss: 0.4300\n",
      " 33/500 [>.............................] - ETA: 7:49 - loss: 1.7371 - regression_loss: 1.3061 - classification_loss: 0.4310\n",
      " 34/500 [=>............................] - ETA: 7:50 - loss: 1.7111 - regression_loss: 1.2869 - classification_loss: 0.4242\n",
      " 35/500 [=>............................] - ETA: 7:49 - loss: 1.7131 - regression_loss: 1.2888 - classification_loss: 0.4243\n",
      " 36/500 [=>............................] - ETA: 7:47 - loss: 1.7117 - regression_loss: 1.2860 - classification_loss: 0.4257\n",
      " 37/500 [=>............................] - ETA: 7:46 - loss: 1.7299 - regression_loss: 1.2999 - classification_loss: 0.4300\n",
      " 38/500 [=>............................] - ETA: 7:46 - loss: 1.7171 - regression_loss: 1.2898 - classification_loss: 0.4273\n",
      " 39/500 [=>............................] - ETA: 7:45 - loss: 1.7288 - regression_loss: 1.2999 - classification_loss: 0.4289\n",
      " 40/500 [=>............................] - ETA: 7:45 - loss: 1.7315 - regression_loss: 1.3065 - classification_loss: 0.4250\n",
      " 41/500 [=>............................] - ETA: 7:45 - loss: 1.7161 - regression_loss: 1.2949 - classification_loss: 0.4212\n",
      " 42/500 [=>............................] - ETA: 7:44 - loss: 1.7503 - regression_loss: 1.3174 - classification_loss: 0.4328\n",
      " 43/500 [=>............................] - ETA: 7:43 - loss: 1.7545 - regression_loss: 1.3251 - classification_loss: 0.4294\n",
      " 44/500 [=>............................] - ETA: 7:42 - loss: 1.7672 - regression_loss: 1.3346 - classification_loss: 0.4326\n",
      " 45/500 [=>............................] - ETA: 7:41 - loss: 1.7605 - regression_loss: 1.3306 - classification_loss: 0.4299\n",
      " 46/500 [=>............................] - ETA: 7:40 - loss: 1.7739 - regression_loss: 1.3440 - classification_loss: 0.4298\n",
      " 47/500 [=>............................] - ETA: 7:39 - loss: 1.7809 - regression_loss: 1.3476 - classification_loss: 0.4333\n",
      " 48/500 [=>............................] - ETA: 7:38 - loss: 1.7728 - regression_loss: 1.3443 - classification_loss: 0.4285\n",
      " 49/500 [=>............................] - ETA: 7:38 - loss: 1.7865 - regression_loss: 1.3501 - classification_loss: 0.4364\n",
      " 50/500 [==>...........................] - ETA: 7:35 - loss: 1.7724 - regression_loss: 1.3395 - classification_loss: 0.4330\n",
      " 51/500 [==>...........................] - ETA: 7:35 - loss: 1.7847 - regression_loss: 1.3493 - classification_loss: 0.4354\n",
      " 52/500 [==>...........................] - ETA: 7:32 - loss: 1.7709 - regression_loss: 1.3391 - classification_loss: 0.4319\n",
      " 53/500 [==>...........................] - ETA: 7:33 - loss: 1.7742 - regression_loss: 1.3416 - classification_loss: 0.4326\n",
      " 54/500 [==>...........................] - ETA: 7:31 - loss: 1.7870 - regression_loss: 1.3500 - classification_loss: 0.4370\n",
      " 55/500 [==>...........................] - ETA: 7:30 - loss: 1.8049 - regression_loss: 1.3599 - classification_loss: 0.4450\n",
      " 56/500 [==>...........................] - ETA: 7:30 - loss: 1.7924 - regression_loss: 1.3496 - classification_loss: 0.4427\n",
      " 57/500 [==>...........................] - ETA: 7:29 - loss: 1.8071 - regression_loss: 1.3604 - classification_loss: 0.4467\n",
      " 58/500 [==>...........................] - ETA: 7:29 - loss: 1.8182 - regression_loss: 1.3680 - classification_loss: 0.4502\n",
      " 59/500 [==>...........................] - ETA: 7:27 - loss: 1.8145 - regression_loss: 1.3676 - classification_loss: 0.4469\n",
      " 60/500 [==>...........................] - ETA: 7:26 - loss: 1.8234 - regression_loss: 1.3711 - classification_loss: 0.4522\n",
      " 61/500 [==>...........................] - ETA: 7:25 - loss: 1.8276 - regression_loss: 1.3758 - classification_loss: 0.4518\n",
      " 62/500 [==>...........................] - ETA: 7:24 - loss: 1.8285 - regression_loss: 1.3738 - classification_loss: 0.4546\n",
      " 63/500 [==>...........................] - ETA: 7:23 - loss: 1.8352 - regression_loss: 1.3810 - classification_loss: 0.4542\n",
      " 64/500 [==>...........................] - ETA: 7:22 - loss: 1.8455 - regression_loss: 1.3905 - classification_loss: 0.4550\n",
      " 65/500 [==>...........................] - ETA: 7:22 - loss: 1.8480 - regression_loss: 1.3923 - classification_loss: 0.4557\n",
      " 66/500 [==>...........................] - ETA: 7:21 - loss: 1.8584 - regression_loss: 1.4014 - classification_loss: 0.4570\n",
      " 67/500 [===>..........................] - ETA: 7:19 - loss: 1.8577 - regression_loss: 1.4005 - classification_loss: 0.4572\n",
      " 68/500 [===>..........................] - ETA: 7:18 - loss: 1.8678 - regression_loss: 1.4092 - classification_loss: 0.4586\n",
      " 69/500 [===>..........................] - ETA: 7:18 - loss: 1.8583 - regression_loss: 1.4022 - classification_loss: 0.4561\n",
      " 70/500 [===>..........................] - ETA: 7:16 - loss: 1.8640 - regression_loss: 1.4081 - classification_loss: 0.4559\n",
      " 71/500 [===>..........................] - ETA: 7:15 - loss: 1.8700 - regression_loss: 1.4122 - classification_loss: 0.4578\n",
      " 72/500 [===>..........................] - ETA: 7:14 - loss: 1.8651 - regression_loss: 1.4085 - classification_loss: 0.4566\n",
      " 73/500 [===>..........................] - ETA: 7:13 - loss: 1.8651 - regression_loss: 1.4088 - classification_loss: 0.4563\n",
      " 74/500 [===>..........................] - ETA: 7:12 - loss: 1.8743 - regression_loss: 1.4155 - classification_loss: 0.4587\n",
      " 75/500 [===>..........................] - ETA: 7:11 - loss: 1.8758 - regression_loss: 1.4177 - classification_loss: 0.4581\n",
      " 76/500 [===>..........................] - ETA: 7:10 - loss: 1.8724 - regression_loss: 1.4157 - classification_loss: 0.4567\n",
      " 77/500 [===>..........................] - ETA: 7:08 - loss: 1.8766 - regression_loss: 1.4196 - classification_loss: 0.4570\n",
      " 78/500 [===>..........................] - ETA: 7:08 - loss: 1.8803 - regression_loss: 1.4231 - classification_loss: 0.4572\n",
      " 79/500 [===>..........................] - ETA: 7:06 - loss: 1.8730 - regression_loss: 1.4173 - classification_loss: 0.4557\n",
      " 80/500 [===>..........................] - ETA: 7:06 - loss: 1.8714 - regression_loss: 1.4175 - classification_loss: 0.4539\n",
      " 81/500 [===>..........................] - ETA: 7:04 - loss: 1.8662 - regression_loss: 1.4144 - classification_loss: 0.4518\n",
      " 82/500 [===>..........................] - ETA: 7:04 - loss: 1.8681 - regression_loss: 1.4166 - classification_loss: 0.4514\n",
      " 83/500 [===>..........................] - ETA: 7:03 - loss: 1.8593 - regression_loss: 1.4100 - classification_loss: 0.4494\n",
      " 84/500 [====>.........................] - ETA: 7:02 - loss: 1.8751 - regression_loss: 1.4217 - classification_loss: 0.4534\n",
      " 85/500 [====>.........................] - ETA: 7:01 - loss: 1.8800 - regression_loss: 1.4247 - classification_loss: 0.4552\n",
      " 86/500 [====>.........................] - ETA: 6:59 - loss: 1.8750 - regression_loss: 1.4206 - classification_loss: 0.4544\n",
      " 87/500 [====>.........................] - ETA: 6:59 - loss: 1.8703 - regression_loss: 1.4185 - classification_loss: 0.4517\n",
      " 88/500 [====>.........................] - ETA: 6:58 - loss: 1.8699 - regression_loss: 1.4185 - classification_loss: 0.4514\n",
      " 89/500 [====>.........................] - ETA: 6:57 - loss: 1.8706 - regression_loss: 1.4179 - classification_loss: 0.4527\n",
      " 90/500 [====>.........................] - ETA: 6:56 - loss: 1.8808 - regression_loss: 1.4252 - classification_loss: 0.4555\n",
      " 91/500 [====>.........................] - ETA: 6:55 - loss: 1.8824 - regression_loss: 1.4252 - classification_loss: 0.4571\n",
      " 92/500 [====>.........................] - ETA: 6:54 - loss: 1.8763 - regression_loss: 1.4216 - classification_loss: 0.4546\n",
      " 93/500 [====>.........................] - ETA: 6:53 - loss: 1.8776 - regression_loss: 1.4212 - classification_loss: 0.4564\n",
      " 94/500 [====>.........................] - ETA: 6:52 - loss: 1.8796 - regression_loss: 1.4238 - classification_loss: 0.4558\n",
      " 95/500 [====>.........................] - ETA: 6:52 - loss: 1.8778 - regression_loss: 1.4230 - classification_loss: 0.4548\n",
      " 96/500 [====>.........................] - ETA: 6:51 - loss: 1.8744 - regression_loss: 1.4198 - classification_loss: 0.4546\n",
      " 97/500 [====>.........................] - ETA: 6:50 - loss: 1.8757 - regression_loss: 1.4203 - classification_loss: 0.4555\n",
      " 98/500 [====>.........................] - ETA: 6:49 - loss: 1.8720 - regression_loss: 1.4177 - classification_loss: 0.4542\n",
      " 99/500 [====>.........................] - ETA: 6:48 - loss: 1.8676 - regression_loss: 1.4143 - classification_loss: 0.4533\n",
      "100/500 [=====>........................] - ETA: 6:47 - loss: 1.8580 - regression_loss: 1.4062 - classification_loss: 0.4518\n",
      "101/500 [=====>........................] - ETA: 6:46 - loss: 1.8524 - regression_loss: 1.4021 - classification_loss: 0.4503\n",
      "102/500 [=====>........................] - ETA: 6:46 - loss: 1.8474 - regression_loss: 1.3968 - classification_loss: 0.4506\n",
      "103/500 [=====>........................] - ETA: 6:44 - loss: 1.8440 - regression_loss: 1.3941 - classification_loss: 0.4499\n",
      "104/500 [=====>........................] - ETA: 6:43 - loss: 1.8576 - regression_loss: 1.4021 - classification_loss: 0.4555\n",
      "105/500 [=====>........................] - ETA: 6:42 - loss: 1.8621 - regression_loss: 1.4046 - classification_loss: 0.4575\n",
      "106/500 [=====>........................] - ETA: 6:42 - loss: 1.8619 - regression_loss: 1.4048 - classification_loss: 0.4572\n",
      "107/500 [=====>........................] - ETA: 6:40 - loss: 1.8567 - regression_loss: 1.4011 - classification_loss: 0.4556\n",
      "108/500 [=====>........................] - ETA: 6:39 - loss: 1.8637 - regression_loss: 1.4055 - classification_loss: 0.4583\n",
      "109/500 [=====>........................] - ETA: 6:45 - loss: 1.8602 - regression_loss: 1.4025 - classification_loss: 0.4577\n",
      "110/500 [=====>........................] - ETA: 6:44 - loss: 1.8593 - regression_loss: 1.4019 - classification_loss: 0.4573\n",
      "111/500 [=====>........................] - ETA: 6:43 - loss: 1.8608 - regression_loss: 1.4036 - classification_loss: 0.4572\n",
      "112/500 [=====>........................] - ETA: 6:42 - loss: 1.8644 - regression_loss: 1.4062 - classification_loss: 0.4582\n",
      "113/500 [=====>........................] - ETA: 6:41 - loss: 1.8650 - regression_loss: 1.4078 - classification_loss: 0.4572\n",
      "114/500 [=====>........................] - ETA: 6:39 - loss: 1.8597 - regression_loss: 1.4045 - classification_loss: 0.4552\n",
      "115/500 [=====>........................] - ETA: 6:38 - loss: 1.8697 - regression_loss: 1.4106 - classification_loss: 0.4591\n",
      "116/500 [=====>........................] - ETA: 6:37 - loss: 1.8673 - regression_loss: 1.4087 - classification_loss: 0.4586\n",
      "117/500 [======>.......................] - ETA: 6:36 - loss: 1.8671 - regression_loss: 1.4088 - classification_loss: 0.4582\n",
      "118/500 [======>.......................] - ETA: 6:35 - loss: 1.8677 - regression_loss: 1.4091 - classification_loss: 0.4585\n",
      "119/500 [======>.......................] - ETA: 6:34 - loss: 1.8677 - regression_loss: 1.4086 - classification_loss: 0.4591\n",
      "120/500 [======>.......................] - ETA: 6:33 - loss: 1.8618 - regression_loss: 1.4042 - classification_loss: 0.4576\n",
      "121/500 [======>.......................] - ETA: 6:32 - loss: 1.8650 - regression_loss: 1.4070 - classification_loss: 0.4579\n",
      "122/500 [======>.......................] - ETA: 6:31 - loss: 1.8633 - regression_loss: 1.4062 - classification_loss: 0.4572\n",
      "123/500 [======>.......................] - ETA: 6:30 - loss: 1.8649 - regression_loss: 1.4079 - classification_loss: 0.4570\n",
      "124/500 [======>.......................] - ETA: 6:29 - loss: 1.8600 - regression_loss: 1.4045 - classification_loss: 0.4556\n",
      "125/500 [======>.......................] - ETA: 6:28 - loss: 1.8560 - regression_loss: 1.4005 - classification_loss: 0.4555\n",
      "126/500 [======>.......................] - ETA: 6:27 - loss: 1.8583 - regression_loss: 1.4013 - classification_loss: 0.4570\n",
      "127/500 [======>.......................] - ETA: 6:26 - loss: 1.8608 - regression_loss: 1.4036 - classification_loss: 0.4571\n",
      "128/500 [======>.......................] - ETA: 6:25 - loss: 1.8571 - regression_loss: 1.4007 - classification_loss: 0.4564\n",
      "129/500 [======>.......................] - ETA: 6:24 - loss: 1.8565 - regression_loss: 1.4000 - classification_loss: 0.4565\n",
      "130/500 [======>.......................] - ETA: 6:23 - loss: 1.8555 - regression_loss: 1.3991 - classification_loss: 0.4565\n",
      "131/500 [======>.......................] - ETA: 6:22 - loss: 1.8547 - regression_loss: 1.3997 - classification_loss: 0.4551\n",
      "132/500 [======>.......................] - ETA: 6:21 - loss: 1.8569 - regression_loss: 1.4002 - classification_loss: 0.4567\n",
      "133/500 [======>.......................] - ETA: 6:20 - loss: 1.8552 - regression_loss: 1.3986 - classification_loss: 0.4566\n",
      "134/500 [=======>......................] - ETA: 6:19 - loss: 1.8550 - regression_loss: 1.3990 - classification_loss: 0.4559\n",
      "135/500 [=======>......................] - ETA: 6:18 - loss: 1.8497 - regression_loss: 1.3954 - classification_loss: 0.4543\n",
      "136/500 [=======>......................] - ETA: 6:17 - loss: 1.8437 - regression_loss: 1.3911 - classification_loss: 0.4526\n",
      "137/500 [=======>......................] - ETA: 6:16 - loss: 1.8432 - regression_loss: 1.3908 - classification_loss: 0.4524\n",
      "138/500 [=======>......................] - ETA: 6:15 - loss: 1.8483 - regression_loss: 1.3932 - classification_loss: 0.4551\n",
      "139/500 [=======>......................] - ETA: 6:14 - loss: 1.8511 - regression_loss: 1.3948 - classification_loss: 0.4563\n",
      "140/500 [=======>......................] - ETA: 6:12 - loss: 1.8534 - regression_loss: 1.3958 - classification_loss: 0.4577\n",
      "141/500 [=======>......................] - ETA: 6:12 - loss: 1.8552 - regression_loss: 1.3983 - classification_loss: 0.4569\n",
      "142/500 [=======>......................] - ETA: 6:10 - loss: 1.8559 - regression_loss: 1.3975 - classification_loss: 0.4585\n",
      "143/500 [=======>......................] - ETA: 6:09 - loss: 1.8597 - regression_loss: 1.4005 - classification_loss: 0.4592\n",
      "144/500 [=======>......................] - ETA: 6:08 - loss: 1.8648 - regression_loss: 1.4045 - classification_loss: 0.4603\n",
      "145/500 [=======>......................] - ETA: 6:07 - loss: 1.8692 - regression_loss: 1.4076 - classification_loss: 0.4617\n",
      "146/500 [=======>......................] - ETA: 6:06 - loss: 1.8729 - regression_loss: 1.4099 - classification_loss: 0.4630\n",
      "147/500 [=======>......................] - ETA: 6:05 - loss: 1.8696 - regression_loss: 1.4075 - classification_loss: 0.4621\n",
      "148/500 [=======>......................] - ETA: 6:04 - loss: 1.8770 - regression_loss: 1.4132 - classification_loss: 0.4638\n",
      "149/500 [=======>......................] - ETA: 6:03 - loss: 1.8776 - regression_loss: 1.4135 - classification_loss: 0.4641\n",
      "150/500 [========>.....................] - ETA: 6:02 - loss: 1.8744 - regression_loss: 1.4113 - classification_loss: 0.4631\n",
      "151/500 [========>.....................] - ETA: 6:01 - loss: 1.8773 - regression_loss: 1.4143 - classification_loss: 0.4630\n",
      "152/500 [========>.....................] - ETA: 6:00 - loss: 1.8793 - regression_loss: 1.4165 - classification_loss: 0.4628\n",
      "153/500 [========>.....................] - ETA: 5:59 - loss: 1.8787 - regression_loss: 1.4153 - classification_loss: 0.4634\n",
      "154/500 [========>.....................] - ETA: 5:58 - loss: 1.8771 - regression_loss: 1.4147 - classification_loss: 0.4624\n",
      "155/500 [========>.....................] - ETA: 5:57 - loss: 1.8777 - regression_loss: 1.4165 - classification_loss: 0.4612\n",
      "156/500 [========>.....................] - ETA: 5:55 - loss: 1.8787 - regression_loss: 1.4176 - classification_loss: 0.4612\n",
      "157/500 [========>.....................] - ETA: 5:55 - loss: 1.8812 - regression_loss: 1.4196 - classification_loss: 0.4616\n",
      "158/500 [========>.....................] - ETA: 5:53 - loss: 1.8833 - regression_loss: 1.4212 - classification_loss: 0.4620\n",
      "159/500 [========>.....................] - ETA: 5:52 - loss: 1.8773 - regression_loss: 1.4166 - classification_loss: 0.4608\n",
      "160/500 [========>.....................] - ETA: 5:51 - loss: 1.8801 - regression_loss: 1.4183 - classification_loss: 0.4618\n",
      "161/500 [========>.....................] - ETA: 5:50 - loss: 1.8822 - regression_loss: 1.4202 - classification_loss: 0.4620\n",
      "162/500 [========>.....................] - ETA: 5:49 - loss: 1.8850 - regression_loss: 1.4213 - classification_loss: 0.4637\n",
      "163/500 [========>.....................] - ETA: 5:48 - loss: 1.8852 - regression_loss: 1.4215 - classification_loss: 0.4637\n",
      "164/500 [========>.....................] - ETA: 5:47 - loss: 1.8857 - regression_loss: 1.4221 - classification_loss: 0.4637\n",
      "165/500 [========>.....................] - ETA: 5:46 - loss: 1.8854 - regression_loss: 1.4222 - classification_loss: 0.4632\n",
      "166/500 [========>.....................] - ETA: 5:45 - loss: 1.8921 - regression_loss: 1.4266 - classification_loss: 0.4654\n",
      "167/500 [=========>....................] - ETA: 5:44 - loss: 1.8952 - regression_loss: 1.4290 - classification_loss: 0.4662\n",
      "168/500 [=========>....................] - ETA: 5:43 - loss: 1.8979 - regression_loss: 1.4310 - classification_loss: 0.4669\n",
      "169/500 [=========>....................] - ETA: 5:42 - loss: 1.9011 - regression_loss: 1.4326 - classification_loss: 0.4685\n",
      "170/500 [=========>....................] - ETA: 5:40 - loss: 1.8987 - regression_loss: 1.4295 - classification_loss: 0.4692\n",
      "171/500 [=========>....................] - ETA: 5:40 - loss: 1.9007 - regression_loss: 1.4287 - classification_loss: 0.4720\n",
      "172/500 [=========>....................] - ETA: 5:39 - loss: 1.9048 - regression_loss: 1.4313 - classification_loss: 0.4736\n",
      "173/500 [=========>....................] - ETA: 5:38 - loss: 1.9026 - regression_loss: 1.4298 - classification_loss: 0.4729\n",
      "174/500 [=========>....................] - ETA: 5:37 - loss: 1.9038 - regression_loss: 1.4315 - classification_loss: 0.4723\n",
      "175/500 [=========>....................] - ETA: 5:36 - loss: 1.9065 - regression_loss: 1.4338 - classification_loss: 0.4727\n",
      "176/500 [=========>....................] - ETA: 5:35 - loss: 1.9040 - regression_loss: 1.4328 - classification_loss: 0.4712\n",
      "177/500 [=========>....................] - ETA: 5:33 - loss: 1.9022 - regression_loss: 1.4318 - classification_loss: 0.4703\n",
      "178/500 [=========>....................] - ETA: 5:32 - loss: 1.9022 - regression_loss: 1.4318 - classification_loss: 0.4705\n",
      "179/500 [=========>....................] - ETA: 5:31 - loss: 1.8999 - regression_loss: 1.4302 - classification_loss: 0.4697\n",
      "180/500 [=========>....................] - ETA: 5:30 - loss: 1.8971 - regression_loss: 1.4286 - classification_loss: 0.4685\n",
      "181/500 [=========>....................] - ETA: 5:29 - loss: 1.8940 - regression_loss: 1.4265 - classification_loss: 0.4675\n",
      "182/500 [=========>....................] - ETA: 5:28 - loss: 1.8957 - regression_loss: 1.4268 - classification_loss: 0.4689\n",
      "183/500 [=========>....................] - ETA: 5:27 - loss: 1.8995 - regression_loss: 1.4290 - classification_loss: 0.4705\n",
      "184/500 [==========>...................] - ETA: 5:26 - loss: 1.9050 - regression_loss: 1.4329 - classification_loss: 0.4721\n",
      "185/500 [==========>...................] - ETA: 5:25 - loss: 1.9022 - regression_loss: 1.4311 - classification_loss: 0.4711\n",
      "186/500 [==========>...................] - ETA: 5:24 - loss: 1.8999 - regression_loss: 1.4293 - classification_loss: 0.4706\n",
      "187/500 [==========>...................] - ETA: 5:23 - loss: 1.9033 - regression_loss: 1.4315 - classification_loss: 0.4718\n",
      "188/500 [==========>...................] - ETA: 5:22 - loss: 1.9039 - regression_loss: 1.4310 - classification_loss: 0.4729\n",
      "189/500 [==========>...................] - ETA: 5:21 - loss: 1.9044 - regression_loss: 1.4314 - classification_loss: 0.4730\n",
      "190/500 [==========>...................] - ETA: 5:20 - loss: 1.9023 - regression_loss: 1.4289 - classification_loss: 0.4734\n",
      "191/500 [==========>...................] - ETA: 5:19 - loss: 1.9018 - regression_loss: 1.4278 - classification_loss: 0.4739\n",
      "192/500 [==========>...................] - ETA: 5:18 - loss: 1.9021 - regression_loss: 1.4274 - classification_loss: 0.4747\n",
      "193/500 [==========>...................] - ETA: 5:17 - loss: 1.9057 - regression_loss: 1.4301 - classification_loss: 0.4756\n",
      "194/500 [==========>...................] - ETA: 5:16 - loss: 1.9037 - regression_loss: 1.4287 - classification_loss: 0.4750\n",
      "195/500 [==========>...................] - ETA: 5:15 - loss: 1.9055 - regression_loss: 1.4308 - classification_loss: 0.4747\n",
      "196/500 [==========>...................] - ETA: 5:14 - loss: 1.9043 - regression_loss: 1.4303 - classification_loss: 0.4740\n",
      "197/500 [==========>...................] - ETA: 5:13 - loss: 1.9069 - regression_loss: 1.4314 - classification_loss: 0.4755\n",
      "198/500 [==========>...................] - ETA: 5:12 - loss: 1.9087 - regression_loss: 1.4320 - classification_loss: 0.4766\n",
      "199/500 [==========>...................] - ETA: 5:11 - loss: 1.9076 - regression_loss: 1.4319 - classification_loss: 0.4757\n",
      "200/500 [===========>..................] - ETA: 5:10 - loss: 1.9080 - regression_loss: 1.4322 - classification_loss: 0.4758\n",
      "201/500 [===========>..................] - ETA: 5:09 - loss: 1.9065 - regression_loss: 1.4308 - classification_loss: 0.4757\n",
      "202/500 [===========>..................] - ETA: 5:08 - loss: 1.9081 - regression_loss: 1.4319 - classification_loss: 0.4762\n",
      "203/500 [===========>..................] - ETA: 5:07 - loss: 1.9127 - regression_loss: 1.4355 - classification_loss: 0.4772\n",
      "204/500 [===========>..................] - ETA: 5:06 - loss: 1.9118 - regression_loss: 1.4349 - classification_loss: 0.4769\n",
      "205/500 [===========>..................] - ETA: 5:05 - loss: 1.9115 - regression_loss: 1.4348 - classification_loss: 0.4767\n",
      "206/500 [===========>..................] - ETA: 5:04 - loss: 1.9141 - regression_loss: 1.4367 - classification_loss: 0.4774\n",
      "207/500 [===========>..................] - ETA: 5:03 - loss: 1.9133 - regression_loss: 1.4363 - classification_loss: 0.4770\n",
      "208/500 [===========>..................] - ETA: 5:01 - loss: 1.9114 - regression_loss: 1.4352 - classification_loss: 0.4762\n",
      "209/500 [===========>..................] - ETA: 5:01 - loss: 1.9111 - regression_loss: 1.4340 - classification_loss: 0.4771\n",
      "210/500 [===========>..................] - ETA: 4:59 - loss: 1.9106 - regression_loss: 1.4332 - classification_loss: 0.4774\n",
      "211/500 [===========>..................] - ETA: 4:59 - loss: 1.9103 - regression_loss: 1.4333 - classification_loss: 0.4770\n",
      "212/500 [===========>..................] - ETA: 4:58 - loss: 1.9127 - regression_loss: 1.4354 - classification_loss: 0.4774\n",
      "213/500 [===========>..................] - ETA: 4:56 - loss: 1.9088 - regression_loss: 1.4325 - classification_loss: 0.4763\n",
      "214/500 [===========>..................] - ETA: 4:55 - loss: 1.9115 - regression_loss: 1.4345 - classification_loss: 0.4770\n",
      "215/500 [===========>..................] - ETA: 4:54 - loss: 1.9104 - regression_loss: 1.4336 - classification_loss: 0.4768\n",
      "216/500 [===========>..................] - ETA: 4:53 - loss: 1.9071 - regression_loss: 1.4311 - classification_loss: 0.4761\n",
      "217/500 [============>.................] - ETA: 4:52 - loss: 1.9050 - regression_loss: 1.4296 - classification_loss: 0.4754\n",
      "218/500 [============>.................] - ETA: 4:51 - loss: 1.9080 - regression_loss: 1.4313 - classification_loss: 0.4767\n",
      "219/500 [============>.................] - ETA: 4:50 - loss: 1.9058 - regression_loss: 1.4298 - classification_loss: 0.4760\n",
      "220/500 [============>.................] - ETA: 4:49 - loss: 1.9041 - regression_loss: 1.4281 - classification_loss: 0.4761\n",
      "221/500 [============>.................] - ETA: 4:48 - loss: 1.9028 - regression_loss: 1.4274 - classification_loss: 0.4755\n",
      "222/500 [============>.................] - ETA: 4:47 - loss: 1.9009 - regression_loss: 1.4262 - classification_loss: 0.4748\n",
      "223/500 [============>.................] - ETA: 4:45 - loss: 1.8994 - regression_loss: 1.4255 - classification_loss: 0.4739\n",
      "224/500 [============>.................] - ETA: 4:45 - loss: 1.9054 - regression_loss: 1.4303 - classification_loss: 0.4752\n",
      "225/500 [============>.................] - ETA: 4:44 - loss: 1.9083 - regression_loss: 1.4323 - classification_loss: 0.4760\n",
      "226/500 [============>.................] - ETA: 4:43 - loss: 1.9054 - regression_loss: 1.4299 - classification_loss: 0.4754\n",
      "227/500 [============>.................] - ETA: 4:41 - loss: 1.9084 - regression_loss: 1.4331 - classification_loss: 0.4754\n",
      "228/500 [============>.................] - ETA: 4:40 - loss: 1.9080 - regression_loss: 1.4326 - classification_loss: 0.4754\n",
      "229/500 [============>.................] - ETA: 4:39 - loss: 1.9119 - regression_loss: 1.4348 - classification_loss: 0.4771\n",
      "230/500 [============>.................] - ETA: 4:38 - loss: 1.9171 - regression_loss: 1.4382 - classification_loss: 0.4789\n",
      "231/500 [============>.................] - ETA: 4:37 - loss: 1.9196 - regression_loss: 1.4404 - classification_loss: 0.4793\n",
      "232/500 [============>.................] - ETA: 4:36 - loss: 1.9195 - regression_loss: 1.4404 - classification_loss: 0.4791\n",
      "233/500 [============>.................] - ETA: 4:35 - loss: 1.9190 - regression_loss: 1.4404 - classification_loss: 0.4786\n",
      "234/500 [=============>................] - ETA: 4:34 - loss: 1.9178 - regression_loss: 1.4391 - classification_loss: 0.4788\n",
      "235/500 [=============>................] - ETA: 4:33 - loss: 1.9213 - regression_loss: 1.4419 - classification_loss: 0.4794\n",
      "236/500 [=============>................] - ETA: 4:32 - loss: 1.9177 - regression_loss: 1.4391 - classification_loss: 0.4786\n",
      "237/500 [=============>................] - ETA: 4:31 - loss: 1.9171 - regression_loss: 1.4388 - classification_loss: 0.4784\n",
      "238/500 [=============>................] - ETA: 4:30 - loss: 1.9155 - regression_loss: 1.4379 - classification_loss: 0.4776\n",
      "239/500 [=============>................] - ETA: 4:29 - loss: 1.9203 - regression_loss: 1.4422 - classification_loss: 0.4781\n",
      "240/500 [=============>................] - ETA: 4:28 - loss: 1.9175 - regression_loss: 1.4403 - classification_loss: 0.4772\n",
      "241/500 [=============>................] - ETA: 4:27 - loss: 1.9170 - regression_loss: 1.4400 - classification_loss: 0.4770\n",
      "242/500 [=============>................] - ETA: 4:26 - loss: 1.9168 - regression_loss: 1.4398 - classification_loss: 0.4770\n",
      "243/500 [=============>................] - ETA: 4:25 - loss: 1.9185 - regression_loss: 1.4406 - classification_loss: 0.4780\n",
      "244/500 [=============>................] - ETA: 4:23 - loss: 1.9194 - regression_loss: 1.4410 - classification_loss: 0.4783\n",
      "245/500 [=============>................] - ETA: 4:22 - loss: 1.9191 - regression_loss: 1.4406 - classification_loss: 0.4785\n",
      "246/500 [=============>................] - ETA: 4:21 - loss: 1.9193 - regression_loss: 1.4408 - classification_loss: 0.4785\n",
      "247/500 [=============>................] - ETA: 4:20 - loss: 1.9200 - regression_loss: 1.4409 - classification_loss: 0.4791\n",
      "248/500 [=============>................] - ETA: 4:19 - loss: 1.9239 - regression_loss: 1.4430 - classification_loss: 0.4810\n",
      "249/500 [=============>................] - ETA: 4:18 - loss: 1.9223 - regression_loss: 1.4415 - classification_loss: 0.4808\n",
      "250/500 [==============>...............] - ETA: 4:17 - loss: 1.9200 - regression_loss: 1.4397 - classification_loss: 0.4803\n",
      "251/500 [==============>...............] - ETA: 4:16 - loss: 1.9210 - regression_loss: 1.4408 - classification_loss: 0.4803\n",
      "252/500 [==============>...............] - ETA: 4:15 - loss: 1.9225 - regression_loss: 1.4407 - classification_loss: 0.4818\n",
      "253/500 [==============>...............] - ETA: 4:14 - loss: 1.9226 - regression_loss: 1.4415 - classification_loss: 0.4812\n",
      "254/500 [==============>...............] - ETA: 4:13 - loss: 1.9227 - regression_loss: 1.4416 - classification_loss: 0.4811\n",
      "255/500 [==============>...............] - ETA: 4:12 - loss: 1.9204 - regression_loss: 1.4400 - classification_loss: 0.4804\n",
      "256/500 [==============>...............] - ETA: 4:11 - loss: 1.9189 - regression_loss: 1.4385 - classification_loss: 0.4804\n",
      "257/500 [==============>...............] - ETA: 4:10 - loss: 1.9210 - regression_loss: 1.4402 - classification_loss: 0.4808\n",
      "258/500 [==============>...............] - ETA: 4:09 - loss: 1.9216 - regression_loss: 1.4409 - classification_loss: 0.4808\n",
      "259/500 [==============>...............] - ETA: 4:08 - loss: 1.9202 - regression_loss: 1.4397 - classification_loss: 0.4805\n",
      "260/500 [==============>...............] - ETA: 4:07 - loss: 1.9188 - regression_loss: 1.4390 - classification_loss: 0.4798\n",
      "261/500 [==============>...............] - ETA: 4:06 - loss: 1.9193 - regression_loss: 1.4394 - classification_loss: 0.4799\n",
      "262/500 [==============>...............] - ETA: 4:05 - loss: 1.9179 - regression_loss: 1.4379 - classification_loss: 0.4800\n",
      "263/500 [==============>...............] - ETA: 4:04 - loss: 1.9170 - regression_loss: 1.4368 - classification_loss: 0.4803\n",
      "264/500 [==============>...............] - ETA: 4:03 - loss: 1.9212 - regression_loss: 1.4400 - classification_loss: 0.4812\n",
      "265/500 [==============>...............] - ETA: 4:02 - loss: 1.9220 - regression_loss: 1.4407 - classification_loss: 0.4814\n",
      "266/500 [==============>...............] - ETA: 4:01 - loss: 1.9206 - regression_loss: 1.4395 - classification_loss: 0.4811\n",
      "267/500 [===============>..............] - ETA: 4:00 - loss: 1.9205 - regression_loss: 1.4392 - classification_loss: 0.4813\n",
      "268/500 [===============>..............] - ETA: 3:59 - loss: 1.9193 - regression_loss: 1.4384 - classification_loss: 0.4810\n",
      "269/500 [===============>..............] - ETA: 3:58 - loss: 1.9200 - regression_loss: 1.4389 - classification_loss: 0.4811\n",
      "270/500 [===============>..............] - ETA: 3:57 - loss: 1.9214 - regression_loss: 1.4396 - classification_loss: 0.4817\n",
      "271/500 [===============>..............] - ETA: 3:56 - loss: 1.9222 - regression_loss: 1.4400 - classification_loss: 0.4822\n",
      "272/500 [===============>..............] - ETA: 3:55 - loss: 1.9212 - regression_loss: 1.4395 - classification_loss: 0.4817\n",
      "273/500 [===============>..............] - ETA: 3:54 - loss: 1.9178 - regression_loss: 1.4370 - classification_loss: 0.4808\n",
      "274/500 [===============>..............] - ETA: 3:53 - loss: 1.9190 - regression_loss: 1.4381 - classification_loss: 0.4809\n",
      "275/500 [===============>..............] - ETA: 3:52 - loss: 1.9152 - regression_loss: 1.4354 - classification_loss: 0.4798\n",
      "276/500 [===============>..............] - ETA: 3:51 - loss: 1.9148 - regression_loss: 1.4349 - classification_loss: 0.4799\n",
      "277/500 [===============>..............] - ETA: 3:50 - loss: 1.9136 - regression_loss: 1.4343 - classification_loss: 0.4793\n",
      "278/500 [===============>..............] - ETA: 3:49 - loss: 1.9119 - regression_loss: 1.4334 - classification_loss: 0.4785\n",
      "279/500 [===============>..............] - ETA: 3:48 - loss: 1.9079 - regression_loss: 1.4304 - classification_loss: 0.4775\n",
      "280/500 [===============>..............] - ETA: 3:46 - loss: 1.9069 - regression_loss: 1.4299 - classification_loss: 0.4770\n",
      "281/500 [===============>..............] - ETA: 3:45 - loss: 1.9074 - regression_loss: 1.4306 - classification_loss: 0.4769\n",
      "282/500 [===============>..............] - ETA: 3:44 - loss: 1.9043 - regression_loss: 1.4280 - classification_loss: 0.4762\n",
      "283/500 [===============>..............] - ETA: 3:43 - loss: 1.9029 - regression_loss: 1.4269 - classification_loss: 0.4760\n",
      "284/500 [================>.............] - ETA: 3:42 - loss: 1.9037 - regression_loss: 1.4276 - classification_loss: 0.4762\n",
      "285/500 [================>.............] - ETA: 3:41 - loss: 1.9035 - regression_loss: 1.4272 - classification_loss: 0.4763\n",
      "286/500 [================>.............] - ETA: 3:40 - loss: 1.9047 - regression_loss: 1.4281 - classification_loss: 0.4766\n",
      "287/500 [================>.............] - ETA: 3:39 - loss: 1.9039 - regression_loss: 1.4273 - classification_loss: 0.4765\n",
      "288/500 [================>.............] - ETA: 3:38 - loss: 1.9048 - regression_loss: 1.4273 - classification_loss: 0.4774\n",
      "289/500 [================>.............] - ETA: 3:37 - loss: 1.9047 - regression_loss: 1.4276 - classification_loss: 0.4771\n",
      "290/500 [================>.............] - ETA: 3:36 - loss: 1.9084 - regression_loss: 1.4302 - classification_loss: 0.4782\n",
      "291/500 [================>.............] - ETA: 3:35 - loss: 1.9099 - regression_loss: 1.4314 - classification_loss: 0.4784\n",
      "292/500 [================>.............] - ETA: 3:34 - loss: 1.9101 - regression_loss: 1.4316 - classification_loss: 0.4785\n",
      "293/500 [================>.............] - ETA: 3:33 - loss: 1.9091 - regression_loss: 1.4306 - classification_loss: 0.4784\n",
      "294/500 [================>.............] - ETA: 3:32 - loss: 1.9102 - regression_loss: 1.4313 - classification_loss: 0.4789\n",
      "295/500 [================>.............] - ETA: 3:31 - loss: 1.9083 - regression_loss: 1.4302 - classification_loss: 0.4781\n",
      "296/500 [================>.............] - ETA: 3:30 - loss: 1.9064 - regression_loss: 1.4289 - classification_loss: 0.4775\n",
      "297/500 [================>.............] - ETA: 3:29 - loss: 1.9060 - regression_loss: 1.4289 - classification_loss: 0.4771\n",
      "298/500 [================>.............] - ETA: 3:28 - loss: 1.9059 - regression_loss: 1.4287 - classification_loss: 0.4772\n",
      "299/500 [================>.............] - ETA: 3:27 - loss: 1.9062 - regression_loss: 1.4289 - classification_loss: 0.4773\n",
      "300/500 [=================>............] - ETA: 3:26 - loss: 1.9040 - regression_loss: 1.4272 - classification_loss: 0.4767\n",
      "301/500 [=================>............] - ETA: 3:25 - loss: 1.9024 - regression_loss: 1.4264 - classification_loss: 0.4760\n",
      "302/500 [=================>............] - ETA: 3:24 - loss: 1.9023 - regression_loss: 1.4265 - classification_loss: 0.4758\n",
      "303/500 [=================>............] - ETA: 3:23 - loss: 1.9050 - regression_loss: 1.4288 - classification_loss: 0.4762\n",
      "304/500 [=================>............] - ETA: 3:22 - loss: 1.9067 - regression_loss: 1.4297 - classification_loss: 0.4770\n",
      "305/500 [=================>............] - ETA: 3:21 - loss: 1.9046 - regression_loss: 1.4282 - classification_loss: 0.4764\n",
      "306/500 [=================>............] - ETA: 3:20 - loss: 1.9045 - regression_loss: 1.4282 - classification_loss: 0.4763\n",
      "307/500 [=================>............] - ETA: 3:19 - loss: 1.9027 - regression_loss: 1.4264 - classification_loss: 0.4764\n",
      "308/500 [=================>............] - ETA: 3:18 - loss: 1.9023 - regression_loss: 1.4264 - classification_loss: 0.4758\n",
      "309/500 [=================>............] - ETA: 3:16 - loss: 1.9040 - regression_loss: 1.4274 - classification_loss: 0.4766\n",
      "310/500 [=================>............] - ETA: 3:15 - loss: 1.9057 - regression_loss: 1.4285 - classification_loss: 0.4771\n",
      "311/500 [=================>............] - ETA: 3:14 - loss: 1.9034 - regression_loss: 1.4267 - classification_loss: 0.4767\n",
      "312/500 [=================>............] - ETA: 3:13 - loss: 1.9028 - regression_loss: 1.4266 - classification_loss: 0.4762\n",
      "313/500 [=================>............] - ETA: 3:12 - loss: 1.9027 - regression_loss: 1.4267 - classification_loss: 0.4759\n",
      "314/500 [=================>............] - ETA: 3:11 - loss: 1.9024 - regression_loss: 1.4267 - classification_loss: 0.4757\n",
      "315/500 [=================>............] - ETA: 3:10 - loss: 1.9012 - regression_loss: 1.4259 - classification_loss: 0.4752\n",
      "316/500 [=================>............] - ETA: 3:09 - loss: 1.9005 - regression_loss: 1.4255 - classification_loss: 0.4751\n",
      "317/500 [==================>...........] - ETA: 3:08 - loss: 1.9011 - regression_loss: 1.4256 - classification_loss: 0.4754\n",
      "318/500 [==================>...........] - ETA: 3:07 - loss: 1.9023 - regression_loss: 1.4267 - classification_loss: 0.4756\n",
      "319/500 [==================>...........] - ETA: 3:06 - loss: 1.9044 - regression_loss: 1.4281 - classification_loss: 0.4762\n",
      "320/500 [==================>...........] - ETA: 3:05 - loss: 1.9026 - regression_loss: 1.4267 - classification_loss: 0.4759\n",
      "321/500 [==================>...........] - ETA: 3:04 - loss: 1.9005 - regression_loss: 1.4254 - classification_loss: 0.4751\n",
      "322/500 [==================>...........] - ETA: 3:03 - loss: 1.8988 - regression_loss: 1.4241 - classification_loss: 0.4747\n",
      "323/500 [==================>...........] - ETA: 3:02 - loss: 1.8978 - regression_loss: 1.4237 - classification_loss: 0.4741\n",
      "324/500 [==================>...........] - ETA: 3:01 - loss: 1.8969 - regression_loss: 1.4229 - classification_loss: 0.4739\n",
      "325/500 [==================>...........] - ETA: 3:00 - loss: 1.8969 - regression_loss: 1.4225 - classification_loss: 0.4744\n",
      "326/500 [==================>...........] - ETA: 2:59 - loss: 1.8972 - regression_loss: 1.4224 - classification_loss: 0.4748\n",
      "327/500 [==================>...........] - ETA: 2:58 - loss: 1.8946 - regression_loss: 1.4204 - classification_loss: 0.4742\n",
      "328/500 [==================>...........] - ETA: 2:57 - loss: 1.8956 - regression_loss: 1.4214 - classification_loss: 0.4742\n",
      "329/500 [==================>...........] - ETA: 2:56 - loss: 1.8943 - regression_loss: 1.4202 - classification_loss: 0.4741\n",
      "330/500 [==================>...........] - ETA: 2:55 - loss: 1.8924 - regression_loss: 1.4187 - classification_loss: 0.4737\n",
      "331/500 [==================>...........] - ETA: 2:54 - loss: 1.8928 - regression_loss: 1.4184 - classification_loss: 0.4744\n",
      "332/500 [==================>...........] - ETA: 2:53 - loss: 1.8948 - regression_loss: 1.4201 - classification_loss: 0.4747\n",
      "333/500 [==================>...........] - ETA: 2:51 - loss: 1.8943 - regression_loss: 1.4201 - classification_loss: 0.4742\n",
      "334/500 [===================>..........] - ETA: 2:50 - loss: 1.8950 - regression_loss: 1.4204 - classification_loss: 0.4746\n",
      "335/500 [===================>..........] - ETA: 2:49 - loss: 1.8948 - regression_loss: 1.4200 - classification_loss: 0.4748\n",
      "336/500 [===================>..........] - ETA: 2:48 - loss: 1.8972 - regression_loss: 1.4216 - classification_loss: 0.4756\n",
      "337/500 [===================>..........] - ETA: 2:47 - loss: 1.8999 - regression_loss: 1.4234 - classification_loss: 0.4765\n",
      "338/500 [===================>..........] - ETA: 2:46 - loss: 1.8999 - regression_loss: 1.4234 - classification_loss: 0.4764\n",
      "339/500 [===================>..........] - ETA: 2:45 - loss: 1.9007 - regression_loss: 1.4242 - classification_loss: 0.4765\n",
      "340/500 [===================>..........] - ETA: 2:44 - loss: 1.8997 - regression_loss: 1.4231 - classification_loss: 0.4766\n",
      "341/500 [===================>..........] - ETA: 2:43 - loss: 1.8990 - regression_loss: 1.4225 - classification_loss: 0.4765\n",
      "342/500 [===================>..........] - ETA: 2:42 - loss: 1.9018 - regression_loss: 1.4244 - classification_loss: 0.4775\n",
      "343/500 [===================>..........] - ETA: 2:41 - loss: 1.9002 - regression_loss: 1.4234 - classification_loss: 0.4768\n",
      "344/500 [===================>..........] - ETA: 2:40 - loss: 1.8994 - regression_loss: 1.4225 - classification_loss: 0.4769\n",
      "345/500 [===================>..........] - ETA: 2:39 - loss: 1.8977 - regression_loss: 1.4209 - classification_loss: 0.4768\n",
      "346/500 [===================>..........] - ETA: 2:38 - loss: 1.8964 - regression_loss: 1.4202 - classification_loss: 0.4762\n",
      "347/500 [===================>..........] - ETA: 2:37 - loss: 1.8955 - regression_loss: 1.4195 - classification_loss: 0.4760\n",
      "348/500 [===================>..........] - ETA: 2:36 - loss: 1.8959 - regression_loss: 1.4199 - classification_loss: 0.4760\n",
      "349/500 [===================>..........] - ETA: 2:35 - loss: 1.8950 - regression_loss: 1.4195 - classification_loss: 0.4754\n",
      "350/500 [====================>.........] - ETA: 2:34 - loss: 1.8926 - regression_loss: 1.4178 - classification_loss: 0.4749\n",
      "351/500 [====================>.........] - ETA: 2:33 - loss: 1.8922 - regression_loss: 1.4172 - classification_loss: 0.4750\n",
      "352/500 [====================>.........] - ETA: 2:32 - loss: 1.8905 - regression_loss: 1.4160 - classification_loss: 0.4745\n",
      "353/500 [====================>.........] - ETA: 2:31 - loss: 1.8904 - regression_loss: 1.4164 - classification_loss: 0.4740\n",
      "354/500 [====================>.........] - ETA: 2:30 - loss: 1.8921 - regression_loss: 1.4174 - classification_loss: 0.4747\n",
      "355/500 [====================>.........] - ETA: 2:29 - loss: 1.8909 - regression_loss: 1.4166 - classification_loss: 0.4743\n",
      "356/500 [====================>.........] - ETA: 2:28 - loss: 1.8935 - regression_loss: 1.4187 - classification_loss: 0.4749\n",
      "357/500 [====================>.........] - ETA: 2:27 - loss: 1.8924 - regression_loss: 1.4181 - classification_loss: 0.4743\n",
      "358/500 [====================>.........] - ETA: 2:26 - loss: 1.8933 - regression_loss: 1.4183 - classification_loss: 0.4749\n",
      "359/500 [====================>.........] - ETA: 2:25 - loss: 1.8951 - regression_loss: 1.4197 - classification_loss: 0.4753\n",
      "360/500 [====================>.........] - ETA: 2:24 - loss: 1.8975 - regression_loss: 1.4214 - classification_loss: 0.4761\n",
      "361/500 [====================>.........] - ETA: 2:23 - loss: 1.8968 - regression_loss: 1.4213 - classification_loss: 0.4755\n",
      "362/500 [====================>.........] - ETA: 2:22 - loss: 1.8969 - regression_loss: 1.4215 - classification_loss: 0.4755\n",
      "363/500 [====================>.........] - ETA: 2:21 - loss: 1.8961 - regression_loss: 1.4211 - classification_loss: 0.4750\n",
      "364/500 [====================>.........] - ETA: 2:20 - loss: 1.8973 - regression_loss: 1.4219 - classification_loss: 0.4754\n",
      "365/500 [====================>.........] - ETA: 2:19 - loss: 1.8998 - regression_loss: 1.4238 - classification_loss: 0.4760\n",
      "366/500 [====================>.........] - ETA: 2:18 - loss: 1.8986 - regression_loss: 1.4230 - classification_loss: 0.4756\n",
      "367/500 [=====================>........] - ETA: 2:16 - loss: 1.8994 - regression_loss: 1.4235 - classification_loss: 0.4760\n",
      "368/500 [=====================>........] - ETA: 2:15 - loss: 1.8981 - regression_loss: 1.4223 - classification_loss: 0.4758\n",
      "369/500 [=====================>........] - ETA: 2:14 - loss: 1.8987 - regression_loss: 1.4227 - classification_loss: 0.4760\n",
      "370/500 [=====================>........] - ETA: 2:13 - loss: 1.8976 - regression_loss: 1.4218 - classification_loss: 0.4758\n",
      "371/500 [=====================>........] - ETA: 2:12 - loss: 1.8974 - regression_loss: 1.4218 - classification_loss: 0.4756\n",
      "372/500 [=====================>........] - ETA: 2:11 - loss: 1.8959 - regression_loss: 1.4203 - classification_loss: 0.4756\n",
      "373/500 [=====================>........] - ETA: 2:10 - loss: 1.8959 - regression_loss: 1.4199 - classification_loss: 0.4760\n",
      "374/500 [=====================>........] - ETA: 2:09 - loss: 1.8967 - regression_loss: 1.4202 - classification_loss: 0.4765\n",
      "375/500 [=====================>........] - ETA: 2:08 - loss: 1.8955 - regression_loss: 1.4195 - classification_loss: 0.4760\n",
      "376/500 [=====================>........] - ETA: 2:07 - loss: 1.8966 - regression_loss: 1.4205 - classification_loss: 0.4761\n",
      "377/500 [=====================>........] - ETA: 2:06 - loss: 1.8966 - regression_loss: 1.4208 - classification_loss: 0.4758\n",
      "378/500 [=====================>........] - ETA: 2:05 - loss: 1.8982 - regression_loss: 1.4218 - classification_loss: 0.4764\n",
      "379/500 [=====================>........] - ETA: 2:04 - loss: 1.8976 - regression_loss: 1.4214 - classification_loss: 0.4763\n",
      "380/500 [=====================>........] - ETA: 2:03 - loss: 1.8984 - regression_loss: 1.4219 - classification_loss: 0.4766\n",
      "381/500 [=====================>........] - ETA: 2:02 - loss: 1.8989 - regression_loss: 1.4223 - classification_loss: 0.4766\n",
      "382/500 [=====================>........] - ETA: 2:01 - loss: 1.8997 - regression_loss: 1.4234 - classification_loss: 0.4762\n",
      "383/500 [=====================>........] - ETA: 2:00 - loss: 1.8983 - regression_loss: 1.4227 - classification_loss: 0.4756\n",
      "384/500 [======================>.......] - ETA: 1:59 - loss: 1.8984 - regression_loss: 1.4230 - classification_loss: 0.4754\n",
      "385/500 [======================>.......] - ETA: 1:58 - loss: 1.8994 - regression_loss: 1.4235 - classification_loss: 0.4759\n",
      "386/500 [======================>.......] - ETA: 1:57 - loss: 1.9008 - regression_loss: 1.4243 - classification_loss: 0.4764\n",
      "387/500 [======================>.......] - ETA: 1:56 - loss: 1.9012 - regression_loss: 1.4245 - classification_loss: 0.4767\n",
      "388/500 [======================>.......] - ETA: 1:55 - loss: 1.9007 - regression_loss: 1.4242 - classification_loss: 0.4765\n",
      "389/500 [======================>.......] - ETA: 1:54 - loss: 1.8999 - regression_loss: 1.4238 - classification_loss: 0.4760\n",
      "390/500 [======================>.......] - ETA: 1:53 - loss: 1.8987 - regression_loss: 1.4229 - classification_loss: 0.4758\n",
      "391/500 [======================>.......] - ETA: 1:52 - loss: 1.8998 - regression_loss: 1.4237 - classification_loss: 0.4761\n",
      "392/500 [======================>.......] - ETA: 1:51 - loss: 1.8980 - regression_loss: 1.4223 - classification_loss: 0.4757\n",
      "393/500 [======================>.......] - ETA: 1:50 - loss: 1.8988 - regression_loss: 1.4231 - classification_loss: 0.4757\n",
      "394/500 [======================>.......] - ETA: 1:49 - loss: 1.8988 - regression_loss: 1.4233 - classification_loss: 0.4755\n",
      "395/500 [======================>.......] - ETA: 1:48 - loss: 1.8983 - regression_loss: 1.4230 - classification_loss: 0.4753\n",
      "396/500 [======================>.......] - ETA: 1:47 - loss: 1.8971 - regression_loss: 1.4223 - classification_loss: 0.4748\n",
      "397/500 [======================>.......] - ETA: 1:45 - loss: 1.8964 - regression_loss: 1.4219 - classification_loss: 0.4745\n",
      "398/500 [======================>.......] - ETA: 1:44 - loss: 1.8953 - regression_loss: 1.4210 - classification_loss: 0.4743\n",
      "399/500 [======================>.......] - ETA: 1:43 - loss: 1.8947 - regression_loss: 1.4205 - classification_loss: 0.4742\n",
      "400/500 [=======================>......] - ETA: 1:42 - loss: 1.8959 - regression_loss: 1.4217 - classification_loss: 0.4742\n",
      "401/500 [=======================>......] - ETA: 1:42 - loss: 1.8993 - regression_loss: 1.4239 - classification_loss: 0.4753\n",
      "402/500 [=======================>......] - ETA: 1:41 - loss: 1.8990 - regression_loss: 1.4234 - classification_loss: 0.4756\n",
      "403/500 [=======================>......] - ETA: 1:40 - loss: 1.8984 - regression_loss: 1.4232 - classification_loss: 0.4753\n",
      "404/500 [=======================>......] - ETA: 1:39 - loss: 1.8986 - regression_loss: 1.4235 - classification_loss: 0.4751\n",
      "405/500 [=======================>......] - ETA: 1:38 - loss: 1.8980 - regression_loss: 1.4233 - classification_loss: 0.4747\n",
      "406/500 [=======================>......] - ETA: 1:37 - loss: 1.8972 - regression_loss: 1.4228 - classification_loss: 0.4744\n",
      "407/500 [=======================>......] - ETA: 1:35 - loss: 1.8982 - regression_loss: 1.4236 - classification_loss: 0.4746\n",
      "408/500 [=======================>......] - ETA: 1:34 - loss: 1.8974 - regression_loss: 1.4232 - classification_loss: 0.4743\n",
      "409/500 [=======================>......] - ETA: 1:33 - loss: 1.8973 - regression_loss: 1.4231 - classification_loss: 0.4742\n",
      "410/500 [=======================>......] - ETA: 1:32 - loss: 1.8967 - regression_loss: 1.4228 - classification_loss: 0.4739\n",
      "411/500 [=======================>......] - ETA: 1:31 - loss: 1.8962 - regression_loss: 1.4225 - classification_loss: 0.4737\n",
      "412/500 [=======================>......] - ETA: 1:30 - loss: 1.8976 - regression_loss: 1.4236 - classification_loss: 0.4740\n",
      "413/500 [=======================>......] - ETA: 1:29 - loss: 1.8980 - regression_loss: 1.4238 - classification_loss: 0.4742\n",
      "414/500 [=======================>......] - ETA: 1:28 - loss: 1.8976 - regression_loss: 1.4236 - classification_loss: 0.4740\n",
      "415/500 [=======================>......] - ETA: 1:27 - loss: 1.8977 - regression_loss: 1.4236 - classification_loss: 0.4741\n",
      "416/500 [=======================>......] - ETA: 1:26 - loss: 1.8965 - regression_loss: 1.4227 - classification_loss: 0.4738\n",
      "417/500 [========================>.....] - ETA: 1:25 - loss: 1.8955 - regression_loss: 1.4217 - classification_loss: 0.4737\n",
      "418/500 [========================>.....] - ETA: 1:24 - loss: 1.8944 - regression_loss: 1.4209 - classification_loss: 0.4735\n",
      "419/500 [========================>.....] - ETA: 1:23 - loss: 1.8938 - regression_loss: 1.4203 - classification_loss: 0.4735\n",
      "420/500 [========================>.....] - ETA: 1:22 - loss: 1.8913 - regression_loss: 1.4184 - classification_loss: 0.4729\n",
      "421/500 [========================>.....] - ETA: 1:21 - loss: 1.8914 - regression_loss: 1.4182 - classification_loss: 0.4732\n",
      "422/500 [========================>.....] - ETA: 1:20 - loss: 1.8902 - regression_loss: 1.4174 - classification_loss: 0.4728\n",
      "423/500 [========================>.....] - ETA: 1:19 - loss: 1.8918 - regression_loss: 1.4183 - classification_loss: 0.4735\n",
      "424/500 [========================>.....] - ETA: 1:18 - loss: 1.8923 - regression_loss: 1.4189 - classification_loss: 0.4734\n",
      "425/500 [========================>.....] - ETA: 1:17 - loss: 1.8910 - regression_loss: 1.4180 - classification_loss: 0.4730\n",
      "426/500 [========================>.....] - ETA: 1:16 - loss: 1.8900 - regression_loss: 1.4173 - classification_loss: 0.4726\n",
      "427/500 [========================>.....] - ETA: 1:15 - loss: 1.8906 - regression_loss: 1.4180 - classification_loss: 0.4726\n",
      "428/500 [========================>.....] - ETA: 1:14 - loss: 1.8899 - regression_loss: 1.4175 - classification_loss: 0.4724\n",
      "429/500 [========================>.....] - ETA: 1:13 - loss: 1.8899 - regression_loss: 1.4175 - classification_loss: 0.4724\n",
      "430/500 [========================>.....] - ETA: 1:12 - loss: 1.8878 - regression_loss: 1.4159 - classification_loss: 0.4719\n",
      "431/500 [========================>.....] - ETA: 1:11 - loss: 1.8893 - regression_loss: 1.4168 - classification_loss: 0.4725\n",
      "432/500 [========================>.....] - ETA: 1:10 - loss: 1.8902 - regression_loss: 1.4177 - classification_loss: 0.4725\n",
      "433/500 [========================>.....] - ETA: 1:09 - loss: 1.8890 - regression_loss: 1.4169 - classification_loss: 0.4721\n",
      "434/500 [=========================>....] - ETA: 1:08 - loss: 1.8874 - regression_loss: 1.4156 - classification_loss: 0.4718\n",
      "435/500 [=========================>....] - ETA: 1:07 - loss: 1.8881 - regression_loss: 1.4161 - classification_loss: 0.4720\n",
      "436/500 [=========================>....] - ETA: 1:06 - loss: 1.8869 - regression_loss: 1.4151 - classification_loss: 0.4718\n",
      "437/500 [=========================>....] - ETA: 1:05 - loss: 1.8850 - regression_loss: 1.4135 - classification_loss: 0.4715\n",
      "438/500 [=========================>....] - ETA: 1:04 - loss: 1.8847 - regression_loss: 1.4129 - classification_loss: 0.4718\n",
      "439/500 [=========================>....] - ETA: 1:02 - loss: 1.8848 - regression_loss: 1.4134 - classification_loss: 0.4715\n",
      "440/500 [=========================>....] - ETA: 1:01 - loss: 1.8837 - regression_loss: 1.4127 - classification_loss: 0.4710\n",
      "441/500 [=========================>....] - ETA: 1:00 - loss: 1.8850 - regression_loss: 1.4133 - classification_loss: 0.4717\n",
      "442/500 [=========================>....] - ETA: 59s - loss: 1.8836 - regression_loss: 1.4122 - classification_loss: 0.4714 \n",
      "443/500 [=========================>....] - ETA: 58s - loss: 1.8838 - regression_loss: 1.4123 - classification_loss: 0.4714\n",
      "444/500 [=========================>....] - ETA: 57s - loss: 1.8840 - regression_loss: 1.4122 - classification_loss: 0.4718\n",
      "445/500 [=========================>....] - ETA: 56s - loss: 1.8838 - regression_loss: 1.4119 - classification_loss: 0.4719\n",
      "446/500 [=========================>....] - ETA: 55s - loss: 1.8834 - regression_loss: 1.4114 - classification_loss: 0.4720\n",
      "447/500 [=========================>....] - ETA: 54s - loss: 1.8843 - regression_loss: 1.4114 - classification_loss: 0.4729\n",
      "448/500 [=========================>....] - ETA: 53s - loss: 1.8849 - regression_loss: 1.4116 - classification_loss: 0.4733\n",
      "449/500 [=========================>....] - ETA: 52s - loss: 1.8844 - regression_loss: 1.4107 - classification_loss: 0.4736\n",
      "450/500 [==========================>...] - ETA: 51s - loss: 1.8842 - regression_loss: 1.4107 - classification_loss: 0.4736\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 1.8840 - regression_loss: 1.4103 - classification_loss: 0.4737\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 1.8827 - regression_loss: 1.4090 - classification_loss: 0.4737\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.8840 - regression_loss: 1.4103 - classification_loss: 0.4737\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.8826 - regression_loss: 1.4094 - classification_loss: 0.4732\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.8818 - regression_loss: 1.4088 - classification_loss: 0.4730\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.8818 - regression_loss: 1.4089 - classification_loss: 0.4729\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.8799 - regression_loss: 1.4076 - classification_loss: 0.4723\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.8800 - regression_loss: 1.4075 - classification_loss: 0.4726\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.8789 - regression_loss: 1.4067 - classification_loss: 0.4722\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.8788 - regression_loss: 1.4067 - classification_loss: 0.4722\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 1.8778 - regression_loss: 1.4059 - classification_loss: 0.4719\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 1.8778 - regression_loss: 1.4059 - classification_loss: 0.4719\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 1.8800 - regression_loss: 1.4071 - classification_loss: 0.4729\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 1.8804 - regression_loss: 1.4074 - classification_loss: 0.4730\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 1.8801 - regression_loss: 1.4072 - classification_loss: 0.4728\n",
      "466/500 [==========================>...] - ETA: 35s - loss: 1.8803 - regression_loss: 1.4075 - classification_loss: 0.4728\n",
      "467/500 [===========================>..] - ETA: 34s - loss: 1.8793 - regression_loss: 1.4069 - classification_loss: 0.4723\n",
      "468/500 [===========================>..] - ETA: 33s - loss: 1.8781 - regression_loss: 1.4061 - classification_loss: 0.4720\n",
      "469/500 [===========================>..] - ETA: 31s - loss: 1.8778 - regression_loss: 1.4057 - classification_loss: 0.4721\n",
      "470/500 [===========================>..] - ETA: 30s - loss: 1.8779 - regression_loss: 1.4056 - classification_loss: 0.4723\n",
      "471/500 [===========================>..] - ETA: 29s - loss: 1.8778 - regression_loss: 1.4057 - classification_loss: 0.4722\n",
      "472/500 [===========================>..] - ETA: 28s - loss: 1.8784 - regression_loss: 1.4062 - classification_loss: 0.4722\n",
      "473/500 [===========================>..] - ETA: 27s - loss: 1.8813 - regression_loss: 1.4079 - classification_loss: 0.4734\n",
      "474/500 [===========================>..] - ETA: 26s - loss: 1.8820 - regression_loss: 1.4086 - classification_loss: 0.4734\n",
      "475/500 [===========================>..] - ETA: 25s - loss: 1.8829 - regression_loss: 1.4093 - classification_loss: 0.4736\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 1.8824 - regression_loss: 1.4092 - classification_loss: 0.4732\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.8809 - regression_loss: 1.4082 - classification_loss: 0.4727\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.8794 - regression_loss: 1.4069 - classification_loss: 0.4725\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.8791 - regression_loss: 1.4065 - classification_loss: 0.4726\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.8776 - regression_loss: 1.4054 - classification_loss: 0.4722\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.8764 - regression_loss: 1.4045 - classification_loss: 0.4719\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.8756 - regression_loss: 1.4036 - classification_loss: 0.4720\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.8753 - regression_loss: 1.4033 - classification_loss: 0.4719\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.8741 - regression_loss: 1.4025 - classification_loss: 0.4716\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.8754 - regression_loss: 1.4032 - classification_loss: 0.4722\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.8755 - regression_loss: 1.4031 - classification_loss: 0.4723\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.8769 - regression_loss: 1.4041 - classification_loss: 0.4728\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.8765 - regression_loss: 1.4040 - classification_loss: 0.4725\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.8771 - regression_loss: 1.4044 - classification_loss: 0.4727\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.8773 - regression_loss: 1.4045 - classification_loss: 0.4728\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.8790 - regression_loss: 1.4054 - classification_loss: 0.4735 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.8793 - regression_loss: 1.4058 - classification_loss: 0.4736\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.8803 - regression_loss: 1.4064 - classification_loss: 0.4740\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.8802 - regression_loss: 1.4062 - classification_loss: 0.4740\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.8804 - regression_loss: 1.4067 - classification_loss: 0.4737\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.8800 - regression_loss: 1.4066 - classification_loss: 0.4734\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.8819 - regression_loss: 1.4078 - classification_loss: 0.4741\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.8823 - regression_loss: 1.4085 - classification_loss: 0.4738\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.8824 - regression_loss: 1.4086 - classification_loss: 0.4738\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.8818 - regression_loss: 1.4081 - classification_loss: 0.4737\n",
      "Epoch 00038: saving model to ./snapshots\\resnet50_csv_38.h5\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 9.999998977483753e-16.\n",
      "\n",
      "500/500 [==============================] - 516s 1s/step - loss: 1.8818 - regression_loss: 1.4081 - classification_loss: 0.4737\n",
      "Epoch 39/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 1.6446 - regression_loss: 1.2208 - classification_loss: 0.4238\n",
      "  2/500 [..............................] - ETA: 3:35 - loss: 1.7339 - regression_loss: 1.2947 - classification_loss: 0.4392\n",
      "  3/500 [..............................] - ETA: 5:25 - loss: 1.6095 - regression_loss: 1.1992 - classification_loss: 0.4103\n",
      "  4/500 [..............................] - ETA: 6:17 - loss: 1.6368 - regression_loss: 1.2272 - classification_loss: 0.4095\n",
      "  5/500 [..............................] - ETA: 6:48 - loss: 1.7368 - regression_loss: 1.3137 - classification_loss: 0.4230\n",
      "  6/500 [..............................] - ETA: 7:08 - loss: 1.6479 - regression_loss: 1.2376 - classification_loss: 0.4103\n",
      "  7/500 [..............................] - ETA: 7:15 - loss: 1.6507 - regression_loss: 1.2517 - classification_loss: 0.3990\n",
      "  8/500 [..............................] - ETA: 7:21 - loss: 1.6739 - regression_loss: 1.2612 - classification_loss: 0.4127\n",
      "  9/500 [..............................] - ETA: 7:25 - loss: 1.7145 - regression_loss: 1.2998 - classification_loss: 0.4148\n",
      " 10/500 [..............................] - ETA: 7:33 - loss: 1.7641 - regression_loss: 1.3455 - classification_loss: 0.4187\n",
      " 11/500 [..............................] - ETA: 7:35 - loss: 1.7755 - regression_loss: 1.3500 - classification_loss: 0.4255\n",
      " 12/500 [..............................] - ETA: 7:40 - loss: 1.8189 - regression_loss: 1.3826 - classification_loss: 0.4363\n",
      " 13/500 [..............................] - ETA: 7:40 - loss: 1.8496 - regression_loss: 1.3959 - classification_loss: 0.4537\n",
      " 14/500 [..............................] - ETA: 7:44 - loss: 1.8426 - regression_loss: 1.3915 - classification_loss: 0.4511\n",
      " 15/500 [..............................] - ETA: 7:48 - loss: 1.8360 - regression_loss: 1.3971 - classification_loss: 0.4389\n",
      " 16/500 [..............................] - ETA: 7:51 - loss: 1.8202 - regression_loss: 1.3895 - classification_loss: 0.4308\n",
      " 17/500 [>.............................] - ETA: 7:50 - loss: 1.8158 - regression_loss: 1.3786 - classification_loss: 0.4371\n",
      " 18/500 [>.............................] - ETA: 7:49 - loss: 1.8003 - regression_loss: 1.3675 - classification_loss: 0.4328\n",
      " 19/500 [>.............................] - ETA: 7:48 - loss: 1.7993 - regression_loss: 1.3627 - classification_loss: 0.4366\n",
      " 20/500 [>.............................] - ETA: 7:43 - loss: 1.8024 - regression_loss: 1.3621 - classification_loss: 0.4404\n",
      " 21/500 [>.............................] - ETA: 7:48 - loss: 1.7792 - regression_loss: 1.3479 - classification_loss: 0.4313\n",
      " 22/500 [>.............................] - ETA: 7:44 - loss: 1.7790 - regression_loss: 1.3407 - classification_loss: 0.4383\n",
      " 23/500 [>.............................] - ETA: 7:46 - loss: 1.7879 - regression_loss: 1.3479 - classification_loss: 0.4400\n",
      " 24/500 [>.............................] - ETA: 7:47 - loss: 1.7947 - regression_loss: 1.3582 - classification_loss: 0.4364\n",
      " 25/500 [>.............................] - ETA: 7:46 - loss: 1.7736 - regression_loss: 1.3405 - classification_loss: 0.4331\n",
      " 26/500 [>.............................] - ETA: 7:45 - loss: 1.7510 - regression_loss: 1.3230 - classification_loss: 0.4280\n",
      " 27/500 [>.............................] - ETA: 7:46 - loss: 1.7766 - regression_loss: 1.3450 - classification_loss: 0.4316\n",
      " 28/500 [>.............................] - ETA: 7:47 - loss: 1.7860 - regression_loss: 1.3481 - classification_loss: 0.4379\n",
      " 29/500 [>.............................] - ETA: 7:46 - loss: 1.7840 - regression_loss: 1.3511 - classification_loss: 0.4329\n",
      " 30/500 [>.............................] - ETA: 7:45 - loss: 1.7957 - regression_loss: 1.3565 - classification_loss: 0.4392\n",
      " 31/500 [>.............................] - ETA: 7:45 - loss: 1.8265 - regression_loss: 1.3767 - classification_loss: 0.4498\n",
      " 32/500 [>.............................] - ETA: 7:46 - loss: 1.8220 - regression_loss: 1.3740 - classification_loss: 0.4480\n",
      " 33/500 [>.............................] - ETA: 7:46 - loss: 1.8289 - regression_loss: 1.3737 - classification_loss: 0.4553\n",
      " 34/500 [=>............................] - ETA: 7:45 - loss: 1.8202 - regression_loss: 1.3692 - classification_loss: 0.4509\n",
      " 35/500 [=>............................] - ETA: 7:45 - loss: 1.8182 - regression_loss: 1.3658 - classification_loss: 0.4524\n",
      " 36/500 [=>............................] - ETA: 7:45 - loss: 1.7971 - regression_loss: 1.3503 - classification_loss: 0.4469\n",
      " 37/500 [=>............................] - ETA: 7:45 - loss: 1.7934 - regression_loss: 1.3450 - classification_loss: 0.4485\n",
      " 38/500 [=>............................] - ETA: 7:41 - loss: 1.8069 - regression_loss: 1.3568 - classification_loss: 0.4501\n",
      " 39/500 [=>............................] - ETA: 7:40 - loss: 1.7953 - regression_loss: 1.3475 - classification_loss: 0.4478\n",
      " 40/500 [=>............................] - ETA: 7:42 - loss: 1.7870 - regression_loss: 1.3405 - classification_loss: 0.4465\n",
      " 41/500 [=>............................] - ETA: 7:42 - loss: 1.7747 - regression_loss: 1.3310 - classification_loss: 0.4437\n",
      " 42/500 [=>............................] - ETA: 7:41 - loss: 1.7664 - regression_loss: 1.3256 - classification_loss: 0.4409\n",
      " 43/500 [=>............................] - ETA: 7:41 - loss: 1.7921 - regression_loss: 1.3474 - classification_loss: 0.4446\n",
      " 44/500 [=>............................] - ETA: 7:41 - loss: 1.7903 - regression_loss: 1.3483 - classification_loss: 0.4420\n",
      " 45/500 [=>............................] - ETA: 7:38 - loss: 1.7962 - regression_loss: 1.3491 - classification_loss: 0.4471\n",
      " 46/500 [=>............................] - ETA: 7:39 - loss: 1.8209 - regression_loss: 1.3696 - classification_loss: 0.4513\n",
      " 47/500 [=>............................] - ETA: 7:38 - loss: 1.8193 - regression_loss: 1.3705 - classification_loss: 0.4488\n",
      " 48/500 [=>............................] - ETA: 7:38 - loss: 1.8367 - regression_loss: 1.3831 - classification_loss: 0.4535\n",
      " 49/500 [=>............................] - ETA: 7:36 - loss: 1.8448 - regression_loss: 1.3871 - classification_loss: 0.4576\n",
      " 50/500 [==>...........................] - ETA: 7:36 - loss: 1.8586 - regression_loss: 1.3987 - classification_loss: 0.4599\n",
      " 51/500 [==>...........................] - ETA: 7:35 - loss: 1.8625 - regression_loss: 1.4024 - classification_loss: 0.4601\n",
      " 52/500 [==>...........................] - ETA: 7:34 - loss: 1.8681 - regression_loss: 1.4082 - classification_loss: 0.4600\n",
      " 53/500 [==>...........................] - ETA: 7:34 - loss: 1.8692 - regression_loss: 1.4098 - classification_loss: 0.4594\n",
      " 54/500 [==>...........................] - ETA: 7:33 - loss: 1.8722 - regression_loss: 1.4127 - classification_loss: 0.4594\n",
      " 55/500 [==>...........................] - ETA: 7:32 - loss: 1.8802 - regression_loss: 1.4175 - classification_loss: 0.4627\n",
      " 56/500 [==>...........................] - ETA: 7:32 - loss: 1.8751 - regression_loss: 1.4127 - classification_loss: 0.4624\n",
      " 57/500 [==>...........................] - ETA: 7:31 - loss: 1.8752 - regression_loss: 1.4124 - classification_loss: 0.4628\n",
      " 58/500 [==>...........................] - ETA: 7:31 - loss: 1.8950 - regression_loss: 1.4237 - classification_loss: 0.4713\n",
      " 59/500 [==>...........................] - ETA: 7:30 - loss: 1.8996 - regression_loss: 1.4243 - classification_loss: 0.4753\n",
      " 60/500 [==>...........................] - ETA: 7:28 - loss: 1.8894 - regression_loss: 1.4164 - classification_loss: 0.4730\n",
      " 61/500 [==>...........................] - ETA: 7:27 - loss: 1.9017 - regression_loss: 1.4240 - classification_loss: 0.4778\n",
      " 62/500 [==>...........................] - ETA: 7:27 - loss: 1.9005 - regression_loss: 1.4229 - classification_loss: 0.4776\n",
      " 63/500 [==>...........................] - ETA: 7:25 - loss: 1.8841 - regression_loss: 1.4112 - classification_loss: 0.4729\n",
      " 64/500 [==>...........................] - ETA: 7:22 - loss: 1.8892 - regression_loss: 1.4128 - classification_loss: 0.4764\n",
      " 65/500 [==>...........................] - ETA: 7:23 - loss: 1.9067 - regression_loss: 1.4226 - classification_loss: 0.4842\n",
      " 66/500 [==>...........................] - ETA: 7:22 - loss: 1.9171 - regression_loss: 1.4277 - classification_loss: 0.4895\n",
      " 67/500 [===>..........................] - ETA: 7:21 - loss: 1.9181 - regression_loss: 1.4281 - classification_loss: 0.4899\n",
      " 68/500 [===>..........................] - ETA: 7:20 - loss: 1.9260 - regression_loss: 1.4346 - classification_loss: 0.4914\n",
      " 69/500 [===>..........................] - ETA: 7:19 - loss: 1.9334 - regression_loss: 1.4404 - classification_loss: 0.4930\n",
      " 70/500 [===>..........................] - ETA: 7:18 - loss: 1.9254 - regression_loss: 1.4334 - classification_loss: 0.4920\n",
      " 71/500 [===>..........................] - ETA: 7:17 - loss: 1.9239 - regression_loss: 1.4311 - classification_loss: 0.4928\n",
      " 72/500 [===>..........................] - ETA: 7:17 - loss: 1.9282 - regression_loss: 1.4349 - classification_loss: 0.4934\n",
      " 73/500 [===>..........................] - ETA: 7:14 - loss: 1.9288 - regression_loss: 1.4351 - classification_loss: 0.4937\n",
      " 74/500 [===>..........................] - ETA: 7:14 - loss: 1.9161 - regression_loss: 1.4252 - classification_loss: 0.4909\n",
      " 75/500 [===>..........................] - ETA: 7:13 - loss: 1.9154 - regression_loss: 1.4244 - classification_loss: 0.4910\n",
      " 76/500 [===>..........................] - ETA: 7:12 - loss: 1.9187 - regression_loss: 1.4256 - classification_loss: 0.4931\n",
      " 77/500 [===>..........................] - ETA: 7:12 - loss: 1.9148 - regression_loss: 1.4223 - classification_loss: 0.4925\n",
      " 78/500 [===>..........................] - ETA: 7:11 - loss: 1.9097 - regression_loss: 1.4188 - classification_loss: 0.4909\n",
      " 79/500 [===>..........................] - ETA: 7:10 - loss: 1.9031 - regression_loss: 1.4137 - classification_loss: 0.4893\n",
      " 80/500 [===>..........................] - ETA: 7:09 - loss: 1.9023 - regression_loss: 1.4142 - classification_loss: 0.4881\n",
      " 81/500 [===>..........................] - ETA: 7:08 - loss: 1.9047 - regression_loss: 1.4146 - classification_loss: 0.4901\n",
      " 82/500 [===>..........................] - ETA: 7:07 - loss: 1.9047 - regression_loss: 1.4143 - classification_loss: 0.4904\n",
      " 83/500 [===>..........................] - ETA: 7:06 - loss: 1.8967 - regression_loss: 1.4087 - classification_loss: 0.4880\n",
      " 84/500 [====>.........................] - ETA: 7:05 - loss: 1.9043 - regression_loss: 1.4151 - classification_loss: 0.4892\n",
      " 85/500 [====>.........................] - ETA: 7:04 - loss: 1.8989 - regression_loss: 1.4120 - classification_loss: 0.4869\n",
      " 86/500 [====>.........................] - ETA: 7:03 - loss: 1.8954 - regression_loss: 1.4103 - classification_loss: 0.4851\n",
      " 87/500 [====>.........................] - ETA: 7:02 - loss: 1.8985 - regression_loss: 1.4139 - classification_loss: 0.4846\n",
      " 88/500 [====>.........................] - ETA: 7:02 - loss: 1.8985 - regression_loss: 1.4137 - classification_loss: 0.4848\n",
      " 89/500 [====>.........................] - ETA: 7:01 - loss: 1.8927 - regression_loss: 1.4099 - classification_loss: 0.4828\n",
      " 90/500 [====>.........................] - ETA: 7:00 - loss: 1.8948 - regression_loss: 1.4117 - classification_loss: 0.4831\n",
      " 91/500 [====>.........................] - ETA: 7:00 - loss: 1.8931 - regression_loss: 1.4114 - classification_loss: 0.4816\n",
      " 92/500 [====>.........................] - ETA: 6:58 - loss: 1.8891 - regression_loss: 1.4099 - classification_loss: 0.4793\n",
      " 93/500 [====>.........................] - ETA: 6:58 - loss: 1.8908 - regression_loss: 1.4116 - classification_loss: 0.4792\n",
      " 94/500 [====>.........................] - ETA: 6:57 - loss: 1.8851 - regression_loss: 1.4077 - classification_loss: 0.4774\n",
      " 95/500 [====>.........................] - ETA: 6:56 - loss: 1.8820 - regression_loss: 1.4056 - classification_loss: 0.4763\n",
      " 96/500 [====>.........................] - ETA: 6:55 - loss: 1.8823 - regression_loss: 1.4061 - classification_loss: 0.4762\n",
      " 97/500 [====>.........................] - ETA: 6:53 - loss: 1.8801 - regression_loss: 1.4051 - classification_loss: 0.4750\n",
      " 98/500 [====>.........................] - ETA: 6:52 - loss: 1.8875 - regression_loss: 1.4103 - classification_loss: 0.4771\n",
      " 99/500 [====>.........................] - ETA: 6:51 - loss: 1.8887 - regression_loss: 1.4112 - classification_loss: 0.4775\n",
      "100/500 [=====>........................] - ETA: 6:50 - loss: 1.8851 - regression_loss: 1.4088 - classification_loss: 0.4763\n",
      "101/500 [=====>........................] - ETA: 6:49 - loss: 1.8853 - regression_loss: 1.4099 - classification_loss: 0.4754\n",
      "102/500 [=====>........................] - ETA: 6:48 - loss: 1.8797 - regression_loss: 1.4060 - classification_loss: 0.4738\n",
      "103/500 [=====>........................] - ETA: 6:47 - loss: 1.8736 - regression_loss: 1.4017 - classification_loss: 0.4720\n",
      "104/500 [=====>........................] - ETA: 6:46 - loss: 1.8750 - regression_loss: 1.4026 - classification_loss: 0.4724\n",
      "105/500 [=====>........................] - ETA: 6:46 - loss: 1.8756 - regression_loss: 1.4030 - classification_loss: 0.4726\n",
      "106/500 [=====>........................] - ETA: 6:45 - loss: 1.8782 - regression_loss: 1.4041 - classification_loss: 0.4741\n",
      "107/500 [=====>........................] - ETA: 6:44 - loss: 1.8825 - regression_loss: 1.4085 - classification_loss: 0.4740\n",
      "108/500 [=====>........................] - ETA: 6:43 - loss: 1.8753 - regression_loss: 1.4024 - classification_loss: 0.4729\n",
      "109/500 [=====>........................] - ETA: 6:41 - loss: 1.8735 - regression_loss: 1.4019 - classification_loss: 0.4716\n",
      "110/500 [=====>........................] - ETA: 6:40 - loss: 1.8672 - regression_loss: 1.3974 - classification_loss: 0.4698\n",
      "111/500 [=====>........................] - ETA: 6:40 - loss: 1.8688 - regression_loss: 1.3993 - classification_loss: 0.4695\n",
      "112/500 [=====>........................] - ETA: 6:39 - loss: 1.8732 - regression_loss: 1.4015 - classification_loss: 0.4717\n",
      "113/500 [=====>........................] - ETA: 6:37 - loss: 1.8732 - regression_loss: 1.4008 - classification_loss: 0.4725\n",
      "114/500 [=====>........................] - ETA: 6:36 - loss: 1.8723 - regression_loss: 1.4002 - classification_loss: 0.4721\n",
      "115/500 [=====>........................] - ETA: 6:36 - loss: 1.8651 - regression_loss: 1.3946 - classification_loss: 0.4705\n",
      "116/500 [=====>........................] - ETA: 6:34 - loss: 1.8605 - regression_loss: 1.3908 - classification_loss: 0.4697\n",
      "117/500 [======>.......................] - ETA: 6:33 - loss: 1.8679 - regression_loss: 1.3962 - classification_loss: 0.4717\n",
      "118/500 [======>.......................] - ETA: 6:32 - loss: 1.8640 - regression_loss: 1.3931 - classification_loss: 0.4709\n",
      "119/500 [======>.......................] - ETA: 6:31 - loss: 1.8703 - regression_loss: 1.3983 - classification_loss: 0.4720\n",
      "120/500 [======>.......................] - ETA: 6:30 - loss: 1.8670 - regression_loss: 1.3952 - classification_loss: 0.4718\n",
      "121/500 [======>.......................] - ETA: 6:29 - loss: 1.8628 - regression_loss: 1.3920 - classification_loss: 0.4708\n",
      "122/500 [======>.......................] - ETA: 6:29 - loss: 1.8588 - regression_loss: 1.3877 - classification_loss: 0.4711\n",
      "123/500 [======>.......................] - ETA: 6:28 - loss: 1.8587 - regression_loss: 1.3872 - classification_loss: 0.4715\n",
      "124/500 [======>.......................] - ETA: 6:27 - loss: 1.8537 - regression_loss: 1.3841 - classification_loss: 0.4696\n",
      "125/500 [======>.......................] - ETA: 6:25 - loss: 1.8594 - regression_loss: 1.3882 - classification_loss: 0.4712\n",
      "126/500 [======>.......................] - ETA: 6:24 - loss: 1.8588 - regression_loss: 1.3880 - classification_loss: 0.4708\n",
      "127/500 [======>.......................] - ETA: 6:23 - loss: 1.8565 - regression_loss: 1.3867 - classification_loss: 0.4698\n",
      "128/500 [======>.......................] - ETA: 6:22 - loss: 1.8572 - regression_loss: 1.3883 - classification_loss: 0.4689\n",
      "129/500 [======>.......................] - ETA: 6:21 - loss: 1.8685 - regression_loss: 1.3952 - classification_loss: 0.4734\n",
      "130/500 [======>.......................] - ETA: 6:20 - loss: 1.8743 - regression_loss: 1.3986 - classification_loss: 0.4757\n",
      "131/500 [======>.......................] - ETA: 6:19 - loss: 1.8705 - regression_loss: 1.3954 - classification_loss: 0.4750\n",
      "132/500 [======>.......................] - ETA: 6:18 - loss: 1.8659 - regression_loss: 1.3921 - classification_loss: 0.4738\n",
      "133/500 [======>.......................] - ETA: 6:16 - loss: 1.8670 - regression_loss: 1.3923 - classification_loss: 0.4746\n",
      "134/500 [=======>......................] - ETA: 6:15 - loss: 1.8620 - regression_loss: 1.3884 - classification_loss: 0.4735\n",
      "135/500 [=======>......................] - ETA: 6:14 - loss: 1.8638 - regression_loss: 1.3899 - classification_loss: 0.4739\n",
      "136/500 [=======>......................] - ETA: 6:13 - loss: 1.8602 - regression_loss: 1.3875 - classification_loss: 0.4727\n",
      "137/500 [=======>......................] - ETA: 6:12 - loss: 1.8640 - regression_loss: 1.3901 - classification_loss: 0.4739\n",
      "138/500 [=======>......................] - ETA: 6:11 - loss: 1.8616 - regression_loss: 1.3889 - classification_loss: 0.4727\n",
      "139/500 [=======>......................] - ETA: 6:10 - loss: 1.8625 - regression_loss: 1.3904 - classification_loss: 0.4721\n",
      "140/500 [=======>......................] - ETA: 6:09 - loss: 1.8628 - regression_loss: 1.3893 - classification_loss: 0.4735\n",
      "141/500 [=======>......................] - ETA: 6:08 - loss: 1.8615 - regression_loss: 1.3885 - classification_loss: 0.4731\n",
      "142/500 [=======>......................] - ETA: 6:07 - loss: 1.8576 - regression_loss: 1.3852 - classification_loss: 0.4724\n",
      "143/500 [=======>......................] - ETA: 6:06 - loss: 1.8629 - regression_loss: 1.3874 - classification_loss: 0.4755\n",
      "144/500 [=======>......................] - ETA: 6:05 - loss: 1.8642 - regression_loss: 1.3888 - classification_loss: 0.4753\n",
      "145/500 [=======>......................] - ETA: 6:03 - loss: 1.8645 - regression_loss: 1.3896 - classification_loss: 0.4749\n",
      "146/500 [=======>......................] - ETA: 6:03 - loss: 1.8621 - regression_loss: 1.3879 - classification_loss: 0.4743\n",
      "147/500 [=======>......................] - ETA: 6:02 - loss: 1.8604 - regression_loss: 1.3867 - classification_loss: 0.4737\n",
      "148/500 [=======>......................] - ETA: 6:00 - loss: 1.8558 - regression_loss: 1.3830 - classification_loss: 0.4728\n",
      "149/500 [=======>......................] - ETA: 5:59 - loss: 1.8592 - regression_loss: 1.3856 - classification_loss: 0.4736\n",
      "150/500 [========>.....................] - ETA: 5:59 - loss: 1.8599 - regression_loss: 1.3858 - classification_loss: 0.4741\n",
      "151/500 [========>.....................] - ETA: 5:58 - loss: 1.8611 - regression_loss: 1.3862 - classification_loss: 0.4749\n",
      "152/500 [========>.....................] - ETA: 5:57 - loss: 1.8645 - regression_loss: 1.3887 - classification_loss: 0.4758\n",
      "153/500 [========>.....................] - ETA: 5:55 - loss: 1.8627 - regression_loss: 1.3869 - classification_loss: 0.4758\n",
      "154/500 [========>.....................] - ETA: 5:54 - loss: 1.8687 - regression_loss: 1.3916 - classification_loss: 0.4771\n",
      "155/500 [========>.....................] - ETA: 5:53 - loss: 1.8689 - regression_loss: 1.3912 - classification_loss: 0.4776\n",
      "156/500 [========>.....................] - ETA: 5:52 - loss: 1.8751 - regression_loss: 1.3967 - classification_loss: 0.4784\n",
      "157/500 [========>.....................] - ETA: 5:52 - loss: 1.8750 - regression_loss: 1.3961 - classification_loss: 0.4789\n",
      "158/500 [========>.....................] - ETA: 5:51 - loss: 1.8722 - regression_loss: 1.3936 - classification_loss: 0.4787\n",
      "159/500 [========>.....................] - ETA: 5:50 - loss: 1.8740 - regression_loss: 1.3953 - classification_loss: 0.4787\n",
      "160/500 [========>.....................] - ETA: 5:49 - loss: 1.8711 - regression_loss: 1.3931 - classification_loss: 0.4781\n",
      "161/500 [========>.....................] - ETA: 5:48 - loss: 1.8719 - regression_loss: 1.3940 - classification_loss: 0.4779\n",
      "162/500 [========>.....................] - ETA: 5:47 - loss: 1.8728 - regression_loss: 1.3940 - classification_loss: 0.4788\n",
      "163/500 [========>.....................] - ETA: 5:46 - loss: 1.8776 - regression_loss: 1.3976 - classification_loss: 0.4800\n",
      "164/500 [========>.....................] - ETA: 5:45 - loss: 1.8778 - regression_loss: 1.3983 - classification_loss: 0.4795\n",
      "165/500 [========>.....................] - ETA: 5:44 - loss: 1.8868 - regression_loss: 1.4036 - classification_loss: 0.4832\n",
      "166/500 [========>.....................] - ETA: 5:43 - loss: 1.8840 - regression_loss: 1.4017 - classification_loss: 0.4823\n",
      "167/500 [=========>....................] - ETA: 5:42 - loss: 1.8838 - regression_loss: 1.4018 - classification_loss: 0.4820\n",
      "168/500 [=========>....................] - ETA: 5:41 - loss: 1.8832 - regression_loss: 1.4010 - classification_loss: 0.4823\n",
      "169/500 [=========>....................] - ETA: 5:40 - loss: 1.8789 - regression_loss: 1.3980 - classification_loss: 0.4809\n",
      "170/500 [=========>....................] - ETA: 5:39 - loss: 1.8823 - regression_loss: 1.4006 - classification_loss: 0.4817\n",
      "171/500 [=========>....................] - ETA: 5:38 - loss: 1.8807 - regression_loss: 1.3993 - classification_loss: 0.4814\n",
      "172/500 [=========>....................] - ETA: 5:36 - loss: 1.8778 - regression_loss: 1.3965 - classification_loss: 0.4813\n",
      "173/500 [=========>....................] - ETA: 5:36 - loss: 1.8762 - regression_loss: 1.3957 - classification_loss: 0.4805\n",
      "174/500 [=========>....................] - ETA: 5:35 - loss: 1.8749 - regression_loss: 1.3955 - classification_loss: 0.4795\n",
      "175/500 [=========>....................] - ETA: 5:34 - loss: 1.8693 - regression_loss: 1.3914 - classification_loss: 0.4779\n",
      "176/500 [=========>....................] - ETA: 5:33 - loss: 1.8665 - regression_loss: 1.3892 - classification_loss: 0.4773\n",
      "177/500 [=========>....................] - ETA: 5:32 - loss: 1.8694 - regression_loss: 1.3909 - classification_loss: 0.4785\n",
      "178/500 [=========>....................] - ETA: 5:31 - loss: 1.8681 - regression_loss: 1.3899 - classification_loss: 0.4782\n",
      "179/500 [=========>....................] - ETA: 5:30 - loss: 1.8646 - regression_loss: 1.3876 - classification_loss: 0.4771\n",
      "180/500 [=========>....................] - ETA: 5:29 - loss: 1.8675 - regression_loss: 1.3899 - classification_loss: 0.4777\n",
      "181/500 [=========>....................] - ETA: 5:28 - loss: 1.8725 - regression_loss: 1.3928 - classification_loss: 0.4797\n",
      "182/500 [=========>....................] - ETA: 5:27 - loss: 1.8700 - regression_loss: 1.3909 - classification_loss: 0.4791\n",
      "183/500 [=========>....................] - ETA: 5:25 - loss: 1.8678 - regression_loss: 1.3895 - classification_loss: 0.4783\n",
      "184/500 [==========>...................] - ETA: 5:24 - loss: 1.8624 - regression_loss: 1.3854 - classification_loss: 0.4770\n",
      "185/500 [==========>...................] - ETA: 5:23 - loss: 1.8660 - regression_loss: 1.3876 - classification_loss: 0.4783\n",
      "186/500 [==========>...................] - ETA: 5:22 - loss: 1.8614 - regression_loss: 1.3843 - classification_loss: 0.4771\n",
      "187/500 [==========>...................] - ETA: 5:21 - loss: 1.8622 - regression_loss: 1.3853 - classification_loss: 0.4769\n",
      "188/500 [==========>...................] - ETA: 5:20 - loss: 1.8599 - regression_loss: 1.3832 - classification_loss: 0.4767\n",
      "189/500 [==========>...................] - ETA: 5:19 - loss: 1.8587 - regression_loss: 1.3830 - classification_loss: 0.4756\n",
      "190/500 [==========>...................] - ETA: 5:18 - loss: 1.8587 - regression_loss: 1.3838 - classification_loss: 0.4749\n",
      "191/500 [==========>...................] - ETA: 5:17 - loss: 1.8612 - regression_loss: 1.3863 - classification_loss: 0.4749\n",
      "192/500 [==========>...................] - ETA: 5:16 - loss: 1.8631 - regression_loss: 1.3875 - classification_loss: 0.4756\n",
      "193/500 [==========>...................] - ETA: 5:15 - loss: 1.8596 - regression_loss: 1.3850 - classification_loss: 0.4746\n",
      "194/500 [==========>...................] - ETA: 5:14 - loss: 1.8611 - regression_loss: 1.3857 - classification_loss: 0.4754\n",
      "195/500 [==========>...................] - ETA: 5:13 - loss: 1.8607 - regression_loss: 1.3839 - classification_loss: 0.4768\n",
      "196/500 [==========>...................] - ETA: 5:12 - loss: 1.8615 - regression_loss: 1.3848 - classification_loss: 0.4767\n",
      "197/500 [==========>...................] - ETA: 5:11 - loss: 1.8592 - regression_loss: 1.3829 - classification_loss: 0.4763\n",
      "198/500 [==========>...................] - ETA: 5:10 - loss: 1.8600 - regression_loss: 1.3832 - classification_loss: 0.4768\n",
      "199/500 [==========>...................] - ETA: 5:09 - loss: 1.8602 - regression_loss: 1.3839 - classification_loss: 0.4763\n",
      "200/500 [===========>..................] - ETA: 5:08 - loss: 1.8634 - regression_loss: 1.3864 - classification_loss: 0.4771\n",
      "201/500 [===========>..................] - ETA: 5:07 - loss: 1.8652 - regression_loss: 1.3876 - classification_loss: 0.4777\n",
      "202/500 [===========>..................] - ETA: 5:06 - loss: 1.8672 - regression_loss: 1.3897 - classification_loss: 0.4775\n",
      "203/500 [===========>..................] - ETA: 5:05 - loss: 1.8694 - regression_loss: 1.3916 - classification_loss: 0.4778\n",
      "204/500 [===========>..................] - ETA: 5:04 - loss: 1.8702 - regression_loss: 1.3923 - classification_loss: 0.4778\n",
      "205/500 [===========>..................] - ETA: 5:03 - loss: 1.8733 - regression_loss: 1.3945 - classification_loss: 0.4788\n",
      "206/500 [===========>..................] - ETA: 5:02 - loss: 1.8731 - regression_loss: 1.3945 - classification_loss: 0.4785\n",
      "207/500 [===========>..................] - ETA: 5:01 - loss: 1.8703 - regression_loss: 1.3930 - classification_loss: 0.4773\n",
      "208/500 [===========>..................] - ETA: 4:59 - loss: 1.8706 - regression_loss: 1.3922 - classification_loss: 0.4784\n",
      "209/500 [===========>..................] - ETA: 4:59 - loss: 1.8701 - regression_loss: 1.3923 - classification_loss: 0.4779\n",
      "210/500 [===========>..................] - ETA: 4:57 - loss: 1.8721 - regression_loss: 1.3936 - classification_loss: 0.4785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211/500 [===========>..................] - ETA: 4:57 - loss: 1.8745 - regression_loss: 1.3958 - classification_loss: 0.4787\n",
      "212/500 [===========>..................] - ETA: 4:55 - loss: 1.8743 - regression_loss: 1.3959 - classification_loss: 0.4784\n",
      "213/500 [===========>..................] - ETA: 4:54 - loss: 1.8708 - regression_loss: 1.3937 - classification_loss: 0.4772\n",
      "214/500 [===========>..................] - ETA: 4:53 - loss: 1.8720 - regression_loss: 1.3947 - classification_loss: 0.4773\n",
      "215/500 [===========>..................] - ETA: 4:52 - loss: 1.8732 - regression_loss: 1.3949 - classification_loss: 0.4784\n",
      "216/500 [===========>..................] - ETA: 4:51 - loss: 1.8715 - regression_loss: 1.3933 - classification_loss: 0.4782\n",
      "217/500 [============>.................] - ETA: 4:50 - loss: 1.8676 - regression_loss: 1.3906 - classification_loss: 0.4770\n",
      "218/500 [============>.................] - ETA: 4:49 - loss: 1.8701 - regression_loss: 1.3925 - classification_loss: 0.4776\n",
      "219/500 [============>.................] - ETA: 4:48 - loss: 1.8710 - regression_loss: 1.3931 - classification_loss: 0.4779\n",
      "220/500 [============>.................] - ETA: 4:47 - loss: 1.8692 - regression_loss: 1.3914 - classification_loss: 0.4779\n",
      "221/500 [============>.................] - ETA: 4:46 - loss: 1.8729 - regression_loss: 1.3945 - classification_loss: 0.4784\n",
      "222/500 [============>.................] - ETA: 4:45 - loss: 1.8723 - regression_loss: 1.3944 - classification_loss: 0.4779\n",
      "223/500 [============>.................] - ETA: 4:44 - loss: 1.8692 - regression_loss: 1.3924 - classification_loss: 0.4768\n",
      "224/500 [============>.................] - ETA: 4:43 - loss: 1.8694 - regression_loss: 1.3925 - classification_loss: 0.4770\n",
      "225/500 [============>.................] - ETA: 4:42 - loss: 1.8678 - regression_loss: 1.3912 - classification_loss: 0.4766\n",
      "226/500 [============>.................] - ETA: 4:40 - loss: 1.8678 - regression_loss: 1.3905 - classification_loss: 0.4773\n",
      "227/500 [============>.................] - ETA: 4:39 - loss: 1.8684 - regression_loss: 1.3908 - classification_loss: 0.4775\n",
      "228/500 [============>.................] - ETA: 4:39 - loss: 1.8706 - regression_loss: 1.3919 - classification_loss: 0.4787\n",
      "229/500 [============>.................] - ETA: 4:37 - loss: 1.8741 - regression_loss: 1.3953 - classification_loss: 0.4788\n",
      "230/500 [============>.................] - ETA: 4:36 - loss: 1.8734 - regression_loss: 1.3938 - classification_loss: 0.4797\n",
      "231/500 [============>.................] - ETA: 4:35 - loss: 1.8740 - regression_loss: 1.3933 - classification_loss: 0.4807\n",
      "232/500 [============>.................] - ETA: 4:34 - loss: 1.8754 - regression_loss: 1.3947 - classification_loss: 0.4806\n",
      "233/500 [============>.................] - ETA: 4:33 - loss: 1.8774 - regression_loss: 1.3961 - classification_loss: 0.4812\n",
      "234/500 [=============>................] - ETA: 4:32 - loss: 1.8806 - regression_loss: 1.3986 - classification_loss: 0.4820\n",
      "235/500 [=============>................] - ETA: 4:31 - loss: 1.8802 - regression_loss: 1.3984 - classification_loss: 0.4817\n",
      "236/500 [=============>................] - ETA: 4:30 - loss: 1.8811 - regression_loss: 1.3995 - classification_loss: 0.4816\n",
      "237/500 [=============>................] - ETA: 4:29 - loss: 1.8799 - regression_loss: 1.3987 - classification_loss: 0.4812\n",
      "238/500 [=============>................] - ETA: 4:28 - loss: 1.8803 - regression_loss: 1.3990 - classification_loss: 0.4813\n",
      "239/500 [=============>................] - ETA: 4:27 - loss: 1.8761 - regression_loss: 1.3961 - classification_loss: 0.4800\n",
      "240/500 [=============>................] - ETA: 4:26 - loss: 1.8773 - regression_loss: 1.3960 - classification_loss: 0.4813\n",
      "241/500 [=============>................] - ETA: 4:25 - loss: 1.8753 - regression_loss: 1.3943 - classification_loss: 0.4810\n",
      "242/500 [=============>................] - ETA: 4:24 - loss: 1.8761 - regression_loss: 1.3953 - classification_loss: 0.4808\n",
      "243/500 [=============>................] - ETA: 4:23 - loss: 1.8759 - regression_loss: 1.3948 - classification_loss: 0.4810\n",
      "244/500 [=============>................] - ETA: 4:22 - loss: 1.8760 - regression_loss: 1.3947 - classification_loss: 0.4813\n",
      "245/500 [=============>................] - ETA: 4:21 - loss: 1.8771 - regression_loss: 1.3954 - classification_loss: 0.4817\n",
      "246/500 [=============>................] - ETA: 4:20 - loss: 1.8755 - regression_loss: 1.3943 - classification_loss: 0.4813\n",
      "247/500 [=============>................] - ETA: 4:20 - loss: 1.8756 - regression_loss: 1.3943 - classification_loss: 0.4813\n",
      "248/500 [=============>................] - ETA: 4:19 - loss: 1.8765 - regression_loss: 1.3956 - classification_loss: 0.4809\n",
      "249/500 [=============>................] - ETA: 4:18 - loss: 1.8738 - regression_loss: 1.3936 - classification_loss: 0.4802\n",
      "250/500 [==============>...............] - ETA: 4:17 - loss: 1.8782 - regression_loss: 1.3966 - classification_loss: 0.4815\n",
      "251/500 [==============>...............] - ETA: 4:16 - loss: 1.8784 - regression_loss: 1.3965 - classification_loss: 0.4819\n",
      "252/500 [==============>...............] - ETA: 4:15 - loss: 1.8773 - regression_loss: 1.3956 - classification_loss: 0.4817\n",
      "253/500 [==============>...............] - ETA: 4:14 - loss: 1.8794 - regression_loss: 1.3979 - classification_loss: 0.4815\n",
      "254/500 [==============>...............] - ETA: 4:13 - loss: 1.8800 - regression_loss: 1.3984 - classification_loss: 0.4816\n",
      "255/500 [==============>...............] - ETA: 4:12 - loss: 1.8783 - regression_loss: 1.3971 - classification_loss: 0.4811\n",
      "256/500 [==============>...............] - ETA: 4:10 - loss: 1.8780 - regression_loss: 1.3974 - classification_loss: 0.4806\n",
      "257/500 [==============>...............] - ETA: 4:10 - loss: 1.8778 - regression_loss: 1.3972 - classification_loss: 0.4807\n",
      "258/500 [==============>...............] - ETA: 4:08 - loss: 1.8761 - regression_loss: 1.3962 - classification_loss: 0.4799\n",
      "259/500 [==============>...............] - ETA: 4:07 - loss: 1.8768 - regression_loss: 1.3968 - classification_loss: 0.4799\n",
      "260/500 [==============>...............] - ETA: 4:06 - loss: 1.8763 - regression_loss: 1.3961 - classification_loss: 0.4801\n",
      "261/500 [==============>...............] - ETA: 4:05 - loss: 1.8742 - regression_loss: 1.3948 - classification_loss: 0.4794\n",
      "262/500 [==============>...............] - ETA: 4:04 - loss: 1.8711 - regression_loss: 1.3923 - classification_loss: 0.4788\n",
      "263/500 [==============>...............] - ETA: 4:03 - loss: 1.8727 - regression_loss: 1.3935 - classification_loss: 0.4792\n",
      "264/500 [==============>...............] - ETA: 4:02 - loss: 1.8700 - regression_loss: 1.3914 - classification_loss: 0.4786\n",
      "265/500 [==============>...............] - ETA: 4:01 - loss: 1.8682 - regression_loss: 1.3899 - classification_loss: 0.4783\n",
      "266/500 [==============>...............] - ETA: 4:00 - loss: 1.8686 - regression_loss: 1.3902 - classification_loss: 0.4784\n",
      "267/500 [===============>..............] - ETA: 4:00 - loss: 1.8674 - regression_loss: 1.3896 - classification_loss: 0.4778\n",
      "268/500 [===============>..............] - ETA: 3:59 - loss: 1.8674 - regression_loss: 1.3898 - classification_loss: 0.4776\n",
      "269/500 [===============>..............] - ETA: 3:57 - loss: 1.8647 - regression_loss: 1.3880 - classification_loss: 0.4766\n",
      "270/500 [===============>..............] - ETA: 3:56 - loss: 1.8678 - regression_loss: 1.3901 - classification_loss: 0.4777\n",
      "271/500 [===============>..............] - ETA: 3:55 - loss: 1.8664 - regression_loss: 1.3893 - classification_loss: 0.4771\n",
      "272/500 [===============>..............] - ETA: 3:54 - loss: 1.8673 - regression_loss: 1.3901 - classification_loss: 0.4772\n",
      "273/500 [===============>..............] - ETA: 3:53 - loss: 1.8669 - regression_loss: 1.3900 - classification_loss: 0.4768\n",
      "274/500 [===============>..............] - ETA: 3:52 - loss: 1.8679 - regression_loss: 1.3906 - classification_loss: 0.4774\n",
      "275/500 [===============>..............] - ETA: 3:51 - loss: 1.8671 - regression_loss: 1.3901 - classification_loss: 0.4769\n",
      "276/500 [===============>..............] - ETA: 3:50 - loss: 1.8659 - regression_loss: 1.3895 - classification_loss: 0.4764\n",
      "277/500 [===============>..............] - ETA: 3:49 - loss: 1.8646 - regression_loss: 1.3885 - classification_loss: 0.4762\n",
      "278/500 [===============>..............] - ETA: 3:48 - loss: 1.8633 - regression_loss: 1.3873 - classification_loss: 0.4760\n",
      "279/500 [===============>..............] - ETA: 3:47 - loss: 1.8651 - regression_loss: 1.3884 - classification_loss: 0.4767\n",
      "280/500 [===============>..............] - ETA: 3:46 - loss: 1.8648 - regression_loss: 1.3881 - classification_loss: 0.4767\n",
      "281/500 [===============>..............] - ETA: 3:45 - loss: 1.8695 - regression_loss: 1.3909 - classification_loss: 0.4786\n",
      "282/500 [===============>..............] - ETA: 3:44 - loss: 1.8682 - regression_loss: 1.3901 - classification_loss: 0.4781\n",
      "283/500 [===============>..............] - ETA: 3:43 - loss: 1.8724 - regression_loss: 1.3928 - classification_loss: 0.4797\n",
      "284/500 [================>.............] - ETA: 3:42 - loss: 1.8737 - regression_loss: 1.3937 - classification_loss: 0.4800\n",
      "285/500 [================>.............] - ETA: 3:41 - loss: 1.8729 - regression_loss: 1.3935 - classification_loss: 0.4794\n",
      "286/500 [================>.............] - ETA: 3:40 - loss: 1.8756 - regression_loss: 1.3954 - classification_loss: 0.4803\n",
      "287/500 [================>.............] - ETA: 3:39 - loss: 1.8737 - regression_loss: 1.3939 - classification_loss: 0.4798\n",
      "288/500 [================>.............] - ETA: 3:38 - loss: 1.8741 - regression_loss: 1.3948 - classification_loss: 0.4793\n",
      "289/500 [================>.............] - ETA: 3:37 - loss: 1.8712 - regression_loss: 1.3928 - classification_loss: 0.4784\n",
      "290/500 [================>.............] - ETA: 3:36 - loss: 1.8748 - regression_loss: 1.3947 - classification_loss: 0.4801\n",
      "291/500 [================>.............] - ETA: 3:35 - loss: 1.8734 - regression_loss: 1.3935 - classification_loss: 0.4800\n",
      "292/500 [================>.............] - ETA: 3:34 - loss: 1.8734 - regression_loss: 1.3937 - classification_loss: 0.4797\n",
      "293/500 [================>.............] - ETA: 3:32 - loss: 1.8723 - regression_loss: 1.3928 - classification_loss: 0.4794\n",
      "294/500 [================>.............] - ETA: 3:31 - loss: 1.8737 - regression_loss: 1.3938 - classification_loss: 0.4798\n",
      "295/500 [================>.............] - ETA: 3:30 - loss: 1.8761 - regression_loss: 1.3954 - classification_loss: 0.4807\n",
      "296/500 [================>.............] - ETA: 3:30 - loss: 1.8762 - regression_loss: 1.3949 - classification_loss: 0.4813\n",
      "297/500 [================>.............] - ETA: 3:29 - loss: 1.8739 - regression_loss: 1.3932 - classification_loss: 0.4807\n",
      "298/500 [================>.............] - ETA: 3:28 - loss: 1.8721 - regression_loss: 1.3920 - classification_loss: 0.4801\n",
      "299/500 [================>.............] - ETA: 3:27 - loss: 1.8720 - regression_loss: 1.3918 - classification_loss: 0.4802\n",
      "300/500 [=================>............] - ETA: 3:26 - loss: 1.8718 - regression_loss: 1.3919 - classification_loss: 0.4799\n",
      "301/500 [=================>............] - ETA: 3:25 - loss: 1.8764 - regression_loss: 1.3949 - classification_loss: 0.4815\n",
      "302/500 [=================>............] - ETA: 3:24 - loss: 1.8760 - regression_loss: 1.3948 - classification_loss: 0.4812\n",
      "303/500 [=================>............] - ETA: 3:23 - loss: 1.8750 - regression_loss: 1.3934 - classification_loss: 0.4816\n",
      "304/500 [=================>............] - ETA: 3:22 - loss: 1.8737 - regression_loss: 1.3922 - classification_loss: 0.4815\n",
      "305/500 [=================>............] - ETA: 3:21 - loss: 1.8728 - regression_loss: 1.3918 - classification_loss: 0.4811\n",
      "306/500 [=================>............] - ETA: 3:20 - loss: 1.8761 - regression_loss: 1.3940 - classification_loss: 0.4821\n",
      "307/500 [=================>............] - ETA: 3:19 - loss: 1.8751 - regression_loss: 1.3929 - classification_loss: 0.4822\n",
      "308/500 [=================>............] - ETA: 3:18 - loss: 1.8765 - regression_loss: 1.3948 - classification_loss: 0.4817\n",
      "309/500 [=================>............] - ETA: 3:17 - loss: 1.8769 - regression_loss: 1.3946 - classification_loss: 0.4823\n",
      "310/500 [=================>............] - ETA: 3:16 - loss: 1.8760 - regression_loss: 1.3938 - classification_loss: 0.4822\n",
      "311/500 [=================>............] - ETA: 3:15 - loss: 1.8750 - regression_loss: 1.3932 - classification_loss: 0.4818\n",
      "312/500 [=================>............] - ETA: 3:14 - loss: 1.8787 - regression_loss: 1.3960 - classification_loss: 0.4826\n",
      "313/500 [=================>............] - ETA: 3:13 - loss: 1.8795 - regression_loss: 1.3963 - classification_loss: 0.4833\n",
      "314/500 [=================>............] - ETA: 3:12 - loss: 1.8774 - regression_loss: 1.3948 - classification_loss: 0.4826\n",
      "315/500 [=================>............] - ETA: 3:11 - loss: 1.8784 - regression_loss: 1.3954 - classification_loss: 0.4830\n",
      "316/500 [=================>............] - ETA: 3:10 - loss: 1.8767 - regression_loss: 1.3941 - classification_loss: 0.4826\n",
      "317/500 [==================>...........] - ETA: 3:09 - loss: 1.8769 - regression_loss: 1.3943 - classification_loss: 0.4826\n",
      "318/500 [==================>...........] - ETA: 3:07 - loss: 1.8744 - regression_loss: 1.3925 - classification_loss: 0.4819\n",
      "319/500 [==================>...........] - ETA: 3:06 - loss: 1.8733 - regression_loss: 1.3913 - classification_loss: 0.4821\n",
      "320/500 [==================>...........] - ETA: 3:05 - loss: 1.8772 - regression_loss: 1.3939 - classification_loss: 0.4833\n",
      "321/500 [==================>...........] - ETA: 3:04 - loss: 1.8759 - regression_loss: 1.3928 - classification_loss: 0.4831\n",
      "322/500 [==================>...........] - ETA: 3:03 - loss: 1.8753 - regression_loss: 1.3923 - classification_loss: 0.4830\n",
      "323/500 [==================>...........] - ETA: 3:02 - loss: 1.8753 - regression_loss: 1.3927 - classification_loss: 0.4826\n",
      "324/500 [==================>...........] - ETA: 3:01 - loss: 1.8749 - regression_loss: 1.3928 - classification_loss: 0.4821\n",
      "325/500 [==================>...........] - ETA: 3:00 - loss: 1.8734 - regression_loss: 1.3919 - classification_loss: 0.4814\n",
      "326/500 [==================>...........] - ETA: 2:59 - loss: 1.8711 - regression_loss: 1.3903 - classification_loss: 0.4808\n",
      "327/500 [==================>...........] - ETA: 2:58 - loss: 1.8732 - regression_loss: 1.3915 - classification_loss: 0.4818\n",
      "328/500 [==================>...........] - ETA: 2:57 - loss: 1.8746 - regression_loss: 1.3922 - classification_loss: 0.4823\n",
      "329/500 [==================>...........] - ETA: 2:56 - loss: 1.8756 - regression_loss: 1.3935 - classification_loss: 0.4820\n",
      "330/500 [==================>...........] - ETA: 2:55 - loss: 1.8768 - regression_loss: 1.3943 - classification_loss: 0.4825\n",
      "331/500 [==================>...........] - ETA: 2:54 - loss: 1.8758 - regression_loss: 1.3937 - classification_loss: 0.4820\n",
      "332/500 [==================>...........] - ETA: 2:53 - loss: 1.8768 - regression_loss: 1.3943 - classification_loss: 0.4825\n",
      "333/500 [==================>...........] - ETA: 2:52 - loss: 1.8776 - regression_loss: 1.3954 - classification_loss: 0.4822\n",
      "334/500 [===================>..........] - ETA: 2:51 - loss: 1.8794 - regression_loss: 1.3969 - classification_loss: 0.4825\n",
      "335/500 [===================>..........] - ETA: 2:50 - loss: 1.8787 - regression_loss: 1.3965 - classification_loss: 0.4823\n",
      "336/500 [===================>..........] - ETA: 2:49 - loss: 1.8765 - regression_loss: 1.3950 - classification_loss: 0.4815\n",
      "337/500 [===================>..........] - ETA: 2:48 - loss: 1.8753 - regression_loss: 1.3940 - classification_loss: 0.4812\n",
      "338/500 [===================>..........] - ETA: 2:47 - loss: 1.8758 - regression_loss: 1.3942 - classification_loss: 0.4816\n",
      "339/500 [===================>..........] - ETA: 2:46 - loss: 1.8735 - regression_loss: 1.3925 - classification_loss: 0.4810\n",
      "340/500 [===================>..........] - ETA: 2:45 - loss: 1.8733 - regression_loss: 1.3925 - classification_loss: 0.4807\n",
      "341/500 [===================>..........] - ETA: 2:44 - loss: 1.8723 - regression_loss: 1.3920 - classification_loss: 0.4803\n",
      "342/500 [===================>..........] - ETA: 2:43 - loss: 1.8720 - regression_loss: 1.3919 - classification_loss: 0.4801\n",
      "343/500 [===================>..........] - ETA: 2:42 - loss: 1.8728 - regression_loss: 1.3927 - classification_loss: 0.4801\n",
      "344/500 [===================>..........] - ETA: 2:41 - loss: 1.8707 - regression_loss: 1.3910 - classification_loss: 0.4797\n",
      "345/500 [===================>..........] - ETA: 2:40 - loss: 1.8725 - regression_loss: 1.3926 - classification_loss: 0.4799\n",
      "346/500 [===================>..........] - ETA: 2:39 - loss: 1.8711 - regression_loss: 1.3916 - classification_loss: 0.4795\n",
      "347/500 [===================>..........] - ETA: 2:38 - loss: 1.8702 - regression_loss: 1.3910 - classification_loss: 0.4792\n",
      "348/500 [===================>..........] - ETA: 2:37 - loss: 1.8719 - regression_loss: 1.3921 - classification_loss: 0.4799\n",
      "349/500 [===================>..........] - ETA: 2:36 - loss: 1.8719 - regression_loss: 1.3924 - classification_loss: 0.4795\n",
      "350/500 [====================>.........] - ETA: 2:34 - loss: 1.8739 - regression_loss: 1.3939 - classification_loss: 0.4800\n",
      "351/500 [====================>.........] - ETA: 2:33 - loss: 1.8747 - regression_loss: 1.3945 - classification_loss: 0.4802\n",
      "352/500 [====================>.........] - ETA: 2:32 - loss: 1.8781 - regression_loss: 1.3968 - classification_loss: 0.4813\n",
      "353/500 [====================>.........] - ETA: 2:31 - loss: 1.8773 - regression_loss: 1.3966 - classification_loss: 0.4807\n",
      "354/500 [====================>.........] - ETA: 2:30 - loss: 1.8751 - regression_loss: 1.3950 - classification_loss: 0.4801\n",
      "355/500 [====================>.........] - ETA: 2:29 - loss: 1.8760 - regression_loss: 1.3957 - classification_loss: 0.4802\n",
      "356/500 [====================>.........] - ETA: 2:28 - loss: 1.8752 - regression_loss: 1.3951 - classification_loss: 0.4801\n",
      "357/500 [====================>.........] - ETA: 2:27 - loss: 1.8748 - regression_loss: 1.3949 - classification_loss: 0.4799\n",
      "358/500 [====================>.........] - ETA: 2:26 - loss: 1.8744 - regression_loss: 1.3947 - classification_loss: 0.4797\n",
      "359/500 [====================>.........] - ETA: 2:25 - loss: 1.8740 - regression_loss: 1.3948 - classification_loss: 0.4792\n",
      "360/500 [====================>.........] - ETA: 2:24 - loss: 1.8770 - regression_loss: 1.3970 - classification_loss: 0.4800\n",
      "361/500 [====================>.........] - ETA: 2:23 - loss: 1.8770 - regression_loss: 1.3968 - classification_loss: 0.4802\n",
      "362/500 [====================>.........] - ETA: 2:22 - loss: 1.8801 - regression_loss: 1.3990 - classification_loss: 0.4811\n",
      "363/500 [====================>.........] - ETA: 2:21 - loss: 1.8796 - regression_loss: 1.3988 - classification_loss: 0.4808\n",
      "364/500 [====================>.........] - ETA: 2:20 - loss: 1.8819 - regression_loss: 1.4009 - classification_loss: 0.4810\n",
      "365/500 [====================>.........] - ETA: 2:19 - loss: 1.8835 - regression_loss: 1.4024 - classification_loss: 0.4811\n",
      "366/500 [====================>.........] - ETA: 2:18 - loss: 1.8819 - regression_loss: 1.4012 - classification_loss: 0.4806\n",
      "367/500 [=====================>........] - ETA: 2:17 - loss: 1.8829 - regression_loss: 1.4019 - classification_loss: 0.4809\n",
      "368/500 [=====================>........] - ETA: 2:16 - loss: 1.8822 - regression_loss: 1.4013 - classification_loss: 0.4809\n",
      "369/500 [=====================>........] - ETA: 2:15 - loss: 1.8847 - regression_loss: 1.4029 - classification_loss: 0.4818\n",
      "370/500 [=====================>........] - ETA: 2:14 - loss: 1.8851 - regression_loss: 1.4030 - classification_loss: 0.4821\n",
      "371/500 [=====================>........] - ETA: 2:13 - loss: 1.8849 - regression_loss: 1.4030 - classification_loss: 0.4819\n",
      "372/500 [=====================>........] - ETA: 2:12 - loss: 1.8838 - regression_loss: 1.4020 - classification_loss: 0.4818\n",
      "373/500 [=====================>........] - ETA: 2:11 - loss: 1.8857 - regression_loss: 1.4034 - classification_loss: 0.4823\n",
      "374/500 [=====================>........] - ETA: 2:10 - loss: 1.8853 - regression_loss: 1.4031 - classification_loss: 0.4822\n",
      "375/500 [=====================>........] - ETA: 2:09 - loss: 1.8840 - regression_loss: 1.4022 - classification_loss: 0.4818\n",
      "376/500 [=====================>........] - ETA: 2:08 - loss: 1.8837 - regression_loss: 1.4022 - classification_loss: 0.4815\n",
      "377/500 [=====================>........] - ETA: 2:07 - loss: 1.8829 - regression_loss: 1.4015 - classification_loss: 0.4813\n",
      "378/500 [=====================>........] - ETA: 2:06 - loss: 1.8822 - regression_loss: 1.4011 - classification_loss: 0.4811\n",
      "379/500 [=====================>........] - ETA: 2:05 - loss: 1.8823 - regression_loss: 1.4016 - classification_loss: 0.4807\n",
      "380/500 [=====================>........] - ETA: 2:04 - loss: 1.8815 - regression_loss: 1.4011 - classification_loss: 0.4804\n",
      "381/500 [=====================>........] - ETA: 2:03 - loss: 1.8818 - regression_loss: 1.4014 - classification_loss: 0.4804\n",
      "382/500 [=====================>........] - ETA: 2:02 - loss: 1.8824 - regression_loss: 1.4019 - classification_loss: 0.4805\n",
      "383/500 [=====================>........] - ETA: 2:01 - loss: 1.8834 - regression_loss: 1.4030 - classification_loss: 0.4804\n",
      "384/500 [======================>.......] - ETA: 2:00 - loss: 1.8842 - regression_loss: 1.4034 - classification_loss: 0.4808\n",
      "385/500 [======================>.......] - ETA: 1:58 - loss: 1.8848 - regression_loss: 1.4033 - classification_loss: 0.4815\n",
      "386/500 [======================>.......] - ETA: 1:57 - loss: 1.8855 - regression_loss: 1.4042 - classification_loss: 0.4813\n",
      "387/500 [======================>.......] - ETA: 1:56 - loss: 1.8863 - regression_loss: 1.4052 - classification_loss: 0.4812\n",
      "388/500 [======================>.......] - ETA: 1:55 - loss: 1.8871 - regression_loss: 1.4058 - classification_loss: 0.4813\n",
      "389/500 [======================>.......] - ETA: 1:54 - loss: 1.8866 - regression_loss: 1.4057 - classification_loss: 0.4809\n",
      "390/500 [======================>.......] - ETA: 1:53 - loss: 1.8860 - regression_loss: 1.4053 - classification_loss: 0.4807\n",
      "391/500 [======================>.......] - ETA: 1:52 - loss: 1.8843 - regression_loss: 1.4040 - classification_loss: 0.4803\n",
      "392/500 [======================>.......] - ETA: 1:51 - loss: 1.8844 - regression_loss: 1.4042 - classification_loss: 0.4802\n",
      "393/500 [======================>.......] - ETA: 1:50 - loss: 1.8852 - regression_loss: 1.4050 - classification_loss: 0.4802\n",
      "394/500 [======================>.......] - ETA: 1:49 - loss: 1.8880 - regression_loss: 1.4070 - classification_loss: 0.4811\n",
      "395/500 [======================>.......] - ETA: 1:48 - loss: 1.8873 - regression_loss: 1.4065 - classification_loss: 0.4808\n",
      "396/500 [======================>.......] - ETA: 1:47 - loss: 1.8875 - regression_loss: 1.4066 - classification_loss: 0.4809\n",
      "397/500 [======================>.......] - ETA: 1:46 - loss: 1.8886 - regression_loss: 1.4075 - classification_loss: 0.4811\n",
      "398/500 [======================>.......] - ETA: 1:45 - loss: 1.8876 - regression_loss: 1.4070 - classification_loss: 0.4806\n",
      "399/500 [======================>.......] - ETA: 1:44 - loss: 1.8870 - regression_loss: 1.4068 - classification_loss: 0.4802\n",
      "400/500 [=======================>......] - ETA: 1:43 - loss: 1.8872 - regression_loss: 1.4070 - classification_loss: 0.4801\n",
      "401/500 [=======================>......] - ETA: 1:42 - loss: 1.8869 - regression_loss: 1.4062 - classification_loss: 0.4807\n",
      "402/500 [=======================>......] - ETA: 1:41 - loss: 1.8849 - regression_loss: 1.4046 - classification_loss: 0.4803\n",
      "403/500 [=======================>......] - ETA: 1:40 - loss: 1.8853 - regression_loss: 1.4050 - classification_loss: 0.4803\n",
      "404/500 [=======================>......] - ETA: 1:39 - loss: 1.8855 - regression_loss: 1.4054 - classification_loss: 0.4801\n",
      "405/500 [=======================>......] - ETA: 1:38 - loss: 1.8838 - regression_loss: 1.4042 - classification_loss: 0.4796\n",
      "406/500 [=======================>......] - ETA: 1:37 - loss: 1.8830 - regression_loss: 1.4034 - classification_loss: 0.4796\n",
      "407/500 [=======================>......] - ETA: 1:36 - loss: 1.8839 - regression_loss: 1.4043 - classification_loss: 0.4796\n",
      "408/500 [=======================>......] - ETA: 1:35 - loss: 1.8838 - regression_loss: 1.4045 - classification_loss: 0.4793\n",
      "409/500 [=======================>......] - ETA: 1:34 - loss: 1.8844 - regression_loss: 1.4048 - classification_loss: 0.4796\n",
      "410/500 [=======================>......] - ETA: 1:33 - loss: 1.8849 - regression_loss: 1.4050 - classification_loss: 0.4798\n",
      "411/500 [=======================>......] - ETA: 1:32 - loss: 1.8847 - regression_loss: 1.4048 - classification_loss: 0.4799\n",
      "412/500 [=======================>......] - ETA: 1:30 - loss: 1.8850 - regression_loss: 1.4050 - classification_loss: 0.4800\n",
      "413/500 [=======================>......] - ETA: 1:29 - loss: 1.8841 - regression_loss: 1.4041 - classification_loss: 0.4799\n",
      "414/500 [=======================>......] - ETA: 1:28 - loss: 1.8849 - regression_loss: 1.4048 - classification_loss: 0.4801\n",
      "415/500 [=======================>......] - ETA: 1:27 - loss: 1.8861 - regression_loss: 1.4059 - classification_loss: 0.4801\n",
      "416/500 [=======================>......] - ETA: 1:26 - loss: 1.8879 - regression_loss: 1.4067 - classification_loss: 0.4812\n",
      "417/500 [========================>.....] - ETA: 1:25 - loss: 1.8882 - regression_loss: 1.4070 - classification_loss: 0.4812\n",
      "418/500 [========================>.....] - ETA: 1:24 - loss: 1.8877 - regression_loss: 1.4066 - classification_loss: 0.4811\n",
      "419/500 [========================>.....] - ETA: 1:23 - loss: 1.8875 - regression_loss: 1.4064 - classification_loss: 0.4811\n",
      "420/500 [========================>.....] - ETA: 1:22 - loss: 1.8895 - regression_loss: 1.4079 - classification_loss: 0.4816\n",
      "421/500 [========================>.....] - ETA: 1:21 - loss: 1.8897 - regression_loss: 1.4077 - classification_loss: 0.4820\n",
      "422/500 [========================>.....] - ETA: 1:20 - loss: 1.8906 - regression_loss: 1.4083 - classification_loss: 0.4823\n",
      "423/500 [========================>.....] - ETA: 1:19 - loss: 1.8904 - regression_loss: 1.4077 - classification_loss: 0.4827\n",
      "424/500 [========================>.....] - ETA: 1:18 - loss: 1.8920 - regression_loss: 1.4096 - classification_loss: 0.4824\n",
      "425/500 [========================>.....] - ETA: 1:17 - loss: 1.8906 - regression_loss: 1.4086 - classification_loss: 0.4819\n",
      "426/500 [========================>.....] - ETA: 1:16 - loss: 1.8910 - regression_loss: 1.4089 - classification_loss: 0.4821\n",
      "427/500 [========================>.....] - ETA: 1:15 - loss: 1.8902 - regression_loss: 1.4084 - classification_loss: 0.4817\n",
      "428/500 [========================>.....] - ETA: 1:14 - loss: 1.8888 - regression_loss: 1.4074 - classification_loss: 0.4814\n",
      "429/500 [========================>.....] - ETA: 1:13 - loss: 1.8891 - regression_loss: 1.4071 - classification_loss: 0.4819\n",
      "430/500 [========================>.....] - ETA: 1:12 - loss: 1.8895 - regression_loss: 1.4073 - classification_loss: 0.4822\n",
      "431/500 [========================>.....] - ETA: 1:11 - loss: 1.8909 - regression_loss: 1.4082 - classification_loss: 0.4827\n",
      "432/500 [========================>.....] - ETA: 1:10 - loss: 1.8896 - regression_loss: 1.4074 - classification_loss: 0.4822\n",
      "433/500 [========================>.....] - ETA: 1:09 - loss: 1.8878 - regression_loss: 1.4062 - classification_loss: 0.4817\n",
      "434/500 [=========================>....] - ETA: 1:08 - loss: 1.8867 - regression_loss: 1.4052 - classification_loss: 0.4815\n",
      "435/500 [=========================>....] - ETA: 1:07 - loss: 1.8865 - regression_loss: 1.4046 - classification_loss: 0.4818\n",
      "436/500 [=========================>....] - ETA: 1:06 - loss: 1.8888 - regression_loss: 1.4059 - classification_loss: 0.4829\n",
      "437/500 [=========================>....] - ETA: 1:05 - loss: 1.8894 - regression_loss: 1.4063 - classification_loss: 0.4831\n",
      "438/500 [=========================>....] - ETA: 1:04 - loss: 1.8881 - regression_loss: 1.4054 - classification_loss: 0.4827\n",
      "439/500 [=========================>....] - ETA: 1:03 - loss: 1.8874 - regression_loss: 1.4049 - classification_loss: 0.4825\n",
      "440/500 [=========================>....] - ETA: 1:02 - loss: 1.8856 - regression_loss: 1.4034 - classification_loss: 0.4821\n",
      "441/500 [=========================>....] - ETA: 1:01 - loss: 1.8865 - regression_loss: 1.4043 - classification_loss: 0.4823\n",
      "442/500 [=========================>....] - ETA: 1:00 - loss: 1.8858 - regression_loss: 1.4038 - classification_loss: 0.4820\n",
      "443/500 [=========================>....] - ETA: 59s - loss: 1.8865 - regression_loss: 1.4045 - classification_loss: 0.4820 \n",
      "444/500 [=========================>....] - ETA: 58s - loss: 1.8858 - regression_loss: 1.4037 - classification_loss: 0.4821\n",
      "445/500 [=========================>....] - ETA: 57s - loss: 1.8853 - regression_loss: 1.4033 - classification_loss: 0.4820\n",
      "446/500 [=========================>....] - ETA: 56s - loss: 1.8842 - regression_loss: 1.4025 - classification_loss: 0.4817\n",
      "447/500 [=========================>....] - ETA: 55s - loss: 1.8834 - regression_loss: 1.4020 - classification_loss: 0.4814\n",
      "448/500 [=========================>....] - ETA: 53s - loss: 1.8836 - regression_loss: 1.4020 - classification_loss: 0.4815\n",
      "449/500 [=========================>....] - ETA: 52s - loss: 1.8829 - regression_loss: 1.4017 - classification_loss: 0.4813\n",
      "450/500 [==========================>...] - ETA: 51s - loss: 1.8821 - regression_loss: 1.4010 - classification_loss: 0.4812\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 1.8815 - regression_loss: 1.4004 - classification_loss: 0.4812\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 1.8824 - regression_loss: 1.4007 - classification_loss: 0.4817\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.8804 - regression_loss: 1.3990 - classification_loss: 0.4813\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.8783 - regression_loss: 1.3975 - classification_loss: 0.4808\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.8767 - regression_loss: 1.3962 - classification_loss: 0.4805\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.8762 - regression_loss: 1.3956 - classification_loss: 0.4806\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.8745 - regression_loss: 1.3947 - classification_loss: 0.4798\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.8766 - regression_loss: 1.3965 - classification_loss: 0.4801\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.8763 - regression_loss: 1.3963 - classification_loss: 0.4800\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.8755 - regression_loss: 1.3960 - classification_loss: 0.4795\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 1.8757 - regression_loss: 1.3959 - classification_loss: 0.4798\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 1.8768 - regression_loss: 1.3965 - classification_loss: 0.4802\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 1.8772 - regression_loss: 1.3970 - classification_loss: 0.4802\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 1.8758 - regression_loss: 1.3958 - classification_loss: 0.4800\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 1.8758 - regression_loss: 1.3956 - classification_loss: 0.4802\n",
      "466/500 [==========================>...] - ETA: 35s - loss: 1.8750 - regression_loss: 1.3949 - classification_loss: 0.4802\n",
      "467/500 [===========================>..] - ETA: 34s - loss: 1.8764 - regression_loss: 1.3956 - classification_loss: 0.4808\n",
      "468/500 [===========================>..] - ETA: 33s - loss: 1.8752 - regression_loss: 1.3947 - classification_loss: 0.4805\n",
      "469/500 [===========================>..] - ETA: 32s - loss: 1.8744 - regression_loss: 1.3944 - classification_loss: 0.4800\n",
      "470/500 [===========================>..] - ETA: 31s - loss: 1.8748 - regression_loss: 1.3950 - classification_loss: 0.4798\n",
      "471/500 [===========================>..] - ETA: 30s - loss: 1.8746 - regression_loss: 1.3951 - classification_loss: 0.4795\n",
      "472/500 [===========================>..] - ETA: 29s - loss: 1.8742 - regression_loss: 1.3948 - classification_loss: 0.4794\n",
      "473/500 [===========================>..] - ETA: 28s - loss: 1.8739 - regression_loss: 1.3947 - classification_loss: 0.4792\n",
      "474/500 [===========================>..] - ETA: 26s - loss: 1.8747 - regression_loss: 1.3953 - classification_loss: 0.4794\n",
      "475/500 [===========================>..] - ETA: 25s - loss: 1.8746 - regression_loss: 1.3954 - classification_loss: 0.4792\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 1.8755 - regression_loss: 1.3961 - classification_loss: 0.4794\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.8762 - regression_loss: 1.3964 - classification_loss: 0.4798\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.8760 - regression_loss: 1.3960 - classification_loss: 0.4800\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.8754 - regression_loss: 1.3954 - classification_loss: 0.4800\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.8772 - regression_loss: 1.3970 - classification_loss: 0.4802\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.8796 - regression_loss: 1.3989 - classification_loss: 0.4806\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.8801 - regression_loss: 1.3993 - classification_loss: 0.4808\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.8794 - regression_loss: 1.3984 - classification_loss: 0.4810\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.8792 - regression_loss: 1.3983 - classification_loss: 0.4809\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.8773 - regression_loss: 1.3968 - classification_loss: 0.4806\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.8778 - regression_loss: 1.3972 - classification_loss: 0.4806\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.8780 - regression_loss: 1.3976 - classification_loss: 0.4804\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.8785 - regression_loss: 1.3982 - classification_loss: 0.4804\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.8789 - regression_loss: 1.3984 - classification_loss: 0.4804\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.8781 - regression_loss: 1.3979 - classification_loss: 0.4802\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.8808 - regression_loss: 1.3995 - classification_loss: 0.4812 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.8810 - regression_loss: 1.3999 - classification_loss: 0.4811\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.8808 - regression_loss: 1.4000 - classification_loss: 0.4809\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.8800 - regression_loss: 1.3993 - classification_loss: 0.4807\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.8786 - regression_loss: 1.3982 - classification_loss: 0.4804\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.8774 - regression_loss: 1.3971 - classification_loss: 0.4803\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.8765 - regression_loss: 1.3968 - classification_loss: 0.4798\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.8775 - regression_loss: 1.3973 - classification_loss: 0.4802\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.8779 - regression_loss: 1.3976 - classification_loss: 0.4803\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.8791 - regression_loss: 1.3984 - classification_loss: 0.4808\n",
      "Epoch 00039: saving model to ./snapshots\\resnet50_csv_39.h5\n",
      "\n",
      "500/500 [==============================] - 519s 1s/step - loss: 1.8791 - regression_loss: 1.3984 - classification_loss: 0.4808\n",
      "Epoch 40/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 1.8488 - regression_loss: 1.4200 - classification_loss: 0.4287\n",
      "  2/500 [..............................] - ETA: 4:03 - loss: 1.8268 - regression_loss: 1.4067 - classification_loss: 0.4201\n",
      "  3/500 [..............................] - ETA: 5:42 - loss: 1.8812 - regression_loss: 1.4427 - classification_loss: 0.4385\n",
      "  4/500 [..............................] - ETA: 6:17 - loss: 1.6233 - regression_loss: 1.2550 - classification_loss: 0.3683\n",
      "  5/500 [..............................] - ETA: 6:50 - loss: 1.5751 - regression_loss: 1.2246 - classification_loss: 0.3505\n",
      "  6/500 [..............................] - ETA: 7:10 - loss: 1.6480 - regression_loss: 1.2922 - classification_loss: 0.3558\n",
      "  7/500 [..............................] - ETA: 7:24 - loss: 1.6966 - regression_loss: 1.3082 - classification_loss: 0.3884\n",
      "  8/500 [..............................] - ETA: 7:34 - loss: 1.7895 - regression_loss: 1.3757 - classification_loss: 0.4138\n",
      "  9/500 [..............................] - ETA: 7:36 - loss: 1.7674 - regression_loss: 1.3604 - classification_loss: 0.4070\n",
      " 10/500 [..............................] - ETA: 7:42 - loss: 1.8937 - regression_loss: 1.4399 - classification_loss: 0.4538\n",
      " 11/500 [..............................] - ETA: 7:43 - loss: 1.8268 - regression_loss: 1.3963 - classification_loss: 0.4305\n",
      " 12/500 [..............................] - ETA: 7:47 - loss: 1.8259 - regression_loss: 1.3914 - classification_loss: 0.4345\n",
      " 13/500 [..............................] - ETA: 7:47 - loss: 1.8330 - regression_loss: 1.3975 - classification_loss: 0.4355\n",
      " 14/500 [..............................] - ETA: 7:47 - loss: 1.8065 - regression_loss: 1.3829 - classification_loss: 0.4236\n",
      " 15/500 [..............................] - ETA: 7:49 - loss: 1.7940 - regression_loss: 1.3723 - classification_loss: 0.4217\n",
      " 16/500 [..............................] - ETA: 7:52 - loss: 1.7969 - regression_loss: 1.3687 - classification_loss: 0.4281\n",
      " 17/500 [>.............................] - ETA: 7:51 - loss: 1.7980 - regression_loss: 1.3649 - classification_loss: 0.4331\n",
      " 18/500 [>.............................] - ETA: 7:53 - loss: 1.7641 - regression_loss: 1.3386 - classification_loss: 0.4254\n",
      " 19/500 [>.............................] - ETA: 7:52 - loss: 1.7581 - regression_loss: 1.3363 - classification_loss: 0.4218\n",
      " 20/500 [>.............................] - ETA: 7:54 - loss: 1.7989 - regression_loss: 1.3693 - classification_loss: 0.4296\n",
      " 21/500 [>.............................] - ETA: 7:56 - loss: 1.7905 - regression_loss: 1.3577 - classification_loss: 0.4328\n",
      " 22/500 [>.............................] - ETA: 7:55 - loss: 1.8000 - regression_loss: 1.3632 - classification_loss: 0.4368\n",
      " 23/500 [>.............................] - ETA: 7:49 - loss: 1.7785 - regression_loss: 1.3489 - classification_loss: 0.4296\n",
      " 24/500 [>.............................] - ETA: 7:52 - loss: 1.7693 - regression_loss: 1.3411 - classification_loss: 0.4282\n",
      " 25/500 [>.............................] - ETA: 7:52 - loss: 1.7703 - regression_loss: 1.3446 - classification_loss: 0.4257\n",
      " 26/500 [>.............................] - ETA: 7:51 - loss: 1.7741 - regression_loss: 1.3479 - classification_loss: 0.4262\n",
      " 27/500 [>.............................] - ETA: 7:52 - loss: 1.7836 - regression_loss: 1.3530 - classification_loss: 0.4306\n",
      " 28/500 [>.............................] - ETA: 7:52 - loss: 1.7994 - regression_loss: 1.3673 - classification_loss: 0.4321\n",
      " 29/500 [>.............................] - ETA: 7:51 - loss: 1.8337 - regression_loss: 1.3894 - classification_loss: 0.4443\n",
      " 30/500 [>.............................] - ETA: 7:46 - loss: 1.8055 - regression_loss: 1.3679 - classification_loss: 0.4376\n",
      " 31/500 [>.............................] - ETA: 7:47 - loss: 1.7827 - regression_loss: 1.3501 - classification_loss: 0.4326\n",
      " 32/500 [>.............................] - ETA: 7:48 - loss: 1.7845 - regression_loss: 1.3540 - classification_loss: 0.4304\n",
      " 33/500 [>.............................] - ETA: 7:46 - loss: 1.7739 - regression_loss: 1.3474 - classification_loss: 0.4265\n",
      " 34/500 [=>............................] - ETA: 7:42 - loss: 1.7828 - regression_loss: 1.3580 - classification_loss: 0.4249\n",
      " 35/500 [=>............................] - ETA: 7:45 - loss: 1.8167 - regression_loss: 1.3788 - classification_loss: 0.4378\n",
      " 36/500 [=>............................] - ETA: 7:43 - loss: 1.7961 - regression_loss: 1.3610 - classification_loss: 0.4351\n",
      " 37/500 [=>............................] - ETA: 7:42 - loss: 1.8052 - regression_loss: 1.3682 - classification_loss: 0.4370\n",
      " 38/500 [=>............................] - ETA: 7:41 - loss: 1.8165 - regression_loss: 1.3727 - classification_loss: 0.4437\n",
      " 39/500 [=>............................] - ETA: 7:40 - loss: 1.8157 - regression_loss: 1.3691 - classification_loss: 0.4466\n",
      " 40/500 [=>............................] - ETA: 7:40 - loss: 1.8065 - regression_loss: 1.3635 - classification_loss: 0.4430\n",
      " 41/500 [=>............................] - ETA: 7:38 - loss: 1.7930 - regression_loss: 1.3525 - classification_loss: 0.4405\n",
      " 42/500 [=>............................] - ETA: 7:37 - loss: 1.7826 - regression_loss: 1.3436 - classification_loss: 0.4390\n",
      " 43/500 [=>............................] - ETA: 7:36 - loss: 1.7836 - regression_loss: 1.3452 - classification_loss: 0.4383\n",
      " 44/500 [=>............................] - ETA: 7:35 - loss: 1.7777 - regression_loss: 1.3403 - classification_loss: 0.4374\n",
      " 45/500 [=>............................] - ETA: 7:35 - loss: 1.7853 - regression_loss: 1.3453 - classification_loss: 0.4400\n",
      " 46/500 [=>............................] - ETA: 7:34 - loss: 1.7786 - regression_loss: 1.3380 - classification_loss: 0.4407\n",
      " 47/500 [=>............................] - ETA: 7:33 - loss: 1.7905 - regression_loss: 1.3467 - classification_loss: 0.4439\n",
      " 48/500 [=>............................] - ETA: 7:32 - loss: 1.8107 - regression_loss: 1.3600 - classification_loss: 0.4507\n",
      " 49/500 [=>............................] - ETA: 7:31 - loss: 1.8270 - regression_loss: 1.3744 - classification_loss: 0.4526\n",
      " 50/500 [==>...........................] - ETA: 7:30 - loss: 1.8204 - regression_loss: 1.3688 - classification_loss: 0.4516\n",
      " 51/500 [==>...........................] - ETA: 7:29 - loss: 1.8163 - regression_loss: 1.3663 - classification_loss: 0.4500\n",
      " 52/500 [==>...........................] - ETA: 7:28 - loss: 1.8422 - regression_loss: 1.3813 - classification_loss: 0.4609\n",
      " 53/500 [==>...........................] - ETA: 7:27 - loss: 1.8387 - regression_loss: 1.3791 - classification_loss: 0.4595\n",
      " 54/500 [==>...........................] - ETA: 7:26 - loss: 1.8507 - regression_loss: 1.3878 - classification_loss: 0.4629\n",
      " 55/500 [==>...........................] - ETA: 7:25 - loss: 1.8475 - regression_loss: 1.3856 - classification_loss: 0.4619\n",
      " 56/500 [==>...........................] - ETA: 7:24 - loss: 1.8339 - regression_loss: 1.3734 - classification_loss: 0.4605\n",
      " 57/500 [==>...........................] - ETA: 7:23 - loss: 1.8286 - regression_loss: 1.3673 - classification_loss: 0.4613\n",
      " 58/500 [==>...........................] - ETA: 7:22 - loss: 1.8459 - regression_loss: 1.3780 - classification_loss: 0.4679\n",
      " 59/500 [==>...........................] - ETA: 7:21 - loss: 1.8634 - regression_loss: 1.3900 - classification_loss: 0.4734\n",
      " 60/500 [==>...........................] - ETA: 7:20 - loss: 1.8695 - regression_loss: 1.3969 - classification_loss: 0.4726\n",
      " 61/500 [==>...........................] - ETA: 7:20 - loss: 1.8829 - regression_loss: 1.4059 - classification_loss: 0.4769\n",
      " 62/500 [==>...........................] - ETA: 7:19 - loss: 1.8796 - regression_loss: 1.4019 - classification_loss: 0.4777\n",
      " 63/500 [==>...........................] - ETA: 7:18 - loss: 1.8765 - regression_loss: 1.3998 - classification_loss: 0.4766\n",
      " 64/500 [==>...........................] - ETA: 7:18 - loss: 1.8686 - regression_loss: 1.3947 - classification_loss: 0.4739\n",
      " 65/500 [==>...........................] - ETA: 7:17 - loss: 1.8585 - regression_loss: 1.3879 - classification_loss: 0.4706\n",
      " 66/500 [==>...........................] - ETA: 7:16 - loss: 1.8681 - regression_loss: 1.3952 - classification_loss: 0.4729\n",
      " 67/500 [===>..........................] - ETA: 7:14 - loss: 1.8710 - regression_loss: 1.3973 - classification_loss: 0.4737\n",
      " 68/500 [===>..........................] - ETA: 7:14 - loss: 1.8803 - regression_loss: 1.4054 - classification_loss: 0.4750\n",
      " 69/500 [===>..........................] - ETA: 7:13 - loss: 1.8795 - regression_loss: 1.4050 - classification_loss: 0.4745\n",
      " 70/500 [===>..........................] - ETA: 7:12 - loss: 1.8783 - regression_loss: 1.4026 - classification_loss: 0.4758\n",
      " 71/500 [===>..........................] - ETA: 7:11 - loss: 1.8755 - regression_loss: 1.4026 - classification_loss: 0.4729\n",
      " 72/500 [===>..........................] - ETA: 7:10 - loss: 1.8625 - regression_loss: 1.3923 - classification_loss: 0.4701\n",
      " 73/500 [===>..........................] - ETA: 7:10 - loss: 1.8613 - regression_loss: 1.3925 - classification_loss: 0.4688\n",
      " 74/500 [===>..........................] - ETA: 7:09 - loss: 1.8573 - regression_loss: 1.3907 - classification_loss: 0.4666\n",
      " 75/500 [===>..........................] - ETA: 7:09 - loss: 1.8711 - regression_loss: 1.3980 - classification_loss: 0.4730\n",
      " 76/500 [===>..........................] - ETA: 7:08 - loss: 1.8793 - regression_loss: 1.4025 - classification_loss: 0.4768\n",
      " 77/500 [===>..........................] - ETA: 7:06 - loss: 1.8804 - regression_loss: 1.4050 - classification_loss: 0.4754\n",
      " 78/500 [===>..........................] - ETA: 7:05 - loss: 1.8726 - regression_loss: 1.3995 - classification_loss: 0.4731\n",
      " 79/500 [===>..........................] - ETA: 7:05 - loss: 1.8718 - regression_loss: 1.3994 - classification_loss: 0.4724\n",
      " 80/500 [===>..........................] - ETA: 7:04 - loss: 1.8778 - regression_loss: 1.4041 - classification_loss: 0.4737\n",
      " 81/500 [===>..........................] - ETA: 7:03 - loss: 1.8938 - regression_loss: 1.4142 - classification_loss: 0.4796\n",
      " 82/500 [===>..........................] - ETA: 7:02 - loss: 1.9008 - regression_loss: 1.4192 - classification_loss: 0.4816\n",
      " 83/500 [===>..........................] - ETA: 7:00 - loss: 1.9057 - regression_loss: 1.4200 - classification_loss: 0.4856\n",
      " 84/500 [====>.........................] - ETA: 6:59 - loss: 1.9024 - regression_loss: 1.4173 - classification_loss: 0.4851\n",
      " 85/500 [====>.........................] - ETA: 6:58 - loss: 1.8988 - regression_loss: 1.4130 - classification_loss: 0.4859\n",
      " 86/500 [====>.........................] - ETA: 6:58 - loss: 1.8985 - regression_loss: 1.4143 - classification_loss: 0.4842\n",
      " 87/500 [====>.........................] - ETA: 6:57 - loss: 1.8933 - regression_loss: 1.4090 - classification_loss: 0.4843\n",
      " 88/500 [====>.........................] - ETA: 6:55 - loss: 1.8853 - regression_loss: 1.4035 - classification_loss: 0.4818\n",
      " 89/500 [====>.........................] - ETA: 6:53 - loss: 1.8780 - regression_loss: 1.3979 - classification_loss: 0.4801\n",
      " 90/500 [====>.........................] - ETA: 6:53 - loss: 1.8830 - regression_loss: 1.4011 - classification_loss: 0.4818\n",
      " 91/500 [====>.........................] - ETA: 6:52 - loss: 1.8856 - regression_loss: 1.4017 - classification_loss: 0.4839\n",
      " 92/500 [====>.........................] - ETA: 6:51 - loss: 1.8789 - regression_loss: 1.3979 - classification_loss: 0.4810\n",
      " 93/500 [====>.........................] - ETA: 6:50 - loss: 1.8861 - regression_loss: 1.4046 - classification_loss: 0.4815\n",
      " 94/500 [====>.........................] - ETA: 6:50 - loss: 1.8875 - regression_loss: 1.4059 - classification_loss: 0.4816\n",
      " 95/500 [====>.........................] - ETA: 6:47 - loss: 1.8813 - regression_loss: 1.4012 - classification_loss: 0.4800\n",
      " 96/500 [====>.........................] - ETA: 6:47 - loss: 1.8786 - regression_loss: 1.3996 - classification_loss: 0.4790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 97/500 [====>.........................] - ETA: 6:45 - loss: 1.8772 - regression_loss: 1.3966 - classification_loss: 0.4805\n",
      " 98/500 [====>.........................] - ETA: 6:45 - loss: 1.8823 - regression_loss: 1.4015 - classification_loss: 0.4808\n",
      " 99/500 [====>.........................] - ETA: 6:44 - loss: 1.8838 - regression_loss: 1.4017 - classification_loss: 0.4821\n",
      "100/500 [=====>........................] - ETA: 6:43 - loss: 1.8907 - regression_loss: 1.4049 - classification_loss: 0.4858\n",
      "101/500 [=====>........................] - ETA: 6:42 - loss: 1.8907 - regression_loss: 1.4055 - classification_loss: 0.4851\n",
      "102/500 [=====>........................] - ETA: 6:42 - loss: 1.8983 - regression_loss: 1.4114 - classification_loss: 0.4869\n",
      "103/500 [=====>........................] - ETA: 6:41 - loss: 1.9038 - regression_loss: 1.4148 - classification_loss: 0.4890\n",
      "104/500 [=====>........................] - ETA: 6:40 - loss: 1.9059 - regression_loss: 1.4164 - classification_loss: 0.4894\n",
      "105/500 [=====>........................] - ETA: 6:39 - loss: 1.9116 - regression_loss: 1.4204 - classification_loss: 0.4912\n",
      "106/500 [=====>........................] - ETA: 6:38 - loss: 1.9015 - regression_loss: 1.4129 - classification_loss: 0.4887\n",
      "107/500 [=====>........................] - ETA: 6:37 - loss: 1.8955 - regression_loss: 1.4095 - classification_loss: 0.4860\n",
      "108/500 [=====>........................] - ETA: 6:36 - loss: 1.8920 - regression_loss: 1.4072 - classification_loss: 0.4848\n",
      "109/500 [=====>........................] - ETA: 6:34 - loss: 1.8890 - regression_loss: 1.4053 - classification_loss: 0.4837\n",
      "110/500 [=====>........................] - ETA: 6:34 - loss: 1.8881 - regression_loss: 1.4051 - classification_loss: 0.4830\n",
      "111/500 [=====>........................] - ETA: 6:33 - loss: 1.8838 - regression_loss: 1.4019 - classification_loss: 0.4819\n",
      "112/500 [=====>........................] - ETA: 6:32 - loss: 1.8824 - regression_loss: 1.4012 - classification_loss: 0.4812\n",
      "113/500 [=====>........................] - ETA: 6:31 - loss: 1.8862 - regression_loss: 1.4056 - classification_loss: 0.4805\n",
      "114/500 [=====>........................] - ETA: 6:29 - loss: 1.8934 - regression_loss: 1.4083 - classification_loss: 0.4851\n",
      "115/500 [=====>........................] - ETA: 6:28 - loss: 1.8893 - regression_loss: 1.4064 - classification_loss: 0.4828\n",
      "116/500 [=====>........................] - ETA: 6:27 - loss: 1.8866 - regression_loss: 1.4044 - classification_loss: 0.4822\n",
      "117/500 [======>.......................] - ETA: 6:26 - loss: 1.8936 - regression_loss: 1.4091 - classification_loss: 0.4845\n",
      "118/500 [======>.......................] - ETA: 6:26 - loss: 1.8884 - regression_loss: 1.4051 - classification_loss: 0.4834\n",
      "119/500 [======>.......................] - ETA: 6:25 - loss: 1.8965 - regression_loss: 1.4109 - classification_loss: 0.4856\n",
      "120/500 [======>.......................] - ETA: 6:24 - loss: 1.9010 - regression_loss: 1.4159 - classification_loss: 0.4850\n",
      "121/500 [======>.......................] - ETA: 6:23 - loss: 1.9040 - regression_loss: 1.4174 - classification_loss: 0.4865\n",
      "122/500 [======>.......................] - ETA: 6:22 - loss: 1.9104 - regression_loss: 1.4221 - classification_loss: 0.4883\n",
      "123/500 [======>.......................] - ETA: 6:20 - loss: 1.9086 - regression_loss: 1.4202 - classification_loss: 0.4884\n",
      "124/500 [======>.......................] - ETA: 6:20 - loss: 1.9114 - regression_loss: 1.4237 - classification_loss: 0.4877\n",
      "125/500 [======>.......................] - ETA: 6:19 - loss: 1.9116 - regression_loss: 1.4236 - classification_loss: 0.4880\n",
      "126/500 [======>.......................] - ETA: 6:18 - loss: 1.9043 - regression_loss: 1.4177 - classification_loss: 0.4866\n",
      "127/500 [======>.......................] - ETA: 6:16 - loss: 1.9030 - regression_loss: 1.4158 - classification_loss: 0.4871\n",
      "128/500 [======>.......................] - ETA: 6:16 - loss: 1.9062 - regression_loss: 1.4177 - classification_loss: 0.4885\n",
      "129/500 [======>.......................] - ETA: 6:15 - loss: 1.9093 - regression_loss: 1.4175 - classification_loss: 0.4918\n",
      "130/500 [======>.......................] - ETA: 6:14 - loss: 1.9069 - regression_loss: 1.4152 - classification_loss: 0.4917\n",
      "131/500 [======>.......................] - ETA: 6:13 - loss: 1.9091 - regression_loss: 1.4182 - classification_loss: 0.4909\n",
      "132/500 [======>.......................] - ETA: 6:12 - loss: 1.9064 - regression_loss: 1.4150 - classification_loss: 0.4914\n",
      "133/500 [======>.......................] - ETA: 6:10 - loss: 1.9058 - regression_loss: 1.4151 - classification_loss: 0.4907\n",
      "134/500 [=======>......................] - ETA: 6:10 - loss: 1.9089 - regression_loss: 1.4182 - classification_loss: 0.4907\n",
      "135/500 [=======>......................] - ETA: 6:09 - loss: 1.9069 - regression_loss: 1.4165 - classification_loss: 0.4904\n",
      "136/500 [=======>......................] - ETA: 6:08 - loss: 1.9065 - regression_loss: 1.4169 - classification_loss: 0.4895\n",
      "137/500 [=======>......................] - ETA: 6:06 - loss: 1.9051 - regression_loss: 1.4158 - classification_loss: 0.4893\n",
      "138/500 [=======>......................] - ETA: 6:06 - loss: 1.9081 - regression_loss: 1.4185 - classification_loss: 0.4896\n",
      "139/500 [=======>......................] - ETA: 6:05 - loss: 1.9089 - regression_loss: 1.4194 - classification_loss: 0.4896\n",
      "140/500 [=======>......................] - ETA: 6:04 - loss: 1.9109 - regression_loss: 1.4200 - classification_loss: 0.4908\n",
      "141/500 [=======>......................] - ETA: 6:03 - loss: 1.9110 - regression_loss: 1.4198 - classification_loss: 0.4912\n",
      "142/500 [=======>......................] - ETA: 6:02 - loss: 1.9132 - regression_loss: 1.4221 - classification_loss: 0.4911\n",
      "143/500 [=======>......................] - ETA: 6:01 - loss: 1.9159 - regression_loss: 1.4247 - classification_loss: 0.4911\n",
      "144/500 [=======>......................] - ETA: 6:00 - loss: 1.9194 - regression_loss: 1.4278 - classification_loss: 0.4915\n",
      "145/500 [=======>......................] - ETA: 5:59 - loss: 1.9219 - regression_loss: 1.4294 - classification_loss: 0.4925\n",
      "146/500 [=======>......................] - ETA: 5:58 - loss: 1.9203 - regression_loss: 1.4286 - classification_loss: 0.4918\n",
      "147/500 [=======>......................] - ETA: 5:56 - loss: 1.9245 - regression_loss: 1.4328 - classification_loss: 0.4918\n",
      "148/500 [=======>......................] - ETA: 5:56 - loss: 1.9272 - regression_loss: 1.4347 - classification_loss: 0.4924\n",
      "149/500 [=======>......................] - ETA: 5:55 - loss: 1.9260 - regression_loss: 1.4342 - classification_loss: 0.4918\n",
      "150/500 [========>.....................] - ETA: 5:54 - loss: 1.9208 - regression_loss: 1.4304 - classification_loss: 0.4905\n",
      "151/500 [========>.....................] - ETA: 5:53 - loss: 1.9200 - regression_loss: 1.4301 - classification_loss: 0.4900\n",
      "152/500 [========>.....................] - ETA: 5:52 - loss: 1.9179 - regression_loss: 1.4283 - classification_loss: 0.4896\n",
      "153/500 [========>.....................] - ETA: 5:50 - loss: 1.9209 - regression_loss: 1.4307 - classification_loss: 0.4902\n",
      "154/500 [========>.....................] - ETA: 5:50 - loss: 1.9198 - regression_loss: 1.4300 - classification_loss: 0.4898\n",
      "155/500 [========>.....................] - ETA: 5:49 - loss: 1.9213 - regression_loss: 1.4318 - classification_loss: 0.4896\n",
      "156/500 [========>.....................] - ETA: 5:48 - loss: 1.9191 - regression_loss: 1.4297 - classification_loss: 0.4893\n",
      "157/500 [========>.....................] - ETA: 5:47 - loss: 1.9171 - regression_loss: 1.4291 - classification_loss: 0.4880\n",
      "158/500 [========>.....................] - ETA: 5:46 - loss: 1.9205 - regression_loss: 1.4323 - classification_loss: 0.4883\n",
      "159/500 [========>.....................] - ETA: 5:45 - loss: 1.9204 - regression_loss: 1.4315 - classification_loss: 0.4889\n",
      "160/500 [========>.....................] - ETA: 5:44 - loss: 1.9164 - regression_loss: 1.4291 - classification_loss: 0.4874\n",
      "161/500 [========>.....................] - ETA: 5:43 - loss: 1.9110 - regression_loss: 1.4245 - classification_loss: 0.4865\n",
      "162/500 [========>.....................] - ETA: 5:42 - loss: 1.9142 - regression_loss: 1.4279 - classification_loss: 0.4863\n",
      "163/500 [========>.....................] - ETA: 5:41 - loss: 1.9156 - regression_loss: 1.4270 - classification_loss: 0.4886\n",
      "164/500 [========>.....................] - ETA: 5:40 - loss: 1.9158 - regression_loss: 1.4267 - classification_loss: 0.4890\n",
      "165/500 [========>.....................] - ETA: 5:39 - loss: 1.9177 - regression_loss: 1.4285 - classification_loss: 0.4892\n",
      "166/500 [========>.....................] - ETA: 5:38 - loss: 1.9155 - regression_loss: 1.4276 - classification_loss: 0.4880\n",
      "167/500 [=========>....................] - ETA: 5:37 - loss: 1.9170 - regression_loss: 1.4293 - classification_loss: 0.4878\n",
      "168/500 [=========>....................] - ETA: 5:35 - loss: 1.9172 - regression_loss: 1.4293 - classification_loss: 0.4879\n",
      "169/500 [=========>....................] - ETA: 5:35 - loss: 1.9208 - regression_loss: 1.4319 - classification_loss: 0.4889\n",
      "170/500 [=========>....................] - ETA: 5:34 - loss: 1.9219 - regression_loss: 1.4336 - classification_loss: 0.4883\n",
      "171/500 [=========>....................] - ETA: 5:33 - loss: 1.9213 - regression_loss: 1.4330 - classification_loss: 0.4883\n",
      "172/500 [=========>....................] - ETA: 5:32 - loss: 1.9220 - regression_loss: 1.4341 - classification_loss: 0.4878\n",
      "173/500 [=========>....................] - ETA: 5:31 - loss: 1.9204 - regression_loss: 1.4325 - classification_loss: 0.4879\n",
      "174/500 [=========>....................] - ETA: 5:29 - loss: 1.9268 - regression_loss: 1.4367 - classification_loss: 0.4901\n",
      "175/500 [=========>....................] - ETA: 5:29 - loss: 1.9267 - regression_loss: 1.4360 - classification_loss: 0.4907\n",
      "176/500 [=========>....................] - ETA: 5:28 - loss: 1.9240 - regression_loss: 1.4342 - classification_loss: 0.4898\n",
      "177/500 [=========>....................] - ETA: 5:27 - loss: 1.9276 - regression_loss: 1.4376 - classification_loss: 0.4900\n",
      "178/500 [=========>....................] - ETA: 5:26 - loss: 1.9245 - regression_loss: 1.4351 - classification_loss: 0.4895\n",
      "179/500 [=========>....................] - ETA: 5:25 - loss: 1.9211 - regression_loss: 1.4331 - classification_loss: 0.4879\n",
      "180/500 [=========>....................] - ETA: 5:24 - loss: 1.9206 - regression_loss: 1.4326 - classification_loss: 0.4879\n",
      "181/500 [=========>....................] - ETA: 5:23 - loss: 1.9177 - regression_loss: 1.4312 - classification_loss: 0.4864\n",
      "182/500 [=========>....................] - ETA: 5:21 - loss: 1.9127 - regression_loss: 1.4276 - classification_loss: 0.4851\n",
      "183/500 [=========>....................] - ETA: 5:21 - loss: 1.9118 - regression_loss: 1.4267 - classification_loss: 0.4851\n",
      "184/500 [==========>...................] - ETA: 5:20 - loss: 1.9093 - regression_loss: 1.4250 - classification_loss: 0.4844\n",
      "185/500 [==========>...................] - ETA: 5:19 - loss: 1.9102 - regression_loss: 1.4253 - classification_loss: 0.4849\n",
      "186/500 [==========>...................] - ETA: 5:18 - loss: 1.9071 - regression_loss: 1.4231 - classification_loss: 0.4840\n",
      "187/500 [==========>...................] - ETA: 5:17 - loss: 1.9086 - regression_loss: 1.4242 - classification_loss: 0.4844\n",
      "188/500 [==========>...................] - ETA: 5:16 - loss: 1.9103 - regression_loss: 1.4256 - classification_loss: 0.4847\n",
      "189/500 [==========>...................] - ETA: 5:15 - loss: 1.9084 - regression_loss: 1.4244 - classification_loss: 0.4840\n",
      "190/500 [==========>...................] - ETA: 5:14 - loss: 1.9064 - regression_loss: 1.4229 - classification_loss: 0.4835\n",
      "191/500 [==========>...................] - ETA: 5:13 - loss: 1.9092 - regression_loss: 1.4233 - classification_loss: 0.4859\n",
      "192/500 [==========>...................] - ETA: 5:12 - loss: 1.9098 - regression_loss: 1.4242 - classification_loss: 0.4856\n",
      "193/500 [==========>...................] - ETA: 5:11 - loss: 1.9103 - regression_loss: 1.4244 - classification_loss: 0.4858\n",
      "194/500 [==========>...................] - ETA: 5:10 - loss: 1.9077 - regression_loss: 1.4226 - classification_loss: 0.4851\n",
      "195/500 [==========>...................] - ETA: 5:09 - loss: 1.9052 - regression_loss: 1.4210 - classification_loss: 0.4842\n",
      "196/500 [==========>...................] - ETA: 5:08 - loss: 1.9093 - regression_loss: 1.4230 - classification_loss: 0.4863\n",
      "197/500 [==========>...................] - ETA: 5:07 - loss: 1.9103 - regression_loss: 1.4239 - classification_loss: 0.4865\n",
      "198/500 [==========>...................] - ETA: 5:06 - loss: 1.9120 - regression_loss: 1.4259 - classification_loss: 0.4861\n",
      "199/500 [==========>...................] - ETA: 5:05 - loss: 1.9084 - regression_loss: 1.4235 - classification_loss: 0.4849\n",
      "200/500 [===========>..................] - ETA: 5:04 - loss: 1.9082 - regression_loss: 1.4239 - classification_loss: 0.4843\n",
      "201/500 [===========>..................] - ETA: 5:03 - loss: 1.9090 - regression_loss: 1.4247 - classification_loss: 0.4843\n",
      "202/500 [===========>..................] - ETA: 5:02 - loss: 1.9083 - regression_loss: 1.4247 - classification_loss: 0.4836\n",
      "203/500 [===========>..................] - ETA: 5:01 - loss: 1.9050 - regression_loss: 1.4223 - classification_loss: 0.4826\n",
      "204/500 [===========>..................] - ETA: 5:00 - loss: 1.9066 - regression_loss: 1.4244 - classification_loss: 0.4822\n",
      "205/500 [===========>..................] - ETA: 5:00 - loss: 1.9076 - regression_loss: 1.4252 - classification_loss: 0.4824\n",
      "206/500 [===========>..................] - ETA: 4:59 - loss: 1.9058 - regression_loss: 1.4236 - classification_loss: 0.4822\n",
      "207/500 [===========>..................] - ETA: 4:58 - loss: 1.9088 - regression_loss: 1.4253 - classification_loss: 0.4835\n",
      "208/500 [===========>..................] - ETA: 4:57 - loss: 1.9076 - regression_loss: 1.4245 - classification_loss: 0.4831\n",
      "209/500 [===========>..................] - ETA: 4:56 - loss: 1.9043 - regression_loss: 1.4222 - classification_loss: 0.4821\n",
      "210/500 [===========>..................] - ETA: 4:55 - loss: 1.9055 - regression_loss: 1.4227 - classification_loss: 0.4827\n",
      "211/500 [===========>..................] - ETA: 4:54 - loss: 1.9038 - regression_loss: 1.4216 - classification_loss: 0.4821\n",
      "212/500 [===========>..................] - ETA: 4:53 - loss: 1.9090 - regression_loss: 1.4250 - classification_loss: 0.4840\n",
      "213/500 [===========>..................] - ETA: 4:52 - loss: 1.9134 - regression_loss: 1.4280 - classification_loss: 0.4854\n",
      "214/500 [===========>..................] - ETA: 4:51 - loss: 1.9151 - regression_loss: 1.4283 - classification_loss: 0.4868\n",
      "215/500 [===========>..................] - ETA: 4:50 - loss: 1.9174 - regression_loss: 1.4298 - classification_loss: 0.4876\n",
      "216/500 [===========>..................] - ETA: 4:49 - loss: 1.9163 - regression_loss: 1.4289 - classification_loss: 0.4873\n",
      "217/500 [============>.................] - ETA: 4:48 - loss: 1.9129 - regression_loss: 1.4263 - classification_loss: 0.4866\n",
      "218/500 [============>.................] - ETA: 4:47 - loss: 1.9102 - regression_loss: 1.4245 - classification_loss: 0.4857\n",
      "219/500 [============>.................] - ETA: 4:46 - loss: 1.9081 - regression_loss: 1.4231 - classification_loss: 0.4850\n",
      "220/500 [============>.................] - ETA: 4:45 - loss: 1.9079 - regression_loss: 1.4234 - classification_loss: 0.4845\n",
      "221/500 [============>.................] - ETA: 4:44 - loss: 1.9083 - regression_loss: 1.4241 - classification_loss: 0.4843\n",
      "222/500 [============>.................] - ETA: 4:43 - loss: 1.9091 - regression_loss: 1.4252 - classification_loss: 0.4839\n",
      "223/500 [============>.................] - ETA: 4:42 - loss: 1.9080 - regression_loss: 1.4243 - classification_loss: 0.4837\n",
      "224/500 [============>.................] - ETA: 4:41 - loss: 1.9081 - regression_loss: 1.4247 - classification_loss: 0.4834\n",
      "225/500 [============>.................] - ETA: 4:40 - loss: 1.9091 - regression_loss: 1.4248 - classification_loss: 0.4843\n",
      "226/500 [============>.................] - ETA: 4:39 - loss: 1.9068 - regression_loss: 1.4236 - classification_loss: 0.4832\n",
      "227/500 [============>.................] - ETA: 4:38 - loss: 1.9036 - regression_loss: 1.4213 - classification_loss: 0.4824\n",
      "228/500 [============>.................] - ETA: 4:37 - loss: 1.9058 - regression_loss: 1.4231 - classification_loss: 0.4826\n",
      "229/500 [============>.................] - ETA: 4:36 - loss: 1.9083 - regression_loss: 1.4254 - classification_loss: 0.4828\n",
      "230/500 [============>.................] - ETA: 4:35 - loss: 1.9111 - regression_loss: 1.4272 - classification_loss: 0.4839\n",
      "231/500 [============>.................] - ETA: 4:34 - loss: 1.9112 - regression_loss: 1.4274 - classification_loss: 0.4838\n",
      "232/500 [============>.................] - ETA: 4:33 - loss: 1.9108 - regression_loss: 1.4276 - classification_loss: 0.4833\n",
      "233/500 [============>.................] - ETA: 4:32 - loss: 1.9130 - regression_loss: 1.4295 - classification_loss: 0.4835\n",
      "234/500 [=============>................] - ETA: 4:31 - loss: 1.9148 - regression_loss: 1.4305 - classification_loss: 0.4843\n",
      "235/500 [=============>................] - ETA: 4:30 - loss: 1.9124 - regression_loss: 1.4290 - classification_loss: 0.4834\n",
      "236/500 [=============>................] - ETA: 4:29 - loss: 1.9134 - regression_loss: 1.4299 - classification_loss: 0.4835\n",
      "237/500 [=============>................] - ETA: 4:28 - loss: 1.9121 - regression_loss: 1.4286 - classification_loss: 0.4834\n",
      "238/500 [=============>................] - ETA: 4:27 - loss: 1.9103 - regression_loss: 1.4276 - classification_loss: 0.4827\n",
      "239/500 [=============>................] - ETA: 4:26 - loss: 1.9094 - regression_loss: 1.4271 - classification_loss: 0.4822\n",
      "240/500 [=============>................] - ETA: 4:25 - loss: 1.9068 - regression_loss: 1.4250 - classification_loss: 0.4819\n",
      "241/500 [=============>................] - ETA: 4:24 - loss: 1.9078 - regression_loss: 1.4259 - classification_loss: 0.4818\n",
      "242/500 [=============>................] - ETA: 4:23 - loss: 1.9047 - regression_loss: 1.4238 - classification_loss: 0.4809\n",
      "243/500 [=============>................] - ETA: 4:22 - loss: 1.9054 - regression_loss: 1.4244 - classification_loss: 0.4810\n",
      "244/500 [=============>................] - ETA: 4:21 - loss: 1.9048 - regression_loss: 1.4230 - classification_loss: 0.4818\n",
      "245/500 [=============>................] - ETA: 4:20 - loss: 1.9038 - regression_loss: 1.4224 - classification_loss: 0.4814\n",
      "246/500 [=============>................] - ETA: 4:19 - loss: 1.9018 - regression_loss: 1.4203 - classification_loss: 0.4815\n",
      "247/500 [=============>................] - ETA: 4:18 - loss: 1.9051 - regression_loss: 1.4223 - classification_loss: 0.4828\n",
      "248/500 [=============>................] - ETA: 4:17 - loss: 1.9053 - regression_loss: 1.4231 - classification_loss: 0.4822\n",
      "249/500 [=============>................] - ETA: 4:16 - loss: 1.9057 - regression_loss: 1.4235 - classification_loss: 0.4822\n",
      "250/500 [==============>...............] - ETA: 4:15 - loss: 1.9065 - regression_loss: 1.4247 - classification_loss: 0.4818\n",
      "251/500 [==============>...............] - ETA: 4:14 - loss: 1.9044 - regression_loss: 1.4231 - classification_loss: 0.4813\n",
      "252/500 [==============>...............] - ETA: 4:13 - loss: 1.9074 - regression_loss: 1.4256 - classification_loss: 0.4818\n",
      "253/500 [==============>...............] - ETA: 4:12 - loss: 1.9068 - regression_loss: 1.4252 - classification_loss: 0.4816\n",
      "254/500 [==============>...............] - ETA: 4:11 - loss: 1.9073 - regression_loss: 1.4253 - classification_loss: 0.4820\n",
      "255/500 [==============>...............] - ETA: 4:10 - loss: 1.9122 - regression_loss: 1.4282 - classification_loss: 0.4840\n",
      "256/500 [==============>...............] - ETA: 4:09 - loss: 1.9106 - regression_loss: 1.4273 - classification_loss: 0.4833\n",
      "257/500 [==============>...............] - ETA: 4:08 - loss: 1.9122 - regression_loss: 1.4291 - classification_loss: 0.4830\n",
      "258/500 [==============>...............] - ETA: 4:07 - loss: 1.9113 - regression_loss: 1.4287 - classification_loss: 0.4826\n",
      "259/500 [==============>...............] - ETA: 4:06 - loss: 1.9099 - regression_loss: 1.4277 - classification_loss: 0.4822\n",
      "260/500 [==============>...............] - ETA: 4:05 - loss: 1.9118 - regression_loss: 1.4292 - classification_loss: 0.4827\n",
      "261/500 [==============>...............] - ETA: 4:04 - loss: 1.9120 - regression_loss: 1.4295 - classification_loss: 0.4825\n",
      "262/500 [==============>...............] - ETA: 4:03 - loss: 1.9156 - regression_loss: 1.4325 - classification_loss: 0.4831\n",
      "263/500 [==============>...............] - ETA: 4:02 - loss: 1.9117 - regression_loss: 1.4296 - classification_loss: 0.4821\n",
      "264/500 [==============>...............] - ETA: 4:01 - loss: 1.9132 - regression_loss: 1.4309 - classification_loss: 0.4823\n",
      "265/500 [==============>...............] - ETA: 3:59 - loss: 1.9138 - regression_loss: 1.4317 - classification_loss: 0.4821\n",
      "266/500 [==============>...............] - ETA: 3:59 - loss: 1.9167 - regression_loss: 1.4332 - classification_loss: 0.4834\n",
      "267/500 [===============>..............] - ETA: 3:58 - loss: 1.9170 - regression_loss: 1.4327 - classification_loss: 0.4843\n",
      "268/500 [===============>..............] - ETA: 3:57 - loss: 1.9146 - regression_loss: 1.4310 - classification_loss: 0.4836\n",
      "269/500 [===============>..............] - ETA: 3:56 - loss: 1.9122 - regression_loss: 1.4292 - classification_loss: 0.4830\n",
      "270/500 [===============>..............] - ETA: 3:55 - loss: 1.9086 - regression_loss: 1.4263 - classification_loss: 0.4823\n",
      "271/500 [===============>..............] - ETA: 3:54 - loss: 1.9111 - regression_loss: 1.4285 - classification_loss: 0.4826\n",
      "272/500 [===============>..............] - ETA: 3:53 - loss: 1.9138 - regression_loss: 1.4303 - classification_loss: 0.4835\n",
      "273/500 [===============>..............] - ETA: 3:52 - loss: 1.9116 - regression_loss: 1.4290 - classification_loss: 0.4825\n",
      "274/500 [===============>..............] - ETA: 3:51 - loss: 1.9099 - regression_loss: 1.4279 - classification_loss: 0.4820\n",
      "275/500 [===============>..............] - ETA: 3:50 - loss: 1.9099 - regression_loss: 1.4281 - classification_loss: 0.4819\n",
      "276/500 [===============>..............] - ETA: 3:49 - loss: 1.9095 - regression_loss: 1.4276 - classification_loss: 0.4819\n",
      "277/500 [===============>..............] - ETA: 3:48 - loss: 1.9084 - regression_loss: 1.4270 - classification_loss: 0.4814\n",
      "278/500 [===============>..............] - ETA: 3:47 - loss: 1.9095 - regression_loss: 1.4278 - classification_loss: 0.4817\n",
      "279/500 [===============>..............] - ETA: 3:46 - loss: 1.9127 - regression_loss: 1.4298 - classification_loss: 0.4829\n",
      "280/500 [===============>..............] - ETA: 3:45 - loss: 1.9109 - regression_loss: 1.4284 - classification_loss: 0.4826\n",
      "281/500 [===============>..............] - ETA: 3:44 - loss: 1.9112 - regression_loss: 1.4283 - classification_loss: 0.4829\n",
      "282/500 [===============>..............] - ETA: 3:43 - loss: 1.9117 - regression_loss: 1.4288 - classification_loss: 0.4829\n",
      "283/500 [===============>..............] - ETA: 3:42 - loss: 1.9143 - regression_loss: 1.4310 - classification_loss: 0.4834\n",
      "284/500 [================>.............] - ETA: 3:41 - loss: 1.9151 - regression_loss: 1.4314 - classification_loss: 0.4837\n",
      "285/500 [================>.............] - ETA: 3:40 - loss: 1.9169 - regression_loss: 1.4330 - classification_loss: 0.4839\n",
      "286/500 [================>.............] - ETA: 3:39 - loss: 1.9158 - regression_loss: 1.4323 - classification_loss: 0.4835\n",
      "287/500 [================>.............] - ETA: 3:37 - loss: 1.9153 - regression_loss: 1.4315 - classification_loss: 0.4838\n",
      "288/500 [================>.............] - ETA: 3:36 - loss: 1.9187 - regression_loss: 1.4341 - classification_loss: 0.4846\n",
      "289/500 [================>.............] - ETA: 3:36 - loss: 1.9165 - regression_loss: 1.4325 - classification_loss: 0.4840\n",
      "290/500 [================>.............] - ETA: 3:35 - loss: 1.9164 - regression_loss: 1.4326 - classification_loss: 0.4838\n",
      "291/500 [================>.............] - ETA: 3:34 - loss: 1.9169 - regression_loss: 1.4331 - classification_loss: 0.4838\n",
      "292/500 [================>.............] - ETA: 3:33 - loss: 1.9162 - regression_loss: 1.4325 - classification_loss: 0.4837\n",
      "293/500 [================>.............] - ETA: 3:31 - loss: 1.9155 - regression_loss: 1.4320 - classification_loss: 0.4835\n",
      "294/500 [================>.............] - ETA: 3:30 - loss: 1.9143 - regression_loss: 1.4309 - classification_loss: 0.4834\n",
      "295/500 [================>.............] - ETA: 3:29 - loss: 1.9145 - regression_loss: 1.4315 - classification_loss: 0.4830\n",
      "296/500 [================>.............] - ETA: 3:28 - loss: 1.9117 - regression_loss: 1.4292 - classification_loss: 0.4825\n",
      "297/500 [================>.............] - ETA: 3:27 - loss: 1.9124 - regression_loss: 1.4297 - classification_loss: 0.4827\n",
      "298/500 [================>.............] - ETA: 3:26 - loss: 1.9144 - regression_loss: 1.4308 - classification_loss: 0.4836\n",
      "299/500 [================>.............] - ETA: 3:25 - loss: 1.9136 - regression_loss: 1.4297 - classification_loss: 0.4839\n",
      "300/500 [=================>............] - ETA: 3:24 - loss: 1.9135 - regression_loss: 1.4297 - classification_loss: 0.4838\n",
      "301/500 [=================>............] - ETA: 3:23 - loss: 1.9125 - regression_loss: 1.4290 - classification_loss: 0.4835\n",
      "302/500 [=================>............] - ETA: 3:22 - loss: 1.9128 - regression_loss: 1.4290 - classification_loss: 0.4838\n",
      "303/500 [=================>............] - ETA: 3:21 - loss: 1.9128 - regression_loss: 1.4294 - classification_loss: 0.4834\n",
      "304/500 [=================>............] - ETA: 3:20 - loss: 1.9129 - regression_loss: 1.4284 - classification_loss: 0.4844\n",
      "305/500 [=================>............] - ETA: 3:19 - loss: 1.9123 - regression_loss: 1.4281 - classification_loss: 0.4842\n",
      "306/500 [=================>............] - ETA: 3:18 - loss: 1.9088 - regression_loss: 1.4254 - classification_loss: 0.4834\n",
      "307/500 [=================>............] - ETA: 3:17 - loss: 1.9064 - regression_loss: 1.4236 - classification_loss: 0.4829\n",
      "308/500 [=================>............] - ETA: 3:16 - loss: 1.9037 - regression_loss: 1.4215 - classification_loss: 0.4822\n",
      "309/500 [=================>............] - ETA: 3:15 - loss: 1.9039 - regression_loss: 1.4220 - classification_loss: 0.4819\n",
      "310/500 [=================>............] - ETA: 3:14 - loss: 1.9030 - regression_loss: 1.4216 - classification_loss: 0.4815\n",
      "311/500 [=================>............] - ETA: 3:13 - loss: 1.9013 - regression_loss: 1.4200 - classification_loss: 0.4813\n",
      "312/500 [=================>............] - ETA: 3:12 - loss: 1.8988 - regression_loss: 1.4181 - classification_loss: 0.4807\n",
      "313/500 [=================>............] - ETA: 3:11 - loss: 1.8989 - regression_loss: 1.4187 - classification_loss: 0.4802\n",
      "314/500 [=================>............] - ETA: 3:10 - loss: 1.8992 - regression_loss: 1.4191 - classification_loss: 0.4801\n",
      "315/500 [=================>............] - ETA: 3:09 - loss: 1.8989 - regression_loss: 1.4187 - classification_loss: 0.4802\n",
      "316/500 [=================>............] - ETA: 3:08 - loss: 1.8994 - regression_loss: 1.4196 - classification_loss: 0.4798\n",
      "317/500 [==================>...........] - ETA: 3:07 - loss: 1.8982 - regression_loss: 1.4189 - classification_loss: 0.4792\n",
      "318/500 [==================>...........] - ETA: 3:06 - loss: 1.8993 - regression_loss: 1.4202 - classification_loss: 0.4792\n",
      "319/500 [==================>...........] - ETA: 3:05 - loss: 1.9000 - regression_loss: 1.4205 - classification_loss: 0.4795\n",
      "320/500 [==================>...........] - ETA: 3:04 - loss: 1.9001 - regression_loss: 1.4202 - classification_loss: 0.4798\n",
      "321/500 [==================>...........] - ETA: 3:03 - loss: 1.9041 - regression_loss: 1.4230 - classification_loss: 0.4811\n",
      "322/500 [==================>...........] - ETA: 3:02 - loss: 1.9044 - regression_loss: 1.4232 - classification_loss: 0.4811\n",
      "323/500 [==================>...........] - ETA: 3:01 - loss: 1.9026 - regression_loss: 1.4214 - classification_loss: 0.4812\n",
      "324/500 [==================>...........] - ETA: 3:00 - loss: 1.9016 - regression_loss: 1.4202 - classification_loss: 0.4814\n",
      "325/500 [==================>...........] - ETA: 2:59 - loss: 1.9001 - regression_loss: 1.4191 - classification_loss: 0.4810\n",
      "326/500 [==================>...........] - ETA: 2:57 - loss: 1.8985 - regression_loss: 1.4181 - classification_loss: 0.4804\n",
      "327/500 [==================>...........] - ETA: 2:57 - loss: 1.8991 - regression_loss: 1.4182 - classification_loss: 0.4809\n",
      "328/500 [==================>...........] - ETA: 2:56 - loss: 1.8991 - regression_loss: 1.4185 - classification_loss: 0.4806\n",
      "329/500 [==================>...........] - ETA: 2:55 - loss: 1.9002 - regression_loss: 1.4193 - classification_loss: 0.4810\n",
      "330/500 [==================>...........] - ETA: 2:53 - loss: 1.9001 - regression_loss: 1.4196 - classification_loss: 0.4806\n",
      "331/500 [==================>...........] - ETA: 2:52 - loss: 1.8997 - regression_loss: 1.4189 - classification_loss: 0.4808\n",
      "332/500 [==================>...........] - ETA: 2:51 - loss: 1.8979 - regression_loss: 1.4176 - classification_loss: 0.4803\n",
      "333/500 [==================>...........] - ETA: 2:50 - loss: 1.8971 - regression_loss: 1.4170 - classification_loss: 0.4801\n",
      "334/500 [===================>..........] - ETA: 2:49 - loss: 1.8990 - regression_loss: 1.4185 - classification_loss: 0.4805\n",
      "335/500 [===================>..........] - ETA: 2:48 - loss: 1.9011 - regression_loss: 1.4197 - classification_loss: 0.4814\n",
      "336/500 [===================>..........] - ETA: 2:47 - loss: 1.9022 - regression_loss: 1.4204 - classification_loss: 0.4817\n",
      "337/500 [===================>..........] - ETA: 2:46 - loss: 1.9008 - regression_loss: 1.4195 - classification_loss: 0.4813\n",
      "338/500 [===================>..........] - ETA: 2:45 - loss: 1.9009 - regression_loss: 1.4198 - classification_loss: 0.4812\n",
      "339/500 [===================>..........] - ETA: 2:44 - loss: 1.9034 - regression_loss: 1.4214 - classification_loss: 0.4820\n",
      "340/500 [===================>..........] - ETA: 2:43 - loss: 1.9039 - regression_loss: 1.4222 - classification_loss: 0.4817\n",
      "341/500 [===================>..........] - ETA: 2:42 - loss: 1.9048 - regression_loss: 1.4233 - classification_loss: 0.4815\n",
      "342/500 [===================>..........] - ETA: 2:41 - loss: 1.9044 - regression_loss: 1.4231 - classification_loss: 0.4813\n",
      "343/500 [===================>..........] - ETA: 2:40 - loss: 1.9073 - regression_loss: 1.4246 - classification_loss: 0.4827\n",
      "344/500 [===================>..........] - ETA: 2:39 - loss: 1.9065 - regression_loss: 1.4243 - classification_loss: 0.4822\n",
      "345/500 [===================>..........] - ETA: 2:38 - loss: 1.9054 - regression_loss: 1.4233 - classification_loss: 0.4821\n",
      "346/500 [===================>..........] - ETA: 2:37 - loss: 1.9054 - regression_loss: 1.4233 - classification_loss: 0.4821\n",
      "347/500 [===================>..........] - ETA: 2:36 - loss: 1.9050 - regression_loss: 1.4230 - classification_loss: 0.4820\n",
      "348/500 [===================>..........] - ETA: 2:35 - loss: 1.9032 - regression_loss: 1.4217 - classification_loss: 0.4814\n",
      "349/500 [===================>..........] - ETA: 2:34 - loss: 1.9028 - regression_loss: 1.4219 - classification_loss: 0.4809\n",
      "350/500 [====================>.........] - ETA: 2:33 - loss: 1.9031 - regression_loss: 1.4219 - classification_loss: 0.4812\n",
      "351/500 [====================>.........] - ETA: 2:32 - loss: 1.9037 - regression_loss: 1.4226 - classification_loss: 0.4811\n",
      "352/500 [====================>.........] - ETA: 2:31 - loss: 1.9036 - regression_loss: 1.4224 - classification_loss: 0.4812\n",
      "353/500 [====================>.........] - ETA: 2:30 - loss: 1.9025 - regression_loss: 1.4217 - classification_loss: 0.4808\n",
      "354/500 [====================>.........] - ETA: 2:29 - loss: 1.9021 - regression_loss: 1.4214 - classification_loss: 0.4807\n",
      "355/500 [====================>.........] - ETA: 2:28 - loss: 1.9029 - regression_loss: 1.4222 - classification_loss: 0.4807\n",
      "356/500 [====================>.........] - ETA: 2:27 - loss: 1.9039 - regression_loss: 1.4232 - classification_loss: 0.4807\n",
      "357/500 [====================>.........] - ETA: 2:26 - loss: 1.9009 - regression_loss: 1.4209 - classification_loss: 0.4800\n",
      "358/500 [====================>.........] - ETA: 2:25 - loss: 1.9026 - regression_loss: 1.4221 - classification_loss: 0.4805\n",
      "359/500 [====================>.........] - ETA: 2:24 - loss: 1.9013 - regression_loss: 1.4214 - classification_loss: 0.4799\n",
      "360/500 [====================>.........] - ETA: 2:23 - loss: 1.9023 - regression_loss: 1.4217 - classification_loss: 0.4807\n",
      "361/500 [====================>.........] - ETA: 2:22 - loss: 1.9033 - regression_loss: 1.4225 - classification_loss: 0.4809\n",
      "362/500 [====================>.........] - ETA: 2:21 - loss: 1.9033 - regression_loss: 1.4225 - classification_loss: 0.4808\n",
      "363/500 [====================>.........] - ETA: 2:20 - loss: 1.9017 - regression_loss: 1.4217 - classification_loss: 0.4801\n",
      "364/500 [====================>.........] - ETA: 2:19 - loss: 1.9006 - regression_loss: 1.4210 - classification_loss: 0.4796\n",
      "365/500 [====================>.........] - ETA: 2:18 - loss: 1.8997 - regression_loss: 1.4205 - classification_loss: 0.4792\n",
      "366/500 [====================>.........] - ETA: 2:17 - loss: 1.8987 - regression_loss: 1.4196 - classification_loss: 0.4791\n",
      "367/500 [=====================>........] - ETA: 2:16 - loss: 1.8986 - regression_loss: 1.4198 - classification_loss: 0.4788\n",
      "368/500 [=====================>........] - ETA: 2:15 - loss: 1.8964 - regression_loss: 1.4184 - classification_loss: 0.4780\n",
      "369/500 [=====================>........] - ETA: 2:14 - loss: 1.8967 - regression_loss: 1.4187 - classification_loss: 0.4780\n",
      "370/500 [=====================>........] - ETA: 2:13 - loss: 1.8970 - regression_loss: 1.4189 - classification_loss: 0.4781\n",
      "371/500 [=====================>........] - ETA: 2:12 - loss: 1.9006 - regression_loss: 1.4211 - classification_loss: 0.4795\n",
      "372/500 [=====================>........] - ETA: 2:10 - loss: 1.8991 - regression_loss: 1.4202 - classification_loss: 0.4790\n",
      "373/500 [=====================>........] - ETA: 2:09 - loss: 1.9011 - regression_loss: 1.4218 - classification_loss: 0.4792\n",
      "374/500 [=====================>........] - ETA: 2:08 - loss: 1.9005 - regression_loss: 1.4214 - classification_loss: 0.4791\n",
      "375/500 [=====================>........] - ETA: 2:07 - loss: 1.9003 - regression_loss: 1.4213 - classification_loss: 0.4790\n",
      "376/500 [=====================>........] - ETA: 2:06 - loss: 1.8983 - regression_loss: 1.4198 - classification_loss: 0.4785\n",
      "377/500 [=====================>........] - ETA: 2:05 - loss: 1.8985 - regression_loss: 1.4199 - classification_loss: 0.4786\n",
      "378/500 [=====================>........] - ETA: 2:04 - loss: 1.8995 - regression_loss: 1.4204 - classification_loss: 0.4791\n",
      "379/500 [=====================>........] - ETA: 2:03 - loss: 1.8987 - regression_loss: 1.4193 - classification_loss: 0.4794\n",
      "380/500 [=====================>........] - ETA: 2:02 - loss: 1.8988 - regression_loss: 1.4193 - classification_loss: 0.4795\n",
      "381/500 [=====================>........] - ETA: 2:01 - loss: 1.8991 - regression_loss: 1.4197 - classification_loss: 0.4794\n",
      "382/500 [=====================>........] - ETA: 2:00 - loss: 1.8975 - regression_loss: 1.4186 - classification_loss: 0.4789\n",
      "383/500 [=====================>........] - ETA: 1:59 - loss: 1.8974 - regression_loss: 1.4184 - classification_loss: 0.4790\n",
      "384/500 [======================>.......] - ETA: 1:58 - loss: 1.8997 - regression_loss: 1.4197 - classification_loss: 0.4799\n",
      "385/500 [======================>.......] - ETA: 1:57 - loss: 1.8985 - regression_loss: 1.4187 - classification_loss: 0.4798\n",
      "386/500 [======================>.......] - ETA: 1:56 - loss: 1.8988 - regression_loss: 1.4192 - classification_loss: 0.4797\n",
      "387/500 [======================>.......] - ETA: 1:55 - loss: 1.9010 - regression_loss: 1.4204 - classification_loss: 0.4805\n",
      "388/500 [======================>.......] - ETA: 1:54 - loss: 1.9044 - regression_loss: 1.4224 - classification_loss: 0.4820\n",
      "389/500 [======================>.......] - ETA: 1:53 - loss: 1.9034 - regression_loss: 1.4218 - classification_loss: 0.4816\n",
      "390/500 [======================>.......] - ETA: 1:52 - loss: 1.9050 - regression_loss: 1.4229 - classification_loss: 0.4821\n",
      "391/500 [======================>.......] - ETA: 1:51 - loss: 1.9039 - regression_loss: 1.4222 - classification_loss: 0.4817\n",
      "392/500 [======================>.......] - ETA: 1:50 - loss: 1.9048 - regression_loss: 1.4230 - classification_loss: 0.4818\n",
      "393/500 [======================>.......] - ETA: 1:49 - loss: 1.9069 - regression_loss: 1.4246 - classification_loss: 0.4823\n",
      "394/500 [======================>.......] - ETA: 1:48 - loss: 1.9079 - regression_loss: 1.4253 - classification_loss: 0.4826\n",
      "395/500 [======================>.......] - ETA: 1:47 - loss: 1.9103 - regression_loss: 1.4266 - classification_loss: 0.4837\n",
      "396/500 [======================>.......] - ETA: 1:46 - loss: 1.9109 - regression_loss: 1.4272 - classification_loss: 0.4837\n",
      "397/500 [======================>.......] - ETA: 1:45 - loss: 1.9125 - regression_loss: 1.4287 - classification_loss: 0.4838\n",
      "398/500 [======================>.......] - ETA: 1:44 - loss: 1.9113 - regression_loss: 1.4275 - classification_loss: 0.4838\n",
      "399/500 [======================>.......] - ETA: 1:43 - loss: 1.9099 - regression_loss: 1.4264 - classification_loss: 0.4835\n",
      "400/500 [=======================>......] - ETA: 1:42 - loss: 1.9084 - regression_loss: 1.4255 - classification_loss: 0.4830\n",
      "401/500 [=======================>......] - ETA: 1:41 - loss: 1.9106 - regression_loss: 1.4270 - classification_loss: 0.4836\n",
      "402/500 [=======================>......] - ETA: 1:40 - loss: 1.9101 - regression_loss: 1.4269 - classification_loss: 0.4832\n",
      "403/500 [=======================>......] - ETA: 1:39 - loss: 1.9087 - regression_loss: 1.4256 - classification_loss: 0.4831\n",
      "404/500 [=======================>......] - ETA: 1:38 - loss: 1.9085 - regression_loss: 1.4254 - classification_loss: 0.4830\n",
      "405/500 [=======================>......] - ETA: 1:37 - loss: 1.9096 - regression_loss: 1.4259 - classification_loss: 0.4837\n",
      "406/500 [=======================>......] - ETA: 1:36 - loss: 1.9085 - regression_loss: 1.4250 - classification_loss: 0.4834\n",
      "407/500 [=======================>......] - ETA: 1:35 - loss: 1.9092 - regression_loss: 1.4252 - classification_loss: 0.4840\n",
      "408/500 [=======================>......] - ETA: 1:34 - loss: 1.9098 - regression_loss: 1.4257 - classification_loss: 0.4842\n",
      "409/500 [=======================>......] - ETA: 1:33 - loss: 1.9096 - regression_loss: 1.4255 - classification_loss: 0.4841\n",
      "410/500 [=======================>......] - ETA: 1:32 - loss: 1.9106 - regression_loss: 1.4264 - classification_loss: 0.4842\n",
      "411/500 [=======================>......] - ETA: 1:31 - loss: 1.9115 - regression_loss: 1.4271 - classification_loss: 0.4845\n",
      "412/500 [=======================>......] - ETA: 1:30 - loss: 1.9109 - regression_loss: 1.4264 - classification_loss: 0.4845\n",
      "413/500 [=======================>......] - ETA: 1:29 - loss: 1.9093 - regression_loss: 1.4253 - classification_loss: 0.4840\n",
      "414/500 [=======================>......] - ETA: 1:28 - loss: 1.9107 - regression_loss: 1.4266 - classification_loss: 0.4841\n",
      "415/500 [=======================>......] - ETA: 1:27 - loss: 1.9120 - regression_loss: 1.4275 - classification_loss: 0.4845\n",
      "416/500 [=======================>......] - ETA: 1:26 - loss: 1.9097 - regression_loss: 1.4257 - classification_loss: 0.4840\n",
      "417/500 [========================>.....] - ETA: 1:25 - loss: 1.9089 - regression_loss: 1.4248 - classification_loss: 0.4841\n",
      "418/500 [========================>.....] - ETA: 1:23 - loss: 1.9082 - regression_loss: 1.4244 - classification_loss: 0.4838\n",
      "419/500 [========================>.....] - ETA: 1:22 - loss: 1.9094 - regression_loss: 1.4253 - classification_loss: 0.4841\n",
      "420/500 [========================>.....] - ETA: 1:21 - loss: 1.9091 - regression_loss: 1.4254 - classification_loss: 0.4837\n",
      "421/500 [========================>.....] - ETA: 1:20 - loss: 1.9082 - regression_loss: 1.4250 - classification_loss: 0.4832\n",
      "422/500 [========================>.....] - ETA: 1:19 - loss: 1.9084 - regression_loss: 1.4253 - classification_loss: 0.4831\n",
      "423/500 [========================>.....] - ETA: 1:18 - loss: 1.9073 - regression_loss: 1.4245 - classification_loss: 0.4828\n",
      "424/500 [========================>.....] - ETA: 1:17 - loss: 1.9072 - regression_loss: 1.4247 - classification_loss: 0.4826\n",
      "425/500 [========================>.....] - ETA: 1:16 - loss: 1.9049 - regression_loss: 1.4229 - classification_loss: 0.4820\n",
      "426/500 [========================>.....] - ETA: 1:15 - loss: 1.9057 - regression_loss: 1.4237 - classification_loss: 0.4820\n",
      "427/500 [========================>.....] - ETA: 1:14 - loss: 1.9050 - regression_loss: 1.4236 - classification_loss: 0.4815\n",
      "428/500 [========================>.....] - ETA: 1:13 - loss: 1.9043 - regression_loss: 1.4230 - classification_loss: 0.4813\n",
      "429/500 [========================>.....] - ETA: 1:12 - loss: 1.9039 - regression_loss: 1.4228 - classification_loss: 0.4812\n",
      "430/500 [========================>.....] - ETA: 1:11 - loss: 1.9060 - regression_loss: 1.4242 - classification_loss: 0.4818\n",
      "431/500 [========================>.....] - ETA: 1:10 - loss: 1.9062 - regression_loss: 1.4247 - classification_loss: 0.4815\n",
      "432/500 [========================>.....] - ETA: 1:09 - loss: 1.9065 - regression_loss: 1.4244 - classification_loss: 0.4821\n",
      "433/500 [========================>.....] - ETA: 1:08 - loss: 1.9048 - regression_loss: 1.4231 - classification_loss: 0.4817\n",
      "434/500 [=========================>....] - ETA: 1:07 - loss: 1.9072 - regression_loss: 1.4246 - classification_loss: 0.4827\n",
      "435/500 [=========================>....] - ETA: 1:06 - loss: 1.9063 - regression_loss: 1.4238 - classification_loss: 0.4825\n",
      "436/500 [=========================>....] - ETA: 1:05 - loss: 1.9049 - regression_loss: 1.4225 - classification_loss: 0.4824\n",
      "437/500 [=========================>....] - ETA: 1:04 - loss: 1.9049 - regression_loss: 1.4221 - classification_loss: 0.4829\n",
      "438/500 [=========================>....] - ETA: 1:03 - loss: 1.9056 - regression_loss: 1.4228 - classification_loss: 0.4828\n",
      "439/500 [=========================>....] - ETA: 1:02 - loss: 1.9059 - regression_loss: 1.4234 - classification_loss: 0.4825\n",
      "440/500 [=========================>....] - ETA: 1:01 - loss: 1.9056 - regression_loss: 1.4232 - classification_loss: 0.4824\n",
      "441/500 [=========================>....] - ETA: 1:00 - loss: 1.9053 - regression_loss: 1.4230 - classification_loss: 0.4823\n",
      "442/500 [=========================>....] - ETA: 59s - loss: 1.9046 - regression_loss: 1.4226 - classification_loss: 0.4820 \n",
      "443/500 [=========================>....] - ETA: 58s - loss: 1.9070 - regression_loss: 1.4239 - classification_loss: 0.4831\n",
      "444/500 [=========================>....] - ETA: 57s - loss: 1.9072 - regression_loss: 1.4241 - classification_loss: 0.4831\n",
      "445/500 [=========================>....] - ETA: 56s - loss: 1.9070 - regression_loss: 1.4236 - classification_loss: 0.4834\n",
      "446/500 [=========================>....] - ETA: 55s - loss: 1.9070 - regression_loss: 1.4239 - classification_loss: 0.4831\n",
      "447/500 [=========================>....] - ETA: 54s - loss: 1.9058 - regression_loss: 1.4230 - classification_loss: 0.4829\n",
      "448/500 [=========================>....] - ETA: 53s - loss: 1.9064 - regression_loss: 1.4236 - classification_loss: 0.4828\n",
      "449/500 [=========================>....] - ETA: 52s - loss: 1.9040 - regression_loss: 1.4215 - classification_loss: 0.4825\n",
      "450/500 [==========================>...] - ETA: 51s - loss: 1.9022 - regression_loss: 1.4203 - classification_loss: 0.4820\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 1.9006 - regression_loss: 1.4187 - classification_loss: 0.4818\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 1.9009 - regression_loss: 1.4184 - classification_loss: 0.4824\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.9021 - regression_loss: 1.4195 - classification_loss: 0.4826\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.9012 - regression_loss: 1.4190 - classification_loss: 0.4822\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.9019 - regression_loss: 1.4196 - classification_loss: 0.4824\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.9036 - regression_loss: 1.4206 - classification_loss: 0.4830\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.9043 - regression_loss: 1.4211 - classification_loss: 0.4832\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.9058 - regression_loss: 1.4220 - classification_loss: 0.4838\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.9057 - regression_loss: 1.4221 - classification_loss: 0.4837\n",
      "460/500 [==========================>...] - ETA: 40s - loss: 1.9047 - regression_loss: 1.4214 - classification_loss: 0.4833\n",
      "461/500 [==========================>...] - ETA: 39s - loss: 1.9061 - regression_loss: 1.4226 - classification_loss: 0.4836\n",
      "462/500 [==========================>...] - ETA: 38s - loss: 1.9068 - regression_loss: 1.4230 - classification_loss: 0.4837\n",
      "463/500 [==========================>...] - ETA: 37s - loss: 1.9057 - regression_loss: 1.4222 - classification_loss: 0.4835\n",
      "464/500 [==========================>...] - ETA: 36s - loss: 1.9068 - regression_loss: 1.4232 - classification_loss: 0.4837\n",
      "465/500 [==========================>...] - ETA: 35s - loss: 1.9057 - regression_loss: 1.4225 - classification_loss: 0.4832\n",
      "466/500 [==========================>...] - ETA: 34s - loss: 1.9058 - regression_loss: 1.4227 - classification_loss: 0.4832\n",
      "467/500 [===========================>..] - ETA: 33s - loss: 1.9043 - regression_loss: 1.4218 - classification_loss: 0.4825\n",
      "468/500 [===========================>..] - ETA: 32s - loss: 1.9037 - regression_loss: 1.4213 - classification_loss: 0.4824\n",
      "469/500 [===========================>..] - ETA: 31s - loss: 1.9032 - regression_loss: 1.4208 - classification_loss: 0.4824\n",
      "470/500 [===========================>..] - ETA: 30s - loss: 1.9032 - regression_loss: 1.4209 - classification_loss: 0.4824\n",
      "471/500 [===========================>..] - ETA: 29s - loss: 1.9043 - regression_loss: 1.4217 - classification_loss: 0.4826\n",
      "472/500 [===========================>..] - ETA: 28s - loss: 1.9068 - regression_loss: 1.4233 - classification_loss: 0.4835\n",
      "473/500 [===========================>..] - ETA: 27s - loss: 1.9064 - regression_loss: 1.4233 - classification_loss: 0.4832\n",
      "474/500 [===========================>..] - ETA: 26s - loss: 1.9065 - regression_loss: 1.4234 - classification_loss: 0.4831\n",
      "475/500 [===========================>..] - ETA: 25s - loss: 1.9085 - regression_loss: 1.4248 - classification_loss: 0.4837\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 1.9067 - regression_loss: 1.4234 - classification_loss: 0.4833\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.9078 - regression_loss: 1.4244 - classification_loss: 0.4834\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.9091 - regression_loss: 1.4253 - classification_loss: 0.4838\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.9091 - regression_loss: 1.4254 - classification_loss: 0.4837\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.9083 - regression_loss: 1.4248 - classification_loss: 0.4834\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.9073 - regression_loss: 1.4240 - classification_loss: 0.4833\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.9088 - regression_loss: 1.4249 - classification_loss: 0.4839\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.9085 - regression_loss: 1.4246 - classification_loss: 0.4839\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.9077 - regression_loss: 1.4240 - classification_loss: 0.4838\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.9074 - regression_loss: 1.4241 - classification_loss: 0.4833\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.9077 - regression_loss: 1.4244 - classification_loss: 0.4832\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.9099 - regression_loss: 1.4259 - classification_loss: 0.4840\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.9092 - regression_loss: 1.4252 - classification_loss: 0.4840\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.9089 - regression_loss: 1.4253 - classification_loss: 0.4836\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.9078 - regression_loss: 1.4246 - classification_loss: 0.4832\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.9086 - regression_loss: 1.4255 - classification_loss: 0.4831 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.9086 - regression_loss: 1.4257 - classification_loss: 0.4829\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.9090 - regression_loss: 1.4261 - classification_loss: 0.4829\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.9085 - regression_loss: 1.4257 - classification_loss: 0.4829\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.9077 - regression_loss: 1.4251 - classification_loss: 0.4825\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.9080 - regression_loss: 1.4254 - classification_loss: 0.4826\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.9082 - regression_loss: 1.4258 - classification_loss: 0.4824\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.9088 - regression_loss: 1.4258 - classification_loss: 0.4830\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.9070 - regression_loss: 1.4244 - classification_loss: 0.4826\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9064 - regression_loss: 1.4239 - classification_loss: 0.4825\n",
      "Epoch 00040: saving model to ./snapshots\\resnet50_csv_40.h5\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.999998977483754e-17.\n",
      "\n",
      "500/500 [==============================] - 513s 1s/step - loss: 1.9064 - regression_loss: 1.4239 - classification_loss: 0.4825\n",
      "Epoch 41/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 1.6904 - regression_loss: 1.2532 - classification_loss: 0.4372\n",
      "  2/500 [..............................] - ETA: 4:28 - loss: 1.8718 - regression_loss: 1.4309 - classification_loss: 0.4409\n",
      "  3/500 [..............................] - ETA: 5:21 - loss: 1.6420 - regression_loss: 1.2486 - classification_loss: 0.3934\n",
      "  4/500 [..............................] - ETA: 6:11 - loss: 1.6236 - regression_loss: 1.1937 - classification_loss: 0.4299\n",
      "  5/500 [..............................] - ETA: 6:43 - loss: 1.5200 - regression_loss: 1.1106 - classification_loss: 0.4095\n",
      "  6/500 [..............................] - ETA: 7:04 - loss: 1.5907 - regression_loss: 1.1557 - classification_loss: 0.4350\n",
      "  7/500 [..............................] - ETA: 7:12 - loss: 1.5302 - regression_loss: 1.1221 - classification_loss: 0.4081\n",
      "  8/500 [..............................] - ETA: 7:24 - loss: 1.5937 - regression_loss: 1.1837 - classification_loss: 0.4100\n",
      "  9/500 [..............................] - ETA: 7:15 - loss: 1.5937 - regression_loss: 1.1835 - classification_loss: 0.4103\n",
      " 10/500 [..............................] - ETA: 7:27 - loss: 1.5977 - regression_loss: 1.1875 - classification_loss: 0.4102\n",
      " 11/500 [..............................] - ETA: 7:33 - loss: 1.6196 - regression_loss: 1.1964 - classification_loss: 0.4231\n",
      " 12/500 [..............................] - ETA: 7:35 - loss: 1.6489 - regression_loss: 1.2030 - classification_loss: 0.4459\n",
      " 13/500 [..............................] - ETA: 7:27 - loss: 1.6650 - regression_loss: 1.2080 - classification_loss: 0.4570\n",
      " 14/500 [..............................] - ETA: 7:37 - loss: 1.6723 - regression_loss: 1.2178 - classification_loss: 0.4545\n",
      " 15/500 [..............................] - ETA: 7:38 - loss: 1.6705 - regression_loss: 1.2195 - classification_loss: 0.4509\n",
      " 16/500 [..............................] - ETA: 7:41 - loss: 1.6855 - regression_loss: 1.2301 - classification_loss: 0.4554\n",
      " 17/500 [>.............................] - ETA: 7:41 - loss: 1.6752 - regression_loss: 1.2234 - classification_loss: 0.4518\n",
      " 18/500 [>.............................] - ETA: 7:41 - loss: 1.6982 - regression_loss: 1.2454 - classification_loss: 0.4528\n",
      " 19/500 [>.............................] - ETA: 7:41 - loss: 1.7144 - regression_loss: 1.2548 - classification_loss: 0.4596\n",
      " 20/500 [>.............................] - ETA: 7:40 - loss: 1.7264 - regression_loss: 1.2623 - classification_loss: 0.4640\n",
      " 21/500 [>.............................] - ETA: 7:40 - loss: 1.7246 - regression_loss: 1.2655 - classification_loss: 0.4591\n",
      " 22/500 [>.............................] - ETA: 7:35 - loss: 1.7355 - regression_loss: 1.2817 - classification_loss: 0.4539\n",
      " 23/500 [>.............................] - ETA: 7:35 - loss: 1.7484 - regression_loss: 1.2934 - classification_loss: 0.4550\n",
      " 24/500 [>.............................] - ETA: 7:41 - loss: 1.7605 - regression_loss: 1.3014 - classification_loss: 0.4591\n",
      " 25/500 [>.............................] - ETA: 7:40 - loss: 1.7345 - regression_loss: 1.2844 - classification_loss: 0.4501\n",
      " 26/500 [>.............................] - ETA: 7:39 - loss: 1.7506 - regression_loss: 1.2945 - classification_loss: 0.4561\n",
      " 27/500 [>.............................] - ETA: 7:38 - loss: 1.7768 - regression_loss: 1.3122 - classification_loss: 0.4647\n",
      " 28/500 [>.............................] - ETA: 7:38 - loss: 1.7839 - regression_loss: 1.3176 - classification_loss: 0.4663\n",
      " 29/500 [>.............................] - ETA: 7:37 - loss: 1.7791 - regression_loss: 1.3155 - classification_loss: 0.4636\n",
      " 30/500 [>.............................] - ETA: 7:37 - loss: 1.7642 - regression_loss: 1.3040 - classification_loss: 0.4602\n",
      " 31/500 [>.............................] - ETA: 7:36 - loss: 1.7561 - regression_loss: 1.2982 - classification_loss: 0.4578\n",
      " 32/500 [>.............................] - ETA: 7:37 - loss: 1.7950 - regression_loss: 1.3267 - classification_loss: 0.4683\n",
      " 33/500 [>.............................] - ETA: 7:34 - loss: 1.8092 - regression_loss: 1.3379 - classification_loss: 0.4713\n",
      " 34/500 [=>............................] - ETA: 7:33 - loss: 1.8095 - regression_loss: 1.3373 - classification_loss: 0.4722\n",
      " 35/500 [=>............................] - ETA: 7:35 - loss: 1.8122 - regression_loss: 1.3436 - classification_loss: 0.4686\n",
      " 36/500 [=>............................] - ETA: 7:34 - loss: 1.7975 - regression_loss: 1.3337 - classification_loss: 0.4638\n",
      " 37/500 [=>............................] - ETA: 7:34 - loss: 1.7858 - regression_loss: 1.3270 - classification_loss: 0.4588\n",
      " 38/500 [=>............................] - ETA: 7:35 - loss: 1.7855 - regression_loss: 1.3299 - classification_loss: 0.4556\n",
      " 39/500 [=>............................] - ETA: 7:35 - loss: 1.7986 - regression_loss: 1.3442 - classification_loss: 0.4545\n",
      " 40/500 [=>............................] - ETA: 7:34 - loss: 1.7911 - regression_loss: 1.3405 - classification_loss: 0.4506\n",
      " 41/500 [=>............................] - ETA: 7:33 - loss: 1.7954 - regression_loss: 1.3403 - classification_loss: 0.4552\n",
      " 42/500 [=>............................] - ETA: 7:33 - loss: 1.7827 - regression_loss: 1.3329 - classification_loss: 0.4498\n",
      " 43/500 [=>............................] - ETA: 7:32 - loss: 1.8060 - regression_loss: 1.3482 - classification_loss: 0.4579\n",
      " 44/500 [=>............................] - ETA: 7:32 - loss: 1.8182 - regression_loss: 1.3605 - classification_loss: 0.4577\n",
      " 45/500 [=>............................] - ETA: 7:31 - loss: 1.7977 - regression_loss: 1.3457 - classification_loss: 0.4520\n",
      " 46/500 [=>............................] - ETA: 7:30 - loss: 1.8091 - regression_loss: 1.3558 - classification_loss: 0.4533\n",
      " 47/500 [=>............................] - ETA: 7:28 - loss: 1.7977 - regression_loss: 1.3462 - classification_loss: 0.4516\n",
      " 48/500 [=>............................] - ETA: 7:27 - loss: 1.7858 - regression_loss: 1.3379 - classification_loss: 0.4479\n",
      " 49/500 [=>............................] - ETA: 7:26 - loss: 1.7950 - regression_loss: 1.3421 - classification_loss: 0.4529\n",
      " 50/500 [==>...........................] - ETA: 7:26 - loss: 1.8011 - regression_loss: 1.3476 - classification_loss: 0.4535\n",
      " 51/500 [==>...........................] - ETA: 7:26 - loss: 1.7976 - regression_loss: 1.3445 - classification_loss: 0.4531\n",
      " 52/500 [==>...........................] - ETA: 7:25 - loss: 1.7920 - regression_loss: 1.3414 - classification_loss: 0.4506\n",
      " 53/500 [==>...........................] - ETA: 7:25 - loss: 1.7801 - regression_loss: 1.3335 - classification_loss: 0.4466\n",
      " 54/500 [==>...........................] - ETA: 7:24 - loss: 1.7706 - regression_loss: 1.3253 - classification_loss: 0.4454\n",
      " 55/500 [==>...........................] - ETA: 7:23 - loss: 1.7626 - regression_loss: 1.3192 - classification_loss: 0.4434\n",
      " 56/500 [==>...........................] - ETA: 7:23 - loss: 1.7599 - regression_loss: 1.3163 - classification_loss: 0.4436\n",
      " 57/500 [==>...........................] - ETA: 7:22 - loss: 1.7609 - regression_loss: 1.3168 - classification_loss: 0.4441\n",
      " 58/500 [==>...........................] - ETA: 7:21 - loss: 1.7558 - regression_loss: 1.3146 - classification_loss: 0.4412\n",
      " 59/500 [==>...........................] - ETA: 7:21 - loss: 1.7553 - regression_loss: 1.3120 - classification_loss: 0.4433\n",
      " 60/500 [==>...........................] - ETA: 7:20 - loss: 1.7621 - regression_loss: 1.3166 - classification_loss: 0.4455\n",
      " 61/500 [==>...........................] - ETA: 7:17 - loss: 1.7585 - regression_loss: 1.3145 - classification_loss: 0.4440\n",
      " 62/500 [==>...........................] - ETA: 7:18 - loss: 1.7656 - regression_loss: 1.3227 - classification_loss: 0.4429\n",
      " 63/500 [==>...........................] - ETA: 7:16 - loss: 1.7652 - regression_loss: 1.3225 - classification_loss: 0.4427\n",
      " 64/500 [==>...........................] - ETA: 7:16 - loss: 1.7676 - regression_loss: 1.3248 - classification_loss: 0.4428\n",
      " 65/500 [==>...........................] - ETA: 7:16 - loss: 1.7635 - regression_loss: 1.3208 - classification_loss: 0.4427\n",
      " 66/500 [==>...........................] - ETA: 7:15 - loss: 1.7580 - regression_loss: 1.3170 - classification_loss: 0.4409\n",
      " 67/500 [===>..........................] - ETA: 7:14 - loss: 1.7511 - regression_loss: 1.3111 - classification_loss: 0.4400\n",
      " 68/500 [===>..........................] - ETA: 7:14 - loss: 1.7482 - regression_loss: 1.3096 - classification_loss: 0.4387\n",
      " 69/500 [===>..........................] - ETA: 7:13 - loss: 1.7520 - regression_loss: 1.3121 - classification_loss: 0.4399\n",
      " 70/500 [===>..........................] - ETA: 7:13 - loss: 1.7504 - regression_loss: 1.3102 - classification_loss: 0.4402\n",
      " 71/500 [===>..........................] - ETA: 7:26 - loss: 1.7473 - regression_loss: 1.3085 - classification_loss: 0.4388\n",
      " 72/500 [===>..........................] - ETA: 7:25 - loss: 1.7658 - regression_loss: 1.3201 - classification_loss: 0.4458\n",
      " 73/500 [===>..........................] - ETA: 7:25 - loss: 1.7736 - regression_loss: 1.3258 - classification_loss: 0.4478\n",
      " 74/500 [===>..........................] - ETA: 7:23 - loss: 1.7768 - regression_loss: 1.3266 - classification_loss: 0.4503\n",
      " 75/500 [===>..........................] - ETA: 7:22 - loss: 1.7706 - regression_loss: 1.3220 - classification_loss: 0.4486\n",
      " 76/500 [===>..........................] - ETA: 7:20 - loss: 1.7681 - regression_loss: 1.3191 - classification_loss: 0.4490\n",
      " 77/500 [===>..........................] - ETA: 7:19 - loss: 1.7791 - regression_loss: 1.3252 - classification_loss: 0.4539\n",
      " 78/500 [===>..........................] - ETA: 7:18 - loss: 1.7742 - regression_loss: 1.3231 - classification_loss: 0.4511\n",
      " 79/500 [===>..........................] - ETA: 7:16 - loss: 1.7872 - regression_loss: 1.3341 - classification_loss: 0.4531\n",
      " 80/500 [===>..........................] - ETA: 7:16 - loss: 1.7937 - regression_loss: 1.3393 - classification_loss: 0.4544\n",
      " 81/500 [===>..........................] - ETA: 7:14 - loss: 1.7904 - regression_loss: 1.3369 - classification_loss: 0.4534\n",
      " 82/500 [===>..........................] - ETA: 7:14 - loss: 1.7930 - regression_loss: 1.3376 - classification_loss: 0.4554\n",
      " 83/500 [===>..........................] - ETA: 7:12 - loss: 1.7899 - regression_loss: 1.3366 - classification_loss: 0.4533\n",
      " 84/500 [====>.........................] - ETA: 7:11 - loss: 1.7876 - regression_loss: 1.3353 - classification_loss: 0.4523\n",
      " 85/500 [====>.........................] - ETA: 7:10 - loss: 1.7817 - regression_loss: 1.3299 - classification_loss: 0.4518\n",
      " 86/500 [====>.........................] - ETA: 7:09 - loss: 1.7834 - regression_loss: 1.3317 - classification_loss: 0.4517\n",
      " 87/500 [====>.........................] - ETA: 7:07 - loss: 1.7862 - regression_loss: 1.3342 - classification_loss: 0.4520\n",
      " 88/500 [====>.........................] - ETA: 7:07 - loss: 1.7900 - regression_loss: 1.3368 - classification_loss: 0.4532\n",
      " 89/500 [====>.........................] - ETA: 7:06 - loss: 1.7856 - regression_loss: 1.3340 - classification_loss: 0.4517\n",
      " 90/500 [====>.........................] - ETA: 7:05 - loss: 1.7902 - regression_loss: 1.3368 - classification_loss: 0.4535\n",
      " 91/500 [====>.........................] - ETA: 7:04 - loss: 1.7793 - regression_loss: 1.3286 - classification_loss: 0.4507\n",
      " 92/500 [====>.........................] - ETA: 7:03 - loss: 1.7895 - regression_loss: 1.3346 - classification_loss: 0.4549\n",
      " 93/500 [====>.........................] - ETA: 7:02 - loss: 1.7836 - regression_loss: 1.3285 - classification_loss: 0.4551\n",
      " 94/500 [====>.........................] - ETA: 7:01 - loss: 1.7810 - regression_loss: 1.3271 - classification_loss: 0.4540\n",
      " 95/500 [====>.........................] - ETA: 7:00 - loss: 1.7817 - regression_loss: 1.3289 - classification_loss: 0.4528\n",
      " 96/500 [====>.........................] - ETA: 6:58 - loss: 1.7791 - regression_loss: 1.3273 - classification_loss: 0.4518\n",
      " 97/500 [====>.........................] - ETA: 6:57 - loss: 1.7718 - regression_loss: 1.3228 - classification_loss: 0.4491\n",
      " 98/500 [====>.........................] - ETA: 6:55 - loss: 1.7694 - regression_loss: 1.3210 - classification_loss: 0.4484\n",
      " 99/500 [====>.........................] - ETA: 6:55 - loss: 1.7709 - regression_loss: 1.3215 - classification_loss: 0.4495\n",
      "100/500 [=====>........................] - ETA: 6:53 - loss: 1.7734 - regression_loss: 1.3250 - classification_loss: 0.4483\n",
      "101/500 [=====>........................] - ETA: 6:52 - loss: 1.7750 - regression_loss: 1.3249 - classification_loss: 0.4500\n",
      "102/500 [=====>........................] - ETA: 6:52 - loss: 1.7810 - regression_loss: 1.3310 - classification_loss: 0.4501\n",
      "103/500 [=====>........................] - ETA: 6:50 - loss: 1.7822 - regression_loss: 1.3334 - classification_loss: 0.4488\n",
      "104/500 [=====>........................] - ETA: 6:49 - loss: 1.7854 - regression_loss: 1.3366 - classification_loss: 0.4488\n",
      "105/500 [=====>........................] - ETA: 6:48 - loss: 1.7844 - regression_loss: 1.3363 - classification_loss: 0.4481\n",
      "106/500 [=====>........................] - ETA: 6:47 - loss: 1.7849 - regression_loss: 1.3369 - classification_loss: 0.4480\n",
      "107/500 [=====>........................] - ETA: 6:46 - loss: 1.7928 - regression_loss: 1.3421 - classification_loss: 0.4507\n",
      "108/500 [=====>........................] - ETA: 6:44 - loss: 1.7988 - regression_loss: 1.3470 - classification_loss: 0.4518\n",
      "109/500 [=====>........................] - ETA: 6:44 - loss: 1.8023 - regression_loss: 1.3506 - classification_loss: 0.4517\n",
      "110/500 [=====>........................] - ETA: 6:43 - loss: 1.8120 - regression_loss: 1.3573 - classification_loss: 0.4547\n",
      "111/500 [=====>........................] - ETA: 6:42 - loss: 1.8113 - regression_loss: 1.3562 - classification_loss: 0.4552\n",
      "112/500 [=====>........................] - ETA: 6:40 - loss: 1.8111 - regression_loss: 1.3563 - classification_loss: 0.4549\n",
      "113/500 [=====>........................] - ETA: 6:39 - loss: 1.8133 - regression_loss: 1.3587 - classification_loss: 0.4546\n",
      "114/500 [=====>........................] - ETA: 6:38 - loss: 1.8105 - regression_loss: 1.3568 - classification_loss: 0.4537\n",
      "115/500 [=====>........................] - ETA: 6:38 - loss: 1.8151 - regression_loss: 1.3588 - classification_loss: 0.4564\n",
      "116/500 [=====>........................] - ETA: 6:36 - loss: 1.8119 - regression_loss: 1.3570 - classification_loss: 0.4549\n",
      "117/500 [======>.......................] - ETA: 6:35 - loss: 1.8081 - regression_loss: 1.3546 - classification_loss: 0.4535\n",
      "118/500 [======>.......................] - ETA: 6:34 - loss: 1.8035 - regression_loss: 1.3509 - classification_loss: 0.4526\n",
      "119/500 [======>.......................] - ETA: 6:33 - loss: 1.8045 - regression_loss: 1.3504 - classification_loss: 0.4541\n",
      "120/500 [======>.......................] - ETA: 6:32 - loss: 1.8043 - regression_loss: 1.3509 - classification_loss: 0.4534\n",
      "121/500 [======>.......................] - ETA: 6:32 - loss: 1.7984 - regression_loss: 1.3463 - classification_loss: 0.4521\n",
      "122/500 [======>.......................] - ETA: 6:30 - loss: 1.8041 - regression_loss: 1.3511 - classification_loss: 0.4530\n",
      "123/500 [======>.......................] - ETA: 6:29 - loss: 1.8010 - regression_loss: 1.3481 - classification_loss: 0.4529\n",
      "124/500 [======>.......................] - ETA: 6:28 - loss: 1.7975 - regression_loss: 1.3456 - classification_loss: 0.4520\n",
      "125/500 [======>.......................] - ETA: 6:27 - loss: 1.7969 - regression_loss: 1.3450 - classification_loss: 0.4519\n",
      "126/500 [======>.......................] - ETA: 6:26 - loss: 1.7959 - regression_loss: 1.3446 - classification_loss: 0.4513\n",
      "127/500 [======>.......................] - ETA: 6:25 - loss: 1.7951 - regression_loss: 1.3441 - classification_loss: 0.4510\n",
      "128/500 [======>.......................] - ETA: 6:24 - loss: 1.8001 - regression_loss: 1.3471 - classification_loss: 0.4531\n",
      "129/500 [======>.......................] - ETA: 6:23 - loss: 1.8070 - regression_loss: 1.3511 - classification_loss: 0.4559\n",
      "130/500 [======>.......................] - ETA: 6:22 - loss: 1.8066 - regression_loss: 1.3502 - classification_loss: 0.4564\n",
      "131/500 [======>.......................] - ETA: 6:21 - loss: 1.8063 - regression_loss: 1.3511 - classification_loss: 0.4553\n",
      "132/500 [======>.......................] - ETA: 6:20 - loss: 1.8067 - regression_loss: 1.3512 - classification_loss: 0.4556\n",
      "133/500 [======>.......................] - ETA: 6:19 - loss: 1.8047 - regression_loss: 1.3501 - classification_loss: 0.4546\n",
      "134/500 [=======>......................] - ETA: 6:18 - loss: 1.8054 - regression_loss: 1.3510 - classification_loss: 0.4544\n",
      "135/500 [=======>......................] - ETA: 6:17 - loss: 1.8035 - regression_loss: 1.3493 - classification_loss: 0.4542\n",
      "136/500 [=======>......................] - ETA: 6:16 - loss: 1.8081 - regression_loss: 1.3514 - classification_loss: 0.4567\n",
      "137/500 [=======>......................] - ETA: 6:14 - loss: 1.8022 - regression_loss: 1.3465 - classification_loss: 0.4557\n",
      "138/500 [=======>......................] - ETA: 6:14 - loss: 1.7981 - regression_loss: 1.3429 - classification_loss: 0.4552\n",
      "139/500 [=======>......................] - ETA: 6:13 - loss: 1.8022 - regression_loss: 1.3462 - classification_loss: 0.4560\n",
      "140/500 [=======>......................] - ETA: 6:12 - loss: 1.8005 - regression_loss: 1.3454 - classification_loss: 0.4552\n",
      "141/500 [=======>......................] - ETA: 6:11 - loss: 1.7975 - regression_loss: 1.3438 - classification_loss: 0.4537\n",
      "142/500 [=======>......................] - ETA: 6:10 - loss: 1.7921 - regression_loss: 1.3402 - classification_loss: 0.4519\n",
      "143/500 [=======>......................] - ETA: 6:08 - loss: 1.7872 - regression_loss: 1.3367 - classification_loss: 0.4505\n",
      "144/500 [=======>......................] - ETA: 6:07 - loss: 1.7860 - regression_loss: 1.3362 - classification_loss: 0.4498\n",
      "145/500 [=======>......................] - ETA: 6:07 - loss: 1.7897 - regression_loss: 1.3379 - classification_loss: 0.4518\n",
      "146/500 [=======>......................] - ETA: 6:05 - loss: 1.7909 - regression_loss: 1.3380 - classification_loss: 0.4529\n",
      "147/500 [=======>......................] - ETA: 6:04 - loss: 1.7995 - regression_loss: 1.3428 - classification_loss: 0.4567\n",
      "148/500 [=======>......................] - ETA: 6:03 - loss: 1.7934 - regression_loss: 1.3378 - classification_loss: 0.4556\n",
      "149/500 [=======>......................] - ETA: 6:02 - loss: 1.7946 - regression_loss: 1.3383 - classification_loss: 0.4564\n",
      "150/500 [========>.....................] - ETA: 6:01 - loss: 1.7905 - regression_loss: 1.3360 - classification_loss: 0.4545\n",
      "151/500 [========>.....................] - ETA: 6:00 - loss: 1.7892 - regression_loss: 1.3349 - classification_loss: 0.4543\n",
      "152/500 [========>.....................] - ETA: 5:59 - loss: 1.7924 - regression_loss: 1.3367 - classification_loss: 0.4557\n",
      "153/500 [========>.....................] - ETA: 5:58 - loss: 1.7936 - regression_loss: 1.3371 - classification_loss: 0.4564\n",
      "154/500 [========>.....................] - ETA: 5:57 - loss: 1.7938 - regression_loss: 1.3369 - classification_loss: 0.4569\n",
      "155/500 [========>.....................] - ETA: 5:56 - loss: 1.7917 - regression_loss: 1.3344 - classification_loss: 0.4573\n",
      "156/500 [========>.....................] - ETA: 5:55 - loss: 1.7914 - regression_loss: 1.3345 - classification_loss: 0.4568\n",
      "157/500 [========>.....................] - ETA: 5:54 - loss: 1.7936 - regression_loss: 1.3368 - classification_loss: 0.4568\n",
      "158/500 [========>.....................] - ETA: 5:53 - loss: 1.8009 - regression_loss: 1.3419 - classification_loss: 0.4590\n",
      "159/500 [========>.....................] - ETA: 5:52 - loss: 1.7970 - regression_loss: 1.3394 - classification_loss: 0.4576\n",
      "160/500 [========>.....................] - ETA: 5:51 - loss: 1.7991 - regression_loss: 1.3409 - classification_loss: 0.4582\n",
      "161/500 [========>.....................] - ETA: 5:50 - loss: 1.7987 - regression_loss: 1.3410 - classification_loss: 0.4577\n",
      "162/500 [========>.....................] - ETA: 5:49 - loss: 1.7974 - regression_loss: 1.3400 - classification_loss: 0.4574\n",
      "163/500 [========>.....................] - ETA: 5:48 - loss: 1.7964 - regression_loss: 1.3389 - classification_loss: 0.4575\n",
      "164/500 [========>.....................] - ETA: 5:47 - loss: 1.7950 - regression_loss: 1.3370 - classification_loss: 0.4581\n",
      "165/500 [========>.....................] - ETA: 5:46 - loss: 1.7934 - regression_loss: 1.3348 - classification_loss: 0.4586\n",
      "166/500 [========>.....................] - ETA: 5:45 - loss: 1.7953 - regression_loss: 1.3359 - classification_loss: 0.4594\n",
      "167/500 [=========>....................] - ETA: 5:44 - loss: 1.7911 - regression_loss: 1.3324 - classification_loss: 0.4588\n",
      "168/500 [=========>....................] - ETA: 5:43 - loss: 1.7888 - regression_loss: 1.3310 - classification_loss: 0.4579\n",
      "169/500 [=========>....................] - ETA: 5:42 - loss: 1.7907 - regression_loss: 1.3326 - classification_loss: 0.4581\n",
      "170/500 [=========>....................] - ETA: 5:41 - loss: 1.7925 - regression_loss: 1.3345 - classification_loss: 0.4581\n",
      "171/500 [=========>....................] - ETA: 5:40 - loss: 1.7985 - regression_loss: 1.3388 - classification_loss: 0.4596\n",
      "172/500 [=========>....................] - ETA: 5:39 - loss: 1.8000 - regression_loss: 1.3398 - classification_loss: 0.4603\n",
      "173/500 [=========>....................] - ETA: 5:38 - loss: 1.7964 - regression_loss: 1.3371 - classification_loss: 0.4593\n",
      "174/500 [=========>....................] - ETA: 5:36 - loss: 1.7950 - regression_loss: 1.3358 - classification_loss: 0.4592\n",
      "175/500 [=========>....................] - ETA: 5:36 - loss: 1.8030 - regression_loss: 1.3415 - classification_loss: 0.4615\n",
      "176/500 [=========>....................] - ETA: 5:35 - loss: 1.8056 - regression_loss: 1.3435 - classification_loss: 0.4621\n",
      "177/500 [=========>....................] - ETA: 5:33 - loss: 1.8060 - regression_loss: 1.3436 - classification_loss: 0.4624\n",
      "178/500 [=========>....................] - ETA: 5:32 - loss: 1.8061 - regression_loss: 1.3433 - classification_loss: 0.4628\n",
      "179/500 [=========>....................] - ETA: 5:31 - loss: 1.8049 - regression_loss: 1.3430 - classification_loss: 0.4619\n",
      "180/500 [=========>....................] - ETA: 5:30 - loss: 1.8030 - regression_loss: 1.3412 - classification_loss: 0.4617\n",
      "181/500 [=========>....................] - ETA: 5:29 - loss: 1.8033 - regression_loss: 1.3406 - classification_loss: 0.4627\n",
      "182/500 [=========>....................] - ETA: 5:28 - loss: 1.8046 - regression_loss: 1.3404 - classification_loss: 0.4642\n",
      "183/500 [=========>....................] - ETA: 5:27 - loss: 1.8094 - regression_loss: 1.3446 - classification_loss: 0.4648\n",
      "184/500 [==========>...................] - ETA: 5:26 - loss: 1.8088 - regression_loss: 1.3441 - classification_loss: 0.4647\n",
      "185/500 [==========>...................] - ETA: 5:24 - loss: 1.8043 - regression_loss: 1.3405 - classification_loss: 0.4638\n",
      "186/500 [==========>...................] - ETA: 5:23 - loss: 1.8008 - regression_loss: 1.3379 - classification_loss: 0.4629\n",
      "187/500 [==========>...................] - ETA: 5:22 - loss: 1.8018 - regression_loss: 1.3394 - classification_loss: 0.4624\n",
      "188/500 [==========>...................] - ETA: 5:21 - loss: 1.8062 - regression_loss: 1.3421 - classification_loss: 0.4641\n",
      "189/500 [==========>...................] - ETA: 5:20 - loss: 1.8080 - regression_loss: 1.3432 - classification_loss: 0.4648\n",
      "190/500 [==========>...................] - ETA: 5:19 - loss: 1.8105 - regression_loss: 1.3452 - classification_loss: 0.4653\n",
      "191/500 [==========>...................] - ETA: 5:18 - loss: 1.8150 - regression_loss: 1.3485 - classification_loss: 0.4665\n",
      "192/500 [==========>...................] - ETA: 5:17 - loss: 1.8167 - regression_loss: 1.3487 - classification_loss: 0.4679\n",
      "193/500 [==========>...................] - ETA: 5:16 - loss: 1.8176 - regression_loss: 1.3498 - classification_loss: 0.4678\n",
      "194/500 [==========>...................] - ETA: 5:14 - loss: 1.8204 - regression_loss: 1.3516 - classification_loss: 0.4688\n",
      "195/500 [==========>...................] - ETA: 5:14 - loss: 1.8234 - regression_loss: 1.3531 - classification_loss: 0.4703\n",
      "196/500 [==========>...................] - ETA: 5:13 - loss: 1.8250 - regression_loss: 1.3542 - classification_loss: 0.4707\n",
      "197/500 [==========>...................] - ETA: 5:12 - loss: 1.8259 - regression_loss: 1.3556 - classification_loss: 0.4703\n",
      "198/500 [==========>...................] - ETA: 5:11 - loss: 1.8271 - regression_loss: 1.3564 - classification_loss: 0.4707\n",
      "199/500 [==========>...................] - ETA: 5:10 - loss: 1.8317 - regression_loss: 1.3603 - classification_loss: 0.4714\n",
      "200/500 [===========>..................] - ETA: 5:09 - loss: 1.8337 - regression_loss: 1.3630 - classification_loss: 0.4707\n",
      "201/500 [===========>..................] - ETA: 5:08 - loss: 1.8322 - regression_loss: 1.3620 - classification_loss: 0.4702\n",
      "202/500 [===========>..................] - ETA: 5:07 - loss: 1.8308 - regression_loss: 1.3608 - classification_loss: 0.4699\n",
      "203/500 [===========>..................] - ETA: 5:06 - loss: 1.8274 - regression_loss: 1.3583 - classification_loss: 0.4691\n",
      "204/500 [===========>..................] - ETA: 5:04 - loss: 1.8257 - regression_loss: 1.3569 - classification_loss: 0.4689\n",
      "205/500 [===========>..................] - ETA: 5:04 - loss: 1.8264 - regression_loss: 1.3569 - classification_loss: 0.4695\n",
      "206/500 [===========>..................] - ETA: 5:03 - loss: 1.8278 - regression_loss: 1.3578 - classification_loss: 0.4699\n",
      "207/500 [===========>..................] - ETA: 5:02 - loss: 1.8340 - regression_loss: 1.3615 - classification_loss: 0.4725\n",
      "208/500 [===========>..................] - ETA: 5:01 - loss: 1.8353 - regression_loss: 1.3633 - classification_loss: 0.4720\n",
      "209/500 [===========>..................] - ETA: 4:59 - loss: 1.8329 - regression_loss: 1.3611 - classification_loss: 0.4719\n",
      "210/500 [===========>..................] - ETA: 4:58 - loss: 1.8332 - regression_loss: 1.3611 - classification_loss: 0.4721\n",
      "211/500 [===========>..................] - ETA: 4:57 - loss: 1.8363 - regression_loss: 1.3637 - classification_loss: 0.4726\n",
      "212/500 [===========>..................] - ETA: 4:56 - loss: 1.8425 - regression_loss: 1.3678 - classification_loss: 0.4748\n",
      "213/500 [===========>..................] - ETA: 4:55 - loss: 1.8406 - regression_loss: 1.3664 - classification_loss: 0.4742\n",
      "214/500 [===========>..................] - ETA: 4:54 - loss: 1.8430 - regression_loss: 1.3689 - classification_loss: 0.4741\n",
      "215/500 [===========>..................] - ETA: 4:53 - loss: 1.8415 - regression_loss: 1.3683 - classification_loss: 0.4732\n",
      "216/500 [===========>..................] - ETA: 4:52 - loss: 1.8389 - regression_loss: 1.3667 - classification_loss: 0.4721\n",
      "217/500 [============>.................] - ETA: 4:51 - loss: 1.8382 - regression_loss: 1.3665 - classification_loss: 0.4717\n",
      "218/500 [============>.................] - ETA: 4:50 - loss: 1.8396 - regression_loss: 1.3669 - classification_loss: 0.4727\n",
      "219/500 [============>.................] - ETA: 4:49 - loss: 1.8416 - regression_loss: 1.3686 - classification_loss: 0.4730\n",
      "220/500 [============>.................] - ETA: 4:48 - loss: 1.8435 - regression_loss: 1.3696 - classification_loss: 0.4739\n",
      "221/500 [============>.................] - ETA: 4:47 - loss: 1.8476 - regression_loss: 1.3723 - classification_loss: 0.4753\n",
      "222/500 [============>.................] - ETA: 4:46 - loss: 1.8488 - regression_loss: 1.3732 - classification_loss: 0.4756\n",
      "223/500 [============>.................] - ETA: 4:44 - loss: 1.8458 - regression_loss: 1.3710 - classification_loss: 0.4749\n",
      "224/500 [============>.................] - ETA: 4:44 - loss: 1.8443 - regression_loss: 1.3699 - classification_loss: 0.4744\n",
      "225/500 [============>.................] - ETA: 4:43 - loss: 1.8423 - regression_loss: 1.3681 - classification_loss: 0.4742\n",
      "226/500 [============>.................] - ETA: 4:42 - loss: 1.8408 - regression_loss: 1.3663 - classification_loss: 0.4745\n",
      "227/500 [============>.................] - ETA: 4:40 - loss: 1.8411 - regression_loss: 1.3673 - classification_loss: 0.4738\n",
      "228/500 [============>.................] - ETA: 4:39 - loss: 1.8395 - regression_loss: 1.3664 - classification_loss: 0.4731\n",
      "229/500 [============>.................] - ETA: 4:39 - loss: 1.8392 - regression_loss: 1.3665 - classification_loss: 0.4727\n",
      "230/500 [============>.................] - ETA: 4:37 - loss: 1.8383 - regression_loss: 1.3656 - classification_loss: 0.4727\n",
      "231/500 [============>.................] - ETA: 4:37 - loss: 1.8373 - regression_loss: 1.3645 - classification_loss: 0.4727\n",
      "232/500 [============>.................] - ETA: 4:36 - loss: 1.8363 - regression_loss: 1.3643 - classification_loss: 0.4720\n",
      "233/500 [============>.................] - ETA: 4:35 - loss: 1.8340 - regression_loss: 1.3627 - classification_loss: 0.4713\n",
      "234/500 [=============>................] - ETA: 4:34 - loss: 1.8395 - regression_loss: 1.3666 - classification_loss: 0.4728\n",
      "235/500 [=============>................] - ETA: 4:33 - loss: 1.8372 - regression_loss: 1.3653 - classification_loss: 0.4719\n",
      "236/500 [=============>................] - ETA: 4:32 - loss: 1.8408 - regression_loss: 1.3674 - classification_loss: 0.4734\n",
      "237/500 [=============>................] - ETA: 4:31 - loss: 1.8381 - regression_loss: 1.3653 - classification_loss: 0.4728\n",
      "238/500 [=============>................] - ETA: 4:29 - loss: 1.8382 - regression_loss: 1.3655 - classification_loss: 0.4726\n",
      "239/500 [=============>................] - ETA: 4:29 - loss: 1.8392 - regression_loss: 1.3668 - classification_loss: 0.4724\n",
      "240/500 [=============>................] - ETA: 4:28 - loss: 1.8370 - regression_loss: 1.3655 - classification_loss: 0.4715\n",
      "241/500 [=============>................] - ETA: 4:26 - loss: 1.8356 - regression_loss: 1.3644 - classification_loss: 0.4712\n",
      "242/500 [=============>................] - ETA: 4:25 - loss: 1.8349 - regression_loss: 1.3638 - classification_loss: 0.4712\n",
      "243/500 [=============>................] - ETA: 4:24 - loss: 1.8342 - regression_loss: 1.3634 - classification_loss: 0.4708\n",
      "244/500 [=============>................] - ETA: 4:23 - loss: 1.8311 - regression_loss: 1.3612 - classification_loss: 0.4699\n",
      "245/500 [=============>................] - ETA: 4:22 - loss: 1.8329 - regression_loss: 1.3622 - classification_loss: 0.4706\n",
      "246/500 [=============>................] - ETA: 4:21 - loss: 1.8327 - regression_loss: 1.3626 - classification_loss: 0.4701\n",
      "247/500 [=============>................] - ETA: 4:20 - loss: 1.8369 - regression_loss: 1.3653 - classification_loss: 0.4716\n",
      "248/500 [=============>................] - ETA: 4:19 - loss: 1.8345 - regression_loss: 1.3636 - classification_loss: 0.4709\n",
      "249/500 [=============>................] - ETA: 4:18 - loss: 1.8332 - regression_loss: 1.3632 - classification_loss: 0.4700\n",
      "250/500 [==============>...............] - ETA: 4:17 - loss: 1.8366 - regression_loss: 1.3656 - classification_loss: 0.4710\n",
      "251/500 [==============>...............] - ETA: 4:16 - loss: 1.8367 - regression_loss: 1.3652 - classification_loss: 0.4715\n",
      "252/500 [==============>...............] - ETA: 4:15 - loss: 1.8391 - regression_loss: 1.3667 - classification_loss: 0.4724\n",
      "253/500 [==============>...............] - ETA: 4:14 - loss: 1.8370 - regression_loss: 1.3653 - classification_loss: 0.4716\n",
      "254/500 [==============>...............] - ETA: 4:13 - loss: 1.8408 - regression_loss: 1.3687 - classification_loss: 0.4722\n",
      "255/500 [==============>...............] - ETA: 4:12 - loss: 1.8409 - regression_loss: 1.3689 - classification_loss: 0.4720\n",
      "256/500 [==============>...............] - ETA: 4:11 - loss: 1.8392 - regression_loss: 1.3673 - classification_loss: 0.4719\n",
      "257/500 [==============>...............] - ETA: 4:10 - loss: 1.8435 - regression_loss: 1.3694 - classification_loss: 0.4741\n",
      "258/500 [==============>...............] - ETA: 4:09 - loss: 1.8424 - regression_loss: 1.3688 - classification_loss: 0.4736\n",
      "259/500 [==============>...............] - ETA: 4:08 - loss: 1.8408 - regression_loss: 1.3678 - classification_loss: 0.4729\n",
      "260/500 [==============>...............] - ETA: 4:07 - loss: 1.8432 - regression_loss: 1.3693 - classification_loss: 0.4739\n",
      "261/500 [==============>...............] - ETA: 4:06 - loss: 1.8472 - regression_loss: 1.3722 - classification_loss: 0.4750\n",
      "262/500 [==============>...............] - ETA: 4:05 - loss: 1.8447 - regression_loss: 1.3703 - classification_loss: 0.4744\n",
      "263/500 [==============>...............] - ETA: 4:04 - loss: 1.8471 - regression_loss: 1.3713 - classification_loss: 0.4757\n",
      "264/500 [==============>...............] - ETA: 4:02 - loss: 1.8476 - regression_loss: 1.3716 - classification_loss: 0.4760\n",
      "265/500 [==============>...............] - ETA: 4:01 - loss: 1.8480 - regression_loss: 1.3720 - classification_loss: 0.4759\n",
      "266/500 [==============>...............] - ETA: 4:00 - loss: 1.8454 - regression_loss: 1.3704 - classification_loss: 0.4751\n",
      "267/500 [===============>..............] - ETA: 3:59 - loss: 1.8500 - regression_loss: 1.3731 - classification_loss: 0.4769\n",
      "268/500 [===============>..............] - ETA: 3:58 - loss: 1.8534 - regression_loss: 1.3755 - classification_loss: 0.4779\n",
      "269/500 [===============>..............] - ETA: 3:57 - loss: 1.8521 - regression_loss: 1.3748 - classification_loss: 0.4773\n",
      "270/500 [===============>..............] - ETA: 3:56 - loss: 1.8521 - regression_loss: 1.3751 - classification_loss: 0.4770\n",
      "271/500 [===============>..............] - ETA: 3:55 - loss: 1.8533 - regression_loss: 1.3765 - classification_loss: 0.4768\n",
      "272/500 [===============>..............] - ETA: 3:54 - loss: 1.8580 - regression_loss: 1.3802 - classification_loss: 0.4778\n",
      "273/500 [===============>..............] - ETA: 3:53 - loss: 1.8557 - regression_loss: 1.3786 - classification_loss: 0.4771\n",
      "274/500 [===============>..............] - ETA: 3:52 - loss: 1.8546 - regression_loss: 1.3779 - classification_loss: 0.4767\n",
      "275/500 [===============>..............] - ETA: 3:51 - loss: 1.8539 - regression_loss: 1.3775 - classification_loss: 0.4764\n",
      "276/500 [===============>..............] - ETA: 3:50 - loss: 1.8533 - regression_loss: 1.3770 - classification_loss: 0.4763\n",
      "277/500 [===============>..............] - ETA: 3:49 - loss: 1.8563 - regression_loss: 1.3795 - classification_loss: 0.4768\n",
      "278/500 [===============>..............] - ETA: 3:48 - loss: 1.8549 - regression_loss: 1.3788 - classification_loss: 0.4761\n",
      "279/500 [===============>..............] - ETA: 3:47 - loss: 1.8551 - regression_loss: 1.3791 - classification_loss: 0.4759\n",
      "280/500 [===============>..............] - ETA: 3:46 - loss: 1.8560 - regression_loss: 1.3796 - classification_loss: 0.4764\n",
      "281/500 [===============>..............] - ETA: 3:45 - loss: 1.8554 - regression_loss: 1.3789 - classification_loss: 0.4765\n",
      "282/500 [===============>..............] - ETA: 3:44 - loss: 1.8572 - regression_loss: 1.3802 - classification_loss: 0.4770\n",
      "283/500 [===============>..............] - ETA: 3:43 - loss: 1.8564 - regression_loss: 1.3797 - classification_loss: 0.4767\n",
      "284/500 [================>.............] - ETA: 3:42 - loss: 1.8556 - regression_loss: 1.3791 - classification_loss: 0.4765\n",
      "285/500 [================>.............] - ETA: 3:41 - loss: 1.8549 - regression_loss: 1.3781 - classification_loss: 0.4768\n",
      "286/500 [================>.............] - ETA: 3:40 - loss: 1.8528 - regression_loss: 1.3765 - classification_loss: 0.4762\n",
      "287/500 [================>.............] - ETA: 3:39 - loss: 1.8553 - regression_loss: 1.3781 - classification_loss: 0.4772\n",
      "288/500 [================>.............] - ETA: 3:38 - loss: 1.8538 - regression_loss: 1.3768 - classification_loss: 0.4770\n",
      "289/500 [================>.............] - ETA: 3:37 - loss: 1.8510 - regression_loss: 1.3747 - classification_loss: 0.4763\n",
      "290/500 [================>.............] - ETA: 3:36 - loss: 1.8552 - regression_loss: 1.3773 - classification_loss: 0.4779\n",
      "291/500 [================>.............] - ETA: 3:35 - loss: 1.8550 - regression_loss: 1.3778 - classification_loss: 0.4772\n",
      "292/500 [================>.............] - ETA: 3:34 - loss: 1.8533 - regression_loss: 1.3766 - classification_loss: 0.4767\n",
      "293/500 [================>.............] - ETA: 3:33 - loss: 1.8515 - regression_loss: 1.3751 - classification_loss: 0.4765\n",
      "294/500 [================>.............] - ETA: 3:31 - loss: 1.8531 - regression_loss: 1.3761 - classification_loss: 0.4770\n",
      "295/500 [================>.............] - ETA: 3:30 - loss: 1.8523 - regression_loss: 1.3753 - classification_loss: 0.4770\n",
      "296/500 [================>.............] - ETA: 3:29 - loss: 1.8531 - regression_loss: 1.3762 - classification_loss: 0.4769\n",
      "297/500 [================>.............] - ETA: 3:28 - loss: 1.8522 - regression_loss: 1.3757 - classification_loss: 0.4764\n",
      "298/500 [================>.............] - ETA: 3:27 - loss: 1.8530 - regression_loss: 1.3769 - classification_loss: 0.4760\n",
      "299/500 [================>.............] - ETA: 3:26 - loss: 1.8555 - regression_loss: 1.3785 - classification_loss: 0.4770\n",
      "300/500 [=================>............] - ETA: 3:25 - loss: 1.8537 - regression_loss: 1.3771 - classification_loss: 0.4766\n",
      "301/500 [=================>............] - ETA: 3:24 - loss: 1.8511 - regression_loss: 1.3752 - classification_loss: 0.4759\n",
      "302/500 [=================>............] - ETA: 3:23 - loss: 1.8549 - regression_loss: 1.3779 - classification_loss: 0.4770\n",
      "303/500 [=================>............] - ETA: 3:22 - loss: 1.8578 - regression_loss: 1.3797 - classification_loss: 0.4781\n",
      "304/500 [=================>............] - ETA: 3:21 - loss: 1.8588 - regression_loss: 1.3804 - classification_loss: 0.4783\n",
      "305/500 [=================>............] - ETA: 3:20 - loss: 1.8564 - regression_loss: 1.3787 - classification_loss: 0.4776\n",
      "306/500 [=================>............] - ETA: 3:19 - loss: 1.8586 - regression_loss: 1.3806 - classification_loss: 0.4781\n",
      "307/500 [=================>............] - ETA: 3:18 - loss: 1.8600 - regression_loss: 1.3816 - classification_loss: 0.4784\n",
      "308/500 [=================>............] - ETA: 3:17 - loss: 1.8594 - regression_loss: 1.3811 - classification_loss: 0.4782\n",
      "309/500 [=================>............] - ETA: 3:16 - loss: 1.8584 - regression_loss: 1.3808 - classification_loss: 0.4775\n",
      "310/500 [=================>............] - ETA: 3:15 - loss: 1.8570 - regression_loss: 1.3794 - classification_loss: 0.4776\n",
      "311/500 [=================>............] - ETA: 3:14 - loss: 1.8569 - regression_loss: 1.3794 - classification_loss: 0.4775\n",
      "312/500 [=================>............] - ETA: 3:13 - loss: 1.8576 - regression_loss: 1.3798 - classification_loss: 0.4777\n",
      "313/500 [=================>............] - ETA: 3:12 - loss: 1.8605 - regression_loss: 1.3820 - classification_loss: 0.4785\n",
      "314/500 [=================>............] - ETA: 3:11 - loss: 1.8633 - regression_loss: 1.3835 - classification_loss: 0.4797\n",
      "315/500 [=================>............] - ETA: 3:10 - loss: 1.8649 - regression_loss: 1.3842 - classification_loss: 0.4807\n",
      "316/500 [=================>............] - ETA: 3:09 - loss: 1.8646 - regression_loss: 1.3845 - classification_loss: 0.4801\n",
      "317/500 [==================>...........] - ETA: 3:08 - loss: 1.8655 - regression_loss: 1.3856 - classification_loss: 0.4799\n",
      "318/500 [==================>...........] - ETA: 3:07 - loss: 1.8634 - regression_loss: 1.3840 - classification_loss: 0.4794\n",
      "319/500 [==================>...........] - ETA: 3:06 - loss: 1.8642 - regression_loss: 1.3847 - classification_loss: 0.4795\n",
      "320/500 [==================>...........] - ETA: 3:05 - loss: 1.8666 - regression_loss: 1.3855 - classification_loss: 0.4811\n",
      "321/500 [==================>...........] - ETA: 3:05 - loss: 1.8635 - regression_loss: 1.3831 - classification_loss: 0.4804\n",
      "322/500 [==================>...........] - ETA: 3:04 - loss: 1.8621 - regression_loss: 1.3822 - classification_loss: 0.4799\n",
      "323/500 [==================>...........] - ETA: 3:03 - loss: 1.8635 - regression_loss: 1.3830 - classification_loss: 0.4805\n",
      "324/500 [==================>...........] - ETA: 3:02 - loss: 1.8610 - regression_loss: 1.3811 - classification_loss: 0.4799\n",
      "325/500 [==================>...........] - ETA: 3:01 - loss: 1.8594 - regression_loss: 1.3801 - classification_loss: 0.4793\n",
      "326/500 [==================>...........] - ETA: 3:00 - loss: 1.8634 - regression_loss: 1.3832 - classification_loss: 0.4802\n",
      "327/500 [==================>...........] - ETA: 2:59 - loss: 1.8632 - regression_loss: 1.3835 - classification_loss: 0.4797\n",
      "328/500 [==================>...........] - ETA: 2:58 - loss: 1.8625 - regression_loss: 1.3832 - classification_loss: 0.4793\n",
      "329/500 [==================>...........] - ETA: 2:57 - loss: 1.8612 - regression_loss: 1.3825 - classification_loss: 0.4787\n",
      "330/500 [==================>...........] - ETA: 2:56 - loss: 1.8608 - regression_loss: 1.3825 - classification_loss: 0.4783\n",
      "331/500 [==================>...........] - ETA: 2:54 - loss: 1.8619 - regression_loss: 1.3834 - classification_loss: 0.4785\n",
      "332/500 [==================>...........] - ETA: 2:53 - loss: 1.8615 - regression_loss: 1.3834 - classification_loss: 0.4781\n",
      "333/500 [==================>...........] - ETA: 2:52 - loss: 1.8620 - regression_loss: 1.3838 - classification_loss: 0.4783\n",
      "334/500 [===================>..........] - ETA: 2:51 - loss: 1.8637 - regression_loss: 1.3853 - classification_loss: 0.4784\n",
      "335/500 [===================>..........] - ETA: 2:50 - loss: 1.8631 - regression_loss: 1.3849 - classification_loss: 0.4781\n",
      "336/500 [===================>..........] - ETA: 2:49 - loss: 1.8619 - regression_loss: 1.3842 - classification_loss: 0.4776\n",
      "337/500 [===================>..........] - ETA: 2:48 - loss: 1.8594 - regression_loss: 1.3825 - classification_loss: 0.4769\n",
      "338/500 [===================>..........] - ETA: 2:47 - loss: 1.8619 - regression_loss: 1.3844 - classification_loss: 0.4775\n",
      "339/500 [===================>..........] - ETA: 2:46 - loss: 1.8638 - regression_loss: 1.3858 - classification_loss: 0.4780\n",
      "340/500 [===================>..........] - ETA: 2:45 - loss: 1.8651 - regression_loss: 1.3865 - classification_loss: 0.4786\n",
      "341/500 [===================>..........] - ETA: 2:44 - loss: 1.8664 - regression_loss: 1.3865 - classification_loss: 0.4799\n",
      "342/500 [===================>..........] - ETA: 2:43 - loss: 1.8671 - regression_loss: 1.3869 - classification_loss: 0.4802\n",
      "343/500 [===================>..........] - ETA: 2:42 - loss: 1.8698 - regression_loss: 1.3885 - classification_loss: 0.4813\n",
      "344/500 [===================>..........] - ETA: 2:41 - loss: 1.8701 - regression_loss: 1.3889 - classification_loss: 0.4811\n",
      "345/500 [===================>..........] - ETA: 2:40 - loss: 1.8727 - regression_loss: 1.3910 - classification_loss: 0.4817\n",
      "346/500 [===================>..........] - ETA: 2:39 - loss: 1.8746 - regression_loss: 1.3918 - classification_loss: 0.4828\n",
      "347/500 [===================>..........] - ETA: 2:38 - loss: 1.8741 - regression_loss: 1.3911 - classification_loss: 0.4829\n",
      "348/500 [===================>..........] - ETA: 2:37 - loss: 1.8713 - regression_loss: 1.3891 - classification_loss: 0.4823\n",
      "349/500 [===================>..........] - ETA: 2:36 - loss: 1.8717 - regression_loss: 1.3892 - classification_loss: 0.4825\n",
      "350/500 [====================>.........] - ETA: 2:35 - loss: 1.8720 - regression_loss: 1.3889 - classification_loss: 0.4831\n",
      "351/500 [====================>.........] - ETA: 2:34 - loss: 1.8741 - regression_loss: 1.3902 - classification_loss: 0.4838\n",
      "352/500 [====================>.........] - ETA: 2:33 - loss: 1.8757 - regression_loss: 1.3914 - classification_loss: 0.4843\n",
      "353/500 [====================>.........] - ETA: 2:32 - loss: 1.8759 - regression_loss: 1.3916 - classification_loss: 0.4843\n",
      "354/500 [====================>.........] - ETA: 2:31 - loss: 1.8743 - regression_loss: 1.3906 - classification_loss: 0.4838\n",
      "355/500 [====================>.........] - ETA: 2:30 - loss: 1.8751 - regression_loss: 1.3907 - classification_loss: 0.4844\n",
      "356/500 [====================>.........] - ETA: 2:29 - loss: 1.8762 - regression_loss: 1.3917 - classification_loss: 0.4845\n",
      "357/500 [====================>.........] - ETA: 2:28 - loss: 1.8767 - regression_loss: 1.3924 - classification_loss: 0.4843\n",
      "358/500 [====================>.........] - ETA: 2:27 - loss: 1.8775 - regression_loss: 1.3926 - classification_loss: 0.4849\n",
      "359/500 [====================>.........] - ETA: 2:25 - loss: 1.8792 - regression_loss: 1.3934 - classification_loss: 0.4858\n",
      "360/500 [====================>.........] - ETA: 2:24 - loss: 1.8775 - regression_loss: 1.3921 - classification_loss: 0.4854\n",
      "361/500 [====================>.........] - ETA: 2:23 - loss: 1.8764 - regression_loss: 1.3915 - classification_loss: 0.4849\n",
      "362/500 [====================>.........] - ETA: 2:22 - loss: 1.8758 - regression_loss: 1.3910 - classification_loss: 0.4848\n",
      "363/500 [====================>.........] - ETA: 2:21 - loss: 1.8773 - regression_loss: 1.3918 - classification_loss: 0.4854\n",
      "364/500 [====================>.........] - ETA: 2:20 - loss: 1.8777 - regression_loss: 1.3925 - classification_loss: 0.4852\n",
      "365/500 [====================>.........] - ETA: 2:19 - loss: 1.8788 - regression_loss: 1.3933 - classification_loss: 0.4854\n",
      "366/500 [====================>.........] - ETA: 2:18 - loss: 1.8792 - regression_loss: 1.3938 - classification_loss: 0.4855\n",
      "367/500 [=====================>........] - ETA: 2:17 - loss: 1.8783 - regression_loss: 1.3931 - classification_loss: 0.4852\n",
      "368/500 [=====================>........] - ETA: 2:16 - loss: 1.8785 - regression_loss: 1.3933 - classification_loss: 0.4852\n",
      "369/500 [=====================>........] - ETA: 2:15 - loss: 1.8785 - regression_loss: 1.3934 - classification_loss: 0.4851\n",
      "370/500 [=====================>........] - ETA: 2:14 - loss: 1.8769 - regression_loss: 1.3919 - classification_loss: 0.4850\n",
      "371/500 [=====================>........] - ETA: 2:13 - loss: 1.8768 - regression_loss: 1.3914 - classification_loss: 0.4855\n",
      "372/500 [=====================>........] - ETA: 2:12 - loss: 1.8756 - regression_loss: 1.3906 - classification_loss: 0.4850\n",
      "373/500 [=====================>........] - ETA: 2:11 - loss: 1.8747 - regression_loss: 1.3898 - classification_loss: 0.4848\n",
      "374/500 [=====================>........] - ETA: 2:10 - loss: 1.8759 - regression_loss: 1.3908 - classification_loss: 0.4851\n",
      "375/500 [=====================>........] - ETA: 2:09 - loss: 1.8745 - regression_loss: 1.3895 - classification_loss: 0.4850\n",
      "376/500 [=====================>........] - ETA: 2:08 - loss: 1.8737 - regression_loss: 1.3890 - classification_loss: 0.4848\n",
      "377/500 [=====================>........] - ETA: 2:07 - loss: 1.8742 - regression_loss: 1.3897 - classification_loss: 0.4845\n",
      "378/500 [=====================>........] - ETA: 2:06 - loss: 1.8733 - regression_loss: 1.3890 - classification_loss: 0.4842\n",
      "379/500 [=====================>........] - ETA: 2:05 - loss: 1.8760 - regression_loss: 1.3901 - classification_loss: 0.4859\n",
      "380/500 [=====================>........] - ETA: 2:04 - loss: 1.8767 - regression_loss: 1.3909 - classification_loss: 0.4858\n",
      "381/500 [=====================>........] - ETA: 2:03 - loss: 1.8752 - regression_loss: 1.3900 - classification_loss: 0.4852\n",
      "382/500 [=====================>........] - ETA: 2:01 - loss: 1.8758 - regression_loss: 1.3907 - classification_loss: 0.4852\n",
      "383/500 [=====================>........] - ETA: 2:00 - loss: 1.8756 - regression_loss: 1.3903 - classification_loss: 0.4853\n",
      "384/500 [======================>.......] - ETA: 1:59 - loss: 1.8764 - regression_loss: 1.3911 - classification_loss: 0.4853\n",
      "385/500 [======================>.......] - ETA: 1:58 - loss: 1.8755 - regression_loss: 1.3906 - classification_loss: 0.4848\n",
      "386/500 [======================>.......] - ETA: 1:57 - loss: 1.8752 - regression_loss: 1.3903 - classification_loss: 0.4849\n",
      "387/500 [======================>.......] - ETA: 1:56 - loss: 1.8744 - regression_loss: 1.3897 - classification_loss: 0.4847\n",
      "388/500 [======================>.......] - ETA: 1:55 - loss: 1.8762 - regression_loss: 1.3908 - classification_loss: 0.4854\n",
      "389/500 [======================>.......] - ETA: 1:54 - loss: 1.8752 - regression_loss: 1.3900 - classification_loss: 0.4852\n",
      "390/500 [======================>.......] - ETA: 1:53 - loss: 1.8760 - regression_loss: 1.3907 - classification_loss: 0.4853\n",
      "391/500 [======================>.......] - ETA: 1:52 - loss: 1.8757 - regression_loss: 1.3907 - classification_loss: 0.4850\n",
      "392/500 [======================>.......] - ETA: 1:51 - loss: 1.8781 - regression_loss: 1.3925 - classification_loss: 0.4856\n",
      "393/500 [======================>.......] - ETA: 1:50 - loss: 1.8776 - regression_loss: 1.3923 - classification_loss: 0.4853\n",
      "394/500 [======================>.......] - ETA: 1:49 - loss: 1.8775 - regression_loss: 1.3923 - classification_loss: 0.4852\n",
      "395/500 [======================>.......] - ETA: 1:48 - loss: 1.8769 - regression_loss: 1.3921 - classification_loss: 0.4848\n",
      "396/500 [======================>.......] - ETA: 1:47 - loss: 1.8760 - regression_loss: 1.3915 - classification_loss: 0.4845\n",
      "397/500 [======================>.......] - ETA: 1:46 - loss: 1.8765 - regression_loss: 1.3922 - classification_loss: 0.4843\n",
      "398/500 [======================>.......] - ETA: 1:45 - loss: 1.8762 - regression_loss: 1.3917 - classification_loss: 0.4845\n",
      "399/500 [======================>.......] - ETA: 1:44 - loss: 1.8760 - regression_loss: 1.3916 - classification_loss: 0.4845\n",
      "400/500 [=======================>......] - ETA: 1:43 - loss: 1.8761 - regression_loss: 1.3919 - classification_loss: 0.4843\n",
      "401/500 [=======================>......] - ETA: 1:42 - loss: 1.8767 - regression_loss: 1.3919 - classification_loss: 0.4847\n",
      "402/500 [=======================>......] - ETA: 1:41 - loss: 1.8787 - regression_loss: 1.3936 - classification_loss: 0.4850\n",
      "403/500 [=======================>......] - ETA: 1:40 - loss: 1.8779 - regression_loss: 1.3931 - classification_loss: 0.4849\n",
      "404/500 [=======================>......] - ETA: 1:39 - loss: 1.8773 - regression_loss: 1.3922 - classification_loss: 0.4851\n",
      "405/500 [=======================>......] - ETA: 1:38 - loss: 1.8787 - regression_loss: 1.3936 - classification_loss: 0.4851\n",
      "406/500 [=======================>......] - ETA: 1:36 - loss: 1.8804 - regression_loss: 1.3949 - classification_loss: 0.4856\n",
      "407/500 [=======================>......] - ETA: 1:35 - loss: 1.8799 - regression_loss: 1.3947 - classification_loss: 0.4852\n",
      "408/500 [=======================>......] - ETA: 1:34 - loss: 1.8816 - regression_loss: 1.3962 - classification_loss: 0.4854\n",
      "409/500 [=======================>......] - ETA: 1:33 - loss: 1.8801 - regression_loss: 1.3948 - classification_loss: 0.4853\n",
      "410/500 [=======================>......] - ETA: 1:32 - loss: 1.8809 - regression_loss: 1.3951 - classification_loss: 0.4857\n",
      "411/500 [=======================>......] - ETA: 1:31 - loss: 1.8816 - regression_loss: 1.3959 - classification_loss: 0.4857\n",
      "412/500 [=======================>......] - ETA: 1:30 - loss: 1.8803 - regression_loss: 1.3949 - classification_loss: 0.4854\n",
      "413/500 [=======================>......] - ETA: 1:29 - loss: 1.8782 - regression_loss: 1.3932 - classification_loss: 0.4849\n",
      "414/500 [=======================>......] - ETA: 1:28 - loss: 1.8779 - regression_loss: 1.3926 - classification_loss: 0.4852\n",
      "415/500 [=======================>......] - ETA: 1:27 - loss: 1.8770 - regression_loss: 1.3920 - classification_loss: 0.4850\n",
      "416/500 [=======================>......] - ETA: 1:26 - loss: 1.8768 - regression_loss: 1.3921 - classification_loss: 0.4847\n",
      "417/500 [========================>.....] - ETA: 1:25 - loss: 1.8756 - regression_loss: 1.3912 - classification_loss: 0.4845\n",
      "418/500 [========================>.....] - ETA: 1:24 - loss: 1.8740 - regression_loss: 1.3900 - classification_loss: 0.4840\n",
      "419/500 [========================>.....] - ETA: 1:23 - loss: 1.8767 - regression_loss: 1.3914 - classification_loss: 0.4853\n",
      "420/500 [========================>.....] - ETA: 1:22 - loss: 1.8793 - regression_loss: 1.3928 - classification_loss: 0.4864\n",
      "421/500 [========================>.....] - ETA: 1:21 - loss: 1.8803 - regression_loss: 1.3938 - classification_loss: 0.4865\n",
      "422/500 [========================>.....] - ETA: 1:20 - loss: 1.8809 - regression_loss: 1.3941 - classification_loss: 0.4868\n",
      "423/500 [========================>.....] - ETA: 1:19 - loss: 1.8791 - regression_loss: 1.3929 - classification_loss: 0.4862\n",
      "424/500 [========================>.....] - ETA: 1:18 - loss: 1.8797 - regression_loss: 1.3937 - classification_loss: 0.4860\n",
      "425/500 [========================>.....] - ETA: 1:17 - loss: 1.8807 - regression_loss: 1.3945 - classification_loss: 0.4862\n",
      "426/500 [========================>.....] - ETA: 1:16 - loss: 1.8809 - regression_loss: 1.3950 - classification_loss: 0.4860\n",
      "427/500 [========================>.....] - ETA: 1:15 - loss: 1.8805 - regression_loss: 1.3946 - classification_loss: 0.4859\n",
      "428/500 [========================>.....] - ETA: 1:14 - loss: 1.8815 - regression_loss: 1.3956 - classification_loss: 0.4859\n",
      "429/500 [========================>.....] - ETA: 1:13 - loss: 1.8809 - regression_loss: 1.3955 - classification_loss: 0.4854\n",
      "430/500 [========================>.....] - ETA: 1:12 - loss: 1.8818 - regression_loss: 1.3959 - classification_loss: 0.4859\n",
      "431/500 [========================>.....] - ETA: 1:11 - loss: 1.8817 - regression_loss: 1.3960 - classification_loss: 0.4857\n",
      "432/500 [========================>.....] - ETA: 1:10 - loss: 1.8804 - regression_loss: 1.3951 - classification_loss: 0.4853\n",
      "433/500 [========================>.....] - ETA: 1:09 - loss: 1.8790 - regression_loss: 1.3942 - classification_loss: 0.4848\n",
      "434/500 [=========================>....] - ETA: 1:08 - loss: 1.8787 - regression_loss: 1.3940 - classification_loss: 0.4846\n",
      "435/500 [=========================>....] - ETA: 1:07 - loss: 1.8787 - regression_loss: 1.3942 - classification_loss: 0.4845\n",
      "436/500 [=========================>....] - ETA: 1:06 - loss: 1.8778 - regression_loss: 1.3936 - classification_loss: 0.4843\n",
      "437/500 [=========================>....] - ETA: 1:05 - loss: 1.8771 - regression_loss: 1.3931 - classification_loss: 0.4840\n",
      "438/500 [=========================>....] - ETA: 1:03 - loss: 1.8770 - regression_loss: 1.3928 - classification_loss: 0.4842\n",
      "439/500 [=========================>....] - ETA: 1:02 - loss: 1.8781 - regression_loss: 1.3938 - classification_loss: 0.4842\n",
      "440/500 [=========================>....] - ETA: 1:01 - loss: 1.8784 - regression_loss: 1.3941 - classification_loss: 0.4843\n",
      "441/500 [=========================>....] - ETA: 1:00 - loss: 1.8792 - regression_loss: 1.3948 - classification_loss: 0.4844\n",
      "442/500 [=========================>....] - ETA: 59s - loss: 1.8791 - regression_loss: 1.3948 - classification_loss: 0.4843 \n",
      "443/500 [=========================>....] - ETA: 58s - loss: 1.8791 - regression_loss: 1.3946 - classification_loss: 0.4844\n",
      "444/500 [=========================>....] - ETA: 57s - loss: 1.8798 - regression_loss: 1.3953 - classification_loss: 0.4845\n",
      "445/500 [=========================>....] - ETA: 56s - loss: 1.8800 - regression_loss: 1.3951 - classification_loss: 0.4848\n",
      "446/500 [=========================>....] - ETA: 55s - loss: 1.8802 - regression_loss: 1.3955 - classification_loss: 0.4847\n",
      "447/500 [=========================>....] - ETA: 54s - loss: 1.8804 - regression_loss: 1.3957 - classification_loss: 0.4846\n",
      "448/500 [=========================>....] - ETA: 53s - loss: 1.8809 - regression_loss: 1.3962 - classification_loss: 0.4847\n",
      "449/500 [=========================>....] - ETA: 52s - loss: 1.8811 - regression_loss: 1.3964 - classification_loss: 0.4847\n",
      "450/500 [==========================>...] - ETA: 51s - loss: 1.8818 - regression_loss: 1.3970 - classification_loss: 0.4848\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 1.8794 - regression_loss: 1.3954 - classification_loss: 0.4840\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 1.8787 - regression_loss: 1.3947 - classification_loss: 0.4840\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.8784 - regression_loss: 1.3941 - classification_loss: 0.4843\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.8781 - regression_loss: 1.3939 - classification_loss: 0.4843\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.8773 - regression_loss: 1.3934 - classification_loss: 0.4839\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.8767 - regression_loss: 1.3928 - classification_loss: 0.4839\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.8761 - regression_loss: 1.3921 - classification_loss: 0.4840\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.8783 - regression_loss: 1.3936 - classification_loss: 0.4847\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.8778 - regression_loss: 1.3927 - classification_loss: 0.4851\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.8762 - regression_loss: 1.3914 - classification_loss: 0.4847\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 1.8779 - regression_loss: 1.3924 - classification_loss: 0.4855\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 1.8769 - regression_loss: 1.3913 - classification_loss: 0.4857\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 1.8757 - regression_loss: 1.3903 - classification_loss: 0.4854\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 1.8754 - regression_loss: 1.3900 - classification_loss: 0.4854\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 1.8735 - regression_loss: 1.3885 - classification_loss: 0.4850\n",
      "466/500 [==========================>...] - ETA: 35s - loss: 1.8743 - regression_loss: 1.3893 - classification_loss: 0.4850\n",
      "467/500 [===========================>..] - ETA: 34s - loss: 1.8737 - regression_loss: 1.3889 - classification_loss: 0.4847\n",
      "468/500 [===========================>..] - ETA: 33s - loss: 1.8733 - regression_loss: 1.3884 - classification_loss: 0.4849\n",
      "469/500 [===========================>..] - ETA: 32s - loss: 1.8739 - regression_loss: 1.3884 - classification_loss: 0.4854\n",
      "470/500 [===========================>..] - ETA: 30s - loss: 1.8746 - regression_loss: 1.3889 - classification_loss: 0.4857\n",
      "471/500 [===========================>..] - ETA: 29s - loss: 1.8729 - regression_loss: 1.3877 - classification_loss: 0.4853\n",
      "472/500 [===========================>..] - ETA: 28s - loss: 1.8742 - regression_loss: 1.3886 - classification_loss: 0.4856\n",
      "473/500 [===========================>..] - ETA: 27s - loss: 1.8729 - regression_loss: 1.3877 - classification_loss: 0.4852\n",
      "474/500 [===========================>..] - ETA: 26s - loss: 1.8726 - regression_loss: 1.3873 - classification_loss: 0.4853\n",
      "475/500 [===========================>..] - ETA: 25s - loss: 1.8747 - regression_loss: 1.3885 - classification_loss: 0.4862\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 1.8763 - regression_loss: 1.3900 - classification_loss: 0.4862\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.8769 - regression_loss: 1.3907 - classification_loss: 0.4861\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.8779 - regression_loss: 1.3913 - classification_loss: 0.4866\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.8779 - regression_loss: 1.3911 - classification_loss: 0.4868\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.8769 - regression_loss: 1.3901 - classification_loss: 0.4868\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.8766 - regression_loss: 1.3896 - classification_loss: 0.4869\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.8745 - regression_loss: 1.3880 - classification_loss: 0.4864\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.8758 - regression_loss: 1.3890 - classification_loss: 0.4867\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.8769 - regression_loss: 1.3902 - classification_loss: 0.4867\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.8791 - regression_loss: 1.3914 - classification_loss: 0.4877\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.8792 - regression_loss: 1.3917 - classification_loss: 0.4875\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.8794 - regression_loss: 1.3914 - classification_loss: 0.4880\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.8803 - regression_loss: 1.3921 - classification_loss: 0.4883\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.8802 - regression_loss: 1.3922 - classification_loss: 0.4880\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.8805 - regression_loss: 1.3927 - classification_loss: 0.4879\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.8798 - regression_loss: 1.3921 - classification_loss: 0.4877 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.8807 - regression_loss: 1.3923 - classification_loss: 0.4884\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.8811 - regression_loss: 1.3925 - classification_loss: 0.4886\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.8823 - regression_loss: 1.3932 - classification_loss: 0.4891\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.8815 - regression_loss: 1.3927 - classification_loss: 0.4888\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.8825 - regression_loss: 1.3933 - classification_loss: 0.4892\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.8831 - regression_loss: 1.3939 - classification_loss: 0.4893\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.8838 - regression_loss: 1.3939 - classification_loss: 0.4899\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.8836 - regression_loss: 1.3937 - classification_loss: 0.4898\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.8825 - regression_loss: 1.3931 - classification_loss: 0.4894\n",
      "Epoch 00041: saving model to ./snapshots\\resnet50_csv_41.h5\n",
      "\n",
      "500/500 [==============================] - 517s 1s/step - loss: 1.8825 - regression_loss: 1.3931 - classification_loss: 0.4894\n",
      "Epoch 42/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 2.1145 - regression_loss: 1.6626 - classification_loss: 0.4519\n",
      "  2/500 [..............................] - ETA: 4:27 - loss: 1.6343 - regression_loss: 1.2515 - classification_loss: 0.3827\n",
      "  3/500 [..............................] - ETA: 5:41 - loss: 1.4947 - regression_loss: 1.0974 - classification_loss: 0.3974\n",
      "  4/500 [..............................] - ETA: 5:49 - loss: 1.5809 - regression_loss: 1.1697 - classification_loss: 0.4112\n",
      "  5/500 [..............................] - ETA: 6:36 - loss: 1.7162 - regression_loss: 1.2834 - classification_loss: 0.4327\n",
      "  6/500 [..............................] - ETA: 6:50 - loss: 1.7606 - regression_loss: 1.3387 - classification_loss: 0.4219\n",
      "  7/500 [..............................] - ETA: 7:00 - loss: 1.6698 - regression_loss: 1.2760 - classification_loss: 0.3938\n",
      "  8/500 [..............................] - ETA: 7:13 - loss: 1.7337 - regression_loss: 1.3291 - classification_loss: 0.4046\n",
      "  9/500 [..............................] - ETA: 7:18 - loss: 1.8261 - regression_loss: 1.3587 - classification_loss: 0.4674\n",
      " 10/500 [..............................] - ETA: 7:21 - loss: 1.8944 - regression_loss: 1.4260 - classification_loss: 0.4685\n",
      " 11/500 [..............................] - ETA: 7:24 - loss: 1.8181 - regression_loss: 1.3713 - classification_loss: 0.4468\n",
      " 12/500 [..............................] - ETA: 7:26 - loss: 1.7541 - regression_loss: 1.3231 - classification_loss: 0.4310\n",
      " 13/500 [..............................] - ETA: 7:28 - loss: 1.7913 - regression_loss: 1.3553 - classification_loss: 0.4359\n",
      " 14/500 [..............................] - ETA: 7:32 - loss: 1.8308 - regression_loss: 1.3795 - classification_loss: 0.4513\n",
      " 15/500 [..............................] - ETA: 7:36 - loss: 1.8248 - regression_loss: 1.3767 - classification_loss: 0.4481\n",
      " 16/500 [..............................] - ETA: 7:39 - loss: 1.8851 - regression_loss: 1.4262 - classification_loss: 0.4590\n",
      " 17/500 [>.............................] - ETA: 7:42 - loss: 1.8558 - regression_loss: 1.4062 - classification_loss: 0.4496\n",
      " 18/500 [>.............................] - ETA: 7:35 - loss: 1.9058 - regression_loss: 1.4260 - classification_loss: 0.4799\n",
      " 19/500 [>.............................] - ETA: 7:40 - loss: 1.8880 - regression_loss: 1.4161 - classification_loss: 0.4720\n",
      " 20/500 [>.............................] - ETA: 7:42 - loss: 1.9189 - regression_loss: 1.4415 - classification_loss: 0.4774\n",
      " 21/500 [>.............................] - ETA: 7:44 - loss: 1.9301 - regression_loss: 1.4546 - classification_loss: 0.4755\n",
      " 22/500 [>.............................] - ETA: 7:45 - loss: 1.9808 - regression_loss: 1.4975 - classification_loss: 0.4833\n",
      " 23/500 [>.............................] - ETA: 7:47 - loss: 1.9840 - regression_loss: 1.5058 - classification_loss: 0.4782\n",
      " 24/500 [>.............................] - ETA: 7:46 - loss: 1.9718 - regression_loss: 1.4941 - classification_loss: 0.4777\n",
      " 25/500 [>.............................] - ETA: 7:47 - loss: 1.9729 - regression_loss: 1.4972 - classification_loss: 0.4758\n",
      " 26/500 [>.............................] - ETA: 7:50 - loss: 2.0070 - regression_loss: 1.5126 - classification_loss: 0.4943\n",
      " 27/500 [>.............................] - ETA: 7:45 - loss: 1.9926 - regression_loss: 1.5063 - classification_loss: 0.4863\n",
      " 28/500 [>.............................] - ETA: 7:48 - loss: 1.9627 - regression_loss: 1.4836 - classification_loss: 0.4792\n",
      " 29/500 [>.............................] - ETA: 7:48 - loss: 1.9959 - regression_loss: 1.5000 - classification_loss: 0.4959\n",
      " 30/500 [>.............................] - ETA: 7:49 - loss: 1.9683 - regression_loss: 1.4784 - classification_loss: 0.4899\n",
      " 31/500 [>.............................] - ETA: 7:47 - loss: 1.9645 - regression_loss: 1.4759 - classification_loss: 0.4886\n",
      " 32/500 [>.............................] - ETA: 7:48 - loss: 1.9362 - regression_loss: 1.4560 - classification_loss: 0.4802\n",
      " 33/500 [>.............................] - ETA: 7:47 - loss: 1.9530 - regression_loss: 1.4679 - classification_loss: 0.4851\n",
      " 34/500 [=>............................] - ETA: 7:47 - loss: 1.9509 - regression_loss: 1.4659 - classification_loss: 0.4850\n",
      " 35/500 [=>............................] - ETA: 7:47 - loss: 1.9549 - regression_loss: 1.4747 - classification_loss: 0.4802\n",
      " 36/500 [=>............................] - ETA: 7:43 - loss: 1.9730 - regression_loss: 1.4868 - classification_loss: 0.4862\n",
      " 37/500 [=>............................] - ETA: 7:44 - loss: 1.9475 - regression_loss: 1.4692 - classification_loss: 0.4784\n",
      " 38/500 [=>............................] - ETA: 7:44 - loss: 1.9669 - regression_loss: 1.4866 - classification_loss: 0.4803\n",
      " 39/500 [=>............................] - ETA: 7:44 - loss: 1.9720 - regression_loss: 1.4938 - classification_loss: 0.4781\n",
      " 40/500 [=>............................] - ETA: 7:44 - loss: 1.9643 - regression_loss: 1.4904 - classification_loss: 0.4739\n",
      " 41/500 [=>............................] - ETA: 7:43 - loss: 1.9691 - regression_loss: 1.4933 - classification_loss: 0.4758\n",
      " 42/500 [=>............................] - ETA: 7:41 - loss: 1.9642 - regression_loss: 1.4917 - classification_loss: 0.4725\n",
      " 43/500 [=>............................] - ETA: 7:41 - loss: 1.9853 - regression_loss: 1.5051 - classification_loss: 0.4801\n",
      " 44/500 [=>............................] - ETA: 7:40 - loss: 1.9930 - regression_loss: 1.5098 - classification_loss: 0.4832\n",
      " 45/500 [=>............................] - ETA: 7:39 - loss: 1.9961 - regression_loss: 1.5093 - classification_loss: 0.4869\n",
      " 46/500 [=>............................] - ETA: 7:39 - loss: 1.9967 - regression_loss: 1.5093 - classification_loss: 0.4874\n",
      " 47/500 [=>............................] - ETA: 7:35 - loss: 2.0042 - regression_loss: 1.5087 - classification_loss: 0.4955\n",
      " 48/500 [=>............................] - ETA: 7:37 - loss: 2.0072 - regression_loss: 1.5146 - classification_loss: 0.4926\n",
      " 49/500 [=>............................] - ETA: 7:35 - loss: 2.0210 - regression_loss: 1.5234 - classification_loss: 0.4976\n",
      " 50/500 [==>...........................] - ETA: 7:35 - loss: 2.0072 - regression_loss: 1.5144 - classification_loss: 0.4928\n",
      " 51/500 [==>...........................] - ETA: 7:35 - loss: 1.9982 - regression_loss: 1.5084 - classification_loss: 0.4897\n",
      " 52/500 [==>...........................] - ETA: 7:33 - loss: 1.9966 - regression_loss: 1.5104 - classification_loss: 0.4862\n",
      " 53/500 [==>...........................] - ETA: 7:32 - loss: 2.0052 - regression_loss: 1.5165 - classification_loss: 0.4887\n",
      " 54/500 [==>...........................] - ETA: 7:31 - loss: 1.9989 - regression_loss: 1.5075 - classification_loss: 0.4914\n",
      " 55/500 [==>...........................] - ETA: 7:30 - loss: 2.0030 - regression_loss: 1.5109 - classification_loss: 0.4921\n",
      " 56/500 [==>...........................] - ETA: 7:28 - loss: 1.9867 - regression_loss: 1.4993 - classification_loss: 0.4874\n",
      " 57/500 [==>...........................] - ETA: 7:28 - loss: 1.9812 - regression_loss: 1.4944 - classification_loss: 0.4868\n",
      " 58/500 [==>...........................] - ETA: 7:27 - loss: 1.9666 - regression_loss: 1.4839 - classification_loss: 0.4827\n",
      " 59/500 [==>...........................] - ETA: 7:26 - loss: 1.9597 - regression_loss: 1.4790 - classification_loss: 0.4808\n",
      " 60/500 [==>...........................] - ETA: 7:26 - loss: 1.9586 - regression_loss: 1.4747 - classification_loss: 0.4838\n",
      " 61/500 [==>...........................] - ETA: 7:25 - loss: 1.9744 - regression_loss: 1.4847 - classification_loss: 0.4897\n",
      " 62/500 [==>...........................] - ETA: 7:23 - loss: 1.9759 - regression_loss: 1.4868 - classification_loss: 0.4891\n",
      " 63/500 [==>...........................] - ETA: 7:22 - loss: 1.9701 - regression_loss: 1.4808 - classification_loss: 0.4893\n",
      " 64/500 [==>...........................] - ETA: 7:22 - loss: 1.9771 - regression_loss: 1.4869 - classification_loss: 0.4902\n",
      " 65/500 [==>...........................] - ETA: 7:19 - loss: 1.9659 - regression_loss: 1.4791 - classification_loss: 0.4868\n",
      " 66/500 [==>...........................] - ETA: 7:19 - loss: 1.9609 - regression_loss: 1.4759 - classification_loss: 0.4850\n",
      " 67/500 [===>..........................] - ETA: 7:19 - loss: 1.9650 - regression_loss: 1.4787 - classification_loss: 0.4862\n",
      " 68/500 [===>..........................] - ETA: 7:18 - loss: 1.9663 - regression_loss: 1.4822 - classification_loss: 0.4841\n",
      " 69/500 [===>..........................] - ETA: 7:17 - loss: 1.9572 - regression_loss: 1.4753 - classification_loss: 0.4819\n",
      " 70/500 [===>..........................] - ETA: 7:16 - loss: 1.9533 - regression_loss: 1.4736 - classification_loss: 0.4796\n",
      " 71/500 [===>..........................] - ETA: 7:15 - loss: 1.9511 - regression_loss: 1.4726 - classification_loss: 0.4786\n",
      " 72/500 [===>..........................] - ETA: 7:13 - loss: 1.9578 - regression_loss: 1.4788 - classification_loss: 0.4790\n",
      " 73/500 [===>..........................] - ETA: 7:12 - loss: 1.9520 - regression_loss: 1.4740 - classification_loss: 0.4780\n",
      " 74/500 [===>..........................] - ETA: 7:11 - loss: 1.9471 - regression_loss: 1.4706 - classification_loss: 0.4764\n",
      " 75/500 [===>..........................] - ETA: 7:09 - loss: 1.9433 - regression_loss: 1.4643 - classification_loss: 0.4790\n",
      " 76/500 [===>..........................] - ETA: 7:09 - loss: 1.9350 - regression_loss: 1.4586 - classification_loss: 0.4764\n",
      " 77/500 [===>..........................] - ETA: 7:08 - loss: 1.9395 - regression_loss: 1.4621 - classification_loss: 0.4774\n",
      " 78/500 [===>..........................] - ETA: 7:07 - loss: 1.9328 - regression_loss: 1.4581 - classification_loss: 0.4747\n",
      " 79/500 [===>..........................] - ETA: 7:06 - loss: 1.9273 - regression_loss: 1.4537 - classification_loss: 0.4736\n",
      " 80/500 [===>..........................] - ETA: 7:05 - loss: 1.9317 - regression_loss: 1.4569 - classification_loss: 0.4747\n",
      " 81/500 [===>..........................] - ETA: 7:04 - loss: 1.9325 - regression_loss: 1.4578 - classification_loss: 0.4747\n",
      " 82/500 [===>..........................] - ETA: 7:03 - loss: 1.9313 - regression_loss: 1.4563 - classification_loss: 0.4750\n",
      " 83/500 [===>..........................] - ETA: 7:03 - loss: 1.9423 - regression_loss: 1.4626 - classification_loss: 0.4797\n",
      " 84/500 [====>.........................] - ETA: 7:00 - loss: 1.9368 - regression_loss: 1.4595 - classification_loss: 0.4773\n",
      " 85/500 [====>.........................] - ETA: 7:00 - loss: 1.9387 - regression_loss: 1.4611 - classification_loss: 0.4776\n",
      " 86/500 [====>.........................] - ETA: 6:59 - loss: 1.9331 - regression_loss: 1.4569 - classification_loss: 0.4762\n",
      " 87/500 [====>.........................] - ETA: 6:57 - loss: 1.9328 - regression_loss: 1.4547 - classification_loss: 0.4782\n",
      " 88/500 [====>.........................] - ETA: 6:56 - loss: 1.9303 - regression_loss: 1.4533 - classification_loss: 0.4770\n",
      " 89/500 [====>.........................] - ETA: 6:56 - loss: 1.9300 - regression_loss: 1.4547 - classification_loss: 0.4753\n",
      " 90/500 [====>.........................] - ETA: 6:55 - loss: 1.9255 - regression_loss: 1.4524 - classification_loss: 0.4731\n",
      " 91/500 [====>.........................] - ETA: 6:54 - loss: 1.9244 - regression_loss: 1.4523 - classification_loss: 0.4722\n",
      " 92/500 [====>.........................] - ETA: 6:53 - loss: 1.9249 - regression_loss: 1.4522 - classification_loss: 0.4727\n",
      " 93/500 [====>.........................] - ETA: 6:53 - loss: 1.9175 - regression_loss: 1.4457 - classification_loss: 0.4718\n",
      " 94/500 [====>.........................] - ETA: 6:52 - loss: 1.9205 - regression_loss: 1.4480 - classification_loss: 0.4725\n",
      " 95/500 [====>.........................] - ETA: 6:51 - loss: 1.9244 - regression_loss: 1.4525 - classification_loss: 0.4719\n",
      " 96/500 [====>.........................] - ETA: 6:49 - loss: 1.9205 - regression_loss: 1.4494 - classification_loss: 0.4711\n",
      " 97/500 [====>.........................] - ETA: 6:48 - loss: 1.9324 - regression_loss: 1.4570 - classification_loss: 0.4754\n",
      " 98/500 [====>.........................] - ETA: 6:46 - loss: 1.9417 - regression_loss: 1.4606 - classification_loss: 0.4811\n",
      " 99/500 [====>.........................] - ETA: 6:45 - loss: 1.9342 - regression_loss: 1.4542 - classification_loss: 0.4800\n",
      "100/500 [=====>........................] - ETA: 6:44 - loss: 1.9361 - regression_loss: 1.4562 - classification_loss: 0.4799\n",
      "101/500 [=====>........................] - ETA: 6:44 - loss: 1.9482 - regression_loss: 1.4635 - classification_loss: 0.4847\n",
      "102/500 [=====>........................] - ETA: 6:43 - loss: 1.9451 - regression_loss: 1.4614 - classification_loss: 0.4836\n",
      "103/500 [=====>........................] - ETA: 6:42 - loss: 1.9425 - regression_loss: 1.4605 - classification_loss: 0.4820\n",
      "104/500 [=====>........................] - ETA: 6:42 - loss: 1.9338 - regression_loss: 1.4536 - classification_loss: 0.4803\n",
      "105/500 [=====>........................] - ETA: 6:41 - loss: 1.9387 - regression_loss: 1.4574 - classification_loss: 0.4812\n",
      "106/500 [=====>........................] - ETA: 6:40 - loss: 1.9466 - regression_loss: 1.4622 - classification_loss: 0.4843\n",
      "107/500 [=====>........................] - ETA: 6:39 - loss: 1.9586 - regression_loss: 1.4708 - classification_loss: 0.4878\n",
      "108/500 [=====>........................] - ETA: 6:38 - loss: 1.9530 - regression_loss: 1.4675 - classification_loss: 0.4855\n",
      "109/500 [=====>........................] - ETA: 6:37 - loss: 1.9494 - regression_loss: 1.4650 - classification_loss: 0.4844\n",
      "110/500 [=====>........................] - ETA: 6:36 - loss: 1.9523 - regression_loss: 1.4677 - classification_loss: 0.4846\n",
      "111/500 [=====>........................] - ETA: 6:35 - loss: 1.9504 - regression_loss: 1.4662 - classification_loss: 0.4841\n",
      "112/500 [=====>........................] - ETA: 6:34 - loss: 1.9458 - regression_loss: 1.4627 - classification_loss: 0.4831\n",
      "113/500 [=====>........................] - ETA: 6:33 - loss: 1.9446 - regression_loss: 1.4633 - classification_loss: 0.4812\n",
      "114/500 [=====>........................] - ETA: 6:33 - loss: 1.9425 - regression_loss: 1.4623 - classification_loss: 0.4802\n",
      "115/500 [=====>........................] - ETA: 6:31 - loss: 1.9421 - regression_loss: 1.4618 - classification_loss: 0.4803\n",
      "116/500 [=====>........................] - ETA: 6:31 - loss: 1.9355 - regression_loss: 1.4554 - classification_loss: 0.4801\n",
      "117/500 [======>.......................] - ETA: 6:30 - loss: 1.9283 - regression_loss: 1.4509 - classification_loss: 0.4775\n",
      "118/500 [======>.......................] - ETA: 6:28 - loss: 1.9295 - regression_loss: 1.4518 - classification_loss: 0.4777\n",
      "119/500 [======>.......................] - ETA: 6:28 - loss: 1.9311 - regression_loss: 1.4519 - classification_loss: 0.4792\n",
      "120/500 [======>.......................] - ETA: 6:27 - loss: 1.9342 - regression_loss: 1.4547 - classification_loss: 0.4795\n",
      "121/500 [======>.......................] - ETA: 6:26 - loss: 1.9353 - regression_loss: 1.4545 - classification_loss: 0.4809\n",
      "122/500 [======>.......................] - ETA: 6:25 - loss: 1.9366 - regression_loss: 1.4558 - classification_loss: 0.4808\n",
      "123/500 [======>.......................] - ETA: 6:23 - loss: 1.9322 - regression_loss: 1.4526 - classification_loss: 0.4795\n",
      "124/500 [======>.......................] - ETA: 6:22 - loss: 1.9270 - regression_loss: 1.4491 - classification_loss: 0.4779\n",
      "125/500 [======>.......................] - ETA: 6:21 - loss: 1.9287 - regression_loss: 1.4512 - classification_loss: 0.4775\n",
      "126/500 [======>.......................] - ETA: 6:20 - loss: 1.9227 - regression_loss: 1.4471 - classification_loss: 0.4755\n",
      "127/500 [======>.......................] - ETA: 6:19 - loss: 1.9232 - regression_loss: 1.4478 - classification_loss: 0.4753\n",
      "128/500 [======>.......................] - ETA: 6:18 - loss: 1.9219 - regression_loss: 1.4468 - classification_loss: 0.4751\n",
      "129/500 [======>.......................] - ETA: 6:18 - loss: 1.9188 - regression_loss: 1.4449 - classification_loss: 0.4739\n",
      "130/500 [======>.......................] - ETA: 6:17 - loss: 1.9214 - regression_loss: 1.4478 - classification_loss: 0.4736\n",
      "131/500 [======>.......................] - ETA: 6:15 - loss: 1.9199 - regression_loss: 1.4472 - classification_loss: 0.4727\n",
      "132/500 [======>.......................] - ETA: 6:14 - loss: 1.9172 - regression_loss: 1.4461 - classification_loss: 0.4712\n",
      "133/500 [======>.......................] - ETA: 6:14 - loss: 1.9209 - regression_loss: 1.4487 - classification_loss: 0.4722\n",
      "134/500 [=======>......................] - ETA: 6:12 - loss: 1.9200 - regression_loss: 1.4474 - classification_loss: 0.4725\n",
      "135/500 [=======>......................] - ETA: 6:12 - loss: 1.9228 - regression_loss: 1.4506 - classification_loss: 0.4722\n",
      "136/500 [=======>......................] - ETA: 6:11 - loss: 1.9190 - regression_loss: 1.4478 - classification_loss: 0.4712\n",
      "137/500 [=======>......................] - ETA: 6:10 - loss: 1.9208 - regression_loss: 1.4498 - classification_loss: 0.4710\n",
      "138/500 [=======>......................] - ETA: 6:09 - loss: 1.9173 - regression_loss: 1.4476 - classification_loss: 0.4697\n",
      "139/500 [=======>......................] - ETA: 6:08 - loss: 1.9190 - regression_loss: 1.4490 - classification_loss: 0.4700\n",
      "140/500 [=======>......................] - ETA: 6:07 - loss: 1.9158 - regression_loss: 1.4468 - classification_loss: 0.4690\n",
      "141/500 [=======>......................] - ETA: 6:06 - loss: 1.9140 - regression_loss: 1.4449 - classification_loss: 0.4690\n",
      "142/500 [=======>......................] - ETA: 6:05 - loss: 1.9180 - regression_loss: 1.4466 - classification_loss: 0.4714\n",
      "143/500 [=======>......................] - ETA: 6:04 - loss: 1.9151 - regression_loss: 1.4447 - classification_loss: 0.4704\n",
      "144/500 [=======>......................] - ETA: 6:03 - loss: 1.9118 - regression_loss: 1.4422 - classification_loss: 0.4696\n",
      "145/500 [=======>......................] - ETA: 6:02 - loss: 1.9190 - regression_loss: 1.4471 - classification_loss: 0.4719\n",
      "146/500 [=======>......................] - ETA: 6:01 - loss: 1.9184 - regression_loss: 1.4463 - classification_loss: 0.4720\n",
      "147/500 [=======>......................] - ETA: 6:00 - loss: 1.9236 - regression_loss: 1.4487 - classification_loss: 0.4749\n",
      "148/500 [=======>......................] - ETA: 5:59 - loss: 1.9203 - regression_loss: 1.4465 - classification_loss: 0.4739\n",
      "149/500 [=======>......................] - ETA: 5:58 - loss: 1.9241 - regression_loss: 1.4493 - classification_loss: 0.4748\n",
      "150/500 [========>.....................] - ETA: 5:57 - loss: 1.9208 - regression_loss: 1.4468 - classification_loss: 0.4739\n",
      "151/500 [========>.....................] - ETA: 5:56 - loss: 1.9177 - regression_loss: 1.4447 - classification_loss: 0.4730\n",
      "152/500 [========>.....................] - ETA: 5:55 - loss: 1.9159 - regression_loss: 1.4417 - classification_loss: 0.4742\n",
      "153/500 [========>.....................] - ETA: 5:54 - loss: 1.9114 - regression_loss: 1.4391 - classification_loss: 0.4723\n",
      "154/500 [========>.....................] - ETA: 5:53 - loss: 1.9125 - regression_loss: 1.4390 - classification_loss: 0.4735\n",
      "155/500 [========>.....................] - ETA: 5:52 - loss: 1.9170 - regression_loss: 1.4414 - classification_loss: 0.4756\n",
      "156/500 [========>.....................] - ETA: 5:51 - loss: 1.9172 - regression_loss: 1.4412 - classification_loss: 0.4760\n",
      "157/500 [========>.....................] - ETA: 5:50 - loss: 1.9174 - regression_loss: 1.4416 - classification_loss: 0.4758\n",
      "158/500 [========>.....................] - ETA: 5:49 - loss: 1.9152 - regression_loss: 1.4397 - classification_loss: 0.4754\n",
      "159/500 [========>.....................] - ETA: 5:48 - loss: 1.9134 - regression_loss: 1.4377 - classification_loss: 0.4757\n",
      "160/500 [========>.....................] - ETA: 5:47 - loss: 1.9121 - regression_loss: 1.4363 - classification_loss: 0.4758\n",
      "161/500 [========>.....................] - ETA: 5:46 - loss: 1.9127 - regression_loss: 1.4378 - classification_loss: 0.4749\n",
      "162/500 [========>.....................] - ETA: 5:45 - loss: 1.9123 - regression_loss: 1.4377 - classification_loss: 0.4746\n",
      "163/500 [========>.....................] - ETA: 5:44 - loss: 1.9140 - regression_loss: 1.4383 - classification_loss: 0.4758\n",
      "164/500 [========>.....................] - ETA: 5:43 - loss: 1.9125 - regression_loss: 1.4374 - classification_loss: 0.4751\n",
      "165/500 [========>.....................] - ETA: 5:42 - loss: 1.9126 - regression_loss: 1.4365 - classification_loss: 0.4760\n",
      "166/500 [========>.....................] - ETA: 5:41 - loss: 1.9129 - regression_loss: 1.4352 - classification_loss: 0.4777\n",
      "167/500 [=========>....................] - ETA: 5:40 - loss: 1.9186 - regression_loss: 1.4388 - classification_loss: 0.4798\n",
      "168/500 [=========>....................] - ETA: 5:39 - loss: 1.9233 - regression_loss: 1.4416 - classification_loss: 0.4818\n",
      "169/500 [=========>....................] - ETA: 5:38 - loss: 1.9250 - regression_loss: 1.4423 - classification_loss: 0.4827\n",
      "170/500 [=========>....................] - ETA: 5:37 - loss: 1.9274 - regression_loss: 1.4452 - classification_loss: 0.4822\n",
      "171/500 [=========>....................] - ETA: 5:36 - loss: 1.9259 - regression_loss: 1.4441 - classification_loss: 0.4818\n",
      "172/500 [=========>....................] - ETA: 5:35 - loss: 1.9223 - regression_loss: 1.4420 - classification_loss: 0.4803\n",
      "173/500 [=========>....................] - ETA: 5:34 - loss: 1.9240 - regression_loss: 1.4435 - classification_loss: 0.4805\n",
      "174/500 [=========>....................] - ETA: 5:33 - loss: 1.9211 - regression_loss: 1.4417 - classification_loss: 0.4794\n",
      "175/500 [=========>....................] - ETA: 5:32 - loss: 1.9186 - regression_loss: 1.4405 - classification_loss: 0.4781\n",
      "176/500 [=========>....................] - ETA: 5:31 - loss: 1.9241 - regression_loss: 1.4449 - classification_loss: 0.4792\n",
      "177/500 [=========>....................] - ETA: 5:30 - loss: 1.9250 - regression_loss: 1.4461 - classification_loss: 0.4790\n",
      "178/500 [=========>....................] - ETA: 5:29 - loss: 1.9268 - regression_loss: 1.4465 - classification_loss: 0.4802\n",
      "179/500 [=========>....................] - ETA: 5:27 - loss: 1.9229 - regression_loss: 1.4441 - classification_loss: 0.4788\n",
      "180/500 [=========>....................] - ETA: 5:27 - loss: 1.9248 - regression_loss: 1.4459 - classification_loss: 0.4789\n",
      "181/500 [=========>....................] - ETA: 5:26 - loss: 1.9215 - regression_loss: 1.4436 - classification_loss: 0.4779\n",
      "182/500 [=========>....................] - ETA: 5:25 - loss: 1.9202 - regression_loss: 1.4422 - classification_loss: 0.4780\n",
      "183/500 [=========>....................] - ETA: 5:23 - loss: 1.9168 - regression_loss: 1.4399 - classification_loss: 0.4769\n",
      "184/500 [==========>...................] - ETA: 5:23 - loss: 1.9184 - regression_loss: 1.4408 - classification_loss: 0.4776\n",
      "185/500 [==========>...................] - ETA: 5:21 - loss: 1.9210 - regression_loss: 1.4426 - classification_loss: 0.4785\n",
      "186/500 [==========>...................] - ETA: 5:20 - loss: 1.9225 - regression_loss: 1.4445 - classification_loss: 0.4781\n",
      "187/500 [==========>...................] - ETA: 5:19 - loss: 1.9266 - regression_loss: 1.4464 - classification_loss: 0.4803\n",
      "188/500 [==========>...................] - ETA: 5:18 - loss: 1.9217 - regression_loss: 1.4428 - classification_loss: 0.4789\n",
      "189/500 [==========>...................] - ETA: 5:17 - loss: 1.9263 - regression_loss: 1.4456 - classification_loss: 0.4807\n",
      "190/500 [==========>...................] - ETA: 5:16 - loss: 1.9212 - regression_loss: 1.4417 - classification_loss: 0.4794\n",
      "191/500 [==========>...................] - ETA: 5:15 - loss: 1.9210 - regression_loss: 1.4417 - classification_loss: 0.4792\n",
      "192/500 [==========>...................] - ETA: 5:14 - loss: 1.9186 - regression_loss: 1.4400 - classification_loss: 0.4786\n",
      "193/500 [==========>...................] - ETA: 5:13 - loss: 1.9167 - regression_loss: 1.4382 - classification_loss: 0.4786\n",
      "194/500 [==========>...................] - ETA: 5:12 - loss: 1.9186 - regression_loss: 1.4394 - classification_loss: 0.4792\n",
      "195/500 [==========>...................] - ETA: 5:11 - loss: 1.9177 - regression_loss: 1.4386 - classification_loss: 0.4792\n",
      "196/500 [==========>...................] - ETA: 5:10 - loss: 1.9180 - regression_loss: 1.4390 - classification_loss: 0.4790\n",
      "197/500 [==========>...................] - ETA: 5:10 - loss: 1.9168 - regression_loss: 1.4380 - classification_loss: 0.4789\n",
      "198/500 [==========>...................] - ETA: 5:09 - loss: 1.9143 - regression_loss: 1.4359 - classification_loss: 0.4784\n",
      "199/500 [==========>...................] - ETA: 5:07 - loss: 1.9127 - regression_loss: 1.4342 - classification_loss: 0.4785\n",
      "200/500 [===========>..................] - ETA: 5:07 - loss: 1.9111 - regression_loss: 1.4325 - classification_loss: 0.4786\n",
      "201/500 [===========>..................] - ETA: 5:06 - loss: 1.9160 - regression_loss: 1.4364 - classification_loss: 0.4796\n",
      "202/500 [===========>..................] - ETA: 5:05 - loss: 1.9199 - regression_loss: 1.4381 - classification_loss: 0.4818\n",
      "203/500 [===========>..................] - ETA: 5:04 - loss: 1.9179 - regression_loss: 1.4362 - classification_loss: 0.4816\n",
      "204/500 [===========>..................] - ETA: 5:03 - loss: 1.9148 - regression_loss: 1.4337 - classification_loss: 0.4811\n",
      "205/500 [===========>..................] - ETA: 5:02 - loss: 1.9141 - regression_loss: 1.4331 - classification_loss: 0.4809\n",
      "206/500 [===========>..................] - ETA: 5:00 - loss: 1.9103 - regression_loss: 1.4303 - classification_loss: 0.4800\n",
      "207/500 [===========>..................] - ETA: 5:00 - loss: 1.9119 - regression_loss: 1.4318 - classification_loss: 0.4801\n",
      "208/500 [===========>..................] - ETA: 4:58 - loss: 1.9139 - regression_loss: 1.4324 - classification_loss: 0.4815\n",
      "209/500 [===========>..................] - ETA: 4:57 - loss: 1.9122 - regression_loss: 1.4319 - classification_loss: 0.4803\n",
      "210/500 [===========>..................] - ETA: 4:56 - loss: 1.9202 - regression_loss: 1.4367 - classification_loss: 0.4835\n",
      "211/500 [===========>..................] - ETA: 4:55 - loss: 1.9230 - regression_loss: 1.4389 - classification_loss: 0.4842\n",
      "212/500 [===========>..................] - ETA: 4:54 - loss: 1.9216 - regression_loss: 1.4383 - classification_loss: 0.4834\n",
      "213/500 [===========>..................] - ETA: 4:53 - loss: 1.9198 - regression_loss: 1.4371 - classification_loss: 0.4827\n",
      "214/500 [===========>..................] - ETA: 4:52 - loss: 1.9237 - regression_loss: 1.4400 - classification_loss: 0.4836\n",
      "215/500 [===========>..................] - ETA: 4:51 - loss: 1.9240 - regression_loss: 1.4407 - classification_loss: 0.4833\n",
      "216/500 [===========>..................] - ETA: 4:50 - loss: 1.9256 - regression_loss: 1.4422 - classification_loss: 0.4834\n",
      "217/500 [============>.................] - ETA: 4:49 - loss: 1.9268 - regression_loss: 1.4433 - classification_loss: 0.4835\n",
      "218/500 [============>.................] - ETA: 4:49 - loss: 1.9274 - regression_loss: 1.4441 - classification_loss: 0.4833\n",
      "219/500 [============>.................] - ETA: 4:47 - loss: 1.9255 - regression_loss: 1.4421 - classification_loss: 0.4833\n",
      "220/500 [============>.................] - ETA: 4:46 - loss: 1.9209 - regression_loss: 1.4391 - classification_loss: 0.4818\n",
      "221/500 [============>.................] - ETA: 4:45 - loss: 1.9212 - regression_loss: 1.4394 - classification_loss: 0.4818\n",
      "222/500 [============>.................] - ETA: 4:44 - loss: 1.9222 - regression_loss: 1.4406 - classification_loss: 0.4816\n",
      "223/500 [============>.................] - ETA: 4:43 - loss: 1.9233 - regression_loss: 1.4417 - classification_loss: 0.4816\n",
      "224/500 [============>.................] - ETA: 4:42 - loss: 1.9247 - regression_loss: 1.4427 - classification_loss: 0.4820\n",
      "225/500 [============>.................] - ETA: 4:41 - loss: 1.9245 - regression_loss: 1.4427 - classification_loss: 0.4819\n",
      "226/500 [============>.................] - ETA: 4:40 - loss: 1.9257 - regression_loss: 1.4439 - classification_loss: 0.4818\n",
      "227/500 [============>.................] - ETA: 4:39 - loss: 1.9286 - regression_loss: 1.4453 - classification_loss: 0.4833\n",
      "228/500 [============>.................] - ETA: 4:38 - loss: 1.9271 - regression_loss: 1.4441 - classification_loss: 0.4830\n",
      "229/500 [============>.................] - ETA: 4:37 - loss: 1.9241 - regression_loss: 1.4422 - classification_loss: 0.4819\n",
      "230/500 [============>.................] - ETA: 4:36 - loss: 1.9250 - regression_loss: 1.4422 - classification_loss: 0.4828\n",
      "231/500 [============>.................] - ETA: 4:35 - loss: 1.9224 - regression_loss: 1.4404 - classification_loss: 0.4820\n",
      "232/500 [============>.................] - ETA: 4:34 - loss: 1.9187 - regression_loss: 1.4374 - classification_loss: 0.4813\n",
      "233/500 [============>.................] - ETA: 4:33 - loss: 1.9193 - regression_loss: 1.4376 - classification_loss: 0.4816\n",
      "234/500 [=============>................] - ETA: 4:32 - loss: 1.9192 - regression_loss: 1.4380 - classification_loss: 0.4812\n",
      "235/500 [=============>................] - ETA: 4:31 - loss: 1.9141 - regression_loss: 1.4342 - classification_loss: 0.4798\n",
      "236/500 [=============>................] - ETA: 4:30 - loss: 1.9167 - regression_loss: 1.4355 - classification_loss: 0.4812\n",
      "237/500 [=============>................] - ETA: 4:29 - loss: 1.9152 - regression_loss: 1.4340 - classification_loss: 0.4812\n",
      "238/500 [=============>................] - ETA: 4:28 - loss: 1.9161 - regression_loss: 1.4346 - classification_loss: 0.4814\n",
      "239/500 [=============>................] - ETA: 4:27 - loss: 1.9206 - regression_loss: 1.4378 - classification_loss: 0.4827\n",
      "240/500 [=============>................] - ETA: 4:26 - loss: 1.9231 - regression_loss: 1.4400 - classification_loss: 0.4831\n",
      "241/500 [=============>................] - ETA: 4:25 - loss: 1.9248 - regression_loss: 1.4416 - classification_loss: 0.4831\n",
      "242/500 [=============>................] - ETA: 4:24 - loss: 1.9245 - regression_loss: 1.4413 - classification_loss: 0.4832\n",
      "243/500 [=============>................] - ETA: 4:23 - loss: 1.9250 - regression_loss: 1.4415 - classification_loss: 0.4835\n",
      "244/500 [=============>................] - ETA: 4:22 - loss: 1.9251 - regression_loss: 1.4415 - classification_loss: 0.4836\n",
      "245/500 [=============>................] - ETA: 4:21 - loss: 1.9238 - regression_loss: 1.4407 - classification_loss: 0.4831\n",
      "246/500 [=============>................] - ETA: 4:20 - loss: 1.9225 - regression_loss: 1.4396 - classification_loss: 0.4829\n",
      "247/500 [=============>................] - ETA: 4:18 - loss: 1.9224 - regression_loss: 1.4399 - classification_loss: 0.4825\n",
      "248/500 [=============>................] - ETA: 4:18 - loss: 1.9228 - regression_loss: 1.4399 - classification_loss: 0.4829\n",
      "249/500 [=============>................] - ETA: 4:16 - loss: 1.9185 - regression_loss: 1.4371 - classification_loss: 0.4814\n",
      "250/500 [==============>...............] - ETA: 4:15 - loss: 1.9182 - regression_loss: 1.4369 - classification_loss: 0.4813\n",
      "251/500 [==============>...............] - ETA: 4:14 - loss: 1.9184 - regression_loss: 1.4369 - classification_loss: 0.4815\n",
      "252/500 [==============>...............] - ETA: 4:13 - loss: 1.9206 - regression_loss: 1.4386 - classification_loss: 0.4821\n",
      "253/500 [==============>...............] - ETA: 4:12 - loss: 1.9203 - regression_loss: 1.4388 - classification_loss: 0.4815\n",
      "254/500 [==============>...............] - ETA: 4:14 - loss: 1.9200 - regression_loss: 1.4385 - classification_loss: 0.4815\n",
      "255/500 [==============>...............] - ETA: 4:12 - loss: 1.9201 - regression_loss: 1.4390 - classification_loss: 0.4810\n",
      "256/500 [==============>...............] - ETA: 4:11 - loss: 1.9200 - regression_loss: 1.4394 - classification_loss: 0.4806\n",
      "257/500 [==============>...............] - ETA: 4:10 - loss: 1.9215 - regression_loss: 1.4404 - classification_loss: 0.4811\n",
      "258/500 [==============>...............] - ETA: 4:09 - loss: 1.9199 - regression_loss: 1.4393 - classification_loss: 0.4806\n",
      "259/500 [==============>...............] - ETA: 4:08 - loss: 1.9186 - regression_loss: 1.4383 - classification_loss: 0.4803\n",
      "260/500 [==============>...............] - ETA: 4:07 - loss: 1.9190 - regression_loss: 1.4392 - classification_loss: 0.4798\n",
      "261/500 [==============>...............] - ETA: 4:06 - loss: 1.9178 - regression_loss: 1.4382 - classification_loss: 0.4796\n",
      "262/500 [==============>...............] - ETA: 4:05 - loss: 1.9179 - regression_loss: 1.4377 - classification_loss: 0.4802\n",
      "263/500 [==============>...............] - ETA: 4:04 - loss: 1.9158 - regression_loss: 1.4361 - classification_loss: 0.4796\n",
      "264/500 [==============>...............] - ETA: 4:03 - loss: 1.9148 - regression_loss: 1.4351 - classification_loss: 0.4797\n",
      "265/500 [==============>...............] - ETA: 4:02 - loss: 1.9139 - regression_loss: 1.4345 - classification_loss: 0.4793\n",
      "266/500 [==============>...............] - ETA: 4:01 - loss: 1.9118 - regression_loss: 1.4334 - classification_loss: 0.4784\n",
      "267/500 [===============>..............] - ETA: 4:00 - loss: 1.9111 - regression_loss: 1.4333 - classification_loss: 0.4778\n",
      "268/500 [===============>..............] - ETA: 3:59 - loss: 1.9134 - regression_loss: 1.4346 - classification_loss: 0.4788\n",
      "269/500 [===============>..............] - ETA: 3:58 - loss: 1.9135 - regression_loss: 1.4349 - classification_loss: 0.4785\n",
      "270/500 [===============>..............] - ETA: 3:57 - loss: 1.9143 - regression_loss: 1.4359 - classification_loss: 0.4783\n",
      "271/500 [===============>..............] - ETA: 3:56 - loss: 1.9120 - regression_loss: 1.4336 - classification_loss: 0.4784\n",
      "272/500 [===============>..............] - ETA: 3:55 - loss: 1.9133 - regression_loss: 1.4347 - classification_loss: 0.4786\n",
      "273/500 [===============>..............] - ETA: 3:54 - loss: 1.9186 - regression_loss: 1.4384 - classification_loss: 0.4802\n",
      "274/500 [===============>..............] - ETA: 3:52 - loss: 1.9162 - regression_loss: 1.4368 - classification_loss: 0.4794\n",
      "275/500 [===============>..............] - ETA: 3:51 - loss: 1.9150 - regression_loss: 1.4357 - classification_loss: 0.4793\n",
      "276/500 [===============>..............] - ETA: 3:50 - loss: 1.9157 - regression_loss: 1.4364 - classification_loss: 0.4794\n",
      "277/500 [===============>..............] - ETA: 3:49 - loss: 1.9145 - regression_loss: 1.4358 - classification_loss: 0.4787\n",
      "278/500 [===============>..............] - ETA: 3:48 - loss: 1.9130 - regression_loss: 1.4350 - classification_loss: 0.4780\n",
      "279/500 [===============>..............] - ETA: 3:47 - loss: 1.9137 - regression_loss: 1.4342 - classification_loss: 0.4795\n",
      "280/500 [===============>..............] - ETA: 3:46 - loss: 1.9137 - regression_loss: 1.4344 - classification_loss: 0.4793\n",
      "281/500 [===============>..............] - ETA: 3:45 - loss: 1.9108 - regression_loss: 1.4323 - classification_loss: 0.4785\n",
      "282/500 [===============>..............] - ETA: 3:44 - loss: 1.9113 - regression_loss: 1.4327 - classification_loss: 0.4786\n",
      "283/500 [===============>..............] - ETA: 3:43 - loss: 1.9118 - regression_loss: 1.4332 - classification_loss: 0.4786\n",
      "284/500 [================>.............] - ETA: 3:42 - loss: 1.9121 - regression_loss: 1.4330 - classification_loss: 0.4791\n",
      "285/500 [================>.............] - ETA: 3:41 - loss: 1.9108 - regression_loss: 1.4322 - classification_loss: 0.4786\n",
      "286/500 [================>.............] - ETA: 3:40 - loss: 1.9092 - regression_loss: 1.4309 - classification_loss: 0.4784\n",
      "287/500 [================>.............] - ETA: 3:39 - loss: 1.9118 - regression_loss: 1.4326 - classification_loss: 0.4792\n",
      "288/500 [================>.............] - ETA: 3:38 - loss: 1.9112 - regression_loss: 1.4322 - classification_loss: 0.4790\n",
      "289/500 [================>.............] - ETA: 3:37 - loss: 1.9142 - regression_loss: 1.4349 - classification_loss: 0.4793\n",
      "290/500 [================>.............] - ETA: 3:36 - loss: 1.9117 - regression_loss: 1.4328 - classification_loss: 0.4789\n",
      "291/500 [================>.............] - ETA: 3:35 - loss: 1.9116 - regression_loss: 1.4330 - classification_loss: 0.4786\n",
      "292/500 [================>.............] - ETA: 3:34 - loss: 1.9151 - regression_loss: 1.4356 - classification_loss: 0.4795\n",
      "293/500 [================>.............] - ETA: 3:33 - loss: 1.9179 - regression_loss: 1.4364 - classification_loss: 0.4815\n",
      "294/500 [================>.............] - ETA: 3:32 - loss: 1.9216 - regression_loss: 1.4393 - classification_loss: 0.4823\n",
      "295/500 [================>.............] - ETA: 3:31 - loss: 1.9215 - regression_loss: 1.4390 - classification_loss: 0.4826\n",
      "296/500 [================>.............] - ETA: 3:30 - loss: 1.9219 - regression_loss: 1.4389 - classification_loss: 0.4830\n",
      "297/500 [================>.............] - ETA: 3:29 - loss: 1.9227 - regression_loss: 1.4397 - classification_loss: 0.4830\n",
      "298/500 [================>.............] - ETA: 3:28 - loss: 1.9230 - regression_loss: 1.4404 - classification_loss: 0.4826\n",
      "299/500 [================>.............] - ETA: 3:27 - loss: 1.9223 - regression_loss: 1.4397 - classification_loss: 0.4826\n",
      "300/500 [=================>............] - ETA: 3:26 - loss: 1.9216 - regression_loss: 1.4392 - classification_loss: 0.4824\n",
      "301/500 [=================>............] - ETA: 3:25 - loss: 1.9233 - regression_loss: 1.4407 - classification_loss: 0.4826\n",
      "302/500 [=================>............] - ETA: 3:24 - loss: 1.9231 - regression_loss: 1.4409 - classification_loss: 0.4822\n",
      "303/500 [=================>............] - ETA: 3:22 - loss: 1.9209 - regression_loss: 1.4390 - classification_loss: 0.4818\n",
      "304/500 [=================>............] - ETA: 3:21 - loss: 1.9213 - regression_loss: 1.4397 - classification_loss: 0.4816\n",
      "305/500 [=================>............] - ETA: 3:20 - loss: 1.9192 - regression_loss: 1.4381 - classification_loss: 0.4811\n",
      "306/500 [=================>............] - ETA: 3:19 - loss: 1.9177 - regression_loss: 1.4370 - classification_loss: 0.4807\n",
      "307/500 [=================>............] - ETA: 3:18 - loss: 1.9168 - regression_loss: 1.4360 - classification_loss: 0.4808\n",
      "308/500 [=================>............] - ETA: 3:17 - loss: 1.9170 - regression_loss: 1.4361 - classification_loss: 0.4809\n",
      "309/500 [=================>............] - ETA: 3:16 - loss: 1.9164 - regression_loss: 1.4356 - classification_loss: 0.4808\n",
      "310/500 [=================>............] - ETA: 3:15 - loss: 1.9142 - regression_loss: 1.4339 - classification_loss: 0.4804\n",
      "311/500 [=================>............] - ETA: 3:14 - loss: 1.9151 - regression_loss: 1.4344 - classification_loss: 0.4806\n",
      "312/500 [=================>............] - ETA: 3:13 - loss: 1.9134 - regression_loss: 1.4332 - classification_loss: 0.4803\n",
      "313/500 [=================>............] - ETA: 3:12 - loss: 1.9142 - regression_loss: 1.4335 - classification_loss: 0.4806\n",
      "314/500 [=================>............] - ETA: 3:11 - loss: 1.9142 - regression_loss: 1.4335 - classification_loss: 0.4807\n",
      "315/500 [=================>............] - ETA: 3:10 - loss: 1.9144 - regression_loss: 1.4340 - classification_loss: 0.4804\n",
      "316/500 [=================>............] - ETA: 3:09 - loss: 1.9133 - regression_loss: 1.4332 - classification_loss: 0.4800\n",
      "317/500 [==================>...........] - ETA: 3:08 - loss: 1.9122 - regression_loss: 1.4328 - classification_loss: 0.4794\n",
      "318/500 [==================>...........] - ETA: 3:07 - loss: 1.9134 - regression_loss: 1.4340 - classification_loss: 0.4794\n",
      "319/500 [==================>...........] - ETA: 3:06 - loss: 1.9164 - regression_loss: 1.4353 - classification_loss: 0.4811\n",
      "320/500 [==================>...........] - ETA: 3:05 - loss: 1.9170 - regression_loss: 1.4355 - classification_loss: 0.4815\n",
      "321/500 [==================>...........] - ETA: 3:04 - loss: 1.9180 - regression_loss: 1.4364 - classification_loss: 0.4817\n",
      "322/500 [==================>...........] - ETA: 3:03 - loss: 1.9167 - regression_loss: 1.4353 - classification_loss: 0.4814\n",
      "323/500 [==================>...........] - ETA: 3:02 - loss: 1.9178 - regression_loss: 1.4361 - classification_loss: 0.4816\n",
      "324/500 [==================>...........] - ETA: 3:01 - loss: 1.9151 - regression_loss: 1.4342 - classification_loss: 0.4808\n",
      "325/500 [==================>...........] - ETA: 3:00 - loss: 1.9190 - regression_loss: 1.4368 - classification_loss: 0.4821\n",
      "326/500 [==================>...........] - ETA: 2:59 - loss: 1.9209 - regression_loss: 1.4379 - classification_loss: 0.4830\n",
      "327/500 [==================>...........] - ETA: 2:58 - loss: 1.9233 - regression_loss: 1.4402 - classification_loss: 0.4831\n",
      "328/500 [==================>...........] - ETA: 2:56 - loss: 1.9263 - regression_loss: 1.4424 - classification_loss: 0.4839\n",
      "329/500 [==================>...........] - ETA: 2:55 - loss: 1.9238 - regression_loss: 1.4404 - classification_loss: 0.4833\n",
      "330/500 [==================>...........] - ETA: 2:54 - loss: 1.9219 - regression_loss: 1.4393 - classification_loss: 0.4826\n",
      "331/500 [==================>...........] - ETA: 2:53 - loss: 1.9242 - regression_loss: 1.4405 - classification_loss: 0.4837\n",
      "332/500 [==================>...........] - ETA: 2:52 - loss: 1.9227 - regression_loss: 1.4396 - classification_loss: 0.4831\n",
      "333/500 [==================>...........] - ETA: 2:51 - loss: 1.9212 - regression_loss: 1.4385 - classification_loss: 0.4827\n",
      "334/500 [===================>..........] - ETA: 2:50 - loss: 1.9175 - regression_loss: 1.4356 - classification_loss: 0.4819\n",
      "335/500 [===================>..........] - ETA: 2:49 - loss: 1.9203 - regression_loss: 1.4373 - classification_loss: 0.4830\n",
      "336/500 [===================>..........] - ETA: 2:48 - loss: 1.9203 - regression_loss: 1.4376 - classification_loss: 0.4827\n",
      "337/500 [===================>..........] - ETA: 2:47 - loss: 1.9192 - regression_loss: 1.4365 - classification_loss: 0.4827\n",
      "338/500 [===================>..........] - ETA: 2:46 - loss: 1.9187 - regression_loss: 1.4362 - classification_loss: 0.4825\n",
      "339/500 [===================>..........] - ETA: 2:45 - loss: 1.9190 - regression_loss: 1.4363 - classification_loss: 0.4826\n",
      "340/500 [===================>..........] - ETA: 2:44 - loss: 1.9207 - regression_loss: 1.4376 - classification_loss: 0.4830\n",
      "341/500 [===================>..........] - ETA: 2:43 - loss: 1.9202 - regression_loss: 1.4377 - classification_loss: 0.4825\n",
      "342/500 [===================>..........] - ETA: 2:42 - loss: 1.9192 - regression_loss: 1.4369 - classification_loss: 0.4823\n",
      "343/500 [===================>..........] - ETA: 2:41 - loss: 1.9178 - regression_loss: 1.4361 - classification_loss: 0.4817\n",
      "344/500 [===================>..........] - ETA: 2:40 - loss: 1.9196 - regression_loss: 1.4372 - classification_loss: 0.4824\n",
      "345/500 [===================>..........] - ETA: 2:39 - loss: 1.9189 - regression_loss: 1.4367 - classification_loss: 0.4822\n",
      "346/500 [===================>..........] - ETA: 2:38 - loss: 1.9178 - regression_loss: 1.4358 - classification_loss: 0.4821\n",
      "347/500 [===================>..........] - ETA: 2:37 - loss: 1.9163 - regression_loss: 1.4347 - classification_loss: 0.4816\n",
      "348/500 [===================>..........] - ETA: 2:36 - loss: 1.9168 - regression_loss: 1.4352 - classification_loss: 0.4817\n",
      "349/500 [===================>..........] - ETA: 2:35 - loss: 1.9179 - regression_loss: 1.4361 - classification_loss: 0.4819\n",
      "350/500 [====================>.........] - ETA: 2:34 - loss: 1.9204 - regression_loss: 1.4376 - classification_loss: 0.4827\n",
      "351/500 [====================>.........] - ETA: 2:33 - loss: 1.9194 - regression_loss: 1.4369 - classification_loss: 0.4824\n",
      "352/500 [====================>.........] - ETA: 2:32 - loss: 1.9195 - regression_loss: 1.4370 - classification_loss: 0.4825\n",
      "353/500 [====================>.........] - ETA: 2:31 - loss: 1.9184 - regression_loss: 1.4360 - classification_loss: 0.4824\n",
      "354/500 [====================>.........] - ETA: 2:30 - loss: 1.9189 - regression_loss: 1.4366 - classification_loss: 0.4823\n",
      "355/500 [====================>.........] - ETA: 2:28 - loss: 1.9188 - regression_loss: 1.4363 - classification_loss: 0.4824\n",
      "356/500 [====================>.........] - ETA: 2:28 - loss: 1.9205 - regression_loss: 1.4372 - classification_loss: 0.4833\n",
      "357/500 [====================>.........] - ETA: 2:26 - loss: 1.9209 - regression_loss: 1.4374 - classification_loss: 0.4836\n",
      "358/500 [====================>.........] - ETA: 2:25 - loss: 1.9202 - regression_loss: 1.4369 - classification_loss: 0.4833\n",
      "359/500 [====================>.........] - ETA: 2:24 - loss: 1.9208 - regression_loss: 1.4375 - classification_loss: 0.4833\n",
      "360/500 [====================>.........] - ETA: 2:23 - loss: 1.9219 - regression_loss: 1.4384 - classification_loss: 0.4835\n",
      "361/500 [====================>.........] - ETA: 2:22 - loss: 1.9236 - regression_loss: 1.4398 - classification_loss: 0.4838\n",
      "362/500 [====================>.........] - ETA: 2:21 - loss: 1.9232 - regression_loss: 1.4395 - classification_loss: 0.4836\n",
      "363/500 [====================>.........] - ETA: 2:20 - loss: 1.9253 - regression_loss: 1.4409 - classification_loss: 0.4843\n",
      "364/500 [====================>.........] - ETA: 2:19 - loss: 1.9262 - regression_loss: 1.4419 - classification_loss: 0.4843\n",
      "365/500 [====================>.........] - ETA: 2:18 - loss: 1.9256 - regression_loss: 1.4414 - classification_loss: 0.4842\n",
      "366/500 [====================>.........] - ETA: 2:17 - loss: 1.9243 - regression_loss: 1.4407 - classification_loss: 0.4836\n",
      "367/500 [=====================>........] - ETA: 2:16 - loss: 1.9234 - regression_loss: 1.4399 - classification_loss: 0.4836\n",
      "368/500 [=====================>........] - ETA: 2:15 - loss: 1.9218 - regression_loss: 1.4386 - classification_loss: 0.4832\n",
      "369/500 [=====================>........] - ETA: 2:14 - loss: 1.9216 - regression_loss: 1.4387 - classification_loss: 0.4829\n",
      "370/500 [=====================>........] - ETA: 2:13 - loss: 1.9210 - regression_loss: 1.4383 - classification_loss: 0.4827\n",
      "371/500 [=====================>........] - ETA: 2:12 - loss: 1.9207 - regression_loss: 1.4384 - classification_loss: 0.4823\n",
      "372/500 [=====================>........] - ETA: 2:11 - loss: 1.9184 - regression_loss: 1.4367 - classification_loss: 0.4817\n",
      "373/500 [=====================>........] - ETA: 2:10 - loss: 1.9182 - regression_loss: 1.4368 - classification_loss: 0.4814\n",
      "374/500 [=====================>........] - ETA: 2:09 - loss: 1.9169 - regression_loss: 1.4356 - classification_loss: 0.4813\n",
      "375/500 [=====================>........] - ETA: 2:08 - loss: 1.9167 - regression_loss: 1.4350 - classification_loss: 0.4817\n",
      "376/500 [=====================>........] - ETA: 2:07 - loss: 1.9180 - regression_loss: 1.4360 - classification_loss: 0.4820\n",
      "377/500 [=====================>........] - ETA: 2:06 - loss: 1.9194 - regression_loss: 1.4374 - classification_loss: 0.4820\n",
      "378/500 [=====================>........] - ETA: 2:05 - loss: 1.9200 - regression_loss: 1.4383 - classification_loss: 0.4817\n",
      "379/500 [=====================>........] - ETA: 2:04 - loss: 1.9196 - regression_loss: 1.4375 - classification_loss: 0.4821\n",
      "380/500 [=====================>........] - ETA: 2:03 - loss: 1.9208 - regression_loss: 1.4384 - classification_loss: 0.4824\n",
      "381/500 [=====================>........] - ETA: 2:02 - loss: 1.9198 - regression_loss: 1.4377 - classification_loss: 0.4821\n",
      "382/500 [=====================>........] - ETA: 2:01 - loss: 1.9194 - regression_loss: 1.4376 - classification_loss: 0.4818\n",
      "383/500 [=====================>........] - ETA: 2:00 - loss: 1.9179 - regression_loss: 1.4364 - classification_loss: 0.4815\n",
      "384/500 [======================>.......] - ETA: 1:59 - loss: 1.9211 - regression_loss: 1.4383 - classification_loss: 0.4828\n",
      "385/500 [======================>.......] - ETA: 1:58 - loss: 1.9194 - regression_loss: 1.4370 - classification_loss: 0.4825\n",
      "386/500 [======================>.......] - ETA: 1:57 - loss: 1.9211 - regression_loss: 1.4385 - classification_loss: 0.4826\n",
      "387/500 [======================>.......] - ETA: 1:56 - loss: 1.9213 - regression_loss: 1.4380 - classification_loss: 0.4833\n",
      "388/500 [======================>.......] - ETA: 1:55 - loss: 1.9210 - regression_loss: 1.4380 - classification_loss: 0.4831\n",
      "389/500 [======================>.......] - ETA: 1:54 - loss: 1.9213 - regression_loss: 1.4382 - classification_loss: 0.4831\n",
      "390/500 [======================>.......] - ETA: 1:53 - loss: 1.9214 - regression_loss: 1.4383 - classification_loss: 0.4831\n",
      "391/500 [======================>.......] - ETA: 1:52 - loss: 1.9221 - regression_loss: 1.4390 - classification_loss: 0.4830\n",
      "392/500 [======================>.......] - ETA: 1:51 - loss: 1.9225 - regression_loss: 1.4396 - classification_loss: 0.4829\n",
      "393/500 [======================>.......] - ETA: 1:50 - loss: 1.9217 - regression_loss: 1.4391 - classification_loss: 0.4826\n",
      "394/500 [======================>.......] - ETA: 1:48 - loss: 1.9212 - regression_loss: 1.4387 - classification_loss: 0.4826\n",
      "395/500 [======================>.......] - ETA: 1:47 - loss: 1.9209 - regression_loss: 1.4382 - classification_loss: 0.4827\n",
      "396/500 [======================>.......] - ETA: 1:46 - loss: 1.9191 - regression_loss: 1.4368 - classification_loss: 0.4823\n",
      "397/500 [======================>.......] - ETA: 1:45 - loss: 1.9199 - regression_loss: 1.4375 - classification_loss: 0.4824\n",
      "398/500 [======================>.......] - ETA: 1:44 - loss: 1.9200 - regression_loss: 1.4374 - classification_loss: 0.4826\n",
      "399/500 [======================>.......] - ETA: 1:43 - loss: 1.9196 - regression_loss: 1.4370 - classification_loss: 0.4827\n",
      "400/500 [=======================>......] - ETA: 1:42 - loss: 1.9194 - regression_loss: 1.4368 - classification_loss: 0.4826\n",
      "401/500 [=======================>......] - ETA: 1:41 - loss: 1.9199 - regression_loss: 1.4373 - classification_loss: 0.4826\n",
      "402/500 [=======================>......] - ETA: 1:40 - loss: 1.9189 - regression_loss: 1.4367 - classification_loss: 0.4822\n",
      "403/500 [=======================>......] - ETA: 1:39 - loss: 1.9199 - regression_loss: 1.4369 - classification_loss: 0.4831\n",
      "404/500 [=======================>......] - ETA: 1:38 - loss: 1.9195 - regression_loss: 1.4366 - classification_loss: 0.4829\n",
      "405/500 [=======================>......] - ETA: 1:37 - loss: 1.9215 - regression_loss: 1.4386 - classification_loss: 0.4829\n",
      "406/500 [=======================>......] - ETA: 1:36 - loss: 1.9218 - regression_loss: 1.4388 - classification_loss: 0.4830\n",
      "407/500 [=======================>......] - ETA: 1:35 - loss: 1.9199 - regression_loss: 1.4377 - classification_loss: 0.4823\n",
      "408/500 [=======================>......] - ETA: 1:34 - loss: 1.9186 - regression_loss: 1.4363 - classification_loss: 0.4824\n",
      "409/500 [=======================>......] - ETA: 1:33 - loss: 1.9176 - regression_loss: 1.4354 - classification_loss: 0.4822\n",
      "410/500 [=======================>......] - ETA: 1:32 - loss: 1.9177 - regression_loss: 1.4355 - classification_loss: 0.4822\n",
      "411/500 [=======================>......] - ETA: 1:31 - loss: 1.9178 - regression_loss: 1.4353 - classification_loss: 0.4825\n",
      "412/500 [=======================>......] - ETA: 1:30 - loss: 1.9190 - regression_loss: 1.4361 - classification_loss: 0.4829\n",
      "413/500 [=======================>......] - ETA: 1:29 - loss: 1.9206 - regression_loss: 1.4373 - classification_loss: 0.4833\n",
      "414/500 [=======================>......] - ETA: 1:28 - loss: 1.9215 - regression_loss: 1.4379 - classification_loss: 0.4836\n",
      "415/500 [=======================>......] - ETA: 1:27 - loss: 1.9217 - regression_loss: 1.4382 - classification_loss: 0.4835\n",
      "416/500 [=======================>......] - ETA: 1:26 - loss: 1.9206 - regression_loss: 1.4374 - classification_loss: 0.4833\n",
      "417/500 [========================>.....] - ETA: 1:25 - loss: 1.9188 - regression_loss: 1.4360 - classification_loss: 0.4829\n",
      "418/500 [========================>.....] - ETA: 1:24 - loss: 1.9185 - regression_loss: 1.4356 - classification_loss: 0.4830\n",
      "419/500 [========================>.....] - ETA: 1:23 - loss: 1.9176 - regression_loss: 1.4347 - classification_loss: 0.4829\n",
      "420/500 [========================>.....] - ETA: 1:22 - loss: 1.9179 - regression_loss: 1.4352 - classification_loss: 0.4826\n",
      "421/500 [========================>.....] - ETA: 1:21 - loss: 1.9196 - regression_loss: 1.4362 - classification_loss: 0.4834\n",
      "422/500 [========================>.....] - ETA: 1:20 - loss: 1.9193 - regression_loss: 1.4361 - classification_loss: 0.4832\n",
      "423/500 [========================>.....] - ETA: 1:19 - loss: 1.9189 - regression_loss: 1.4359 - classification_loss: 0.4830\n",
      "424/500 [========================>.....] - ETA: 1:18 - loss: 1.9208 - regression_loss: 1.4373 - classification_loss: 0.4836\n",
      "425/500 [========================>.....] - ETA: 1:17 - loss: 1.9216 - regression_loss: 1.4376 - classification_loss: 0.4840\n",
      "426/500 [========================>.....] - ETA: 1:16 - loss: 1.9209 - regression_loss: 1.4371 - classification_loss: 0.4837\n",
      "427/500 [========================>.....] - ETA: 1:15 - loss: 1.9214 - regression_loss: 1.4372 - classification_loss: 0.4842\n",
      "428/500 [========================>.....] - ETA: 1:14 - loss: 1.9202 - regression_loss: 1.4362 - classification_loss: 0.4840\n",
      "429/500 [========================>.....] - ETA: 1:13 - loss: 1.9205 - regression_loss: 1.4365 - classification_loss: 0.4840\n",
      "430/500 [========================>.....] - ETA: 1:12 - loss: 1.9195 - regression_loss: 1.4357 - classification_loss: 0.4839\n",
      "431/500 [========================>.....] - ETA: 1:11 - loss: 1.9204 - regression_loss: 1.4364 - classification_loss: 0.4840\n",
      "432/500 [========================>.....] - ETA: 1:10 - loss: 1.9199 - regression_loss: 1.4362 - classification_loss: 0.4836\n",
      "433/500 [========================>.....] - ETA: 1:08 - loss: 1.9194 - regression_loss: 1.4357 - classification_loss: 0.4836\n",
      "434/500 [=========================>....] - ETA: 1:07 - loss: 1.9202 - regression_loss: 1.4365 - classification_loss: 0.4836\n",
      "435/500 [=========================>....] - ETA: 1:06 - loss: 1.9199 - regression_loss: 1.4364 - classification_loss: 0.4836\n",
      "436/500 [=========================>....] - ETA: 1:05 - loss: 1.9221 - regression_loss: 1.4375 - classification_loss: 0.4846\n",
      "437/500 [=========================>....] - ETA: 1:04 - loss: 1.9222 - regression_loss: 1.4371 - classification_loss: 0.4851\n",
      "438/500 [=========================>....] - ETA: 1:03 - loss: 1.9228 - regression_loss: 1.4375 - classification_loss: 0.4853\n",
      "439/500 [=========================>....] - ETA: 1:02 - loss: 1.9215 - regression_loss: 1.4366 - classification_loss: 0.4849\n",
      "440/500 [=========================>....] - ETA: 1:01 - loss: 1.9201 - regression_loss: 1.4357 - classification_loss: 0.4844\n",
      "441/500 [=========================>....] - ETA: 1:00 - loss: 1.9189 - regression_loss: 1.4347 - classification_loss: 0.4842\n",
      "442/500 [=========================>....] - ETA: 59s - loss: 1.9183 - regression_loss: 1.4341 - classification_loss: 0.4841 \n",
      "443/500 [=========================>....] - ETA: 58s - loss: 1.9179 - regression_loss: 1.4338 - classification_loss: 0.4841\n",
      "444/500 [=========================>....] - ETA: 57s - loss: 1.9174 - regression_loss: 1.4333 - classification_loss: 0.4841\n",
      "445/500 [=========================>....] - ETA: 56s - loss: 1.9156 - regression_loss: 1.4319 - classification_loss: 0.4837\n",
      "446/500 [=========================>....] - ETA: 55s - loss: 1.9155 - regression_loss: 1.4321 - classification_loss: 0.4835\n",
      "447/500 [=========================>....] - ETA: 54s - loss: 1.9152 - regression_loss: 1.4318 - classification_loss: 0.4834\n",
      "448/500 [=========================>....] - ETA: 53s - loss: 1.9163 - regression_loss: 1.4330 - classification_loss: 0.4834\n",
      "449/500 [=========================>....] - ETA: 52s - loss: 1.9174 - regression_loss: 1.4340 - classification_loss: 0.4834\n",
      "450/500 [==========================>...] - ETA: 51s - loss: 1.9200 - regression_loss: 1.4354 - classification_loss: 0.4846\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 1.9196 - regression_loss: 1.4353 - classification_loss: 0.4843\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 1.9188 - regression_loss: 1.4346 - classification_loss: 0.4841\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.9213 - regression_loss: 1.4364 - classification_loss: 0.4849\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.9205 - regression_loss: 1.4361 - classification_loss: 0.4844\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.9196 - regression_loss: 1.4356 - classification_loss: 0.4839\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.9212 - regression_loss: 1.4370 - classification_loss: 0.4842\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.9207 - regression_loss: 1.4364 - classification_loss: 0.4843\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.9224 - regression_loss: 1.4375 - classification_loss: 0.4849\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.9216 - regression_loss: 1.4370 - classification_loss: 0.4846\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.9233 - regression_loss: 1.4382 - classification_loss: 0.4851\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 1.9256 - regression_loss: 1.4395 - classification_loss: 0.4861\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 1.9267 - regression_loss: 1.4402 - classification_loss: 0.4866\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 1.9265 - regression_loss: 1.4399 - classification_loss: 0.4866\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 1.9258 - regression_loss: 1.4394 - classification_loss: 0.4864\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 1.9268 - regression_loss: 1.4400 - classification_loss: 0.4868\n",
      "466/500 [==========================>...] - ETA: 34s - loss: 1.9265 - regression_loss: 1.4395 - classification_loss: 0.4870\n",
      "467/500 [===========================>..] - ETA: 33s - loss: 1.9248 - regression_loss: 1.4382 - classification_loss: 0.4866\n",
      "468/500 [===========================>..] - ETA: 32s - loss: 1.9251 - regression_loss: 1.4383 - classification_loss: 0.4867\n",
      "469/500 [===========================>..] - ETA: 31s - loss: 1.9247 - regression_loss: 1.4384 - classification_loss: 0.4863\n",
      "470/500 [===========================>..] - ETA: 30s - loss: 1.9240 - regression_loss: 1.4380 - classification_loss: 0.4860\n",
      "471/500 [===========================>..] - ETA: 29s - loss: 1.9243 - regression_loss: 1.4383 - classification_loss: 0.4861\n",
      "472/500 [===========================>..] - ETA: 28s - loss: 1.9234 - regression_loss: 1.4378 - classification_loss: 0.4856\n",
      "473/500 [===========================>..] - ETA: 27s - loss: 1.9243 - regression_loss: 1.4382 - classification_loss: 0.4861\n",
      "474/500 [===========================>..] - ETA: 26s - loss: 1.9231 - regression_loss: 1.4374 - classification_loss: 0.4857\n",
      "475/500 [===========================>..] - ETA: 25s - loss: 1.9256 - regression_loss: 1.4390 - classification_loss: 0.4866\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 1.9271 - regression_loss: 1.4404 - classification_loss: 0.4867\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.9270 - regression_loss: 1.4403 - classification_loss: 0.4867\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.9256 - regression_loss: 1.4395 - classification_loss: 0.4861\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.9266 - regression_loss: 1.4401 - classification_loss: 0.4865\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.9256 - regression_loss: 1.4394 - classification_loss: 0.4862\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.9254 - regression_loss: 1.4392 - classification_loss: 0.4861\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.9246 - regression_loss: 1.4388 - classification_loss: 0.4858\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.9239 - regression_loss: 1.4386 - classification_loss: 0.4853\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.9241 - regression_loss: 1.4390 - classification_loss: 0.4850\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.9232 - regression_loss: 1.4387 - classification_loss: 0.4845\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.9236 - regression_loss: 1.4384 - classification_loss: 0.4852\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.9236 - regression_loss: 1.4384 - classification_loss: 0.4852\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.9230 - regression_loss: 1.4376 - classification_loss: 0.4854\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.9236 - regression_loss: 1.4382 - classification_loss: 0.4855\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.9256 - regression_loss: 1.4396 - classification_loss: 0.4859\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.9266 - regression_loss: 1.4403 - classification_loss: 0.4864 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.9267 - regression_loss: 1.4405 - classification_loss: 0.4862\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.9265 - regression_loss: 1.4404 - classification_loss: 0.4861\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.9255 - regression_loss: 1.4400 - classification_loss: 0.4855\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.9249 - regression_loss: 1.4396 - classification_loss: 0.4853\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.9254 - regression_loss: 1.4402 - classification_loss: 0.4852\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.9254 - regression_loss: 1.4400 - classification_loss: 0.4854\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.9242 - regression_loss: 1.4392 - classification_loss: 0.4850\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.9263 - regression_loss: 1.4404 - classification_loss: 0.4859\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9258 - regression_loss: 1.4402 - classification_loss: 0.4856\n",
      "Epoch 00042: saving model to ./snapshots\\resnet50_csv_42.h5\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 9.999998845134856e-18.\n",
      "\n",
      "500/500 [==============================] - 515s 1s/step - loss: 1.9258 - regression_loss: 1.4402 - classification_loss: 0.4856\n",
      "Epoch 43/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 1.1478 - regression_loss: 0.7451 - classification_loss: 0.4027\n",
      "  2/500 [..............................] - ETA: 4:59 - loss: 1.5396 - regression_loss: 1.0578 - classification_loss: 0.4818\n",
      "  3/500 [..............................] - ETA: 6:01 - loss: 1.7705 - regression_loss: 1.2063 - classification_loss: 0.5642\n",
      "  4/500 [..............................] - ETA: 6:33 - loss: 1.8689 - regression_loss: 1.2499 - classification_loss: 0.6190\n",
      "  5/500 [..............................] - ETA: 7:14 - loss: 1.7443 - regression_loss: 1.1918 - classification_loss: 0.5525\n",
      "  6/500 [..............................] - ETA: 7:29 - loss: 1.7938 - regression_loss: 1.2360 - classification_loss: 0.5578\n",
      "  7/500 [..............................] - ETA: 7:33 - loss: 1.8259 - regression_loss: 1.2781 - classification_loss: 0.5479\n",
      "  8/500 [..............................] - ETA: 7:43 - loss: 1.8055 - regression_loss: 1.2609 - classification_loss: 0.5447\n",
      "  9/500 [..............................] - ETA: 7:46 - loss: 1.8268 - regression_loss: 1.2930 - classification_loss: 0.5338\n",
      " 10/500 [..............................] - ETA: 7:47 - loss: 1.8550 - regression_loss: 1.3358 - classification_loss: 0.5192\n",
      " 11/500 [..............................] - ETA: 7:47 - loss: 1.7970 - regression_loss: 1.3041 - classification_loss: 0.4929\n",
      " 12/500 [..............................] - ETA: 7:51 - loss: 1.8516 - regression_loss: 1.3518 - classification_loss: 0.4998\n",
      " 13/500 [..............................] - ETA: 7:59 - loss: 1.7942 - regression_loss: 1.3172 - classification_loss: 0.4769\n",
      " 14/500 [..............................] - ETA: 7:58 - loss: 1.8367 - regression_loss: 1.3553 - classification_loss: 0.4814\n",
      " 15/500 [..............................] - ETA: 8:01 - loss: 1.8584 - regression_loss: 1.3763 - classification_loss: 0.4822\n",
      " 16/500 [..............................] - ETA: 7:59 - loss: 1.8529 - regression_loss: 1.3753 - classification_loss: 0.4776\n",
      " 17/500 [>.............................] - ETA: 8:01 - loss: 1.8192 - regression_loss: 1.3477 - classification_loss: 0.4715\n",
      " 18/500 [>.............................] - ETA: 8:00 - loss: 1.7866 - regression_loss: 1.3266 - classification_loss: 0.4599\n",
      " 19/500 [>.............................] - ETA: 7:58 - loss: 1.7504 - regression_loss: 1.2982 - classification_loss: 0.4523\n",
      " 20/500 [>.............................] - ETA: 7:57 - loss: 1.7486 - regression_loss: 1.2985 - classification_loss: 0.4501\n",
      " 21/500 [>.............................] - ETA: 7:56 - loss: 1.7921 - regression_loss: 1.3421 - classification_loss: 0.4500\n",
      " 22/500 [>.............................] - ETA: 7:55 - loss: 1.7730 - regression_loss: 1.3273 - classification_loss: 0.4457\n",
      " 23/500 [>.............................] - ETA: 7:56 - loss: 1.7600 - regression_loss: 1.3198 - classification_loss: 0.4402\n",
      " 24/500 [>.............................] - ETA: 7:54 - loss: 1.7438 - regression_loss: 1.3069 - classification_loss: 0.4369\n",
      " 25/500 [>.............................] - ETA: 7:53 - loss: 1.7324 - regression_loss: 1.2989 - classification_loss: 0.4334\n",
      " 26/500 [>.............................] - ETA: 7:54 - loss: 1.7141 - regression_loss: 1.2833 - classification_loss: 0.4308\n",
      " 27/500 [>.............................] - ETA: 7:54 - loss: 1.6954 - regression_loss: 1.2674 - classification_loss: 0.4280\n",
      " 28/500 [>.............................] - ETA: 7:53 - loss: 1.7120 - regression_loss: 1.2753 - classification_loss: 0.4367\n",
      " 29/500 [>.............................] - ETA: 7:51 - loss: 1.7386 - regression_loss: 1.2984 - classification_loss: 0.4402\n",
      " 30/500 [>.............................] - ETA: 7:52 - loss: 1.7502 - regression_loss: 1.3083 - classification_loss: 0.4419\n",
      " 31/500 [>.............................] - ETA: 7:50 - loss: 1.7754 - regression_loss: 1.3213 - classification_loss: 0.4541\n",
      " 32/500 [>.............................] - ETA: 7:46 - loss: 1.7769 - regression_loss: 1.3248 - classification_loss: 0.4521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 33/500 [>.............................] - ETA: 7:47 - loss: 1.7958 - regression_loss: 1.3380 - classification_loss: 0.4578\n",
      " 34/500 [=>............................] - ETA: 7:47 - loss: 1.7810 - regression_loss: 1.3284 - classification_loss: 0.4526\n",
      " 35/500 [=>............................] - ETA: 7:48 - loss: 1.7952 - regression_loss: 1.3364 - classification_loss: 0.4587\n",
      " 36/500 [=>............................] - ETA: 7:48 - loss: 1.8292 - regression_loss: 1.3655 - classification_loss: 0.4636\n",
      " 37/500 [=>............................] - ETA: 7:48 - loss: 1.8292 - regression_loss: 1.3668 - classification_loss: 0.4624\n",
      " 38/500 [=>............................] - ETA: 7:47 - loss: 1.8509 - regression_loss: 1.3833 - classification_loss: 0.4676\n",
      " 39/500 [=>............................] - ETA: 7:45 - loss: 1.8373 - regression_loss: 1.3745 - classification_loss: 0.4628\n",
      " 40/500 [=>............................] - ETA: 7:44 - loss: 1.8445 - regression_loss: 1.3769 - classification_loss: 0.4677\n",
      " 41/500 [=>............................] - ETA: 7:43 - loss: 1.8339 - regression_loss: 1.3702 - classification_loss: 0.4637\n",
      " 42/500 [=>............................] - ETA: 7:43 - loss: 1.8319 - regression_loss: 1.3674 - classification_loss: 0.4645\n",
      " 43/500 [=>............................] - ETA: 7:39 - loss: 1.8164 - regression_loss: 1.3548 - classification_loss: 0.4617\n",
      " 44/500 [=>............................] - ETA: 7:40 - loss: 1.8205 - regression_loss: 1.3580 - classification_loss: 0.4625\n",
      " 45/500 [=>............................] - ETA: 7:40 - loss: 1.8135 - regression_loss: 1.3544 - classification_loss: 0.4591\n",
      " 46/500 [=>............................] - ETA: 7:38 - loss: 1.8048 - regression_loss: 1.3483 - classification_loss: 0.4565\n",
      " 47/500 [=>............................] - ETA: 8:01 - loss: 1.8018 - regression_loss: 1.3465 - classification_loss: 0.4553\n",
      " 48/500 [=>............................] - ETA: 7:59 - loss: 1.7968 - regression_loss: 1.3451 - classification_loss: 0.4517\n",
      " 49/500 [=>............................] - ETA: 7:58 - loss: 1.7952 - regression_loss: 1.3420 - classification_loss: 0.4532\n",
      " 50/500 [==>...........................] - ETA: 7:57 - loss: 1.8078 - regression_loss: 1.3526 - classification_loss: 0.4552\n",
      " 51/500 [==>...........................] - ETA: 7:56 - loss: 1.8066 - regression_loss: 1.3527 - classification_loss: 0.4539\n",
      " 52/500 [==>...........................] - ETA: 7:54 - loss: 1.7985 - regression_loss: 1.3444 - classification_loss: 0.4541\n",
      " 53/500 [==>...........................] - ETA: 7:53 - loss: 1.8047 - regression_loss: 1.3495 - classification_loss: 0.4552\n",
      " 54/500 [==>...........................] - ETA: 7:52 - loss: 1.8045 - regression_loss: 1.3507 - classification_loss: 0.4537\n",
      " 55/500 [==>...........................] - ETA: 7:52 - loss: 1.7983 - regression_loss: 1.3494 - classification_loss: 0.4488\n",
      " 56/500 [==>...........................] - ETA: 7:50 - loss: 1.7977 - regression_loss: 1.3500 - classification_loss: 0.4477\n",
      " 57/500 [==>...........................] - ETA: 7:47 - loss: 1.7882 - regression_loss: 1.3440 - classification_loss: 0.4442\n",
      " 58/500 [==>...........................] - ETA: 7:46 - loss: 1.7878 - regression_loss: 1.3425 - classification_loss: 0.4453\n",
      " 59/500 [==>...........................] - ETA: 7:45 - loss: 1.7847 - regression_loss: 1.3379 - classification_loss: 0.4468\n",
      " 60/500 [==>...........................] - ETA: 7:45 - loss: 1.8030 - regression_loss: 1.3517 - classification_loss: 0.4513\n",
      " 61/500 [==>...........................] - ETA: 7:43 - loss: 1.8217 - regression_loss: 1.3628 - classification_loss: 0.4589\n",
      " 62/500 [==>...........................] - ETA: 7:41 - loss: 1.8274 - regression_loss: 1.3692 - classification_loss: 0.4582\n",
      " 63/500 [==>...........................] - ETA: 7:40 - loss: 1.8361 - regression_loss: 1.3771 - classification_loss: 0.4590\n",
      " 64/500 [==>...........................] - ETA: 7:40 - loss: 1.8392 - regression_loss: 1.3807 - classification_loss: 0.4585\n",
      " 65/500 [==>...........................] - ETA: 7:39 - loss: 1.8346 - regression_loss: 1.3773 - classification_loss: 0.4573\n",
      " 66/500 [==>...........................] - ETA: 7:38 - loss: 1.8338 - regression_loss: 1.3764 - classification_loss: 0.4574\n",
      " 67/500 [===>..........................] - ETA: 7:36 - loss: 1.8455 - regression_loss: 1.3850 - classification_loss: 0.4604\n",
      " 68/500 [===>..........................] - ETA: 7:35 - loss: 1.8433 - regression_loss: 1.3838 - classification_loss: 0.4594\n",
      " 69/500 [===>..........................] - ETA: 7:33 - loss: 1.8565 - regression_loss: 1.3919 - classification_loss: 0.4646\n",
      " 70/500 [===>..........................] - ETA: 7:32 - loss: 1.8546 - regression_loss: 1.3906 - classification_loss: 0.4639\n",
      " 71/500 [===>..........................] - ETA: 7:30 - loss: 1.8607 - regression_loss: 1.3943 - classification_loss: 0.4664\n",
      " 72/500 [===>..........................] - ETA: 7:29 - loss: 1.8536 - regression_loss: 1.3887 - classification_loss: 0.4648\n",
      " 73/500 [===>..........................] - ETA: 7:28 - loss: 1.8650 - regression_loss: 1.3983 - classification_loss: 0.4667\n",
      " 74/500 [===>..........................] - ETA: 7:27 - loss: 1.8692 - regression_loss: 1.4012 - classification_loss: 0.4680\n",
      " 75/500 [===>..........................] - ETA: 7:25 - loss: 1.8664 - regression_loss: 1.3957 - classification_loss: 0.4707\n",
      " 76/500 [===>..........................] - ETA: 7:24 - loss: 1.8642 - regression_loss: 1.3939 - classification_loss: 0.4704\n",
      " 77/500 [===>..........................] - ETA: 7:23 - loss: 1.8518 - regression_loss: 1.3846 - classification_loss: 0.4673\n",
      " 78/500 [===>..........................] - ETA: 7:22 - loss: 1.8662 - regression_loss: 1.3928 - classification_loss: 0.4734\n",
      " 79/500 [===>..........................] - ETA: 7:22 - loss: 1.8704 - regression_loss: 1.3949 - classification_loss: 0.4756\n",
      " 80/500 [===>..........................] - ETA: 7:21 - loss: 1.8637 - regression_loss: 1.3908 - classification_loss: 0.4729\n",
      " 81/500 [===>..........................] - ETA: 7:19 - loss: 1.8790 - regression_loss: 1.4013 - classification_loss: 0.4777\n",
      " 82/500 [===>..........................] - ETA: 7:18 - loss: 1.8805 - regression_loss: 1.4044 - classification_loss: 0.4762\n",
      " 83/500 [===>..........................] - ETA: 7:17 - loss: 1.8710 - regression_loss: 1.3976 - classification_loss: 0.4734\n",
      " 84/500 [====>.........................] - ETA: 7:14 - loss: 1.8646 - regression_loss: 1.3917 - classification_loss: 0.4729\n",
      " 85/500 [====>.........................] - ETA: 7:14 - loss: 1.8703 - regression_loss: 1.3956 - classification_loss: 0.4747\n",
      " 86/500 [====>.........................] - ETA: 7:14 - loss: 1.8658 - regression_loss: 1.3926 - classification_loss: 0.4733\n",
      " 87/500 [====>.........................] - ETA: 7:13 - loss: 1.8642 - regression_loss: 1.3908 - classification_loss: 0.4734\n",
      " 88/500 [====>.........................] - ETA: 7:11 - loss: 1.8674 - regression_loss: 1.3928 - classification_loss: 0.4746\n",
      " 89/500 [====>.........................] - ETA: 7:10 - loss: 1.8690 - regression_loss: 1.3938 - classification_loss: 0.4752\n",
      " 90/500 [====>.........................] - ETA: 7:09 - loss: 1.8663 - regression_loss: 1.3920 - classification_loss: 0.4744\n",
      " 91/500 [====>.........................] - ETA: 7:07 - loss: 1.8685 - regression_loss: 1.3921 - classification_loss: 0.4764\n",
      " 92/500 [====>.........................] - ETA: 7:07 - loss: 1.8665 - regression_loss: 1.3907 - classification_loss: 0.4758\n",
      " 93/500 [====>.........................] - ETA: 7:05 - loss: 1.8618 - regression_loss: 1.3863 - classification_loss: 0.4755\n",
      " 94/500 [====>.........................] - ETA: 7:04 - loss: 1.8563 - regression_loss: 1.3831 - classification_loss: 0.4732\n",
      " 95/500 [====>.........................] - ETA: 7:02 - loss: 1.8558 - regression_loss: 1.3828 - classification_loss: 0.4729\n",
      " 96/500 [====>.........................] - ETA: 7:01 - loss: 1.8564 - regression_loss: 1.3850 - classification_loss: 0.4713\n",
      " 97/500 [====>.........................] - ETA: 6:59 - loss: 1.8554 - regression_loss: 1.3837 - classification_loss: 0.4716\n",
      " 98/500 [====>.........................] - ETA: 6:58 - loss: 1.8525 - regression_loss: 1.3819 - classification_loss: 0.4706\n",
      " 99/500 [====>.........................] - ETA: 6:58 - loss: 1.8506 - regression_loss: 1.3809 - classification_loss: 0.4697\n",
      "100/500 [=====>........................] - ETA: 6:57 - loss: 1.8490 - regression_loss: 1.3795 - classification_loss: 0.4695\n",
      "101/500 [=====>........................] - ETA: 6:56 - loss: 1.8432 - regression_loss: 1.3753 - classification_loss: 0.4679\n",
      "102/500 [=====>........................] - ETA: 6:55 - loss: 1.8436 - regression_loss: 1.3758 - classification_loss: 0.4678\n",
      "103/500 [=====>........................] - ETA: 6:53 - loss: 1.8364 - regression_loss: 1.3704 - classification_loss: 0.4660\n",
      "104/500 [=====>........................] - ETA: 6:53 - loss: 1.8403 - regression_loss: 1.3740 - classification_loss: 0.4663\n",
      "105/500 [=====>........................] - ETA: 6:51 - loss: 1.8504 - regression_loss: 1.3814 - classification_loss: 0.4690\n",
      "106/500 [=====>........................] - ETA: 6:50 - loss: 1.8478 - regression_loss: 1.3799 - classification_loss: 0.4680\n",
      "107/500 [=====>........................] - ETA: 6:48 - loss: 1.8427 - regression_loss: 1.3772 - classification_loss: 0.4655\n",
      "108/500 [=====>........................] - ETA: 6:48 - loss: 1.8520 - regression_loss: 1.3831 - classification_loss: 0.4689\n",
      "109/500 [=====>........................] - ETA: 6:47 - loss: 1.8540 - regression_loss: 1.3847 - classification_loss: 0.4693\n",
      "110/500 [=====>........................] - ETA: 6:46 - loss: 1.8546 - regression_loss: 1.3857 - classification_loss: 0.4690\n",
      "111/500 [=====>........................] - ETA: 6:44 - loss: 1.8463 - regression_loss: 1.3801 - classification_loss: 0.4662\n",
      "112/500 [=====>........................] - ETA: 6:43 - loss: 1.8495 - regression_loss: 1.3822 - classification_loss: 0.4673\n",
      "113/500 [=====>........................] - ETA: 6:42 - loss: 1.8527 - regression_loss: 1.3834 - classification_loss: 0.4693\n",
      "114/500 [=====>........................] - ETA: 6:41 - loss: 1.8568 - regression_loss: 1.3863 - classification_loss: 0.4705\n",
      "115/500 [=====>........................] - ETA: 6:40 - loss: 1.8510 - regression_loss: 1.3820 - classification_loss: 0.4690\n",
      "116/500 [=====>........................] - ETA: 6:38 - loss: 1.8467 - regression_loss: 1.3777 - classification_loss: 0.4689\n",
      "117/500 [======>.......................] - ETA: 6:38 - loss: 1.8548 - regression_loss: 1.3842 - classification_loss: 0.4706\n",
      "118/500 [======>.......................] - ETA: 6:37 - loss: 1.8581 - regression_loss: 1.3859 - classification_loss: 0.4722\n",
      "119/500 [======>.......................] - ETA: 6:35 - loss: 1.8545 - regression_loss: 1.3839 - classification_loss: 0.4706\n",
      "120/500 [======>.......................] - ETA: 6:34 - loss: 1.8515 - regression_loss: 1.3828 - classification_loss: 0.4687\n",
      "121/500 [======>.......................] - ETA: 6:33 - loss: 1.8565 - regression_loss: 1.3851 - classification_loss: 0.4714\n",
      "122/500 [======>.......................] - ETA: 6:32 - loss: 1.8578 - regression_loss: 1.3864 - classification_loss: 0.4714\n",
      "123/500 [======>.......................] - ETA: 6:31 - loss: 1.8587 - regression_loss: 1.3870 - classification_loss: 0.4717\n",
      "124/500 [======>.......................] - ETA: 6:29 - loss: 1.8593 - regression_loss: 1.3873 - classification_loss: 0.4720\n",
      "125/500 [======>.......................] - ETA: 6:29 - loss: 1.8625 - regression_loss: 1.3898 - classification_loss: 0.4728\n",
      "126/500 [======>.......................] - ETA: 6:28 - loss: 1.8603 - regression_loss: 1.3884 - classification_loss: 0.4719\n",
      "127/500 [======>.......................] - ETA: 6:27 - loss: 1.8617 - regression_loss: 1.3908 - classification_loss: 0.4709\n",
      "128/500 [======>.......................] - ETA: 6:26 - loss: 1.8713 - regression_loss: 1.3969 - classification_loss: 0.4744\n",
      "129/500 [======>.......................] - ETA: 6:24 - loss: 1.8693 - regression_loss: 1.3954 - classification_loss: 0.4739\n",
      "130/500 [======>.......................] - ETA: 6:23 - loss: 1.8772 - regression_loss: 1.4022 - classification_loss: 0.4750\n",
      "131/500 [======>.......................] - ETA: 6:22 - loss: 1.8741 - regression_loss: 1.3998 - classification_loss: 0.4743\n",
      "132/500 [======>.......................] - ETA: 6:21 - loss: 1.8850 - regression_loss: 1.4066 - classification_loss: 0.4784\n",
      "133/500 [======>.......................] - ETA: 6:21 - loss: 1.8912 - regression_loss: 1.4122 - classification_loss: 0.4790\n",
      "134/500 [=======>......................] - ETA: 6:19 - loss: 1.8864 - regression_loss: 1.4086 - classification_loss: 0.4778\n",
      "135/500 [=======>......................] - ETA: 6:18 - loss: 1.8899 - regression_loss: 1.4109 - classification_loss: 0.4790\n",
      "136/500 [=======>......................] - ETA: 6:18 - loss: 1.8914 - regression_loss: 1.4119 - classification_loss: 0.4795\n",
      "137/500 [=======>......................] - ETA: 6:17 - loss: 1.8916 - regression_loss: 1.4116 - classification_loss: 0.4801\n",
      "138/500 [=======>......................] - ETA: 6:15 - loss: 1.8961 - regression_loss: 1.4154 - classification_loss: 0.4807\n",
      "139/500 [=======>......................] - ETA: 6:14 - loss: 1.8970 - regression_loss: 1.4151 - classification_loss: 0.4819\n",
      "140/500 [=======>......................] - ETA: 6:13 - loss: 1.8896 - regression_loss: 1.4098 - classification_loss: 0.4798\n",
      "141/500 [=======>......................] - ETA: 6:12 - loss: 1.8928 - regression_loss: 1.4131 - classification_loss: 0.4797\n",
      "142/500 [=======>......................] - ETA: 6:11 - loss: 1.8928 - regression_loss: 1.4124 - classification_loss: 0.4804\n",
      "143/500 [=======>......................] - ETA: 6:10 - loss: 1.8893 - regression_loss: 1.4099 - classification_loss: 0.4794\n",
      "144/500 [=======>......................] - ETA: 6:09 - loss: 1.8932 - regression_loss: 1.4126 - classification_loss: 0.4806\n",
      "145/500 [=======>......................] - ETA: 6:08 - loss: 1.8900 - regression_loss: 1.4102 - classification_loss: 0.4798\n",
      "146/500 [=======>......................] - ETA: 6:07 - loss: 1.8903 - regression_loss: 1.4111 - classification_loss: 0.4792\n",
      "147/500 [=======>......................] - ETA: 6:06 - loss: 1.8892 - regression_loss: 1.4101 - classification_loss: 0.4791\n",
      "148/500 [=======>......................] - ETA: 6:04 - loss: 1.8924 - regression_loss: 1.4111 - classification_loss: 0.4813\n",
      "149/500 [=======>......................] - ETA: 6:03 - loss: 1.8920 - regression_loss: 1.4107 - classification_loss: 0.4814\n",
      "150/500 [========>.....................] - ETA: 6:02 - loss: 1.8938 - regression_loss: 1.4110 - classification_loss: 0.4828\n",
      "151/500 [========>.....................] - ETA: 6:02 - loss: 1.8961 - regression_loss: 1.4128 - classification_loss: 0.4833\n",
      "152/500 [========>.....................] - ETA: 6:00 - loss: 1.8939 - regression_loss: 1.4113 - classification_loss: 0.4826\n",
      "153/500 [========>.....................] - ETA: 5:59 - loss: 1.8919 - regression_loss: 1.4107 - classification_loss: 0.4812\n",
      "154/500 [========>.....................] - ETA: 5:58 - loss: 1.8902 - regression_loss: 1.4093 - classification_loss: 0.4810\n",
      "155/500 [========>.....................] - ETA: 5:57 - loss: 1.8946 - regression_loss: 1.4122 - classification_loss: 0.4824\n",
      "156/500 [========>.....................] - ETA: 5:56 - loss: 1.8946 - regression_loss: 1.4102 - classification_loss: 0.4843\n",
      "157/500 [========>.....................] - ETA: 5:55 - loss: 1.8899 - regression_loss: 1.4070 - classification_loss: 0.4829\n",
      "158/500 [========>.....................] - ETA: 5:54 - loss: 1.8905 - regression_loss: 1.4077 - classification_loss: 0.4828\n",
      "159/500 [========>.....................] - ETA: 5:53 - loss: 1.8896 - regression_loss: 1.4060 - classification_loss: 0.4836\n",
      "160/500 [========>.....................] - ETA: 5:52 - loss: 1.8928 - regression_loss: 1.4088 - classification_loss: 0.4840\n",
      "161/500 [========>.....................] - ETA: 5:50 - loss: 1.8955 - regression_loss: 1.4106 - classification_loss: 0.4849\n",
      "162/500 [========>.....................] - ETA: 5:50 - loss: 1.8939 - regression_loss: 1.4093 - classification_loss: 0.4846\n",
      "163/500 [========>.....................] - ETA: 5:49 - loss: 1.8914 - regression_loss: 1.4080 - classification_loss: 0.4834\n",
      "164/500 [========>.....................] - ETA: 5:48 - loss: 1.8962 - regression_loss: 1.4124 - classification_loss: 0.4838\n",
      "165/500 [========>.....................] - ETA: 5:47 - loss: 1.9039 - regression_loss: 1.4170 - classification_loss: 0.4869\n",
      "166/500 [========>.....................] - ETA: 5:46 - loss: 1.9056 - regression_loss: 1.4186 - classification_loss: 0.4870\n",
      "167/500 [=========>....................] - ETA: 5:44 - loss: 1.9104 - regression_loss: 1.4223 - classification_loss: 0.4880\n",
      "168/500 [=========>....................] - ETA: 5:43 - loss: 1.9079 - regression_loss: 1.4204 - classification_loss: 0.4875\n",
      "169/500 [=========>....................] - ETA: 5:42 - loss: 1.9083 - regression_loss: 1.4210 - classification_loss: 0.4873\n",
      "170/500 [=========>....................] - ETA: 5:41 - loss: 1.9126 - regression_loss: 1.4224 - classification_loss: 0.4902\n",
      "171/500 [=========>....................] - ETA: 5:40 - loss: 1.9179 - regression_loss: 1.4268 - classification_loss: 0.4911\n",
      "172/500 [=========>....................] - ETA: 5:39 - loss: 1.9194 - regression_loss: 1.4289 - classification_loss: 0.4905\n",
      "173/500 [=========>....................] - ETA: 5:38 - loss: 1.9223 - regression_loss: 1.4314 - classification_loss: 0.4909\n",
      "174/500 [=========>....................] - ETA: 5:37 - loss: 1.9186 - regression_loss: 1.4285 - classification_loss: 0.4901\n",
      "175/500 [=========>....................] - ETA: 5:36 - loss: 1.9127 - regression_loss: 1.4239 - classification_loss: 0.4888\n",
      "176/500 [=========>....................] - ETA: 5:35 - loss: 1.9128 - regression_loss: 1.4251 - classification_loss: 0.4877\n",
      "177/500 [=========>....................] - ETA: 5:34 - loss: 1.9085 - regression_loss: 1.4220 - classification_loss: 0.4865\n",
      "178/500 [=========>....................] - ETA: 5:33 - loss: 1.9065 - regression_loss: 1.4215 - classification_loss: 0.4850\n",
      "179/500 [=========>....................] - ETA: 5:32 - loss: 1.9024 - regression_loss: 1.4185 - classification_loss: 0.4838\n",
      "180/500 [=========>....................] - ETA: 5:31 - loss: 1.9073 - regression_loss: 1.4229 - classification_loss: 0.4844\n",
      "181/500 [=========>....................] - ETA: 5:30 - loss: 1.9056 - regression_loss: 1.4225 - classification_loss: 0.4831\n",
      "182/500 [=========>....................] - ETA: 5:29 - loss: 1.9031 - regression_loss: 1.4205 - classification_loss: 0.4826\n",
      "183/500 [=========>....................] - ETA: 5:27 - loss: 1.9063 - regression_loss: 1.4234 - classification_loss: 0.4829\n",
      "184/500 [==========>...................] - ETA: 5:26 - loss: 1.9056 - regression_loss: 1.4230 - classification_loss: 0.4827\n",
      "185/500 [==========>...................] - ETA: 5:25 - loss: 1.9058 - regression_loss: 1.4226 - classification_loss: 0.4832\n",
      "186/500 [==========>...................] - ETA: 5:24 - loss: 1.9019 - regression_loss: 1.4204 - classification_loss: 0.4816\n",
      "187/500 [==========>...................] - ETA: 5:23 - loss: 1.9062 - regression_loss: 1.4232 - classification_loss: 0.4829\n",
      "188/500 [==========>...................] - ETA: 5:22 - loss: 1.9115 - regression_loss: 1.4256 - classification_loss: 0.4859\n",
      "189/500 [==========>...................] - ETA: 5:21 - loss: 1.9067 - regression_loss: 1.4219 - classification_loss: 0.4848\n",
      "190/500 [==========>...................] - ETA: 5:20 - loss: 1.9052 - regression_loss: 1.4204 - classification_loss: 0.4849\n",
      "191/500 [==========>...................] - ETA: 5:19 - loss: 1.9046 - regression_loss: 1.4201 - classification_loss: 0.4844\n",
      "192/500 [==========>...................] - ETA: 5:18 - loss: 1.9030 - regression_loss: 1.4191 - classification_loss: 0.4839\n",
      "193/500 [==========>...................] - ETA: 5:17 - loss: 1.9013 - regression_loss: 1.4180 - classification_loss: 0.4834\n",
      "194/500 [==========>...................] - ETA: 5:16 - loss: 1.9076 - regression_loss: 1.4219 - classification_loss: 0.4857\n",
      "195/500 [==========>...................] - ETA: 5:15 - loss: 1.9081 - regression_loss: 1.4224 - classification_loss: 0.4857\n",
      "196/500 [==========>...................] - ETA: 5:14 - loss: 1.9054 - regression_loss: 1.4202 - classification_loss: 0.4852\n",
      "197/500 [==========>...................] - ETA: 5:13 - loss: 1.9058 - regression_loss: 1.4208 - classification_loss: 0.4850\n",
      "198/500 [==========>...................] - ETA: 5:12 - loss: 1.9040 - regression_loss: 1.4198 - classification_loss: 0.4842\n",
      "199/500 [==========>...................] - ETA: 5:11 - loss: 1.9033 - regression_loss: 1.4188 - classification_loss: 0.4845\n",
      "200/500 [===========>..................] - ETA: 5:10 - loss: 1.9069 - regression_loss: 1.4202 - classification_loss: 0.4866\n",
      "201/500 [===========>..................] - ETA: 5:09 - loss: 1.9048 - regression_loss: 1.4189 - classification_loss: 0.4859\n",
      "202/500 [===========>..................] - ETA: 5:08 - loss: 1.9025 - regression_loss: 1.4173 - classification_loss: 0.4851\n",
      "203/500 [===========>..................] - ETA: 5:07 - loss: 1.9023 - regression_loss: 1.4169 - classification_loss: 0.4854\n",
      "204/500 [===========>..................] - ETA: 5:06 - loss: 1.9025 - regression_loss: 1.4175 - classification_loss: 0.4850\n",
      "205/500 [===========>..................] - ETA: 5:05 - loss: 1.8992 - regression_loss: 1.4145 - classification_loss: 0.4846\n",
      "206/500 [===========>..................] - ETA: 5:04 - loss: 1.9008 - regression_loss: 1.4161 - classification_loss: 0.4847\n",
      "207/500 [===========>..................] - ETA: 5:03 - loss: 1.9019 - regression_loss: 1.4171 - classification_loss: 0.4848\n",
      "208/500 [===========>..................] - ETA: 5:02 - loss: 1.8990 - regression_loss: 1.4150 - classification_loss: 0.4840\n",
      "209/500 [===========>..................] - ETA: 5:01 - loss: 1.9008 - regression_loss: 1.4167 - classification_loss: 0.4841\n",
      "210/500 [===========>..................] - ETA: 5:00 - loss: 1.8977 - regression_loss: 1.4145 - classification_loss: 0.4832\n",
      "211/500 [===========>..................] - ETA: 4:59 - loss: 1.8948 - regression_loss: 1.4124 - classification_loss: 0.4823\n",
      "212/500 [===========>..................] - ETA: 4:58 - loss: 1.8940 - regression_loss: 1.4121 - classification_loss: 0.4818\n",
      "213/500 [===========>..................] - ETA: 4:57 - loss: 1.8939 - regression_loss: 1.4126 - classification_loss: 0.4813\n",
      "214/500 [===========>..................] - ETA: 4:56 - loss: 1.8952 - regression_loss: 1.4138 - classification_loss: 0.4815\n",
      "215/500 [===========>..................] - ETA: 4:55 - loss: 1.8924 - regression_loss: 1.4116 - classification_loss: 0.4807\n",
      "216/500 [===========>..................] - ETA: 4:54 - loss: 1.8922 - regression_loss: 1.4111 - classification_loss: 0.4811\n",
      "217/500 [============>.................] - ETA: 4:53 - loss: 1.8969 - regression_loss: 1.4145 - classification_loss: 0.4824\n",
      "218/500 [============>.................] - ETA: 4:52 - loss: 1.8988 - regression_loss: 1.4156 - classification_loss: 0.4832\n",
      "219/500 [============>.................] - ETA: 4:51 - loss: 1.8973 - regression_loss: 1.4144 - classification_loss: 0.4828\n",
      "220/500 [============>.................] - ETA: 4:50 - loss: 1.8965 - regression_loss: 1.4136 - classification_loss: 0.4830\n",
      "221/500 [============>.................] - ETA: 4:49 - loss: 1.8977 - regression_loss: 1.4138 - classification_loss: 0.4839\n",
      "222/500 [============>.................] - ETA: 4:48 - loss: 1.8980 - regression_loss: 1.4146 - classification_loss: 0.4834\n",
      "223/500 [============>.................] - ETA: 4:47 - loss: 1.8992 - regression_loss: 1.4159 - classification_loss: 0.4833\n",
      "224/500 [============>.................] - ETA: 4:46 - loss: 1.8979 - regression_loss: 1.4147 - classification_loss: 0.4832\n",
      "225/500 [============>.................] - ETA: 4:45 - loss: 1.9009 - regression_loss: 1.4164 - classification_loss: 0.4845\n",
      "226/500 [============>.................] - ETA: 4:43 - loss: 1.9023 - regression_loss: 1.4175 - classification_loss: 0.4848\n",
      "227/500 [============>.................] - ETA: 4:42 - loss: 1.9031 - regression_loss: 1.4184 - classification_loss: 0.4847\n",
      "228/500 [============>.................] - ETA: 4:41 - loss: 1.9010 - regression_loss: 1.4172 - classification_loss: 0.4838\n",
      "229/500 [============>.................] - ETA: 4:40 - loss: 1.9025 - regression_loss: 1.4183 - classification_loss: 0.4841\n",
      "230/500 [============>.................] - ETA: 4:39 - loss: 1.8999 - regression_loss: 1.4163 - classification_loss: 0.4836\n",
      "231/500 [============>.................] - ETA: 4:38 - loss: 1.8990 - regression_loss: 1.4157 - classification_loss: 0.4833\n",
      "232/500 [============>.................] - ETA: 4:37 - loss: 1.8973 - regression_loss: 1.4146 - classification_loss: 0.4827\n",
      "233/500 [============>.................] - ETA: 4:36 - loss: 1.9004 - regression_loss: 1.4173 - classification_loss: 0.4831\n",
      "234/500 [=============>................] - ETA: 4:35 - loss: 1.9024 - regression_loss: 1.4198 - classification_loss: 0.4826\n",
      "235/500 [=============>................] - ETA: 4:34 - loss: 1.8988 - regression_loss: 1.4173 - classification_loss: 0.4815\n",
      "236/500 [=============>................] - ETA: 4:33 - loss: 1.9000 - regression_loss: 1.4186 - classification_loss: 0.4814\n",
      "237/500 [=============>................] - ETA: 4:32 - loss: 1.8990 - regression_loss: 1.4181 - classification_loss: 0.4808\n",
      "238/500 [=============>................] - ETA: 4:31 - loss: 1.9047 - regression_loss: 1.4224 - classification_loss: 0.4823\n",
      "239/500 [=============>................] - ETA: 4:30 - loss: 1.9030 - regression_loss: 1.4213 - classification_loss: 0.4818\n",
      "240/500 [=============>................] - ETA: 4:29 - loss: 1.9080 - regression_loss: 1.4247 - classification_loss: 0.4833\n",
      "241/500 [=============>................] - ETA: 4:28 - loss: 1.9084 - regression_loss: 1.4256 - classification_loss: 0.4828\n",
      "242/500 [=============>................] - ETA: 4:27 - loss: 1.9064 - regression_loss: 1.4239 - classification_loss: 0.4825\n",
      "243/500 [=============>................] - ETA: 4:26 - loss: 1.9071 - regression_loss: 1.4242 - classification_loss: 0.4829\n",
      "244/500 [=============>................] - ETA: 4:25 - loss: 1.9077 - regression_loss: 1.4241 - classification_loss: 0.4836\n",
      "245/500 [=============>................] - ETA: 4:24 - loss: 1.9090 - regression_loss: 1.4250 - classification_loss: 0.4841\n",
      "246/500 [=============>................] - ETA: 4:23 - loss: 1.9099 - regression_loss: 1.4260 - classification_loss: 0.4839\n",
      "247/500 [=============>................] - ETA: 4:21 - loss: 1.9057 - regression_loss: 1.4229 - classification_loss: 0.4828\n",
      "248/500 [=============>................] - ETA: 4:20 - loss: 1.9061 - regression_loss: 1.4233 - classification_loss: 0.4828\n",
      "249/500 [=============>................] - ETA: 4:19 - loss: 1.9047 - regression_loss: 1.4222 - classification_loss: 0.4826\n",
      "250/500 [==============>...............] - ETA: 4:18 - loss: 1.9035 - regression_loss: 1.4213 - classification_loss: 0.4821\n",
      "251/500 [==============>...............] - ETA: 4:17 - loss: 1.9023 - regression_loss: 1.4208 - classification_loss: 0.4815\n",
      "252/500 [==============>...............] - ETA: 4:16 - loss: 1.9060 - regression_loss: 1.4234 - classification_loss: 0.4826\n",
      "253/500 [==============>...............] - ETA: 4:15 - loss: 1.9068 - regression_loss: 1.4244 - classification_loss: 0.4824\n",
      "254/500 [==============>...............] - ETA: 4:14 - loss: 1.9042 - regression_loss: 1.4226 - classification_loss: 0.4816\n",
      "255/500 [==============>...............] - ETA: 4:13 - loss: 1.9048 - regression_loss: 1.4236 - classification_loss: 0.4812\n",
      "256/500 [==============>...............] - ETA: 4:12 - loss: 1.9064 - regression_loss: 1.4244 - classification_loss: 0.4821\n",
      "257/500 [==============>...............] - ETA: 4:11 - loss: 1.9048 - regression_loss: 1.4234 - classification_loss: 0.4814\n",
      "258/500 [==============>...............] - ETA: 4:10 - loss: 1.9071 - regression_loss: 1.4248 - classification_loss: 0.4822\n",
      "259/500 [==============>...............] - ETA: 4:09 - loss: 1.9066 - regression_loss: 1.4240 - classification_loss: 0.4826\n",
      "260/500 [==============>...............] - ETA: 4:08 - loss: 1.9048 - regression_loss: 1.4228 - classification_loss: 0.4820\n",
      "261/500 [==============>...............] - ETA: 4:07 - loss: 1.9064 - regression_loss: 1.4229 - classification_loss: 0.4835\n",
      "262/500 [==============>...............] - ETA: 4:06 - loss: 1.9079 - regression_loss: 1.4241 - classification_loss: 0.4838\n",
      "263/500 [==============>...............] - ETA: 4:05 - loss: 1.9080 - regression_loss: 1.4240 - classification_loss: 0.4840\n",
      "264/500 [==============>...............] - ETA: 4:04 - loss: 1.9094 - regression_loss: 1.4252 - classification_loss: 0.4842\n",
      "265/500 [==============>...............] - ETA: 4:03 - loss: 1.9094 - regression_loss: 1.4256 - classification_loss: 0.4837\n",
      "266/500 [==============>...............] - ETA: 4:02 - loss: 1.9070 - regression_loss: 1.4239 - classification_loss: 0.4831\n",
      "267/500 [===============>..............] - ETA: 4:01 - loss: 1.9092 - regression_loss: 1.4260 - classification_loss: 0.4832\n",
      "268/500 [===============>..............] - ETA: 4:00 - loss: 1.9111 - regression_loss: 1.4270 - classification_loss: 0.4841\n",
      "269/500 [===============>..............] - ETA: 3:59 - loss: 1.9086 - regression_loss: 1.4251 - classification_loss: 0.4835\n",
      "270/500 [===============>..............] - ETA: 3:58 - loss: 1.9060 - regression_loss: 1.4234 - classification_loss: 0.4827\n",
      "271/500 [===============>..............] - ETA: 3:57 - loss: 1.9058 - regression_loss: 1.4234 - classification_loss: 0.4823\n",
      "272/500 [===============>..............] - ETA: 3:56 - loss: 1.9070 - regression_loss: 1.4246 - classification_loss: 0.4824\n",
      "273/500 [===============>..............] - ETA: 3:55 - loss: 1.9047 - regression_loss: 1.4229 - classification_loss: 0.4818\n",
      "274/500 [===============>..............] - ETA: 3:54 - loss: 1.9066 - regression_loss: 1.4248 - classification_loss: 0.4818\n",
      "275/500 [===============>..............] - ETA: 3:53 - loss: 1.9069 - regression_loss: 1.4255 - classification_loss: 0.4814\n",
      "276/500 [===============>..............] - ETA: 3:52 - loss: 1.9061 - regression_loss: 1.4253 - classification_loss: 0.4808\n",
      "277/500 [===============>..............] - ETA: 3:51 - loss: 1.9048 - regression_loss: 1.4249 - classification_loss: 0.4799\n",
      "278/500 [===============>..............] - ETA: 3:50 - loss: 1.9074 - regression_loss: 1.4270 - classification_loss: 0.4805\n",
      "279/500 [===============>..............] - ETA: 3:49 - loss: 1.9073 - regression_loss: 1.4268 - classification_loss: 0.4805\n",
      "280/500 [===============>..............] - ETA: 3:47 - loss: 1.9077 - regression_loss: 1.4276 - classification_loss: 0.4801\n",
      "281/500 [===============>..............] - ETA: 3:46 - loss: 1.9072 - regression_loss: 1.4271 - classification_loss: 0.4800\n",
      "282/500 [===============>..............] - ETA: 3:45 - loss: 1.9052 - regression_loss: 1.4260 - classification_loss: 0.4793\n",
      "283/500 [===============>..............] - ETA: 3:44 - loss: 1.9055 - regression_loss: 1.4265 - classification_loss: 0.4790\n",
      "284/500 [================>.............] - ETA: 3:43 - loss: 1.9060 - regression_loss: 1.4267 - classification_loss: 0.4793\n",
      "285/500 [================>.............] - ETA: 3:42 - loss: 1.9050 - regression_loss: 1.4261 - classification_loss: 0.4789\n",
      "286/500 [================>.............] - ETA: 3:41 - loss: 1.9050 - regression_loss: 1.4261 - classification_loss: 0.4789\n",
      "287/500 [================>.............] - ETA: 3:40 - loss: 1.9059 - regression_loss: 1.4274 - classification_loss: 0.4786\n",
      "288/500 [================>.............] - ETA: 3:39 - loss: 1.9052 - regression_loss: 1.4270 - classification_loss: 0.4781\n",
      "289/500 [================>.............] - ETA: 3:38 - loss: 1.9070 - regression_loss: 1.4285 - classification_loss: 0.4785\n",
      "290/500 [================>.............] - ETA: 3:37 - loss: 1.9074 - regression_loss: 1.4291 - classification_loss: 0.4783\n",
      "291/500 [================>.............] - ETA: 3:36 - loss: 1.9093 - regression_loss: 1.4302 - classification_loss: 0.4791\n",
      "292/500 [================>.............] - ETA: 3:35 - loss: 1.9076 - regression_loss: 1.4288 - classification_loss: 0.4789\n",
      "293/500 [================>.............] - ETA: 3:34 - loss: 1.9066 - regression_loss: 1.4275 - classification_loss: 0.4791\n",
      "294/500 [================>.............] - ETA: 3:33 - loss: 1.9060 - regression_loss: 1.4271 - classification_loss: 0.4788\n",
      "295/500 [================>.............] - ETA: 3:32 - loss: 1.9080 - regression_loss: 1.4292 - classification_loss: 0.4788\n",
      "296/500 [================>.............] - ETA: 3:31 - loss: 1.9068 - regression_loss: 1.4280 - classification_loss: 0.4788\n",
      "297/500 [================>.............] - ETA: 3:30 - loss: 1.9049 - regression_loss: 1.4265 - classification_loss: 0.4784\n",
      "298/500 [================>.............] - ETA: 3:29 - loss: 1.9014 - regression_loss: 1.4240 - classification_loss: 0.4774\n",
      "299/500 [================>.............] - ETA: 3:28 - loss: 1.9045 - regression_loss: 1.4254 - classification_loss: 0.4791\n",
      "300/500 [=================>............] - ETA: 3:27 - loss: 1.9037 - regression_loss: 1.4247 - classification_loss: 0.4789\n",
      "301/500 [=================>............] - ETA: 3:26 - loss: 1.9070 - regression_loss: 1.4264 - classification_loss: 0.4805\n",
      "302/500 [=================>............] - ETA: 3:25 - loss: 1.9091 - regression_loss: 1.4286 - classification_loss: 0.4806\n",
      "303/500 [=================>............] - ETA: 3:24 - loss: 1.9064 - regression_loss: 1.4263 - classification_loss: 0.4801\n",
      "304/500 [=================>............] - ETA: 3:23 - loss: 1.9104 - regression_loss: 1.4287 - classification_loss: 0.4816\n",
      "305/500 [=================>............] - ETA: 3:22 - loss: 1.9126 - regression_loss: 1.4300 - classification_loss: 0.4826\n",
      "306/500 [=================>............] - ETA: 3:21 - loss: 1.9161 - regression_loss: 1.4321 - classification_loss: 0.4840\n",
      "307/500 [=================>............] - ETA: 3:20 - loss: 1.9164 - regression_loss: 1.4324 - classification_loss: 0.4841\n",
      "308/500 [=================>............] - ETA: 3:19 - loss: 1.9179 - regression_loss: 1.4336 - classification_loss: 0.4843\n",
      "309/500 [=================>............] - ETA: 3:18 - loss: 1.9157 - regression_loss: 1.4319 - classification_loss: 0.4838\n",
      "310/500 [=================>............] - ETA: 3:17 - loss: 1.9149 - regression_loss: 1.4310 - classification_loss: 0.4839\n",
      "311/500 [=================>............] - ETA: 3:16 - loss: 1.9141 - regression_loss: 1.4307 - classification_loss: 0.4834\n",
      "312/500 [=================>............] - ETA: 3:15 - loss: 1.9144 - regression_loss: 1.4310 - classification_loss: 0.4834\n",
      "313/500 [=================>............] - ETA: 3:14 - loss: 1.9118 - regression_loss: 1.4290 - classification_loss: 0.4828\n",
      "314/500 [=================>............] - ETA: 3:13 - loss: 1.9114 - regression_loss: 1.4290 - classification_loss: 0.4824\n",
      "315/500 [=================>............] - ETA: 3:12 - loss: 1.9099 - regression_loss: 1.4282 - classification_loss: 0.4817\n",
      "316/500 [=================>............] - ETA: 3:11 - loss: 1.9128 - regression_loss: 1.4302 - classification_loss: 0.4826\n",
      "317/500 [==================>...........] - ETA: 3:10 - loss: 1.9104 - regression_loss: 1.4284 - classification_loss: 0.4820\n",
      "318/500 [==================>...........] - ETA: 3:09 - loss: 1.9111 - regression_loss: 1.4289 - classification_loss: 0.4821\n",
      "319/500 [==================>...........] - ETA: 3:08 - loss: 1.9105 - regression_loss: 1.4287 - classification_loss: 0.4818\n",
      "320/500 [==================>...........] - ETA: 3:06 - loss: 1.9115 - regression_loss: 1.4293 - classification_loss: 0.4822\n",
      "321/500 [==================>...........] - ETA: 3:05 - loss: 1.9110 - regression_loss: 1.4292 - classification_loss: 0.4818\n",
      "322/500 [==================>...........] - ETA: 3:04 - loss: 1.9106 - regression_loss: 1.4291 - classification_loss: 0.4815\n",
      "323/500 [==================>...........] - ETA: 3:03 - loss: 1.9131 - regression_loss: 1.4308 - classification_loss: 0.4823\n",
      "324/500 [==================>...........] - ETA: 3:02 - loss: 1.9126 - regression_loss: 1.4306 - classification_loss: 0.4820\n",
      "325/500 [==================>...........] - ETA: 3:01 - loss: 1.9106 - regression_loss: 1.4289 - classification_loss: 0.4817\n",
      "326/500 [==================>...........] - ETA: 3:00 - loss: 1.9099 - regression_loss: 1.4285 - classification_loss: 0.4815\n",
      "327/500 [==================>...........] - ETA: 2:59 - loss: 1.9105 - regression_loss: 1.4290 - classification_loss: 0.4815\n",
      "328/500 [==================>...........] - ETA: 2:58 - loss: 1.9104 - regression_loss: 1.4291 - classification_loss: 0.4814\n",
      "329/500 [==================>...........] - ETA: 2:57 - loss: 1.9096 - regression_loss: 1.4285 - classification_loss: 0.4810\n",
      "330/500 [==================>...........] - ETA: 2:56 - loss: 1.9087 - regression_loss: 1.4284 - classification_loss: 0.4803\n",
      "331/500 [==================>...........] - ETA: 2:55 - loss: 1.9080 - regression_loss: 1.4280 - classification_loss: 0.4800\n",
      "332/500 [==================>...........] - ETA: 2:54 - loss: 1.9069 - regression_loss: 1.4271 - classification_loss: 0.4798\n",
      "333/500 [==================>...........] - ETA: 2:53 - loss: 1.9099 - regression_loss: 1.4292 - classification_loss: 0.4807\n",
      "334/500 [===================>..........] - ETA: 2:52 - loss: 1.9081 - regression_loss: 1.4280 - classification_loss: 0.4801\n",
      "335/500 [===================>..........] - ETA: 2:51 - loss: 1.9067 - regression_loss: 1.4268 - classification_loss: 0.4799\n",
      "336/500 [===================>..........] - ETA: 2:50 - loss: 1.9065 - regression_loss: 1.4265 - classification_loss: 0.4800\n",
      "337/500 [===================>..........] - ETA: 2:49 - loss: 1.9072 - regression_loss: 1.4274 - classification_loss: 0.4798\n",
      "338/500 [===================>..........] - ETA: 2:48 - loss: 1.9059 - regression_loss: 1.4263 - classification_loss: 0.4796\n",
      "339/500 [===================>..........] - ETA: 2:47 - loss: 1.9079 - regression_loss: 1.4270 - classification_loss: 0.4809\n",
      "340/500 [===================>..........] - ETA: 2:46 - loss: 1.9098 - regression_loss: 1.4281 - classification_loss: 0.4818\n",
      "341/500 [===================>..........] - ETA: 2:45 - loss: 1.9090 - regression_loss: 1.4277 - classification_loss: 0.4813\n",
      "342/500 [===================>..........] - ETA: 2:43 - loss: 1.9082 - regression_loss: 1.4271 - classification_loss: 0.4812\n",
      "343/500 [===================>..........] - ETA: 2:42 - loss: 1.9073 - regression_loss: 1.4265 - classification_loss: 0.4809\n",
      "344/500 [===================>..........] - ETA: 2:41 - loss: 1.9073 - regression_loss: 1.4261 - classification_loss: 0.4812\n",
      "345/500 [===================>..........] - ETA: 2:40 - loss: 1.9051 - regression_loss: 1.4240 - classification_loss: 0.4811\n",
      "346/500 [===================>..........] - ETA: 2:39 - loss: 1.9043 - regression_loss: 1.4238 - classification_loss: 0.4804\n",
      "347/500 [===================>..........] - ETA: 2:38 - loss: 1.9029 - regression_loss: 1.4231 - classification_loss: 0.4798\n",
      "348/500 [===================>..........] - ETA: 2:37 - loss: 1.9039 - regression_loss: 1.4237 - classification_loss: 0.4802\n",
      "349/500 [===================>..........] - ETA: 2:36 - loss: 1.9045 - regression_loss: 1.4239 - classification_loss: 0.4805\n",
      "350/500 [====================>.........] - ETA: 2:35 - loss: 1.9039 - regression_loss: 1.4237 - classification_loss: 0.4803\n",
      "351/500 [====================>.........] - ETA: 2:34 - loss: 1.9061 - regression_loss: 1.4249 - classification_loss: 0.4812\n",
      "352/500 [====================>.........] - ETA: 2:33 - loss: 1.9047 - regression_loss: 1.4238 - classification_loss: 0.4810\n",
      "353/500 [====================>.........] - ETA: 2:32 - loss: 1.9043 - regression_loss: 1.4231 - classification_loss: 0.4811\n",
      "354/500 [====================>.........] - ETA: 2:31 - loss: 1.9053 - regression_loss: 1.4238 - classification_loss: 0.4815\n",
      "355/500 [====================>.........] - ETA: 2:30 - loss: 1.9043 - regression_loss: 1.4232 - classification_loss: 0.4811\n",
      "356/500 [====================>.........] - ETA: 2:29 - loss: 1.9026 - regression_loss: 1.4220 - classification_loss: 0.4806\n",
      "357/500 [====================>.........] - ETA: 2:28 - loss: 1.9015 - regression_loss: 1.4215 - classification_loss: 0.4800\n",
      "358/500 [====================>.........] - ETA: 2:27 - loss: 1.9026 - regression_loss: 1.4224 - classification_loss: 0.4802\n",
      "359/500 [====================>.........] - ETA: 2:26 - loss: 1.9045 - regression_loss: 1.4241 - classification_loss: 0.4805\n",
      "360/500 [====================>.........] - ETA: 2:25 - loss: 1.9053 - regression_loss: 1.4252 - classification_loss: 0.4801\n",
      "361/500 [====================>.........] - ETA: 2:24 - loss: 1.9049 - regression_loss: 1.4248 - classification_loss: 0.4801\n",
      "362/500 [====================>.........] - ETA: 2:23 - loss: 1.9034 - regression_loss: 1.4237 - classification_loss: 0.4797\n",
      "363/500 [====================>.........] - ETA: 2:22 - loss: 1.9047 - regression_loss: 1.4249 - classification_loss: 0.4798\n",
      "364/500 [====================>.........] - ETA: 2:20 - loss: 1.9068 - regression_loss: 1.4260 - classification_loss: 0.4808\n",
      "365/500 [====================>.........] - ETA: 2:19 - loss: 1.9068 - regression_loss: 1.4259 - classification_loss: 0.4809\n",
      "366/500 [====================>.........] - ETA: 2:18 - loss: 1.9079 - regression_loss: 1.4261 - classification_loss: 0.4818\n",
      "367/500 [=====================>........] - ETA: 2:17 - loss: 1.9075 - regression_loss: 1.4259 - classification_loss: 0.4815\n",
      "368/500 [=====================>........] - ETA: 2:16 - loss: 1.9092 - regression_loss: 1.4277 - classification_loss: 0.4816\n",
      "369/500 [=====================>........] - ETA: 2:15 - loss: 1.9096 - regression_loss: 1.4283 - classification_loss: 0.4813\n",
      "370/500 [=====================>........] - ETA: 2:14 - loss: 1.9101 - regression_loss: 1.4291 - classification_loss: 0.4810\n",
      "371/500 [=====================>........] - ETA: 2:13 - loss: 1.9099 - regression_loss: 1.4287 - classification_loss: 0.4812\n",
      "372/500 [=====================>........] - ETA: 2:12 - loss: 1.9094 - regression_loss: 1.4283 - classification_loss: 0.4812\n",
      "373/500 [=====================>........] - ETA: 2:11 - loss: 1.9134 - regression_loss: 1.4307 - classification_loss: 0.4827\n",
      "374/500 [=====================>........] - ETA: 2:10 - loss: 1.9112 - regression_loss: 1.4290 - classification_loss: 0.4822\n",
      "375/500 [=====================>........] - ETA: 2:09 - loss: 1.9113 - regression_loss: 1.4290 - classification_loss: 0.4823\n",
      "376/500 [=====================>........] - ETA: 2:08 - loss: 1.9093 - regression_loss: 1.4277 - classification_loss: 0.4817\n",
      "377/500 [=====================>........] - ETA: 2:07 - loss: 1.9104 - regression_loss: 1.4283 - classification_loss: 0.4820\n",
      "378/500 [=====================>........] - ETA: 2:06 - loss: 1.9097 - regression_loss: 1.4276 - classification_loss: 0.4821\n",
      "379/500 [=====================>........] - ETA: 2:05 - loss: 1.9090 - regression_loss: 1.4274 - classification_loss: 0.4816\n",
      "380/500 [=====================>........] - ETA: 2:04 - loss: 1.9114 - regression_loss: 1.4294 - classification_loss: 0.4820\n",
      "381/500 [=====================>........] - ETA: 2:03 - loss: 1.9116 - regression_loss: 1.4295 - classification_loss: 0.4821\n",
      "382/500 [=====================>........] - ETA: 2:02 - loss: 1.9093 - regression_loss: 1.4278 - classification_loss: 0.4815\n",
      "383/500 [=====================>........] - ETA: 2:01 - loss: 1.9077 - regression_loss: 1.4264 - classification_loss: 0.4814\n",
      "384/500 [======================>.......] - ETA: 2:00 - loss: 1.9080 - regression_loss: 1.4259 - classification_loss: 0.4821\n",
      "385/500 [======================>.......] - ETA: 1:59 - loss: 1.9074 - regression_loss: 1.4251 - classification_loss: 0.4823\n",
      "386/500 [======================>.......] - ETA: 1:58 - loss: 1.9085 - regression_loss: 1.4262 - classification_loss: 0.4823\n",
      "387/500 [======================>.......] - ETA: 1:57 - loss: 1.9078 - regression_loss: 1.4257 - classification_loss: 0.4821\n",
      "388/500 [======================>.......] - ETA: 1:56 - loss: 1.9062 - regression_loss: 1.4243 - classification_loss: 0.4819\n",
      "389/500 [======================>.......] - ETA: 1:55 - loss: 1.9057 - regression_loss: 1.4243 - classification_loss: 0.4815\n",
      "390/500 [======================>.......] - ETA: 1:54 - loss: 1.9068 - regression_loss: 1.4249 - classification_loss: 0.4819\n",
      "391/500 [======================>.......] - ETA: 1:52 - loss: 1.9074 - regression_loss: 1.4255 - classification_loss: 0.4819\n",
      "392/500 [======================>.......] - ETA: 1:51 - loss: 1.9059 - regression_loss: 1.4244 - classification_loss: 0.4815\n",
      "393/500 [======================>.......] - ETA: 1:50 - loss: 1.9076 - regression_loss: 1.4251 - classification_loss: 0.4825\n",
      "394/500 [======================>.......] - ETA: 1:49 - loss: 1.9093 - regression_loss: 1.4258 - classification_loss: 0.4835\n",
      "395/500 [======================>.......] - ETA: 1:48 - loss: 1.9080 - regression_loss: 1.4248 - classification_loss: 0.4831\n",
      "396/500 [======================>.......] - ETA: 1:47 - loss: 1.9084 - regression_loss: 1.4249 - classification_loss: 0.4835\n",
      "397/500 [======================>.......] - ETA: 1:46 - loss: 1.9061 - regression_loss: 1.4231 - classification_loss: 0.4830\n",
      "398/500 [======================>.......] - ETA: 1:45 - loss: 1.9076 - regression_loss: 1.4243 - classification_loss: 0.4833\n",
      "399/500 [======================>.......] - ETA: 1:44 - loss: 1.9096 - regression_loss: 1.4254 - classification_loss: 0.4842\n",
      "400/500 [=======================>......] - ETA: 1:43 - loss: 1.9105 - regression_loss: 1.4263 - classification_loss: 0.4842\n",
      "401/500 [=======================>......] - ETA: 1:42 - loss: 1.9104 - regression_loss: 1.4264 - classification_loss: 0.4840\n",
      "402/500 [=======================>......] - ETA: 1:41 - loss: 1.9096 - regression_loss: 1.4258 - classification_loss: 0.4838\n",
      "403/500 [=======================>......] - ETA: 1:40 - loss: 1.9095 - regression_loss: 1.4258 - classification_loss: 0.4836\n",
      "404/500 [=======================>......] - ETA: 1:39 - loss: 1.9090 - regression_loss: 1.4255 - classification_loss: 0.4835\n",
      "405/500 [=======================>......] - ETA: 1:38 - loss: 1.9089 - regression_loss: 1.4255 - classification_loss: 0.4834\n",
      "406/500 [=======================>......] - ETA: 1:37 - loss: 1.9109 - regression_loss: 1.4268 - classification_loss: 0.4842\n",
      "407/500 [=======================>......] - ETA: 1:36 - loss: 1.9117 - regression_loss: 1.4275 - classification_loss: 0.4842\n",
      "408/500 [=======================>......] - ETA: 1:35 - loss: 1.9118 - regression_loss: 1.4276 - classification_loss: 0.4841\n",
      "409/500 [=======================>......] - ETA: 1:34 - loss: 1.9106 - regression_loss: 1.4269 - classification_loss: 0.4837\n",
      "410/500 [=======================>......] - ETA: 1:33 - loss: 1.9099 - regression_loss: 1.4265 - classification_loss: 0.4834\n",
      "411/500 [=======================>......] - ETA: 1:32 - loss: 1.9097 - regression_loss: 1.4260 - classification_loss: 0.4837\n",
      "412/500 [=======================>......] - ETA: 1:31 - loss: 1.9102 - regression_loss: 1.4266 - classification_loss: 0.4836\n",
      "413/500 [=======================>......] - ETA: 1:30 - loss: 1.9094 - regression_loss: 1.4261 - classification_loss: 0.4833\n",
      "414/500 [=======================>......] - ETA: 1:29 - loss: 1.9079 - regression_loss: 1.4253 - classification_loss: 0.4827\n",
      "415/500 [=======================>......] - ETA: 1:28 - loss: 1.9085 - regression_loss: 1.4256 - classification_loss: 0.4829\n",
      "416/500 [=======================>......] - ETA: 1:26 - loss: 1.9096 - regression_loss: 1.4261 - classification_loss: 0.4835\n",
      "417/500 [========================>.....] - ETA: 1:25 - loss: 1.9082 - regression_loss: 1.4247 - classification_loss: 0.4835\n",
      "418/500 [========================>.....] - ETA: 1:24 - loss: 1.9098 - regression_loss: 1.4259 - classification_loss: 0.4839\n",
      "419/500 [========================>.....] - ETA: 1:23 - loss: 1.9078 - regression_loss: 1.4246 - classification_loss: 0.4832\n",
      "420/500 [========================>.....] - ETA: 1:22 - loss: 1.9075 - regression_loss: 1.4245 - classification_loss: 0.4830\n",
      "421/500 [========================>.....] - ETA: 1:21 - loss: 1.9089 - regression_loss: 1.4250 - classification_loss: 0.4839\n",
      "422/500 [========================>.....] - ETA: 1:20 - loss: 1.9077 - regression_loss: 1.4241 - classification_loss: 0.4835\n",
      "423/500 [========================>.....] - ETA: 1:19 - loss: 1.9069 - regression_loss: 1.4235 - classification_loss: 0.4834\n",
      "424/500 [========================>.....] - ETA: 1:18 - loss: 1.9052 - regression_loss: 1.4222 - classification_loss: 0.4830\n",
      "425/500 [========================>.....] - ETA: 1:17 - loss: 1.9058 - regression_loss: 1.4224 - classification_loss: 0.4834\n",
      "426/500 [========================>.....] - ETA: 1:16 - loss: 1.9059 - regression_loss: 1.4231 - classification_loss: 0.4829\n",
      "427/500 [========================>.....] - ETA: 1:15 - loss: 1.9053 - regression_loss: 1.4227 - classification_loss: 0.4826\n",
      "428/500 [========================>.....] - ETA: 1:14 - loss: 1.9046 - regression_loss: 1.4220 - classification_loss: 0.4826\n",
      "429/500 [========================>.....] - ETA: 1:13 - loss: 1.9037 - regression_loss: 1.4212 - classification_loss: 0.4824\n",
      "430/500 [========================>.....] - ETA: 1:12 - loss: 1.9019 - regression_loss: 1.4200 - classification_loss: 0.4819\n",
      "431/500 [========================>.....] - ETA: 1:11 - loss: 1.9022 - regression_loss: 1.4204 - classification_loss: 0.4818\n",
      "432/500 [========================>.....] - ETA: 1:10 - loss: 1.9046 - regression_loss: 1.4215 - classification_loss: 0.4831\n",
      "433/500 [========================>.....] - ETA: 1:09 - loss: 1.9032 - regression_loss: 1.4206 - classification_loss: 0.4826\n",
      "434/500 [=========================>....] - ETA: 1:08 - loss: 1.9029 - regression_loss: 1.4205 - classification_loss: 0.4824\n",
      "435/500 [=========================>....] - ETA: 1:07 - loss: 1.9031 - regression_loss: 1.4206 - classification_loss: 0.4825\n",
      "436/500 [=========================>....] - ETA: 1:06 - loss: 1.9038 - regression_loss: 1.4207 - classification_loss: 0.4831\n",
      "437/500 [=========================>....] - ETA: 1:05 - loss: 1.9047 - regression_loss: 1.4213 - classification_loss: 0.4834\n",
      "438/500 [=========================>....] - ETA: 1:04 - loss: 1.9054 - regression_loss: 1.4213 - classification_loss: 0.4841\n",
      "439/500 [=========================>....] - ETA: 1:03 - loss: 1.9060 - regression_loss: 1.4219 - classification_loss: 0.4841\n",
      "440/500 [=========================>....] - ETA: 1:02 - loss: 1.9051 - regression_loss: 1.4213 - classification_loss: 0.4839\n",
      "441/500 [=========================>....] - ETA: 1:01 - loss: 1.9049 - regression_loss: 1.4212 - classification_loss: 0.4836\n",
      "442/500 [=========================>....] - ETA: 1:00 - loss: 1.9054 - regression_loss: 1.4216 - classification_loss: 0.4838\n",
      "443/500 [=========================>....] - ETA: 59s - loss: 1.9054 - regression_loss: 1.4220 - classification_loss: 0.4835 \n",
      "444/500 [=========================>....] - ETA: 57s - loss: 1.9059 - regression_loss: 1.4218 - classification_loss: 0.4840\n",
      "445/500 [=========================>....] - ETA: 56s - loss: 1.9050 - regression_loss: 1.4215 - classification_loss: 0.4835\n",
      "446/500 [=========================>....] - ETA: 55s - loss: 1.9042 - regression_loss: 1.4208 - classification_loss: 0.4834\n",
      "447/500 [=========================>....] - ETA: 54s - loss: 1.9032 - regression_loss: 1.4201 - classification_loss: 0.4831\n",
      "448/500 [=========================>....] - ETA: 53s - loss: 1.9034 - regression_loss: 1.4202 - classification_loss: 0.4831\n",
      "449/500 [=========================>....] - ETA: 52s - loss: 1.9028 - regression_loss: 1.4195 - classification_loss: 0.4832\n",
      "450/500 [==========================>...] - ETA: 51s - loss: 1.9044 - regression_loss: 1.4207 - classification_loss: 0.4838\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 1.9053 - regression_loss: 1.4210 - classification_loss: 0.4843\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 1.9070 - regression_loss: 1.4222 - classification_loss: 0.4848\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.9093 - regression_loss: 1.4240 - classification_loss: 0.4853\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.9092 - regression_loss: 1.4238 - classification_loss: 0.4853\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.9106 - regression_loss: 1.4242 - classification_loss: 0.4864\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.9109 - regression_loss: 1.4245 - classification_loss: 0.4864\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.9096 - regression_loss: 1.4234 - classification_loss: 0.4861\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.9082 - regression_loss: 1.4224 - classification_loss: 0.4858\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.9087 - regression_loss: 1.4226 - classification_loss: 0.4861\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.9085 - regression_loss: 1.4225 - classification_loss: 0.4860\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 1.9098 - regression_loss: 1.4233 - classification_loss: 0.4864\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 1.9102 - regression_loss: 1.4236 - classification_loss: 0.4866\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 1.9102 - regression_loss: 1.4234 - classification_loss: 0.4868\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 1.9091 - regression_loss: 1.4224 - classification_loss: 0.4867\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 1.9110 - regression_loss: 1.4240 - classification_loss: 0.4871\n",
      "466/500 [==========================>...] - ETA: 35s - loss: 1.9099 - regression_loss: 1.4231 - classification_loss: 0.4868\n",
      "467/500 [===========================>..] - ETA: 34s - loss: 1.9088 - regression_loss: 1.4221 - classification_loss: 0.4867\n",
      "468/500 [===========================>..] - ETA: 33s - loss: 1.9072 - regression_loss: 1.4209 - classification_loss: 0.4863\n",
      "469/500 [===========================>..] - ETA: 32s - loss: 1.9064 - regression_loss: 1.4202 - classification_loss: 0.4862\n",
      "470/500 [===========================>..] - ETA: 31s - loss: 1.9051 - regression_loss: 1.4191 - classification_loss: 0.4860\n",
      "471/500 [===========================>..] - ETA: 30s - loss: 1.9042 - regression_loss: 1.4183 - classification_loss: 0.4859\n",
      "472/500 [===========================>..] - ETA: 29s - loss: 1.9054 - regression_loss: 1.4190 - classification_loss: 0.4863\n",
      "473/500 [===========================>..] - ETA: 28s - loss: 1.9064 - regression_loss: 1.4203 - classification_loss: 0.4861\n",
      "474/500 [===========================>..] - ETA: 27s - loss: 1.9086 - regression_loss: 1.4221 - classification_loss: 0.4864\n",
      "475/500 [===========================>..] - ETA: 26s - loss: 1.9079 - regression_loss: 1.4214 - classification_loss: 0.4865\n",
      "476/500 [===========================>..] - ETA: 25s - loss: 1.9073 - regression_loss: 1.4212 - classification_loss: 0.4861\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.9071 - regression_loss: 1.4213 - classification_loss: 0.4858\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.9078 - regression_loss: 1.4222 - classification_loss: 0.4855\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.9087 - regression_loss: 1.4231 - classification_loss: 0.4856\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.9076 - regression_loss: 1.4223 - classification_loss: 0.4853\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.9076 - regression_loss: 1.4223 - classification_loss: 0.4853\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.9079 - regression_loss: 1.4226 - classification_loss: 0.4853\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.9071 - regression_loss: 1.4223 - classification_loss: 0.4849\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.9066 - regression_loss: 1.4217 - classification_loss: 0.4848\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.9070 - regression_loss: 1.4222 - classification_loss: 0.4848\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.9060 - regression_loss: 1.4217 - classification_loss: 0.4844\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.9050 - regression_loss: 1.4210 - classification_loss: 0.4840\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.9050 - regression_loss: 1.4210 - classification_loss: 0.4840\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.9062 - regression_loss: 1.4221 - classification_loss: 0.4841\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.9046 - regression_loss: 1.4210 - classification_loss: 0.4836\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.9064 - regression_loss: 1.4223 - classification_loss: 0.4841 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.9083 - regression_loss: 1.4237 - classification_loss: 0.4847\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.9095 - regression_loss: 1.4248 - classification_loss: 0.4847\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.9104 - regression_loss: 1.4254 - classification_loss: 0.4850\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.9095 - regression_loss: 1.4248 - classification_loss: 0.4847\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.9109 - regression_loss: 1.4258 - classification_loss: 0.4851\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.9107 - regression_loss: 1.4258 - classification_loss: 0.4849\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.9118 - regression_loss: 1.4263 - classification_loss: 0.4855\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.9123 - regression_loss: 1.4267 - classification_loss: 0.4856\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9120 - regression_loss: 1.4268 - classification_loss: 0.4852\n",
      "Epoch 00043: saving model to ./snapshots\\resnet50_csv_43.h5\n",
      "\n",
      "500/500 [==============================] - 522s 1s/step - loss: 1.9120 - regression_loss: 1.4268 - classification_loss: 0.4852\n",
      "Epoch 44/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 2.0392 - regression_loss: 1.4581 - classification_loss: 0.5811\n",
      "  2/500 [..............................] - ETA: 4:04 - loss: 1.9479 - regression_loss: 1.4455 - classification_loss: 0.5024\n",
      "  3/500 [..............................] - ETA: 5:25 - loss: 1.8014 - regression_loss: 1.3688 - classification_loss: 0.4326\n",
      "  4/500 [..............................] - ETA: 6:17 - loss: 1.8804 - regression_loss: 1.3967 - classification_loss: 0.4837\n",
      "  5/500 [..............................] - ETA: 6:38 - loss: 1.9546 - regression_loss: 1.4564 - classification_loss: 0.4982\n",
      "  6/500 [..............................] - ETA: 6:52 - loss: 1.8598 - regression_loss: 1.3742 - classification_loss: 0.4856\n",
      "  7/500 [..............................] - ETA: 6:58 - loss: 1.8357 - regression_loss: 1.3608 - classification_loss: 0.4750\n",
      "  8/500 [..............................] - ETA: 7:09 - loss: 1.8003 - regression_loss: 1.3419 - classification_loss: 0.4585\n",
      "  9/500 [..............................] - ETA: 7:14 - loss: 1.7838 - regression_loss: 1.3333 - classification_loss: 0.4506\n",
      " 10/500 [..............................] - ETA: 7:19 - loss: 1.7770 - regression_loss: 1.3358 - classification_loss: 0.4412\n",
      " 11/500 [..............................] - ETA: 7:21 - loss: 1.7565 - regression_loss: 1.3325 - classification_loss: 0.4240\n",
      " 12/500 [..............................] - ETA: 7:28 - loss: 1.7314 - regression_loss: 1.3160 - classification_loss: 0.4155\n",
      " 13/500 [..............................] - ETA: 7:30 - loss: 1.7505 - regression_loss: 1.3331 - classification_loss: 0.4174\n",
      " 14/500 [..............................] - ETA: 7:29 - loss: 1.7694 - regression_loss: 1.3447 - classification_loss: 0.4247\n",
      " 15/500 [..............................] - ETA: 7:33 - loss: 1.7513 - regression_loss: 1.3335 - classification_loss: 0.4178\n",
      " 16/500 [..............................] - ETA: 7:33 - loss: 1.7378 - regression_loss: 1.3199 - classification_loss: 0.4179\n",
      " 17/500 [>.............................] - ETA: 7:37 - loss: 1.6797 - regression_loss: 1.2746 - classification_loss: 0.4050\n",
      " 18/500 [>.............................] - ETA: 7:37 - loss: 1.6423 - regression_loss: 1.2472 - classification_loss: 0.3951\n",
      " 19/500 [>.............................] - ETA: 7:37 - loss: 1.6653 - regression_loss: 1.2676 - classification_loss: 0.3977\n",
      " 20/500 [>.............................] - ETA: 7:39 - loss: 1.7196 - regression_loss: 1.3099 - classification_loss: 0.4097\n",
      " 21/500 [>.............................] - ETA: 7:37 - loss: 1.6856 - regression_loss: 1.2829 - classification_loss: 0.4027\n",
      " 22/500 [>.............................] - ETA: 7:38 - loss: 1.7024 - regression_loss: 1.2994 - classification_loss: 0.4031\n",
      " 23/500 [>.............................] - ETA: 7:38 - loss: 1.6958 - regression_loss: 1.2989 - classification_loss: 0.3970\n",
      " 24/500 [>.............................] - ETA: 7:39 - loss: 1.7051 - regression_loss: 1.3043 - classification_loss: 0.4008\n",
      " 25/500 [>.............................] - ETA: 7:40 - loss: 1.7301 - regression_loss: 1.3089 - classification_loss: 0.4212\n",
      " 26/500 [>.............................] - ETA: 7:40 - loss: 1.7114 - regression_loss: 1.2969 - classification_loss: 0.4145\n",
      " 27/500 [>.............................] - ETA: 7:41 - loss: 1.7023 - regression_loss: 1.2912 - classification_loss: 0.4111\n",
      " 28/500 [>.............................] - ETA: 7:42 - loss: 1.7286 - regression_loss: 1.3106 - classification_loss: 0.4180\n",
      " 29/500 [>.............................] - ETA: 7:43 - loss: 1.7444 - regression_loss: 1.3191 - classification_loss: 0.4253\n",
      " 30/500 [>.............................] - ETA: 7:44 - loss: 1.7474 - regression_loss: 1.3212 - classification_loss: 0.4263\n",
      " 31/500 [>.............................] - ETA: 7:42 - loss: 1.7664 - regression_loss: 1.3362 - classification_loss: 0.4303\n",
      " 32/500 [>.............................] - ETA: 7:43 - loss: 1.7565 - regression_loss: 1.3302 - classification_loss: 0.4263\n",
      " 33/500 [>.............................] - ETA: 7:43 - loss: 1.7802 - regression_loss: 1.3480 - classification_loss: 0.4321\n",
      " 34/500 [=>............................] - ETA: 7:44 - loss: 1.7807 - regression_loss: 1.3494 - classification_loss: 0.4313\n",
      " 35/500 [=>............................] - ETA: 7:43 - loss: 1.7825 - regression_loss: 1.3408 - classification_loss: 0.4417\n",
      " 36/500 [=>............................] - ETA: 7:42 - loss: 1.7688 - regression_loss: 1.3307 - classification_loss: 0.4382\n",
      " 37/500 [=>............................] - ETA: 7:42 - loss: 1.7627 - regression_loss: 1.3245 - classification_loss: 0.4382\n",
      " 38/500 [=>............................] - ETA: 7:42 - loss: 1.7775 - regression_loss: 1.3317 - classification_loss: 0.4458\n",
      " 39/500 [=>............................] - ETA: 7:41 - loss: 1.7933 - regression_loss: 1.3428 - classification_loss: 0.4505\n",
      " 40/500 [=>............................] - ETA: 7:41 - loss: 1.7974 - regression_loss: 1.3448 - classification_loss: 0.4526\n",
      " 41/500 [=>............................] - ETA: 7:39 - loss: 1.8123 - regression_loss: 1.3578 - classification_loss: 0.4546\n",
      " 42/500 [=>............................] - ETA: 7:38 - loss: 1.8108 - regression_loss: 1.3538 - classification_loss: 0.4570\n",
      " 43/500 [=>............................] - ETA: 7:38 - loss: 1.7975 - regression_loss: 1.3416 - classification_loss: 0.4559\n",
      " 44/500 [=>............................] - ETA: 7:38 - loss: 1.7987 - regression_loss: 1.3409 - classification_loss: 0.4579\n",
      " 45/500 [=>............................] - ETA: 7:38 - loss: 1.7879 - regression_loss: 1.3326 - classification_loss: 0.4553\n",
      " 46/500 [=>............................] - ETA: 7:37 - loss: 1.7808 - regression_loss: 1.3245 - classification_loss: 0.4563\n",
      " 47/500 [=>............................] - ETA: 7:37 - loss: 1.7739 - regression_loss: 1.3232 - classification_loss: 0.4507\n",
      " 48/500 [=>............................] - ETA: 7:37 - loss: 1.7648 - regression_loss: 1.3164 - classification_loss: 0.4484\n",
      " 49/500 [=>............................] - ETA: 7:37 - loss: 1.7575 - regression_loss: 1.3112 - classification_loss: 0.4463\n",
      " 50/500 [==>...........................] - ETA: 7:35 - loss: 1.7566 - regression_loss: 1.3121 - classification_loss: 0.4445\n",
      " 51/500 [==>...........................] - ETA: 7:35 - loss: 1.7636 - regression_loss: 1.3135 - classification_loss: 0.4501\n",
      " 52/500 [==>...........................] - ETA: 7:35 - loss: 1.7849 - regression_loss: 1.3285 - classification_loss: 0.4564\n",
      " 53/500 [==>...........................] - ETA: 7:34 - loss: 1.7979 - regression_loss: 1.3360 - classification_loss: 0.4618\n",
      " 54/500 [==>...........................] - ETA: 7:32 - loss: 1.7862 - regression_loss: 1.3289 - classification_loss: 0.4573\n",
      " 55/500 [==>...........................] - ETA: 7:32 - loss: 1.7814 - regression_loss: 1.3247 - classification_loss: 0.4567\n",
      " 56/500 [==>...........................] - ETA: 7:31 - loss: 1.7802 - regression_loss: 1.3258 - classification_loss: 0.4544\n",
      " 57/500 [==>...........................] - ETA: 7:30 - loss: 1.7977 - regression_loss: 1.3364 - classification_loss: 0.4614\n",
      " 58/500 [==>...........................] - ETA: 7:29 - loss: 1.7922 - regression_loss: 1.3341 - classification_loss: 0.4582\n",
      " 59/500 [==>...........................] - ETA: 7:26 - loss: 1.7956 - regression_loss: 1.3373 - classification_loss: 0.4583\n",
      " 60/500 [==>...........................] - ETA: 7:27 - loss: 1.8073 - regression_loss: 1.3460 - classification_loss: 0.4613\n",
      " 61/500 [==>...........................] - ETA: 7:26 - loss: 1.8061 - regression_loss: 1.3449 - classification_loss: 0.4611\n",
      " 62/500 [==>...........................] - ETA: 7:25 - loss: 1.8070 - regression_loss: 1.3446 - classification_loss: 0.4624\n",
      " 63/500 [==>...........................] - ETA: 7:24 - loss: 1.8164 - regression_loss: 1.3488 - classification_loss: 0.4676\n",
      " 64/500 [==>...........................] - ETA: 7:23 - loss: 1.8257 - regression_loss: 1.3569 - classification_loss: 0.4689\n",
      " 65/500 [==>...........................] - ETA: 7:22 - loss: 1.8134 - regression_loss: 1.3488 - classification_loss: 0.4646\n",
      " 66/500 [==>...........................] - ETA: 7:21 - loss: 1.8024 - regression_loss: 1.3397 - classification_loss: 0.4627\n",
      " 67/500 [===>..........................] - ETA: 7:21 - loss: 1.7910 - regression_loss: 1.3315 - classification_loss: 0.4596\n",
      " 68/500 [===>..........................] - ETA: 7:20 - loss: 1.7996 - regression_loss: 1.3348 - classification_loss: 0.4648\n",
      " 69/500 [===>..........................] - ETA: 7:20 - loss: 1.7962 - regression_loss: 1.3334 - classification_loss: 0.4628\n",
      " 70/500 [===>..........................] - ETA: 7:19 - loss: 1.7926 - regression_loss: 1.3323 - classification_loss: 0.4604\n",
      " 71/500 [===>..........................] - ETA: 7:18 - loss: 1.7895 - regression_loss: 1.3322 - classification_loss: 0.4573\n",
      " 72/500 [===>..........................] - ETA: 7:17 - loss: 1.7899 - regression_loss: 1.3343 - classification_loss: 0.4556\n",
      " 73/500 [===>..........................] - ETA: 7:16 - loss: 1.7879 - regression_loss: 1.3329 - classification_loss: 0.4550\n",
      " 74/500 [===>..........................] - ETA: 7:15 - loss: 1.7866 - regression_loss: 1.3322 - classification_loss: 0.4544\n",
      " 75/500 [===>..........................] - ETA: 7:14 - loss: 1.7913 - regression_loss: 1.3373 - classification_loss: 0.4540\n",
      " 76/500 [===>..........................] - ETA: 7:13 - loss: 1.7971 - regression_loss: 1.3413 - classification_loss: 0.4558\n",
      " 77/500 [===>..........................] - ETA: 7:11 - loss: 1.8066 - regression_loss: 1.3472 - classification_loss: 0.4594\n",
      " 78/500 [===>..........................] - ETA: 7:10 - loss: 1.7997 - regression_loss: 1.3421 - classification_loss: 0.4575\n",
      " 79/500 [===>..........................] - ETA: 7:10 - loss: 1.7973 - regression_loss: 1.3426 - classification_loss: 0.4546\n",
      " 80/500 [===>..........................] - ETA: 7:08 - loss: 1.7914 - regression_loss: 1.3373 - classification_loss: 0.4541\n",
      " 81/500 [===>..........................] - ETA: 7:07 - loss: 1.7896 - regression_loss: 1.3381 - classification_loss: 0.4515\n",
      " 82/500 [===>..........................] - ETA: 7:07 - loss: 1.7961 - regression_loss: 1.3432 - classification_loss: 0.4529\n",
      " 83/500 [===>..........................] - ETA: 7:06 - loss: 1.8010 - regression_loss: 1.3483 - classification_loss: 0.4527\n",
      " 84/500 [====>.........................] - ETA: 7:05 - loss: 1.8072 - regression_loss: 1.3517 - classification_loss: 0.4555\n",
      " 85/500 [====>.........................] - ETA: 7:04 - loss: 1.8065 - regression_loss: 1.3514 - classification_loss: 0.4551\n",
      " 86/500 [====>.........................] - ETA: 7:03 - loss: 1.8143 - regression_loss: 1.3588 - classification_loss: 0.4556\n",
      " 87/500 [====>.........................] - ETA: 7:02 - loss: 1.8152 - regression_loss: 1.3609 - classification_loss: 0.4543\n",
      " 88/500 [====>.........................] - ETA: 7:01 - loss: 1.8124 - regression_loss: 1.3586 - classification_loss: 0.4538\n",
      " 89/500 [====>.........................] - ETA: 7:00 - loss: 1.8145 - regression_loss: 1.3601 - classification_loss: 0.4543\n",
      " 90/500 [====>.........................] - ETA: 6:59 - loss: 1.8090 - regression_loss: 1.3558 - classification_loss: 0.4532\n",
      " 91/500 [====>.........................] - ETA: 6:58 - loss: 1.8125 - regression_loss: 1.3580 - classification_loss: 0.4545\n",
      " 92/500 [====>.........................] - ETA: 6:57 - loss: 1.8053 - regression_loss: 1.3526 - classification_loss: 0.4527\n",
      " 93/500 [====>.........................] - ETA: 6:56 - loss: 1.8058 - regression_loss: 1.3524 - classification_loss: 0.4534\n",
      " 94/500 [====>.........................] - ETA: 6:55 - loss: 1.8075 - regression_loss: 1.3532 - classification_loss: 0.4543\n",
      " 95/500 [====>.........................] - ETA: 6:54 - loss: 1.8053 - regression_loss: 1.3517 - classification_loss: 0.4536\n",
      " 96/500 [====>.........................] - ETA: 6:53 - loss: 1.8041 - regression_loss: 1.3501 - classification_loss: 0.4540\n",
      " 97/500 [====>.........................] - ETA: 6:52 - loss: 1.8054 - regression_loss: 1.3524 - classification_loss: 0.4530\n",
      " 98/500 [====>.........................] - ETA: 6:51 - loss: 1.8107 - regression_loss: 1.3565 - classification_loss: 0.4542\n",
      " 99/500 [====>.........................] - ETA: 6:50 - loss: 1.8154 - regression_loss: 1.3593 - classification_loss: 0.4561\n",
      "100/500 [=====>........................] - ETA: 6:49 - loss: 1.8166 - regression_loss: 1.3612 - classification_loss: 0.4554\n",
      "101/500 [=====>........................] - ETA: 6:48 - loss: 1.8175 - regression_loss: 1.3614 - classification_loss: 0.4561\n",
      "102/500 [=====>........................] - ETA: 6:47 - loss: 1.8185 - regression_loss: 1.3624 - classification_loss: 0.4561\n",
      "103/500 [=====>........................] - ETA: 6:46 - loss: 1.8171 - regression_loss: 1.3619 - classification_loss: 0.4552\n",
      "104/500 [=====>........................] - ETA: 6:45 - loss: 1.8147 - regression_loss: 1.3596 - classification_loss: 0.4552\n",
      "105/500 [=====>........................] - ETA: 6:44 - loss: 1.8253 - regression_loss: 1.3685 - classification_loss: 0.4568\n",
      "106/500 [=====>........................] - ETA: 6:43 - loss: 1.8211 - regression_loss: 1.3649 - classification_loss: 0.4562\n",
      "107/500 [=====>........................] - ETA: 6:42 - loss: 1.8226 - regression_loss: 1.3645 - classification_loss: 0.4582\n",
      "108/500 [=====>........................] - ETA: 6:41 - loss: 1.8306 - regression_loss: 1.3688 - classification_loss: 0.4618\n",
      "109/500 [=====>........................] - ETA: 6:41 - loss: 1.8346 - regression_loss: 1.3717 - classification_loss: 0.4629\n",
      "110/500 [=====>........................] - ETA: 6:40 - loss: 1.8441 - regression_loss: 1.3781 - classification_loss: 0.4660\n",
      "111/500 [=====>........................] - ETA: 6:38 - loss: 1.8394 - regression_loss: 1.3752 - classification_loss: 0.4642\n",
      "112/500 [=====>........................] - ETA: 6:38 - loss: 1.8516 - regression_loss: 1.3828 - classification_loss: 0.4688\n",
      "113/500 [=====>........................] - ETA: 6:36 - loss: 1.8494 - regression_loss: 1.3805 - classification_loss: 0.4689\n",
      "114/500 [=====>........................] - ETA: 6:35 - loss: 1.8575 - regression_loss: 1.3836 - classification_loss: 0.4739\n",
      "115/500 [=====>........................] - ETA: 6:34 - loss: 1.8540 - regression_loss: 1.3808 - classification_loss: 0.4732\n",
      "116/500 [=====>........................] - ETA: 6:33 - loss: 1.8627 - regression_loss: 1.3873 - classification_loss: 0.4754\n",
      "117/500 [======>.......................] - ETA: 6:32 - loss: 1.8608 - regression_loss: 1.3864 - classification_loss: 0.4744\n",
      "118/500 [======>.......................] - ETA: 6:32 - loss: 1.8676 - regression_loss: 1.3922 - classification_loss: 0.4753\n",
      "119/500 [======>.......................] - ETA: 6:31 - loss: 1.8636 - regression_loss: 1.3891 - classification_loss: 0.4745\n",
      "120/500 [======>.......................] - ETA: 6:30 - loss: 1.8643 - regression_loss: 1.3899 - classification_loss: 0.4744\n",
      "121/500 [======>.......................] - ETA: 6:29 - loss: 1.8613 - regression_loss: 1.3880 - classification_loss: 0.4733\n",
      "122/500 [======>.......................] - ETA: 6:28 - loss: 1.8603 - regression_loss: 1.3874 - classification_loss: 0.4729\n",
      "123/500 [======>.......................] - ETA: 6:27 - loss: 1.8576 - regression_loss: 1.3853 - classification_loss: 0.4723\n",
      "124/500 [======>.......................] - ETA: 6:25 - loss: 1.8606 - regression_loss: 1.3868 - classification_loss: 0.4738\n",
      "125/500 [======>.......................] - ETA: 6:25 - loss: 1.8626 - regression_loss: 1.3889 - classification_loss: 0.4737\n",
      "126/500 [======>.......................] - ETA: 6:24 - loss: 1.8630 - regression_loss: 1.3882 - classification_loss: 0.4748\n",
      "127/500 [======>.......................] - ETA: 6:23 - loss: 1.8617 - regression_loss: 1.3877 - classification_loss: 0.4740\n",
      "128/500 [======>.......................] - ETA: 6:22 - loss: 1.8640 - regression_loss: 1.3891 - classification_loss: 0.4750\n",
      "129/500 [======>.......................] - ETA: 6:20 - loss: 1.8654 - regression_loss: 1.3905 - classification_loss: 0.4749\n",
      "130/500 [======>.......................] - ETA: 6:20 - loss: 1.8594 - regression_loss: 1.3863 - classification_loss: 0.4731\n",
      "131/500 [======>.......................] - ETA: 6:19 - loss: 1.8605 - regression_loss: 1.3865 - classification_loss: 0.4739\n",
      "132/500 [======>.......................] - ETA: 6:18 - loss: 1.8646 - regression_loss: 1.3888 - classification_loss: 0.4758\n",
      "133/500 [======>.......................] - ETA: 6:16 - loss: 1.8686 - regression_loss: 1.3932 - classification_loss: 0.4754\n",
      "134/500 [=======>......................] - ETA: 6:15 - loss: 1.8615 - regression_loss: 1.3874 - classification_loss: 0.4741\n",
      "135/500 [=======>......................] - ETA: 6:14 - loss: 1.8586 - regression_loss: 1.3853 - classification_loss: 0.4733\n",
      "136/500 [=======>......................] - ETA: 6:13 - loss: 1.8571 - regression_loss: 1.3845 - classification_loss: 0.4726\n",
      "137/500 [=======>......................] - ETA: 6:12 - loss: 1.8597 - regression_loss: 1.3864 - classification_loss: 0.4732\n",
      "138/500 [=======>......................] - ETA: 6:11 - loss: 1.8555 - regression_loss: 1.3829 - classification_loss: 0.4727\n",
      "139/500 [=======>......................] - ETA: 6:10 - loss: 1.8486 - regression_loss: 1.3775 - classification_loss: 0.4711\n",
      "140/500 [=======>......................] - ETA: 6:09 - loss: 1.8462 - regression_loss: 1.3763 - classification_loss: 0.4700\n",
      "141/500 [=======>......................] - ETA: 6:08 - loss: 1.8486 - regression_loss: 1.3793 - classification_loss: 0.4693\n",
      "142/500 [=======>......................] - ETA: 6:07 - loss: 1.8433 - regression_loss: 1.3752 - classification_loss: 0.4681\n",
      "143/500 [=======>......................] - ETA: 6:06 - loss: 1.8426 - regression_loss: 1.3745 - classification_loss: 0.4682\n",
      "144/500 [=======>......................] - ETA: 6:05 - loss: 1.8425 - regression_loss: 1.3743 - classification_loss: 0.4682\n",
      "145/500 [=======>......................] - ETA: 6:04 - loss: 1.8412 - regression_loss: 1.3733 - classification_loss: 0.4680\n",
      "146/500 [=======>......................] - ETA: 6:03 - loss: 1.8359 - regression_loss: 1.3695 - classification_loss: 0.4664\n",
      "147/500 [=======>......................] - ETA: 6:02 - loss: 1.8378 - regression_loss: 1.3714 - classification_loss: 0.4664\n",
      "148/500 [=======>......................] - ETA: 6:01 - loss: 1.8373 - regression_loss: 1.3713 - classification_loss: 0.4660\n",
      "149/500 [=======>......................] - ETA: 6:00 - loss: 1.8344 - regression_loss: 1.3689 - classification_loss: 0.4655\n",
      "150/500 [========>.....................] - ETA: 5:59 - loss: 1.8365 - regression_loss: 1.3705 - classification_loss: 0.4660\n",
      "151/500 [========>.....................] - ETA: 5:58 - loss: 1.8376 - regression_loss: 1.3711 - classification_loss: 0.4665\n",
      "152/500 [========>.....................] - ETA: 5:57 - loss: 1.8471 - regression_loss: 1.3772 - classification_loss: 0.4698\n",
      "153/500 [========>.....................] - ETA: 5:56 - loss: 1.8437 - regression_loss: 1.3747 - classification_loss: 0.4689\n",
      "154/500 [========>.....................] - ETA: 5:55 - loss: 1.8399 - regression_loss: 1.3724 - classification_loss: 0.4675\n",
      "155/500 [========>.....................] - ETA: 5:54 - loss: 1.8484 - regression_loss: 1.3781 - classification_loss: 0.4703\n",
      "156/500 [========>.....................] - ETA: 5:53 - loss: 1.8438 - regression_loss: 1.3749 - classification_loss: 0.4689\n",
      "157/500 [========>.....................] - ETA: 5:52 - loss: 1.8452 - regression_loss: 1.3760 - classification_loss: 0.4692\n",
      "158/500 [========>.....................] - ETA: 5:51 - loss: 1.8398 - regression_loss: 1.3716 - classification_loss: 0.4682\n",
      "159/500 [========>.....................] - ETA: 5:50 - loss: 1.8373 - regression_loss: 1.3700 - classification_loss: 0.4673\n",
      "160/500 [========>.....................] - ETA: 5:49 - loss: 1.8385 - regression_loss: 1.3712 - classification_loss: 0.4673\n",
      "161/500 [========>.....................] - ETA: 5:48 - loss: 1.8339 - regression_loss: 1.3679 - classification_loss: 0.4660\n",
      "162/500 [========>.....................] - ETA: 5:47 - loss: 1.8353 - regression_loss: 1.3693 - classification_loss: 0.4660\n",
      "163/500 [========>.....................] - ETA: 5:47 - loss: 1.8372 - regression_loss: 1.3711 - classification_loss: 0.4661\n",
      "164/500 [========>.....................] - ETA: 5:45 - loss: 1.8394 - regression_loss: 1.3731 - classification_loss: 0.4663\n",
      "165/500 [========>.....................] - ETA: 5:44 - loss: 1.8422 - regression_loss: 1.3760 - classification_loss: 0.4662\n",
      "166/500 [========>.....................] - ETA: 5:43 - loss: 1.8391 - regression_loss: 1.3733 - classification_loss: 0.4658\n",
      "167/500 [=========>....................] - ETA: 5:42 - loss: 1.8368 - regression_loss: 1.3722 - classification_loss: 0.4646\n",
      "168/500 [=========>....................] - ETA: 5:41 - loss: 1.8356 - regression_loss: 1.3720 - classification_loss: 0.4637\n",
      "169/500 [=========>....................] - ETA: 5:40 - loss: 1.8341 - regression_loss: 1.3705 - classification_loss: 0.4636\n",
      "170/500 [=========>....................] - ETA: 5:39 - loss: 1.8339 - regression_loss: 1.3711 - classification_loss: 0.4628\n",
      "171/500 [=========>....................] - ETA: 5:38 - loss: 1.8345 - regression_loss: 1.3704 - classification_loss: 0.4641\n",
      "172/500 [=========>....................] - ETA: 5:37 - loss: 1.8344 - regression_loss: 1.3702 - classification_loss: 0.4642\n",
      "173/500 [=========>....................] - ETA: 5:36 - loss: 1.8316 - regression_loss: 1.3687 - classification_loss: 0.4630\n",
      "174/500 [=========>....................] - ETA: 5:35 - loss: 1.8381 - regression_loss: 1.3728 - classification_loss: 0.4653\n",
      "175/500 [=========>....................] - ETA: 5:34 - loss: 1.8399 - regression_loss: 1.3746 - classification_loss: 0.4654\n",
      "176/500 [=========>....................] - ETA: 5:32 - loss: 1.8455 - regression_loss: 1.3782 - classification_loss: 0.4673\n",
      "177/500 [=========>....................] - ETA: 5:32 - loss: 1.8483 - regression_loss: 1.3795 - classification_loss: 0.4688\n",
      "178/500 [=========>....................] - ETA: 5:30 - loss: 1.8449 - regression_loss: 1.3770 - classification_loss: 0.4679\n",
      "179/500 [=========>....................] - ETA: 5:30 - loss: 1.8515 - regression_loss: 1.3821 - classification_loss: 0.4694\n",
      "180/500 [=========>....................] - ETA: 5:28 - loss: 1.8557 - regression_loss: 1.3836 - classification_loss: 0.4722\n",
      "181/500 [=========>....................] - ETA: 5:27 - loss: 1.8595 - regression_loss: 1.3861 - classification_loss: 0.4734\n",
      "182/500 [=========>....................] - ETA: 5:27 - loss: 1.8574 - regression_loss: 1.3842 - classification_loss: 0.4732\n",
      "183/500 [=========>....................] - ETA: 5:25 - loss: 1.8629 - regression_loss: 1.3874 - classification_loss: 0.4755\n",
      "184/500 [==========>...................] - ETA: 5:24 - loss: 1.8625 - regression_loss: 1.3874 - classification_loss: 0.4751\n",
      "185/500 [==========>...................] - ETA: 5:23 - loss: 1.8648 - regression_loss: 1.3894 - classification_loss: 0.4754\n",
      "186/500 [==========>...................] - ETA: 5:22 - loss: 1.8638 - regression_loss: 1.3886 - classification_loss: 0.4752\n",
      "187/500 [==========>...................] - ETA: 5:21 - loss: 1.8648 - regression_loss: 1.3897 - classification_loss: 0.4750\n",
      "188/500 [==========>...................] - ETA: 5:20 - loss: 1.8646 - regression_loss: 1.3893 - classification_loss: 0.4753\n",
      "189/500 [==========>...................] - ETA: 5:19 - loss: 1.8658 - regression_loss: 1.3909 - classification_loss: 0.4749\n",
      "190/500 [==========>...................] - ETA: 5:18 - loss: 1.8641 - regression_loss: 1.3895 - classification_loss: 0.4746\n",
      "191/500 [==========>...................] - ETA: 5:17 - loss: 1.8652 - regression_loss: 1.3902 - classification_loss: 0.4750\n",
      "192/500 [==========>...................] - ETA: 5:16 - loss: 1.8687 - regression_loss: 1.3925 - classification_loss: 0.4762\n",
      "193/500 [==========>...................] - ETA: 5:15 - loss: 1.8649 - regression_loss: 1.3893 - classification_loss: 0.4756\n",
      "194/500 [==========>...................] - ETA: 5:14 - loss: 1.8693 - regression_loss: 1.3895 - classification_loss: 0.4798\n",
      "195/500 [==========>...................] - ETA: 5:13 - loss: 1.8677 - regression_loss: 1.3887 - classification_loss: 0.4790\n",
      "196/500 [==========>...................] - ETA: 5:12 - loss: 1.8688 - regression_loss: 1.3897 - classification_loss: 0.4791\n",
      "197/500 [==========>...................] - ETA: 5:11 - loss: 1.8693 - regression_loss: 1.3904 - classification_loss: 0.4788\n",
      "198/500 [==========>...................] - ETA: 5:10 - loss: 1.8673 - regression_loss: 1.3894 - classification_loss: 0.4780\n",
      "199/500 [==========>...................] - ETA: 5:09 - loss: 1.8679 - regression_loss: 1.3900 - classification_loss: 0.4780\n",
      "200/500 [===========>..................] - ETA: 5:07 - loss: 1.8663 - regression_loss: 1.3880 - classification_loss: 0.4783\n",
      "201/500 [===========>..................] - ETA: 5:06 - loss: 1.8611 - regression_loss: 1.3842 - classification_loss: 0.4769\n",
      "202/500 [===========>..................] - ETA: 5:05 - loss: 1.8591 - regression_loss: 1.3830 - classification_loss: 0.4761\n",
      "203/500 [===========>..................] - ETA: 5:04 - loss: 1.8547 - regression_loss: 1.3792 - classification_loss: 0.4755\n",
      "204/500 [===========>..................] - ETA: 5:03 - loss: 1.8573 - regression_loss: 1.3804 - classification_loss: 0.4770\n",
      "205/500 [===========>..................] - ETA: 5:02 - loss: 1.8617 - regression_loss: 1.3834 - classification_loss: 0.4783\n",
      "206/500 [===========>..................] - ETA: 5:01 - loss: 1.8663 - regression_loss: 1.3873 - classification_loss: 0.4790\n",
      "207/500 [===========>..................] - ETA: 5:00 - loss: 1.8659 - regression_loss: 1.3871 - classification_loss: 0.4788\n",
      "208/500 [===========>..................] - ETA: 4:59 - loss: 1.8687 - regression_loss: 1.3890 - classification_loss: 0.4797\n",
      "209/500 [===========>..................] - ETA: 4:58 - loss: 1.8705 - regression_loss: 1.3906 - classification_loss: 0.4800\n",
      "210/500 [===========>..................] - ETA: 4:57 - loss: 1.8674 - regression_loss: 1.3880 - classification_loss: 0.4793\n",
      "211/500 [===========>..................] - ETA: 4:56 - loss: 1.8680 - regression_loss: 1.3891 - classification_loss: 0.4789\n",
      "212/500 [===========>..................] - ETA: 4:55 - loss: 1.8668 - regression_loss: 1.3880 - classification_loss: 0.4788\n",
      "213/500 [===========>..................] - ETA: 4:54 - loss: 1.8694 - regression_loss: 1.3901 - classification_loss: 0.4793\n",
      "214/500 [===========>..................] - ETA: 4:53 - loss: 1.8722 - regression_loss: 1.3927 - classification_loss: 0.4795\n",
      "215/500 [===========>..................] - ETA: 4:52 - loss: 1.8776 - regression_loss: 1.3960 - classification_loss: 0.4816\n",
      "216/500 [===========>..................] - ETA: 4:51 - loss: 1.8748 - regression_loss: 1.3939 - classification_loss: 0.4810\n",
      "217/500 [============>.................] - ETA: 4:50 - loss: 1.8762 - regression_loss: 1.3942 - classification_loss: 0.4821\n",
      "218/500 [============>.................] - ETA: 4:49 - loss: 1.8787 - regression_loss: 1.3960 - classification_loss: 0.4827\n",
      "219/500 [============>.................] - ETA: 4:48 - loss: 1.8776 - regression_loss: 1.3951 - classification_loss: 0.4826\n",
      "220/500 [============>.................] - ETA: 4:47 - loss: 1.8752 - regression_loss: 1.3930 - classification_loss: 0.4822\n",
      "221/500 [============>.................] - ETA: 4:46 - loss: 1.8754 - regression_loss: 1.3937 - classification_loss: 0.4818\n",
      "222/500 [============>.................] - ETA: 4:45 - loss: 1.8777 - regression_loss: 1.3956 - classification_loss: 0.4821\n",
      "223/500 [============>.................] - ETA: 4:44 - loss: 1.8786 - regression_loss: 1.3964 - classification_loss: 0.4822\n",
      "224/500 [============>.................] - ETA: 4:43 - loss: 1.8786 - regression_loss: 1.3965 - classification_loss: 0.4821\n",
      "225/500 [============>.................] - ETA: 4:42 - loss: 1.8848 - regression_loss: 1.4001 - classification_loss: 0.4847\n",
      "226/500 [============>.................] - ETA: 4:41 - loss: 1.8845 - regression_loss: 1.4003 - classification_loss: 0.4841\n",
      "227/500 [============>.................] - ETA: 4:40 - loss: 1.8846 - regression_loss: 1.4001 - classification_loss: 0.4845\n",
      "228/500 [============>.................] - ETA: 4:39 - loss: 1.8805 - regression_loss: 1.3975 - classification_loss: 0.4830\n",
      "229/500 [============>.................] - ETA: 4:37 - loss: 1.8795 - regression_loss: 1.3964 - classification_loss: 0.4831\n",
      "230/500 [============>.................] - ETA: 4:36 - loss: 1.8790 - regression_loss: 1.3963 - classification_loss: 0.4827\n",
      "231/500 [============>.................] - ETA: 4:35 - loss: 1.8773 - regression_loss: 1.3951 - classification_loss: 0.4822\n",
      "232/500 [============>.................] - ETA: 4:34 - loss: 1.8758 - regression_loss: 1.3943 - classification_loss: 0.4815\n",
      "233/500 [============>.................] - ETA: 4:33 - loss: 1.8738 - regression_loss: 1.3928 - classification_loss: 0.4809\n",
      "234/500 [=============>................] - ETA: 4:32 - loss: 1.8719 - regression_loss: 1.3918 - classification_loss: 0.4801\n",
      "235/500 [=============>................] - ETA: 4:31 - loss: 1.8717 - regression_loss: 1.3915 - classification_loss: 0.4802\n",
      "236/500 [=============>................] - ETA: 4:30 - loss: 1.8745 - regression_loss: 1.3935 - classification_loss: 0.4810\n",
      "237/500 [=============>................] - ETA: 4:29 - loss: 1.8702 - regression_loss: 1.3904 - classification_loss: 0.4798\n",
      "238/500 [=============>................] - ETA: 4:28 - loss: 1.8727 - regression_loss: 1.3919 - classification_loss: 0.4808\n",
      "239/500 [=============>................] - ETA: 4:27 - loss: 1.8698 - regression_loss: 1.3899 - classification_loss: 0.4799\n",
      "240/500 [=============>................] - ETA: 4:26 - loss: 1.8726 - regression_loss: 1.3913 - classification_loss: 0.4812\n",
      "241/500 [=============>................] - ETA: 4:25 - loss: 1.8703 - regression_loss: 1.3898 - classification_loss: 0.4805\n",
      "242/500 [=============>................] - ETA: 4:24 - loss: 1.8704 - regression_loss: 1.3897 - classification_loss: 0.4806\n",
      "243/500 [=============>................] - ETA: 4:23 - loss: 1.8708 - regression_loss: 1.3901 - classification_loss: 0.4808\n",
      "244/500 [=============>................] - ETA: 4:22 - loss: 1.8695 - regression_loss: 1.3896 - classification_loss: 0.4799\n",
      "245/500 [=============>................] - ETA: 4:21 - loss: 1.8650 - regression_loss: 1.3863 - classification_loss: 0.4786\n",
      "246/500 [=============>................] - ETA: 4:20 - loss: 1.8625 - regression_loss: 1.3848 - classification_loss: 0.4777\n",
      "247/500 [=============>................] - ETA: 4:19 - loss: 1.8616 - regression_loss: 1.3845 - classification_loss: 0.4772\n",
      "248/500 [=============>................] - ETA: 4:18 - loss: 1.8656 - regression_loss: 1.3867 - classification_loss: 0.4789\n",
      "249/500 [=============>................] - ETA: 4:17 - loss: 1.8652 - regression_loss: 1.3862 - classification_loss: 0.4790\n",
      "250/500 [==============>...............] - ETA: 4:16 - loss: 1.8629 - regression_loss: 1.3849 - classification_loss: 0.4780\n",
      "251/500 [==============>...............] - ETA: 4:15 - loss: 1.8623 - regression_loss: 1.3848 - classification_loss: 0.4776\n",
      "252/500 [==============>...............] - ETA: 4:14 - loss: 1.8674 - regression_loss: 1.3883 - classification_loss: 0.4792\n",
      "253/500 [==============>...............] - ETA: 4:13 - loss: 1.8665 - regression_loss: 1.3879 - classification_loss: 0.4787\n",
      "254/500 [==============>...............] - ETA: 4:12 - loss: 1.8682 - regression_loss: 1.3898 - classification_loss: 0.4785\n",
      "255/500 [==============>...............] - ETA: 4:11 - loss: 1.8701 - regression_loss: 1.3906 - classification_loss: 0.4794\n",
      "256/500 [==============>...............] - ETA: 4:10 - loss: 1.8722 - regression_loss: 1.3912 - classification_loss: 0.4810\n",
      "257/500 [==============>...............] - ETA: 4:08 - loss: 1.8750 - regression_loss: 1.3924 - classification_loss: 0.4826\n",
      "258/500 [==============>...............] - ETA: 4:08 - loss: 1.8761 - regression_loss: 1.3932 - classification_loss: 0.4830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259/500 [==============>...............] - ETA: 4:06 - loss: 1.8764 - regression_loss: 1.3931 - classification_loss: 0.4833\n",
      "260/500 [==============>...............] - ETA: 4:05 - loss: 1.8745 - regression_loss: 1.3917 - classification_loss: 0.4829\n",
      "261/500 [==============>...............] - ETA: 4:04 - loss: 1.8745 - regression_loss: 1.3917 - classification_loss: 0.4828\n",
      "262/500 [==============>...............] - ETA: 4:03 - loss: 1.8780 - regression_loss: 1.3939 - classification_loss: 0.4841\n",
      "263/500 [==============>...............] - ETA: 4:02 - loss: 1.8781 - regression_loss: 1.3942 - classification_loss: 0.4840\n",
      "264/500 [==============>...............] - ETA: 4:01 - loss: 1.8787 - regression_loss: 1.3948 - classification_loss: 0.4838\n",
      "265/500 [==============>...............] - ETA: 4:00 - loss: 1.8780 - regression_loss: 1.3944 - classification_loss: 0.4836\n",
      "266/500 [==============>...............] - ETA: 3:59 - loss: 1.8781 - regression_loss: 1.3944 - classification_loss: 0.4836\n",
      "267/500 [===============>..............] - ETA: 3:58 - loss: 1.8767 - regression_loss: 1.3937 - classification_loss: 0.4830\n",
      "268/500 [===============>..............] - ETA: 3:57 - loss: 1.8758 - regression_loss: 1.3931 - classification_loss: 0.4827\n",
      "269/500 [===============>..............] - ETA: 3:56 - loss: 1.8742 - regression_loss: 1.3911 - classification_loss: 0.4831\n",
      "270/500 [===============>..............] - ETA: 3:55 - loss: 1.8711 - regression_loss: 1.3888 - classification_loss: 0.4823\n",
      "271/500 [===============>..............] - ETA: 3:54 - loss: 1.8742 - regression_loss: 1.3901 - classification_loss: 0.4841\n",
      "272/500 [===============>..............] - ETA: 3:53 - loss: 1.8727 - regression_loss: 1.3893 - classification_loss: 0.4834\n",
      "273/500 [===============>..............] - ETA: 3:52 - loss: 1.8719 - regression_loss: 1.3883 - classification_loss: 0.4835\n",
      "274/500 [===============>..............] - ETA: 3:51 - loss: 1.8692 - regression_loss: 1.3864 - classification_loss: 0.4829\n",
      "275/500 [===============>..............] - ETA: 3:50 - loss: 1.8688 - regression_loss: 1.3858 - classification_loss: 0.4830\n",
      "276/500 [===============>..............] - ETA: 3:49 - loss: 1.8680 - regression_loss: 1.3856 - classification_loss: 0.4824\n",
      "277/500 [===============>..............] - ETA: 3:48 - loss: 1.8701 - regression_loss: 1.3871 - classification_loss: 0.4831\n",
      "278/500 [===============>..............] - ETA: 3:47 - loss: 1.8661 - regression_loss: 1.3840 - classification_loss: 0.4821\n",
      "279/500 [===============>..............] - ETA: 3:46 - loss: 1.8651 - regression_loss: 1.3836 - classification_loss: 0.4815\n",
      "280/500 [===============>..............] - ETA: 3:45 - loss: 1.8681 - regression_loss: 1.3855 - classification_loss: 0.4826\n",
      "281/500 [===============>..............] - ETA: 3:44 - loss: 1.8665 - regression_loss: 1.3846 - classification_loss: 0.4820\n",
      "282/500 [===============>..............] - ETA: 3:42 - loss: 1.8689 - regression_loss: 1.3867 - classification_loss: 0.4823\n",
      "283/500 [===============>..............] - ETA: 3:41 - loss: 1.8692 - regression_loss: 1.3868 - classification_loss: 0.4825\n",
      "284/500 [================>.............] - ETA: 3:40 - loss: 1.8664 - regression_loss: 1.3849 - classification_loss: 0.4815\n",
      "285/500 [================>.............] - ETA: 3:39 - loss: 1.8676 - regression_loss: 1.3849 - classification_loss: 0.4828\n",
      "286/500 [================>.............] - ETA: 3:38 - loss: 1.8670 - regression_loss: 1.3843 - classification_loss: 0.4827\n",
      "287/500 [================>.............] - ETA: 3:37 - loss: 1.8674 - regression_loss: 1.3848 - classification_loss: 0.4826\n",
      "288/500 [================>.............] - ETA: 3:36 - loss: 1.8660 - regression_loss: 1.3834 - classification_loss: 0.4825\n",
      "289/500 [================>.............] - ETA: 3:35 - loss: 1.8647 - regression_loss: 1.3820 - classification_loss: 0.4827\n",
      "290/500 [================>.............] - ETA: 3:34 - loss: 1.8650 - regression_loss: 1.3826 - classification_loss: 0.4824\n",
      "291/500 [================>.............] - ETA: 3:33 - loss: 1.8653 - regression_loss: 1.3829 - classification_loss: 0.4824\n",
      "292/500 [================>.............] - ETA: 3:32 - loss: 1.8635 - regression_loss: 1.3815 - classification_loss: 0.4820\n",
      "293/500 [================>.............] - ETA: 3:31 - loss: 1.8645 - regression_loss: 1.3826 - classification_loss: 0.4819\n",
      "294/500 [================>.............] - ETA: 3:30 - loss: 1.8626 - regression_loss: 1.3814 - classification_loss: 0.4812\n",
      "295/500 [================>.............] - ETA: 3:29 - loss: 1.8617 - regression_loss: 1.3809 - classification_loss: 0.4808\n",
      "296/500 [================>.............] - ETA: 3:28 - loss: 1.8598 - regression_loss: 1.3798 - classification_loss: 0.4800\n",
      "297/500 [================>.............] - ETA: 3:27 - loss: 1.8633 - regression_loss: 1.3828 - classification_loss: 0.4805\n",
      "298/500 [================>.............] - ETA: 3:26 - loss: 1.8668 - regression_loss: 1.3856 - classification_loss: 0.4812\n",
      "299/500 [================>.............] - ETA: 3:25 - loss: 1.8655 - regression_loss: 1.3848 - classification_loss: 0.4807\n",
      "300/500 [=================>............] - ETA: 3:24 - loss: 1.8637 - regression_loss: 1.3836 - classification_loss: 0.4800\n",
      "301/500 [=================>............] - ETA: 3:23 - loss: 1.8656 - regression_loss: 1.3849 - classification_loss: 0.4807\n",
      "302/500 [=================>............] - ETA: 3:22 - loss: 1.8673 - regression_loss: 1.3862 - classification_loss: 0.4811\n",
      "303/500 [=================>............] - ETA: 3:21 - loss: 1.8690 - regression_loss: 1.3876 - classification_loss: 0.4815\n",
      "304/500 [=================>............] - ETA: 3:20 - loss: 1.8694 - regression_loss: 1.3877 - classification_loss: 0.4817\n",
      "305/500 [=================>............] - ETA: 3:19 - loss: 1.8694 - regression_loss: 1.3880 - classification_loss: 0.4814\n",
      "306/500 [=================>............] - ETA: 3:18 - loss: 1.8732 - regression_loss: 1.3905 - classification_loss: 0.4827\n",
      "307/500 [=================>............] - ETA: 3:17 - loss: 1.8727 - regression_loss: 1.3900 - classification_loss: 0.4827\n",
      "308/500 [=================>............] - ETA: 3:16 - loss: 1.8734 - regression_loss: 1.3907 - classification_loss: 0.4828\n",
      "309/500 [=================>............] - ETA: 3:15 - loss: 1.8727 - regression_loss: 1.3902 - classification_loss: 0.4826\n",
      "310/500 [=================>............] - ETA: 3:14 - loss: 1.8755 - regression_loss: 1.3917 - classification_loss: 0.4838\n",
      "311/500 [=================>............] - ETA: 3:13 - loss: 1.8752 - regression_loss: 1.3918 - classification_loss: 0.4833\n",
      "312/500 [=================>............] - ETA: 3:12 - loss: 1.8742 - regression_loss: 1.3909 - classification_loss: 0.4833\n",
      "313/500 [=================>............] - ETA: 3:11 - loss: 1.8751 - regression_loss: 1.3919 - classification_loss: 0.4832\n",
      "314/500 [=================>............] - ETA: 3:09 - loss: 1.8765 - regression_loss: 1.3933 - classification_loss: 0.4832\n",
      "315/500 [=================>............] - ETA: 3:08 - loss: 1.8732 - regression_loss: 1.3910 - classification_loss: 0.4822\n",
      "316/500 [=================>............] - ETA: 3:07 - loss: 1.8761 - regression_loss: 1.3936 - classification_loss: 0.4826\n",
      "317/500 [==================>...........] - ETA: 3:06 - loss: 1.8796 - regression_loss: 1.3958 - classification_loss: 0.4838\n",
      "318/500 [==================>...........] - ETA: 3:05 - loss: 1.8826 - regression_loss: 1.3982 - classification_loss: 0.4845\n",
      "319/500 [==================>...........] - ETA: 3:04 - loss: 1.8823 - regression_loss: 1.3983 - classification_loss: 0.4840\n",
      "320/500 [==================>...........] - ETA: 3:03 - loss: 1.8801 - regression_loss: 1.3967 - classification_loss: 0.4834\n",
      "321/500 [==================>...........] - ETA: 3:02 - loss: 1.8772 - regression_loss: 1.3943 - classification_loss: 0.4829\n",
      "322/500 [==================>...........] - ETA: 3:01 - loss: 1.8784 - regression_loss: 1.3958 - classification_loss: 0.4827\n",
      "323/500 [==================>...........] - ETA: 3:00 - loss: 1.8772 - regression_loss: 1.3948 - classification_loss: 0.4824\n",
      "324/500 [==================>...........] - ETA: 2:59 - loss: 1.8783 - regression_loss: 1.3958 - classification_loss: 0.4825\n",
      "325/500 [==================>...........] - ETA: 2:58 - loss: 1.8779 - regression_loss: 1.3957 - classification_loss: 0.4822\n",
      "326/500 [==================>...........] - ETA: 2:57 - loss: 1.8779 - regression_loss: 1.3956 - classification_loss: 0.4822\n",
      "327/500 [==================>...........] - ETA: 2:56 - loss: 1.8792 - regression_loss: 1.3970 - classification_loss: 0.4822\n",
      "328/500 [==================>...........] - ETA: 2:55 - loss: 1.8774 - regression_loss: 1.3957 - classification_loss: 0.4817\n",
      "329/500 [==================>...........] - ETA: 2:54 - loss: 1.8761 - regression_loss: 1.3948 - classification_loss: 0.4813\n",
      "330/500 [==================>...........] - ETA: 2:53 - loss: 1.8739 - regression_loss: 1.3934 - classification_loss: 0.4806\n",
      "331/500 [==================>...........] - ETA: 2:52 - loss: 1.8739 - regression_loss: 1.3931 - classification_loss: 0.4808\n",
      "332/500 [==================>...........] - ETA: 2:51 - loss: 1.8739 - regression_loss: 1.3933 - classification_loss: 0.4807\n",
      "333/500 [==================>...........] - ETA: 2:50 - loss: 1.8722 - regression_loss: 1.3923 - classification_loss: 0.4799\n",
      "334/500 [===================>..........] - ETA: 2:49 - loss: 1.8702 - regression_loss: 1.3911 - classification_loss: 0.4791\n",
      "335/500 [===================>..........] - ETA: 2:48 - loss: 1.8696 - regression_loss: 1.3911 - classification_loss: 0.4784\n",
      "336/500 [===================>..........] - ETA: 2:47 - loss: 1.8690 - regression_loss: 1.3907 - classification_loss: 0.4784\n",
      "337/500 [===================>..........] - ETA: 2:46 - loss: 1.8705 - regression_loss: 1.3924 - classification_loss: 0.4781\n",
      "338/500 [===================>..........] - ETA: 2:45 - loss: 1.8701 - regression_loss: 1.3919 - classification_loss: 0.4783\n",
      "339/500 [===================>..........] - ETA: 2:44 - loss: 1.8726 - regression_loss: 1.3940 - classification_loss: 0.4787\n",
      "340/500 [===================>..........] - ETA: 2:43 - loss: 1.8745 - regression_loss: 1.3947 - classification_loss: 0.4798\n",
      "341/500 [===================>..........] - ETA: 2:42 - loss: 1.8740 - regression_loss: 1.3943 - classification_loss: 0.4796\n",
      "342/500 [===================>..........] - ETA: 2:41 - loss: 1.8741 - regression_loss: 1.3940 - classification_loss: 0.4802\n",
      "343/500 [===================>..........] - ETA: 2:40 - loss: 1.8742 - regression_loss: 1.3939 - classification_loss: 0.4803\n",
      "344/500 [===================>..........] - ETA: 2:39 - loss: 1.8745 - regression_loss: 1.3944 - classification_loss: 0.4801\n",
      "345/500 [===================>..........] - ETA: 2:38 - loss: 1.8771 - regression_loss: 1.3966 - classification_loss: 0.4806\n",
      "346/500 [===================>..........] - ETA: 2:37 - loss: 1.8780 - regression_loss: 1.3973 - classification_loss: 0.4807\n",
      "347/500 [===================>..........] - ETA: 2:36 - loss: 1.8786 - regression_loss: 1.3979 - classification_loss: 0.4807\n",
      "348/500 [===================>..........] - ETA: 2:35 - loss: 1.8791 - regression_loss: 1.3981 - classification_loss: 0.4810\n",
      "349/500 [===================>..........] - ETA: 2:34 - loss: 1.8787 - regression_loss: 1.3978 - classification_loss: 0.4809\n",
      "350/500 [====================>.........] - ETA: 2:33 - loss: 1.8793 - regression_loss: 1.3982 - classification_loss: 0.4811\n",
      "351/500 [====================>.........] - ETA: 2:32 - loss: 1.8774 - regression_loss: 1.3969 - classification_loss: 0.4805\n",
      "352/500 [====================>.........] - ETA: 2:31 - loss: 1.8773 - regression_loss: 1.3967 - classification_loss: 0.4805\n",
      "353/500 [====================>.........] - ETA: 2:30 - loss: 1.8757 - regression_loss: 1.3958 - classification_loss: 0.4800\n",
      "354/500 [====================>.........] - ETA: 2:29 - loss: 1.8742 - regression_loss: 1.3949 - classification_loss: 0.4793\n",
      "355/500 [====================>.........] - ETA: 2:27 - loss: 1.8726 - regression_loss: 1.3938 - classification_loss: 0.4789\n",
      "356/500 [====================>.........] - ETA: 2:26 - loss: 1.8739 - regression_loss: 1.3949 - classification_loss: 0.4790\n",
      "357/500 [====================>.........] - ETA: 2:25 - loss: 1.8724 - regression_loss: 1.3934 - classification_loss: 0.4790\n",
      "358/500 [====================>.........] - ETA: 2:24 - loss: 1.8705 - regression_loss: 1.3920 - classification_loss: 0.4785\n",
      "359/500 [====================>.........] - ETA: 2:23 - loss: 1.8715 - regression_loss: 1.3921 - classification_loss: 0.4794\n",
      "360/500 [====================>.........] - ETA: 2:22 - loss: 1.8712 - regression_loss: 1.3922 - classification_loss: 0.4789\n",
      "361/500 [====================>.........] - ETA: 2:21 - loss: 1.8699 - regression_loss: 1.3906 - classification_loss: 0.4793\n",
      "362/500 [====================>.........] - ETA: 2:20 - loss: 1.8709 - regression_loss: 1.3913 - classification_loss: 0.4796\n",
      "363/500 [====================>.........] - ETA: 2:19 - loss: 1.8725 - regression_loss: 1.3923 - classification_loss: 0.4801\n",
      "364/500 [====================>.........] - ETA: 2:18 - loss: 1.8727 - regression_loss: 1.3925 - classification_loss: 0.4802\n",
      "365/500 [====================>.........] - ETA: 2:17 - loss: 1.8718 - regression_loss: 1.3919 - classification_loss: 0.4799\n",
      "366/500 [====================>.........] - ETA: 2:16 - loss: 1.8718 - regression_loss: 1.3921 - classification_loss: 0.4797\n",
      "367/500 [=====================>........] - ETA: 2:15 - loss: 1.8740 - regression_loss: 1.3935 - classification_loss: 0.4805\n",
      "368/500 [=====================>........] - ETA: 2:14 - loss: 1.8722 - regression_loss: 1.3921 - classification_loss: 0.4801\n",
      "369/500 [=====================>........] - ETA: 2:13 - loss: 1.8729 - regression_loss: 1.3929 - classification_loss: 0.4800\n",
      "370/500 [=====================>........] - ETA: 2:12 - loss: 1.8743 - regression_loss: 1.3939 - classification_loss: 0.4804\n",
      "371/500 [=====================>........] - ETA: 2:11 - loss: 1.8769 - regression_loss: 1.3958 - classification_loss: 0.4811\n",
      "372/500 [=====================>........] - ETA: 2:10 - loss: 1.8770 - regression_loss: 1.3958 - classification_loss: 0.4812\n",
      "373/500 [=====================>........] - ETA: 2:09 - loss: 1.8760 - regression_loss: 1.3951 - classification_loss: 0.4809\n",
      "374/500 [=====================>........] - ETA: 2:08 - loss: 1.8760 - regression_loss: 1.3947 - classification_loss: 0.4813\n",
      "375/500 [=====================>........] - ETA: 2:07 - loss: 1.8751 - regression_loss: 1.3940 - classification_loss: 0.4811\n",
      "376/500 [=====================>........] - ETA: 2:06 - loss: 1.8757 - regression_loss: 1.3950 - classification_loss: 0.4807\n",
      "377/500 [=====================>........] - ETA: 2:05 - loss: 1.8734 - regression_loss: 1.3932 - classification_loss: 0.4801\n",
      "378/500 [=====================>........] - ETA: 2:04 - loss: 1.8760 - regression_loss: 1.3958 - classification_loss: 0.4802\n",
      "379/500 [=====================>........] - ETA: 2:03 - loss: 1.8756 - regression_loss: 1.3955 - classification_loss: 0.4801\n",
      "380/500 [=====================>........] - ETA: 2:02 - loss: 1.8767 - regression_loss: 1.3963 - classification_loss: 0.4804\n",
      "381/500 [=====================>........] - ETA: 2:01 - loss: 1.8786 - regression_loss: 1.3977 - classification_loss: 0.4809\n",
      "382/500 [=====================>........] - ETA: 2:00 - loss: 1.8789 - regression_loss: 1.3976 - classification_loss: 0.4814\n",
      "383/500 [=====================>........] - ETA: 1:59 - loss: 1.8792 - regression_loss: 1.3975 - classification_loss: 0.4817\n",
      "384/500 [======================>.......] - ETA: 1:58 - loss: 1.8789 - regression_loss: 1.3977 - classification_loss: 0.4812\n",
      "385/500 [======================>.......] - ETA: 1:57 - loss: 1.8803 - regression_loss: 1.3989 - classification_loss: 0.4814\n",
      "386/500 [======================>.......] - ETA: 1:56 - loss: 1.8795 - regression_loss: 1.3981 - classification_loss: 0.4813\n",
      "387/500 [======================>.......] - ETA: 1:55 - loss: 1.8802 - regression_loss: 1.3989 - classification_loss: 0.4813\n",
      "388/500 [======================>.......] - ETA: 1:54 - loss: 1.8801 - regression_loss: 1.3984 - classification_loss: 0.4817\n",
      "389/500 [======================>.......] - ETA: 1:53 - loss: 1.8786 - regression_loss: 1.3974 - classification_loss: 0.4812\n",
      "390/500 [======================>.......] - ETA: 1:52 - loss: 1.8792 - regression_loss: 1.3981 - classification_loss: 0.4811\n",
      "391/500 [======================>.......] - ETA: 1:51 - loss: 1.8807 - regression_loss: 1.3987 - classification_loss: 0.4819\n",
      "392/500 [======================>.......] - ETA: 1:50 - loss: 1.8803 - regression_loss: 1.3986 - classification_loss: 0.4818\n",
      "393/500 [======================>.......] - ETA: 1:49 - loss: 1.8785 - regression_loss: 1.3971 - classification_loss: 0.4814\n",
      "394/500 [======================>.......] - ETA: 1:47 - loss: 1.8795 - regression_loss: 1.3982 - classification_loss: 0.4814\n",
      "395/500 [======================>.......] - ETA: 1:47 - loss: 1.8796 - regression_loss: 1.3985 - classification_loss: 0.4811\n",
      "396/500 [======================>.......] - ETA: 1:45 - loss: 1.8788 - regression_loss: 1.3975 - classification_loss: 0.4814\n",
      "397/500 [======================>.......] - ETA: 1:44 - loss: 1.8790 - regression_loss: 1.3973 - classification_loss: 0.4818\n",
      "398/500 [======================>.......] - ETA: 1:43 - loss: 1.8785 - regression_loss: 1.3970 - classification_loss: 0.4815\n",
      "399/500 [======================>.......] - ETA: 1:42 - loss: 1.8795 - regression_loss: 1.3973 - classification_loss: 0.4822\n",
      "400/500 [=======================>......] - ETA: 1:41 - loss: 1.8793 - regression_loss: 1.3975 - classification_loss: 0.4818\n",
      "401/500 [=======================>......] - ETA: 1:40 - loss: 1.8781 - regression_loss: 1.3969 - classification_loss: 0.4812\n",
      "402/500 [=======================>......] - ETA: 1:39 - loss: 1.8781 - regression_loss: 1.3969 - classification_loss: 0.4811\n",
      "403/500 [=======================>......] - ETA: 1:38 - loss: 1.8771 - regression_loss: 1.3961 - classification_loss: 0.4810\n",
      "404/500 [=======================>......] - ETA: 1:37 - loss: 1.8780 - regression_loss: 1.3969 - classification_loss: 0.4812\n",
      "405/500 [=======================>......] - ETA: 1:36 - loss: 1.8775 - regression_loss: 1.3966 - classification_loss: 0.4809\n",
      "406/500 [=======================>......] - ETA: 1:35 - loss: 1.8793 - regression_loss: 1.3980 - classification_loss: 0.4812\n",
      "407/500 [=======================>......] - ETA: 1:34 - loss: 1.8793 - regression_loss: 1.3981 - classification_loss: 0.4812\n",
      "408/500 [=======================>......] - ETA: 1:33 - loss: 1.8788 - regression_loss: 1.3977 - classification_loss: 0.4811\n",
      "409/500 [=======================>......] - ETA: 1:32 - loss: 1.8773 - regression_loss: 1.3966 - classification_loss: 0.4807\n",
      "410/500 [=======================>......] - ETA: 1:31 - loss: 1.8760 - regression_loss: 1.3958 - classification_loss: 0.4802\n",
      "411/500 [=======================>......] - ETA: 1:30 - loss: 1.8749 - regression_loss: 1.3953 - classification_loss: 0.4797\n",
      "412/500 [=======================>......] - ETA: 1:29 - loss: 1.8740 - regression_loss: 1.3948 - classification_loss: 0.4793\n",
      "413/500 [=======================>......] - ETA: 1:28 - loss: 1.8773 - regression_loss: 1.3972 - classification_loss: 0.4800\n",
      "414/500 [=======================>......] - ETA: 1:27 - loss: 1.8756 - regression_loss: 1.3960 - classification_loss: 0.4796\n",
      "415/500 [=======================>......] - ETA: 1:26 - loss: 1.8743 - regression_loss: 1.3951 - classification_loss: 0.4792\n",
      "416/500 [=======================>......] - ETA: 1:25 - loss: 1.8749 - regression_loss: 1.3957 - classification_loss: 0.4791\n",
      "417/500 [========================>.....] - ETA: 1:24 - loss: 1.8755 - regression_loss: 1.3962 - classification_loss: 0.4793\n",
      "418/500 [========================>.....] - ETA: 1:23 - loss: 1.8754 - regression_loss: 1.3963 - classification_loss: 0.4791\n",
      "419/500 [========================>.....] - ETA: 1:22 - loss: 1.8725 - regression_loss: 1.3942 - classification_loss: 0.4783\n",
      "420/500 [========================>.....] - ETA: 1:21 - loss: 1.8732 - regression_loss: 1.3946 - classification_loss: 0.4785\n",
      "421/500 [========================>.....] - ETA: 1:20 - loss: 1.8770 - regression_loss: 1.3972 - classification_loss: 0.4798\n",
      "422/500 [========================>.....] - ETA: 1:19 - loss: 1.8767 - regression_loss: 1.3969 - classification_loss: 0.4798\n",
      "423/500 [========================>.....] - ETA: 1:18 - loss: 1.8765 - regression_loss: 1.3969 - classification_loss: 0.4797\n",
      "424/500 [========================>.....] - ETA: 1:17 - loss: 1.8752 - regression_loss: 1.3959 - classification_loss: 0.4793\n",
      "425/500 [========================>.....] - ETA: 1:16 - loss: 1.8759 - regression_loss: 1.3963 - classification_loss: 0.4795\n",
      "426/500 [========================>.....] - ETA: 1:15 - loss: 1.8773 - regression_loss: 1.3968 - classification_loss: 0.4806\n",
      "427/500 [========================>.....] - ETA: 1:14 - loss: 1.8765 - regression_loss: 1.3963 - classification_loss: 0.4802\n",
      "428/500 [========================>.....] - ETA: 1:13 - loss: 1.8789 - regression_loss: 1.3977 - classification_loss: 0.4813\n",
      "429/500 [========================>.....] - ETA: 1:12 - loss: 1.8771 - regression_loss: 1.3963 - classification_loss: 0.4809\n",
      "430/500 [========================>.....] - ETA: 1:11 - loss: 1.8775 - regression_loss: 1.3964 - classification_loss: 0.4811\n",
      "431/500 [========================>.....] - ETA: 1:10 - loss: 1.8780 - regression_loss: 1.3964 - classification_loss: 0.4815\n",
      "432/500 [========================>.....] - ETA: 1:09 - loss: 1.8791 - regression_loss: 1.3974 - classification_loss: 0.4817\n",
      "433/500 [========================>.....] - ETA: 1:08 - loss: 1.8783 - regression_loss: 1.3970 - classification_loss: 0.4813\n",
      "434/500 [=========================>....] - ETA: 1:07 - loss: 1.8783 - regression_loss: 1.3970 - classification_loss: 0.4812\n",
      "435/500 [=========================>....] - ETA: 1:06 - loss: 1.8784 - regression_loss: 1.3972 - classification_loss: 0.4812\n",
      "436/500 [=========================>....] - ETA: 1:05 - loss: 1.8783 - regression_loss: 1.3974 - classification_loss: 0.4809\n",
      "437/500 [=========================>....] - ETA: 1:04 - loss: 1.8774 - regression_loss: 1.3966 - classification_loss: 0.4808\n",
      "438/500 [=========================>....] - ETA: 1:03 - loss: 1.8772 - regression_loss: 1.3964 - classification_loss: 0.4808\n",
      "439/500 [=========================>....] - ETA: 1:02 - loss: 1.8768 - regression_loss: 1.3961 - classification_loss: 0.4807\n",
      "440/500 [=========================>....] - ETA: 1:01 - loss: 1.8759 - regression_loss: 1.3954 - classification_loss: 0.4805\n",
      "441/500 [=========================>....] - ETA: 1:00 - loss: 1.8766 - regression_loss: 1.3962 - classification_loss: 0.4804\n",
      "442/500 [=========================>....] - ETA: 59s - loss: 1.8756 - regression_loss: 1.3954 - classification_loss: 0.4801 \n",
      "443/500 [=========================>....] - ETA: 58s - loss: 1.8762 - regression_loss: 1.3956 - classification_loss: 0.4805\n",
      "444/500 [=========================>....] - ETA: 57s - loss: 1.8780 - regression_loss: 1.3970 - classification_loss: 0.4811\n",
      "445/500 [=========================>....] - ETA: 56s - loss: 1.8776 - regression_loss: 1.3966 - classification_loss: 0.4810\n",
      "446/500 [=========================>....] - ETA: 55s - loss: 1.8789 - regression_loss: 1.3977 - classification_loss: 0.4812\n",
      "447/500 [=========================>....] - ETA: 54s - loss: 1.8794 - regression_loss: 1.3980 - classification_loss: 0.4814\n",
      "448/500 [=========================>....] - ETA: 53s - loss: 1.8804 - regression_loss: 1.3986 - classification_loss: 0.4818\n",
      "449/500 [=========================>....] - ETA: 52s - loss: 1.8792 - regression_loss: 1.3978 - classification_loss: 0.4814\n",
      "450/500 [==========================>...] - ETA: 51s - loss: 1.8778 - regression_loss: 1.3969 - classification_loss: 0.4808\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 1.8776 - regression_loss: 1.3969 - classification_loss: 0.4808\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 1.8763 - regression_loss: 1.3959 - classification_loss: 0.4804\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.8750 - regression_loss: 1.3949 - classification_loss: 0.4801\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.8750 - regression_loss: 1.3948 - classification_loss: 0.4802\n",
      "455/500 [==========================>...] - ETA: 45s - loss: 1.8749 - regression_loss: 1.3944 - classification_loss: 0.4805\n",
      "456/500 [==========================>...] - ETA: 44s - loss: 1.8736 - regression_loss: 1.3936 - classification_loss: 0.4800\n",
      "457/500 [==========================>...] - ETA: 43s - loss: 1.8735 - regression_loss: 1.3935 - classification_loss: 0.4801\n",
      "458/500 [==========================>...] - ETA: 42s - loss: 1.8745 - regression_loss: 1.3940 - classification_loss: 0.4805\n",
      "459/500 [==========================>...] - ETA: 41s - loss: 1.8740 - regression_loss: 1.3937 - classification_loss: 0.4802\n",
      "460/500 [==========================>...] - ETA: 40s - loss: 1.8748 - regression_loss: 1.3943 - classification_loss: 0.4805\n",
      "461/500 [==========================>...] - ETA: 39s - loss: 1.8748 - regression_loss: 1.3941 - classification_loss: 0.4807\n",
      "462/500 [==========================>...] - ETA: 38s - loss: 1.8775 - regression_loss: 1.3958 - classification_loss: 0.4817\n",
      "463/500 [==========================>...] - ETA: 37s - loss: 1.8777 - regression_loss: 1.3958 - classification_loss: 0.4820\n",
      "464/500 [==========================>...] - ETA: 36s - loss: 1.8784 - regression_loss: 1.3964 - classification_loss: 0.4820\n",
      "465/500 [==========================>...] - ETA: 35s - loss: 1.8793 - regression_loss: 1.3973 - classification_loss: 0.4820\n",
      "466/500 [==========================>...] - ETA: 34s - loss: 1.8794 - regression_loss: 1.3977 - classification_loss: 0.4817\n",
      "467/500 [===========================>..] - ETA: 33s - loss: 1.8799 - regression_loss: 1.3981 - classification_loss: 0.4818\n",
      "468/500 [===========================>..] - ETA: 32s - loss: 1.8787 - regression_loss: 1.3971 - classification_loss: 0.4816\n",
      "469/500 [===========================>..] - ETA: 31s - loss: 1.8785 - regression_loss: 1.3969 - classification_loss: 0.4816\n",
      "470/500 [===========================>..] - ETA: 30s - loss: 1.8793 - regression_loss: 1.3976 - classification_loss: 0.4817\n",
      "471/500 [===========================>..] - ETA: 29s - loss: 1.8804 - regression_loss: 1.3982 - classification_loss: 0.4822\n",
      "472/500 [===========================>..] - ETA: 28s - loss: 1.8814 - regression_loss: 1.3991 - classification_loss: 0.4823\n",
      "473/500 [===========================>..] - ETA: 27s - loss: 1.8826 - regression_loss: 1.4001 - classification_loss: 0.4825\n",
      "474/500 [===========================>..] - ETA: 26s - loss: 1.8834 - regression_loss: 1.4006 - classification_loss: 0.4827\n",
      "475/500 [===========================>..] - ETA: 25s - loss: 1.8827 - regression_loss: 1.4001 - classification_loss: 0.4825\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 1.8825 - regression_loss: 1.4003 - classification_loss: 0.4822\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.8818 - regression_loss: 1.3993 - classification_loss: 0.4825\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.8803 - regression_loss: 1.3981 - classification_loss: 0.4821\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.8800 - regression_loss: 1.3978 - classification_loss: 0.4822\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.8797 - regression_loss: 1.3979 - classification_loss: 0.4819\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.8797 - regression_loss: 1.3978 - classification_loss: 0.4819\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.8799 - regression_loss: 1.3980 - classification_loss: 0.4819\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.8797 - regression_loss: 1.3980 - classification_loss: 0.4817\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.8808 - regression_loss: 1.3986 - classification_loss: 0.4822\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.8827 - regression_loss: 1.3997 - classification_loss: 0.4830\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.8829 - regression_loss: 1.4002 - classification_loss: 0.4827\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.8841 - regression_loss: 1.4009 - classification_loss: 0.4831\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.8829 - regression_loss: 1.4002 - classification_loss: 0.4827\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.8840 - regression_loss: 1.4010 - classification_loss: 0.4830\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.8842 - regression_loss: 1.4013 - classification_loss: 0.4829\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.8863 - regression_loss: 1.4030 - classification_loss: 0.4833 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.8855 - regression_loss: 1.4022 - classification_loss: 0.4834\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.8850 - regression_loss: 1.4019 - classification_loss: 0.4831\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.8878 - regression_loss: 1.4037 - classification_loss: 0.4841\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.8864 - regression_loss: 1.4026 - classification_loss: 0.4838\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.8878 - regression_loss: 1.4036 - classification_loss: 0.4842\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.8876 - regression_loss: 1.4036 - classification_loss: 0.4840\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.8866 - regression_loss: 1.4030 - classification_loss: 0.4836\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.8868 - regression_loss: 1.4034 - classification_loss: 0.4834\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.8866 - regression_loss: 1.4035 - classification_loss: 0.4831\n",
      "Epoch 00044: saving model to ./snapshots\\resnet50_csv_44.h5\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 9.999999010570977e-19.\n",
      "\n",
      "500/500 [==============================] - 512s 1s/step - loss: 1.8866 - regression_loss: 1.4035 - classification_loss: 0.4831\n",
      "Epoch 45/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 1.9513 - regression_loss: 1.4859 - classification_loss: 0.4654\n",
      "  2/500 [..............................] - ETA: 4:04 - loss: 2.3155 - regression_loss: 1.7605 - classification_loss: 0.5549\n",
      "  3/500 [..............................] - ETA: 5:26 - loss: 1.9597 - regression_loss: 1.5099 - classification_loss: 0.4498\n",
      "  4/500 [..............................] - ETA: 6:18 - loss: 2.0279 - regression_loss: 1.5665 - classification_loss: 0.4614\n",
      "  5/500 [..............................] - ETA: 6:27 - loss: 2.0998 - regression_loss: 1.6299 - classification_loss: 0.4698\n",
      "  6/500 [..............................] - ETA: 6:41 - loss: 2.0551 - regression_loss: 1.5565 - classification_loss: 0.4986\n",
      "  7/500 [..............................] - ETA: 7:02 - loss: 2.0502 - regression_loss: 1.5544 - classification_loss: 0.4958\n",
      "  8/500 [..............................] - ETA: 7:15 - loss: 2.0547 - regression_loss: 1.5614 - classification_loss: 0.4933\n",
      "  9/500 [..............................] - ETA: 7:17 - loss: 1.9588 - regression_loss: 1.4825 - classification_loss: 0.4763\n",
      " 10/500 [..............................] - ETA: 7:24 - loss: 1.8397 - regression_loss: 1.3969 - classification_loss: 0.4428\n",
      " 11/500 [..............................] - ETA: 7:26 - loss: 1.8734 - regression_loss: 1.4232 - classification_loss: 0.4503\n",
      " 12/500 [..............................] - ETA: 7:28 - loss: 1.8457 - regression_loss: 1.4013 - classification_loss: 0.4444\n",
      " 13/500 [..............................] - ETA: 7:30 - loss: 1.8811 - regression_loss: 1.4307 - classification_loss: 0.4505\n",
      " 14/500 [..............................] - ETA: 7:26 - loss: 1.8602 - regression_loss: 1.4084 - classification_loss: 0.4519\n",
      " 15/500 [..............................] - ETA: 7:31 - loss: 1.8543 - regression_loss: 1.4055 - classification_loss: 0.4488\n",
      " 16/500 [..............................] - ETA: 7:35 - loss: 1.8609 - regression_loss: 1.4121 - classification_loss: 0.4488\n",
      " 17/500 [>.............................] - ETA: 7:39 - loss: 1.8235 - regression_loss: 1.3840 - classification_loss: 0.4395\n",
      " 18/500 [>.............................] - ETA: 7:42 - loss: 1.8051 - regression_loss: 1.3719 - classification_loss: 0.4332\n",
      " 19/500 [>.............................] - ETA: 7:41 - loss: 1.7603 - regression_loss: 1.3365 - classification_loss: 0.4237\n",
      " 20/500 [>.............................] - ETA: 7:41 - loss: 1.8114 - regression_loss: 1.3674 - classification_loss: 0.4440\n",
      " 21/500 [>.............................] - ETA: 7:43 - loss: 1.7963 - regression_loss: 1.3548 - classification_loss: 0.4415\n",
      " 22/500 [>.............................] - ETA: 7:42 - loss: 1.8038 - regression_loss: 1.3625 - classification_loss: 0.4413\n",
      " 23/500 [>.............................] - ETA: 7:42 - loss: 1.8086 - regression_loss: 1.3601 - classification_loss: 0.4485\n",
      " 24/500 [>.............................] - ETA: 7:44 - loss: 1.8404 - regression_loss: 1.3883 - classification_loss: 0.4521\n",
      " 25/500 [>.............................] - ETA: 7:43 - loss: 1.8177 - regression_loss: 1.3734 - classification_loss: 0.4442\n",
      " 26/500 [>.............................] - ETA: 7:42 - loss: 1.8782 - regression_loss: 1.4102 - classification_loss: 0.4680\n",
      " 27/500 [>.............................] - ETA: 7:41 - loss: 1.8822 - regression_loss: 1.4161 - classification_loss: 0.4660\n",
      " 28/500 [>.............................] - ETA: 7:43 - loss: 1.8711 - regression_loss: 1.4091 - classification_loss: 0.4619\n",
      " 29/500 [>.............................] - ETA: 7:38 - loss: 1.8499 - regression_loss: 1.3959 - classification_loss: 0.4540\n",
      " 30/500 [>.............................] - ETA: 7:40 - loss: 1.8733 - regression_loss: 1.4103 - classification_loss: 0.4630\n",
      " 31/500 [>.............................] - ETA: 7:39 - loss: 1.8626 - regression_loss: 1.4014 - classification_loss: 0.4612\n",
      " 32/500 [>.............................] - ETA: 7:38 - loss: 1.8425 - regression_loss: 1.3911 - classification_loss: 0.4514\n",
      " 33/500 [>.............................] - ETA: 7:34 - loss: 1.8284 - regression_loss: 1.3828 - classification_loss: 0.4457\n",
      " 34/500 [=>............................] - ETA: 7:36 - loss: 1.8517 - regression_loss: 1.3987 - classification_loss: 0.4529\n",
      " 35/500 [=>............................] - ETA: 7:35 - loss: 1.8372 - regression_loss: 1.3902 - classification_loss: 0.4471\n",
      " 36/500 [=>............................] - ETA: 7:34 - loss: 1.8357 - regression_loss: 1.3901 - classification_loss: 0.4456\n",
      " 37/500 [=>............................] - ETA: 7:34 - loss: 1.8301 - regression_loss: 1.3885 - classification_loss: 0.4417\n",
      " 38/500 [=>............................] - ETA: 7:35 - loss: 1.8557 - regression_loss: 1.4025 - classification_loss: 0.4532\n",
      " 39/500 [=>............................] - ETA: 7:34 - loss: 1.8573 - regression_loss: 1.4053 - classification_loss: 0.4521\n",
      " 40/500 [=>............................] - ETA: 7:34 - loss: 1.8776 - regression_loss: 1.4161 - classification_loss: 0.4615\n",
      " 41/500 [=>............................] - ETA: 7:34 - loss: 1.8735 - regression_loss: 1.4104 - classification_loss: 0.4631\n",
      " 42/500 [=>............................] - ETA: 7:33 - loss: 1.8955 - regression_loss: 1.4217 - classification_loss: 0.4738\n",
      " 43/500 [=>............................] - ETA: 7:33 - loss: 1.8894 - regression_loss: 1.4171 - classification_loss: 0.4723\n",
      " 44/500 [=>............................] - ETA: 7:31 - loss: 1.8921 - regression_loss: 1.4210 - classification_loss: 0.4711\n",
      " 45/500 [=>............................] - ETA: 7:31 - loss: 1.8840 - regression_loss: 1.4160 - classification_loss: 0.4680\n",
      " 46/500 [=>............................] - ETA: 7:31 - loss: 1.8781 - regression_loss: 1.4081 - classification_loss: 0.4701\n",
      " 47/500 [=>............................] - ETA: 7:30 - loss: 1.8854 - regression_loss: 1.4172 - classification_loss: 0.4683\n",
      " 48/500 [=>............................] - ETA: 7:30 - loss: 1.8733 - regression_loss: 1.4104 - classification_loss: 0.4630\n",
      " 49/500 [=>............................] - ETA: 7:30 - loss: 1.8875 - regression_loss: 1.4189 - classification_loss: 0.4686\n",
      " 50/500 [==>...........................] - ETA: 7:29 - loss: 1.8967 - regression_loss: 1.4285 - classification_loss: 0.4682\n",
      " 51/500 [==>...........................] - ETA: 7:29 - loss: 1.8937 - regression_loss: 1.4247 - classification_loss: 0.4690\n",
      " 52/500 [==>...........................] - ETA: 7:29 - loss: 1.8810 - regression_loss: 1.4141 - classification_loss: 0.4669\n",
      " 53/500 [==>...........................] - ETA: 7:28 - loss: 1.8847 - regression_loss: 1.4125 - classification_loss: 0.4722\n",
      " 54/500 [==>...........................] - ETA: 7:27 - loss: 1.8909 - regression_loss: 1.4134 - classification_loss: 0.4775\n",
      " 55/500 [==>...........................] - ETA: 7:24 - loss: 1.8948 - regression_loss: 1.4159 - classification_loss: 0.4789\n",
      " 56/500 [==>...........................] - ETA: 7:25 - loss: 1.8834 - regression_loss: 1.4084 - classification_loss: 0.4750\n",
      " 57/500 [==>...........................] - ETA: 7:25 - loss: 1.8940 - regression_loss: 1.4168 - classification_loss: 0.4771\n",
      " 58/500 [==>...........................] - ETA: 7:25 - loss: 1.9010 - regression_loss: 1.4232 - classification_loss: 0.4778\n",
      " 59/500 [==>...........................] - ETA: 7:24 - loss: 1.9009 - regression_loss: 1.4231 - classification_loss: 0.4779\n",
      " 60/500 [==>...........................] - ETA: 7:23 - loss: 1.9047 - regression_loss: 1.4271 - classification_loss: 0.4776\n",
      " 61/500 [==>...........................] - ETA: 7:22 - loss: 1.9025 - regression_loss: 1.4230 - classification_loss: 0.4794\n",
      " 62/500 [==>...........................] - ETA: 7:21 - loss: 1.8966 - regression_loss: 1.4216 - classification_loss: 0.4751\n",
      " 63/500 [==>...........................] - ETA: 7:20 - loss: 1.8814 - regression_loss: 1.4102 - classification_loss: 0.4712\n",
      " 64/500 [==>...........................] - ETA: 7:20 - loss: 1.8820 - regression_loss: 1.4102 - classification_loss: 0.4718\n",
      " 65/500 [==>...........................] - ETA: 7:19 - loss: 1.8921 - regression_loss: 1.4182 - classification_loss: 0.4738\n",
      " 66/500 [==>...........................] - ETA: 7:19 - loss: 1.8869 - regression_loss: 1.4144 - classification_loss: 0.4724\n",
      " 67/500 [===>..........................] - ETA: 7:18 - loss: 1.8872 - regression_loss: 1.4161 - classification_loss: 0.4711\n",
      " 68/500 [===>..........................] - ETA: 7:16 - loss: 1.8830 - regression_loss: 1.4097 - classification_loss: 0.4733\n",
      " 69/500 [===>..........................] - ETA: 7:16 - loss: 1.8899 - regression_loss: 1.4163 - classification_loss: 0.4736\n",
      " 70/500 [===>..........................] - ETA: 7:16 - loss: 1.8896 - regression_loss: 1.4154 - classification_loss: 0.4742\n",
      " 71/500 [===>..........................] - ETA: 7:15 - loss: 1.9016 - regression_loss: 1.4251 - classification_loss: 0.4765\n",
      " 72/500 [===>..........................] - ETA: 7:14 - loss: 1.9065 - regression_loss: 1.4270 - classification_loss: 0.4794\n",
      " 73/500 [===>..........................] - ETA: 7:14 - loss: 1.9114 - regression_loss: 1.4328 - classification_loss: 0.4786\n",
      " 74/500 [===>..........................] - ETA: 7:13 - loss: 1.9053 - regression_loss: 1.4284 - classification_loss: 0.4769\n",
      " 75/500 [===>..........................] - ETA: 7:13 - loss: 1.9176 - regression_loss: 1.4364 - classification_loss: 0.4812\n",
      " 76/500 [===>..........................] - ETA: 7:12 - loss: 1.9172 - regression_loss: 1.4351 - classification_loss: 0.4821\n",
      " 77/500 [===>..........................] - ETA: 7:11 - loss: 1.9313 - regression_loss: 1.4432 - classification_loss: 0.4881\n",
      " 78/500 [===>..........................] - ETA: 7:09 - loss: 1.9413 - regression_loss: 1.4514 - classification_loss: 0.4899\n",
      " 79/500 [===>..........................] - ETA: 7:08 - loss: 1.9419 - regression_loss: 1.4520 - classification_loss: 0.4899\n",
      " 80/500 [===>..........................] - ETA: 7:07 - loss: 1.9330 - regression_loss: 1.4445 - classification_loss: 0.4885\n",
      " 81/500 [===>..........................] - ETA: 7:07 - loss: 1.9311 - regression_loss: 1.4405 - classification_loss: 0.4906\n",
      " 82/500 [===>..........................] - ETA: 7:04 - loss: 1.9273 - regression_loss: 1.4370 - classification_loss: 0.4902\n",
      " 83/500 [===>..........................] - ETA: 7:04 - loss: 1.9300 - regression_loss: 1.4409 - classification_loss: 0.4892\n",
      " 84/500 [====>.........................] - ETA: 7:04 - loss: 1.9364 - regression_loss: 1.4442 - classification_loss: 0.4922\n",
      " 85/500 [====>.........................] - ETA: 7:03 - loss: 1.9397 - regression_loss: 1.4468 - classification_loss: 0.4929\n",
      " 86/500 [====>.........................] - ETA: 7:01 - loss: 1.9424 - regression_loss: 1.4473 - classification_loss: 0.4951\n",
      " 87/500 [====>.........................] - ETA: 7:00 - loss: 1.9350 - regression_loss: 1.4423 - classification_loss: 0.4927\n",
      " 88/500 [====>.........................] - ETA: 6:59 - loss: 1.9302 - regression_loss: 1.4381 - classification_loss: 0.4921\n",
      " 89/500 [====>.........................] - ETA: 6:58 - loss: 1.9318 - regression_loss: 1.4398 - classification_loss: 0.4920\n",
      " 90/500 [====>.........................] - ETA: 6:57 - loss: 1.9282 - regression_loss: 1.4382 - classification_loss: 0.4900\n",
      " 91/500 [====>.........................] - ETA: 6:55 - loss: 1.9287 - regression_loss: 1.4382 - classification_loss: 0.4905\n",
      " 92/500 [====>.........................] - ETA: 6:54 - loss: 1.9292 - regression_loss: 1.4392 - classification_loss: 0.4900\n",
      " 93/500 [====>.........................] - ETA: 6:53 - loss: 1.9284 - regression_loss: 1.4363 - classification_loss: 0.4921\n",
      " 94/500 [====>.........................] - ETA: 6:52 - loss: 1.9273 - regression_loss: 1.4334 - classification_loss: 0.4939\n",
      " 95/500 [====>.........................] - ETA: 6:51 - loss: 1.9190 - regression_loss: 1.4280 - classification_loss: 0.4910\n",
      " 96/500 [====>.........................] - ETA: 6:49 - loss: 1.9254 - regression_loss: 1.4333 - classification_loss: 0.4921\n",
      " 97/500 [====>.........................] - ETA: 6:49 - loss: 1.9225 - regression_loss: 1.4316 - classification_loss: 0.4908\n",
      " 98/500 [====>.........................] - ETA: 6:48 - loss: 1.9245 - regression_loss: 1.4339 - classification_loss: 0.4905\n",
      " 99/500 [====>.........................] - ETA: 6:47 - loss: 1.9211 - regression_loss: 1.4320 - classification_loss: 0.4892\n",
      "100/500 [=====>........................] - ETA: 6:47 - loss: 1.9195 - regression_loss: 1.4317 - classification_loss: 0.4879\n",
      "101/500 [=====>........................] - ETA: 6:46 - loss: 1.9163 - regression_loss: 1.4297 - classification_loss: 0.4867\n",
      "102/500 [=====>........................] - ETA: 6:45 - loss: 1.9229 - regression_loss: 1.4356 - classification_loss: 0.4874\n",
      "103/500 [=====>........................] - ETA: 6:44 - loss: 1.9307 - regression_loss: 1.4402 - classification_loss: 0.4905\n",
      "104/500 [=====>........................] - ETA: 6:42 - loss: 1.9313 - regression_loss: 1.4416 - classification_loss: 0.4897\n",
      "105/500 [=====>........................] - ETA: 6:41 - loss: 1.9309 - regression_loss: 1.4406 - classification_loss: 0.4903\n",
      "106/500 [=====>........................] - ETA: 6:40 - loss: 1.9291 - regression_loss: 1.4396 - classification_loss: 0.4894\n",
      "107/500 [=====>........................] - ETA: 6:39 - loss: 1.9257 - regression_loss: 1.4386 - classification_loss: 0.4871\n",
      "108/500 [=====>........................] - ETA: 6:37 - loss: 1.9332 - regression_loss: 1.4428 - classification_loss: 0.4905\n",
      "109/500 [=====>........................] - ETA: 6:37 - loss: 1.9364 - regression_loss: 1.4417 - classification_loss: 0.4947\n",
      "110/500 [=====>........................] - ETA: 6:36 - loss: 1.9334 - regression_loss: 1.4391 - classification_loss: 0.4942\n",
      "111/500 [=====>........................] - ETA: 6:35 - loss: 1.9316 - regression_loss: 1.4373 - classification_loss: 0.4943\n",
      "112/500 [=====>........................] - ETA: 6:34 - loss: 1.9320 - regression_loss: 1.4375 - classification_loss: 0.4944\n",
      "113/500 [=====>........................] - ETA: 6:33 - loss: 1.9263 - regression_loss: 1.4342 - classification_loss: 0.4920\n",
      "114/500 [=====>........................] - ETA: 6:32 - loss: 1.9278 - regression_loss: 1.4363 - classification_loss: 0.4914\n",
      "115/500 [=====>........................] - ETA: 6:31 - loss: 1.9225 - regression_loss: 1.4325 - classification_loss: 0.4900\n",
      "116/500 [=====>........................] - ETA: 6:31 - loss: 1.9131 - regression_loss: 1.4251 - classification_loss: 0.4880\n",
      "117/500 [======>.......................] - ETA: 6:29 - loss: 1.9082 - regression_loss: 1.4221 - classification_loss: 0.4861\n",
      "118/500 [======>.......................] - ETA: 6:28 - loss: 1.9058 - regression_loss: 1.4195 - classification_loss: 0.4863\n",
      "119/500 [======>.......................] - ETA: 6:28 - loss: 1.8999 - regression_loss: 1.4158 - classification_loss: 0.4841\n",
      "120/500 [======>.......................] - ETA: 6:26 - loss: 1.8984 - regression_loss: 1.4146 - classification_loss: 0.4838\n",
      "121/500 [======>.......................] - ETA: 6:25 - loss: 1.8966 - regression_loss: 1.4133 - classification_loss: 0.4833\n",
      "122/500 [======>.......................] - ETA: 6:25 - loss: 1.8983 - regression_loss: 1.4151 - classification_loss: 0.4832\n",
      "123/500 [======>.......................] - ETA: 6:24 - loss: 1.9006 - regression_loss: 1.4166 - classification_loss: 0.4840\n",
      "124/500 [======>.......................] - ETA: 6:23 - loss: 1.9034 - regression_loss: 1.4189 - classification_loss: 0.4844\n",
      "125/500 [======>.......................] - ETA: 6:22 - loss: 1.9001 - regression_loss: 1.4172 - classification_loss: 0.4830\n",
      "126/500 [======>.......................] - ETA: 6:20 - loss: 1.8964 - regression_loss: 1.4143 - classification_loss: 0.4820\n",
      "127/500 [======>.......................] - ETA: 6:19 - loss: 1.8952 - regression_loss: 1.4149 - classification_loss: 0.4803\n",
      "128/500 [======>.......................] - ETA: 6:18 - loss: 1.8881 - regression_loss: 1.4094 - classification_loss: 0.4787\n",
      "129/500 [======>.......................] - ETA: 6:17 - loss: 1.8914 - regression_loss: 1.4100 - classification_loss: 0.4814\n",
      "130/500 [======>.......................] - ETA: 6:16 - loss: 1.8889 - regression_loss: 1.4092 - classification_loss: 0.4797\n",
      "131/500 [======>.......................] - ETA: 6:15 - loss: 1.8843 - regression_loss: 1.4062 - classification_loss: 0.4780\n",
      "132/500 [======>.......................] - ETA: 6:14 - loss: 1.8807 - regression_loss: 1.4036 - classification_loss: 0.4771\n",
      "133/500 [======>.......................] - ETA: 6:13 - loss: 1.8795 - regression_loss: 1.4024 - classification_loss: 0.4771\n",
      "134/500 [=======>......................] - ETA: 6:12 - loss: 1.8803 - regression_loss: 1.4028 - classification_loss: 0.4775\n",
      "135/500 [=======>......................] - ETA: 6:11 - loss: 1.8857 - regression_loss: 1.4056 - classification_loss: 0.4801\n",
      "136/500 [=======>......................] - ETA: 6:10 - loss: 1.8831 - regression_loss: 1.4039 - classification_loss: 0.4793\n",
      "137/500 [=======>......................] - ETA: 6:10 - loss: 1.8827 - regression_loss: 1.4031 - classification_loss: 0.4796\n",
      "138/500 [=======>......................] - ETA: 6:09 - loss: 1.8831 - regression_loss: 1.4041 - classification_loss: 0.4789\n",
      "139/500 [=======>......................] - ETA: 6:08 - loss: 1.8822 - regression_loss: 1.4031 - classification_loss: 0.4791\n",
      "140/500 [=======>......................] - ETA: 6:07 - loss: 1.8830 - regression_loss: 1.4050 - classification_loss: 0.4781\n",
      "141/500 [=======>......................] - ETA: 6:05 - loss: 1.8852 - regression_loss: 1.4065 - classification_loss: 0.4787\n",
      "142/500 [=======>......................] - ETA: 6:05 - loss: 1.8803 - regression_loss: 1.4027 - classification_loss: 0.4776\n",
      "143/500 [=======>......................] - ETA: 6:04 - loss: 1.8826 - regression_loss: 1.4038 - classification_loss: 0.4788\n",
      "144/500 [=======>......................] - ETA: 6:03 - loss: 1.8811 - regression_loss: 1.4034 - classification_loss: 0.4777\n",
      "145/500 [=======>......................] - ETA: 6:02 - loss: 1.8880 - regression_loss: 1.4073 - classification_loss: 0.4807\n",
      "146/500 [=======>......................] - ETA: 6:01 - loss: 1.8940 - regression_loss: 1.4120 - classification_loss: 0.4820\n",
      "147/500 [=======>......................] - ETA: 6:01 - loss: 1.8966 - regression_loss: 1.4143 - classification_loss: 0.4823\n",
      "148/500 [=======>......................] - ETA: 6:00 - loss: 1.8973 - regression_loss: 1.4144 - classification_loss: 0.4829\n",
      "149/500 [=======>......................] - ETA: 5:59 - loss: 1.8935 - regression_loss: 1.4118 - classification_loss: 0.4817\n",
      "150/500 [========>.....................] - ETA: 5:58 - loss: 1.8980 - regression_loss: 1.4131 - classification_loss: 0.4848\n",
      "151/500 [========>.....................] - ETA: 5:57 - loss: 1.8944 - regression_loss: 1.4111 - classification_loss: 0.4833\n",
      "152/500 [========>.....................] - ETA: 5:56 - loss: 1.8955 - regression_loss: 1.4126 - classification_loss: 0.4829\n",
      "153/500 [========>.....................] - ETA: 5:55 - loss: 1.8929 - regression_loss: 1.4101 - classification_loss: 0.4828\n",
      "154/500 [========>.....................] - ETA: 5:54 - loss: 1.8967 - regression_loss: 1.4116 - classification_loss: 0.4851\n",
      "155/500 [========>.....................] - ETA: 5:53 - loss: 1.8923 - regression_loss: 1.4084 - classification_loss: 0.4839\n",
      "156/500 [========>.....................] - ETA: 5:52 - loss: 1.8884 - regression_loss: 1.4057 - classification_loss: 0.4827\n",
      "157/500 [========>.....................] - ETA: 5:51 - loss: 1.8887 - regression_loss: 1.4057 - classification_loss: 0.4830\n",
      "158/500 [========>.....................] - ETA: 5:50 - loss: 1.8903 - regression_loss: 1.4066 - classification_loss: 0.4837\n",
      "159/500 [========>.....................] - ETA: 5:48 - loss: 1.8840 - regression_loss: 1.4019 - classification_loss: 0.4821\n",
      "160/500 [========>.....................] - ETA: 5:48 - loss: 1.8800 - regression_loss: 1.3992 - classification_loss: 0.4808\n",
      "161/500 [========>.....................] - ETA: 5:47 - loss: 1.8759 - regression_loss: 1.3965 - classification_loss: 0.4793\n",
      "162/500 [========>.....................] - ETA: 5:46 - loss: 1.8734 - regression_loss: 1.3948 - classification_loss: 0.4786\n",
      "163/500 [========>.....................] - ETA: 5:45 - loss: 1.8775 - regression_loss: 1.3972 - classification_loss: 0.4802\n",
      "164/500 [========>.....................] - ETA: 5:44 - loss: 1.8749 - regression_loss: 1.3959 - classification_loss: 0.4790\n",
      "165/500 [========>.....................] - ETA: 5:43 - loss: 1.8752 - regression_loss: 1.3970 - classification_loss: 0.4782\n",
      "166/500 [========>.....................] - ETA: 5:42 - loss: 1.8790 - regression_loss: 1.3995 - classification_loss: 0.4795\n",
      "167/500 [=========>....................] - ETA: 5:41 - loss: 1.8801 - regression_loss: 1.4001 - classification_loss: 0.4801\n",
      "168/500 [=========>....................] - ETA: 5:40 - loss: 1.8767 - regression_loss: 1.3976 - classification_loss: 0.4791\n",
      "169/500 [=========>....................] - ETA: 5:39 - loss: 1.8762 - regression_loss: 1.3977 - classification_loss: 0.4784\n",
      "170/500 [=========>....................] - ETA: 5:38 - loss: 1.8811 - regression_loss: 1.4019 - classification_loss: 0.4792\n",
      "171/500 [=========>....................] - ETA: 5:37 - loss: 1.8821 - regression_loss: 1.4024 - classification_loss: 0.4797\n",
      "172/500 [=========>....................] - ETA: 5:36 - loss: 1.8832 - regression_loss: 1.4020 - classification_loss: 0.4811\n",
      "173/500 [=========>....................] - ETA: 5:35 - loss: 1.8876 - regression_loss: 1.4057 - classification_loss: 0.4819\n",
      "174/500 [=========>....................] - ETA: 5:34 - loss: 1.8934 - regression_loss: 1.4094 - classification_loss: 0.4841\n",
      "175/500 [=========>....................] - ETA: 5:33 - loss: 1.8938 - regression_loss: 1.4105 - classification_loss: 0.4832\n",
      "176/500 [=========>....................] - ETA: 5:31 - loss: 1.8960 - regression_loss: 1.4108 - classification_loss: 0.4852\n",
      "177/500 [=========>....................] - ETA: 5:30 - loss: 1.8984 - regression_loss: 1.4123 - classification_loss: 0.4861\n",
      "178/500 [=========>....................] - ETA: 5:29 - loss: 1.8990 - regression_loss: 1.4137 - classification_loss: 0.4854\n",
      "179/500 [=========>....................] - ETA: 5:28 - loss: 1.9061 - regression_loss: 1.4178 - classification_loss: 0.4883\n",
      "180/500 [=========>....................] - ETA: 5:27 - loss: 1.9114 - regression_loss: 1.4210 - classification_loss: 0.4904\n",
      "181/500 [=========>....................] - ETA: 5:26 - loss: 1.9082 - regression_loss: 1.4186 - classification_loss: 0.4895\n",
      "182/500 [=========>....................] - ETA: 5:26 - loss: 1.9095 - regression_loss: 1.4202 - classification_loss: 0.4892\n",
      "183/500 [=========>....................] - ETA: 5:25 - loss: 1.9145 - regression_loss: 1.4223 - classification_loss: 0.4922\n",
      "184/500 [==========>...................] - ETA: 5:24 - loss: 1.9102 - regression_loss: 1.4192 - classification_loss: 0.4910\n",
      "185/500 [==========>...................] - ETA: 5:23 - loss: 1.9125 - regression_loss: 1.4189 - classification_loss: 0.4936\n",
      "186/500 [==========>...................] - ETA: 5:22 - loss: 1.9108 - regression_loss: 1.4181 - classification_loss: 0.4927\n",
      "187/500 [==========>...................] - ETA: 5:21 - loss: 1.9138 - regression_loss: 1.4199 - classification_loss: 0.4939\n",
      "188/500 [==========>...................] - ETA: 5:20 - loss: 1.9134 - regression_loss: 1.4195 - classification_loss: 0.4939\n",
      "189/500 [==========>...................] - ETA: 5:19 - loss: 1.9170 - regression_loss: 1.4218 - classification_loss: 0.4952\n",
      "190/500 [==========>...................] - ETA: 5:18 - loss: 1.9178 - regression_loss: 1.4228 - classification_loss: 0.4950\n",
      "191/500 [==========>...................] - ETA: 5:17 - loss: 1.9160 - regression_loss: 1.4212 - classification_loss: 0.4948\n",
      "192/500 [==========>...................] - ETA: 5:16 - loss: 1.9159 - regression_loss: 1.4214 - classification_loss: 0.4945\n",
      "193/500 [==========>...................] - ETA: 5:15 - loss: 1.9178 - regression_loss: 1.4227 - classification_loss: 0.4951\n",
      "194/500 [==========>...................] - ETA: 5:14 - loss: 1.9189 - regression_loss: 1.4236 - classification_loss: 0.4953\n",
      "195/500 [==========>...................] - ETA: 5:13 - loss: 1.9221 - regression_loss: 1.4261 - classification_loss: 0.4961\n",
      "196/500 [==========>...................] - ETA: 5:12 - loss: 1.9195 - regression_loss: 1.4244 - classification_loss: 0.4951\n",
      "197/500 [==========>...................] - ETA: 5:11 - loss: 1.9189 - regression_loss: 1.4242 - classification_loss: 0.4947\n",
      "198/500 [==========>...................] - ETA: 5:10 - loss: 1.9164 - regression_loss: 1.4225 - classification_loss: 0.4940\n",
      "199/500 [==========>...................] - ETA: 5:09 - loss: 1.9156 - regression_loss: 1.4224 - classification_loss: 0.4932\n",
      "200/500 [===========>..................] - ETA: 5:08 - loss: 1.9172 - regression_loss: 1.4231 - classification_loss: 0.4940\n",
      "201/500 [===========>..................] - ETA: 5:07 - loss: 1.9183 - regression_loss: 1.4239 - classification_loss: 0.4944\n",
      "202/500 [===========>..................] - ETA: 5:06 - loss: 1.9169 - regression_loss: 1.4229 - classification_loss: 0.4940\n",
      "203/500 [===========>..................] - ETA: 5:05 - loss: 1.9194 - regression_loss: 1.4250 - classification_loss: 0.4945\n",
      "204/500 [===========>..................] - ETA: 5:04 - loss: 1.9156 - regression_loss: 1.4222 - classification_loss: 0.4933\n",
      "205/500 [===========>..................] - ETA: 5:03 - loss: 1.9125 - regression_loss: 1.4205 - classification_loss: 0.4921\n",
      "206/500 [===========>..................] - ETA: 5:02 - loss: 1.9101 - regression_loss: 1.4192 - classification_loss: 0.4910\n",
      "207/500 [===========>..................] - ETA: 5:01 - loss: 1.9071 - regression_loss: 1.4171 - classification_loss: 0.4900\n",
      "208/500 [===========>..................] - ETA: 5:00 - loss: 1.9064 - regression_loss: 1.4165 - classification_loss: 0.4899\n",
      "209/500 [===========>..................] - ETA: 4:59 - loss: 1.9117 - regression_loss: 1.4199 - classification_loss: 0.4919\n",
      "210/500 [===========>..................] - ETA: 4:58 - loss: 1.9152 - regression_loss: 1.4223 - classification_loss: 0.4928\n",
      "211/500 [===========>..................] - ETA: 4:57 - loss: 1.9197 - regression_loss: 1.4255 - classification_loss: 0.4941\n",
      "212/500 [===========>..................] - ETA: 4:56 - loss: 1.9167 - regression_loss: 1.4237 - classification_loss: 0.4930\n",
      "213/500 [===========>..................] - ETA: 4:55 - loss: 1.9214 - regression_loss: 1.4274 - classification_loss: 0.4940\n",
      "214/500 [===========>..................] - ETA: 4:54 - loss: 1.9244 - regression_loss: 1.4296 - classification_loss: 0.4948\n",
      "215/500 [===========>..................] - ETA: 4:53 - loss: 1.9274 - regression_loss: 1.4327 - classification_loss: 0.4947\n",
      "216/500 [===========>..................] - ETA: 4:52 - loss: 1.9241 - regression_loss: 1.4306 - classification_loss: 0.4935\n",
      "217/500 [============>.................] - ETA: 4:51 - loss: 1.9193 - regression_loss: 1.4270 - classification_loss: 0.4923\n",
      "218/500 [============>.................] - ETA: 4:50 - loss: 1.9204 - regression_loss: 1.4275 - classification_loss: 0.4929\n",
      "219/500 [============>.................] - ETA: 4:49 - loss: 1.9209 - regression_loss: 1.4281 - classification_loss: 0.4929\n",
      "220/500 [============>.................] - ETA: 4:48 - loss: 1.9229 - regression_loss: 1.4298 - classification_loss: 0.4932\n",
      "221/500 [============>.................] - ETA: 4:47 - loss: 1.9244 - regression_loss: 1.4308 - classification_loss: 0.4936\n",
      "222/500 [============>.................] - ETA: 4:46 - loss: 1.9233 - regression_loss: 1.4296 - classification_loss: 0.4938\n",
      "223/500 [============>.................] - ETA: 4:45 - loss: 1.9213 - regression_loss: 1.4283 - classification_loss: 0.4930\n",
      "224/500 [============>.................] - ETA: 4:44 - loss: 1.9263 - regression_loss: 1.4313 - classification_loss: 0.4950\n",
      "225/500 [============>.................] - ETA: 4:43 - loss: 1.9257 - regression_loss: 1.4313 - classification_loss: 0.4944\n",
      "226/500 [============>.................] - ETA: 4:42 - loss: 1.9249 - regression_loss: 1.4310 - classification_loss: 0.4940\n",
      "227/500 [============>.................] - ETA: 4:41 - loss: 1.9234 - regression_loss: 1.4299 - classification_loss: 0.4935\n",
      "228/500 [============>.................] - ETA: 4:40 - loss: 1.9239 - regression_loss: 1.4303 - classification_loss: 0.4937\n",
      "229/500 [============>.................] - ETA: 4:39 - loss: 1.9220 - regression_loss: 1.4286 - classification_loss: 0.4935\n",
      "230/500 [============>.................] - ETA: 4:38 - loss: 1.9214 - regression_loss: 1.4284 - classification_loss: 0.4930\n",
      "231/500 [============>.................] - ETA: 4:37 - loss: 1.9232 - regression_loss: 1.4297 - classification_loss: 0.4935\n",
      "232/500 [============>.................] - ETA: 4:36 - loss: 1.9197 - regression_loss: 1.4274 - classification_loss: 0.4923\n",
      "233/500 [============>.................] - ETA: 4:35 - loss: 1.9211 - regression_loss: 1.4282 - classification_loss: 0.4929\n",
      "234/500 [=============>................] - ETA: 4:34 - loss: 1.9200 - regression_loss: 1.4275 - classification_loss: 0.4925\n",
      "235/500 [=============>................] - ETA: 4:33 - loss: 1.9198 - regression_loss: 1.4279 - classification_loss: 0.4919\n",
      "236/500 [=============>................] - ETA: 4:32 - loss: 1.9169 - regression_loss: 1.4257 - classification_loss: 0.4912\n",
      "237/500 [=============>................] - ETA: 4:30 - loss: 1.9187 - regression_loss: 1.4265 - classification_loss: 0.4922\n",
      "238/500 [=============>................] - ETA: 4:29 - loss: 1.9212 - regression_loss: 1.4291 - classification_loss: 0.4921\n",
      "239/500 [=============>................] - ETA: 4:28 - loss: 1.9193 - regression_loss: 1.4278 - classification_loss: 0.4915\n",
      "240/500 [=============>................] - ETA: 4:27 - loss: 1.9163 - regression_loss: 1.4258 - classification_loss: 0.4905\n",
      "241/500 [=============>................] - ETA: 4:26 - loss: 1.9190 - regression_loss: 1.4286 - classification_loss: 0.4905\n",
      "242/500 [=============>................] - ETA: 4:25 - loss: 1.9224 - regression_loss: 1.4307 - classification_loss: 0.4917\n",
      "243/500 [=============>................] - ETA: 4:24 - loss: 1.9216 - regression_loss: 1.4302 - classification_loss: 0.4914\n",
      "244/500 [=============>................] - ETA: 4:23 - loss: 1.9189 - regression_loss: 1.4281 - classification_loss: 0.4908\n",
      "245/500 [=============>................] - ETA: 4:22 - loss: 1.9172 - regression_loss: 1.4269 - classification_loss: 0.4903\n",
      "246/500 [=============>................] - ETA: 4:21 - loss: 1.9181 - regression_loss: 1.4280 - classification_loss: 0.4901\n",
      "247/500 [=============>................] - ETA: 4:20 - loss: 1.9151 - regression_loss: 1.4257 - classification_loss: 0.4894\n",
      "248/500 [=============>................] - ETA: 4:19 - loss: 1.9129 - regression_loss: 1.4236 - classification_loss: 0.4893\n",
      "249/500 [=============>................] - ETA: 4:18 - loss: 1.9152 - regression_loss: 1.4259 - classification_loss: 0.4892\n",
      "250/500 [==============>...............] - ETA: 4:17 - loss: 1.9139 - regression_loss: 1.4252 - classification_loss: 0.4886\n",
      "251/500 [==============>...............] - ETA: 4:16 - loss: 1.9159 - regression_loss: 1.4272 - classification_loss: 0.4887\n",
      "252/500 [==============>...............] - ETA: 4:15 - loss: 1.9145 - regression_loss: 1.4258 - classification_loss: 0.4887\n",
      "253/500 [==============>...............] - ETA: 4:14 - loss: 1.9127 - regression_loss: 1.4242 - classification_loss: 0.4885\n",
      "254/500 [==============>...............] - ETA: 4:13 - loss: 1.9115 - regression_loss: 1.4233 - classification_loss: 0.4882\n",
      "255/500 [==============>...............] - ETA: 4:12 - loss: 1.9110 - regression_loss: 1.4232 - classification_loss: 0.4877\n",
      "256/500 [==============>...............] - ETA: 4:11 - loss: 1.9108 - regression_loss: 1.4227 - classification_loss: 0.4881\n",
      "257/500 [==============>...............] - ETA: 4:10 - loss: 1.9109 - regression_loss: 1.4231 - classification_loss: 0.4878\n",
      "258/500 [==============>...............] - ETA: 4:09 - loss: 1.9105 - regression_loss: 1.4227 - classification_loss: 0.4878\n",
      "259/500 [==============>...............] - ETA: 4:08 - loss: 1.9109 - regression_loss: 1.4229 - classification_loss: 0.4880\n",
      "260/500 [==============>...............] - ETA: 4:07 - loss: 1.9128 - regression_loss: 1.4244 - classification_loss: 0.4884\n",
      "261/500 [==============>...............] - ETA: 4:06 - loss: 1.9136 - regression_loss: 1.4250 - classification_loss: 0.4886\n",
      "262/500 [==============>...............] - ETA: 4:05 - loss: 1.9135 - regression_loss: 1.4253 - classification_loss: 0.4882\n",
      "263/500 [==============>...............] - ETA: 4:04 - loss: 1.9129 - regression_loss: 1.4246 - classification_loss: 0.4883\n",
      "264/500 [==============>...............] - ETA: 4:03 - loss: 1.9115 - regression_loss: 1.4237 - classification_loss: 0.4878\n",
      "265/500 [==============>...............] - ETA: 4:02 - loss: 1.9104 - regression_loss: 1.4232 - classification_loss: 0.4872\n",
      "266/500 [==============>...............] - ETA: 4:01 - loss: 1.9102 - regression_loss: 1.4236 - classification_loss: 0.4866\n",
      "267/500 [===============>..............] - ETA: 4:00 - loss: 1.9147 - regression_loss: 1.4274 - classification_loss: 0.4873\n",
      "268/500 [===============>..............] - ETA: 3:59 - loss: 1.9144 - regression_loss: 1.4266 - classification_loss: 0.4878\n",
      "269/500 [===============>..............] - ETA: 3:58 - loss: 1.9139 - regression_loss: 1.4261 - classification_loss: 0.4877\n",
      "270/500 [===============>..............] - ETA: 3:57 - loss: 1.9136 - regression_loss: 1.4256 - classification_loss: 0.4880\n",
      "271/500 [===============>..............] - ETA: 3:56 - loss: 1.9136 - regression_loss: 1.4260 - classification_loss: 0.4876\n",
      "272/500 [===============>..............] - ETA: 3:55 - loss: 1.9123 - regression_loss: 1.4254 - classification_loss: 0.4869\n",
      "273/500 [===============>..............] - ETA: 3:54 - loss: 1.9128 - regression_loss: 1.4256 - classification_loss: 0.4873\n",
      "274/500 [===============>..............] - ETA: 3:53 - loss: 1.9148 - regression_loss: 1.4270 - classification_loss: 0.4878\n",
      "275/500 [===============>..............] - ETA: 3:52 - loss: 1.9144 - regression_loss: 1.4265 - classification_loss: 0.4879\n",
      "276/500 [===============>..............] - ETA: 3:51 - loss: 1.9145 - regression_loss: 1.4270 - classification_loss: 0.4875\n",
      "277/500 [===============>..............] - ETA: 3:50 - loss: 1.9118 - regression_loss: 1.4252 - classification_loss: 0.4866\n",
      "278/500 [===============>..............] - ETA: 3:49 - loss: 1.9112 - regression_loss: 1.4245 - classification_loss: 0.4867\n",
      "279/500 [===============>..............] - ETA: 3:47 - loss: 1.9137 - regression_loss: 1.4266 - classification_loss: 0.4871\n",
      "280/500 [===============>..............] - ETA: 3:46 - loss: 1.9132 - regression_loss: 1.4267 - classification_loss: 0.4865\n",
      "281/500 [===============>..............] - ETA: 3:45 - loss: 1.9128 - regression_loss: 1.4268 - classification_loss: 0.4861\n",
      "282/500 [===============>..............] - ETA: 3:44 - loss: 1.9107 - regression_loss: 1.4253 - classification_loss: 0.4854\n",
      "283/500 [===============>..............] - ETA: 3:43 - loss: 1.9097 - regression_loss: 1.4250 - classification_loss: 0.4847\n",
      "284/500 [================>.............] - ETA: 3:42 - loss: 1.9123 - regression_loss: 1.4267 - classification_loss: 0.4856\n",
      "285/500 [================>.............] - ETA: 3:41 - loss: 1.9137 - regression_loss: 1.4285 - classification_loss: 0.4852\n",
      "286/500 [================>.............] - ETA: 3:40 - loss: 1.9116 - regression_loss: 1.4270 - classification_loss: 0.4846\n",
      "287/500 [================>.............] - ETA: 3:39 - loss: 1.9131 - regression_loss: 1.4280 - classification_loss: 0.4851\n",
      "288/500 [================>.............] - ETA: 3:38 - loss: 1.9099 - regression_loss: 1.4255 - classification_loss: 0.4844\n",
      "289/500 [================>.............] - ETA: 3:37 - loss: 1.9112 - regression_loss: 1.4268 - classification_loss: 0.4844\n",
      "290/500 [================>.............] - ETA: 3:36 - loss: 1.9093 - regression_loss: 1.4252 - classification_loss: 0.4841\n",
      "291/500 [================>.............] - ETA: 3:35 - loss: 1.9105 - regression_loss: 1.4263 - classification_loss: 0.4842\n",
      "292/500 [================>.............] - ETA: 3:34 - loss: 1.9109 - regression_loss: 1.4259 - classification_loss: 0.4850\n",
      "293/500 [================>.............] - ETA: 3:33 - loss: 1.9081 - regression_loss: 1.4238 - classification_loss: 0.4843\n",
      "294/500 [================>.............] - ETA: 3:32 - loss: 1.9086 - regression_loss: 1.4243 - classification_loss: 0.4842\n",
      "295/500 [================>.............] - ETA: 3:31 - loss: 1.9113 - regression_loss: 1.4267 - classification_loss: 0.4846\n",
      "296/500 [================>.............] - ETA: 3:30 - loss: 1.9102 - regression_loss: 1.4256 - classification_loss: 0.4846\n",
      "297/500 [================>.............] - ETA: 3:29 - loss: 1.9084 - regression_loss: 1.4245 - classification_loss: 0.4839\n",
      "298/500 [================>.............] - ETA: 3:28 - loss: 1.9100 - regression_loss: 1.4256 - classification_loss: 0.4844\n",
      "299/500 [================>.............] - ETA: 3:27 - loss: 1.9108 - regression_loss: 1.4261 - classification_loss: 0.4847\n",
      "300/500 [=================>............] - ETA: 3:26 - loss: 1.9105 - regression_loss: 1.4256 - classification_loss: 0.4849\n",
      "301/500 [=================>............] - ETA: 3:25 - loss: 1.9096 - regression_loss: 1.4254 - classification_loss: 0.4842\n",
      "302/500 [=================>............] - ETA: 3:24 - loss: 1.9082 - regression_loss: 1.4238 - classification_loss: 0.4844\n",
      "303/500 [=================>............] - ETA: 3:23 - loss: 1.9081 - regression_loss: 1.4235 - classification_loss: 0.4846\n",
      "304/500 [=================>............] - ETA: 3:22 - loss: 1.9094 - regression_loss: 1.4246 - classification_loss: 0.4848\n",
      "305/500 [=================>............] - ETA: 3:21 - loss: 1.9104 - regression_loss: 1.4256 - classification_loss: 0.4849\n",
      "306/500 [=================>............] - ETA: 3:20 - loss: 1.9090 - regression_loss: 1.4247 - classification_loss: 0.4843\n",
      "307/500 [=================>............] - ETA: 3:19 - loss: 1.9082 - regression_loss: 1.4239 - classification_loss: 0.4843\n",
      "308/500 [=================>............] - ETA: 3:18 - loss: 1.9097 - regression_loss: 1.4253 - classification_loss: 0.4844\n",
      "309/500 [=================>............] - ETA: 3:17 - loss: 1.9098 - regression_loss: 1.4249 - classification_loss: 0.4849\n",
      "310/500 [=================>............] - ETA: 3:16 - loss: 1.9093 - regression_loss: 1.4245 - classification_loss: 0.4848\n",
      "311/500 [=================>............] - ETA: 3:15 - loss: 1.9103 - regression_loss: 1.4252 - classification_loss: 0.4851\n",
      "312/500 [=================>............] - ETA: 3:14 - loss: 1.9101 - regression_loss: 1.4249 - classification_loss: 0.4852\n",
      "313/500 [=================>............] - ETA: 3:13 - loss: 1.9096 - regression_loss: 1.4246 - classification_loss: 0.4850\n",
      "314/500 [=================>............] - ETA: 3:11 - loss: 1.9079 - regression_loss: 1.4236 - classification_loss: 0.4843\n",
      "315/500 [=================>............] - ETA: 3:10 - loss: 1.9063 - regression_loss: 1.4227 - classification_loss: 0.4836\n",
      "316/500 [=================>............] - ETA: 3:09 - loss: 1.9069 - regression_loss: 1.4231 - classification_loss: 0.4838\n",
      "317/500 [==================>...........] - ETA: 3:08 - loss: 1.9061 - regression_loss: 1.4229 - classification_loss: 0.4832\n",
      "318/500 [==================>...........] - ETA: 3:07 - loss: 1.9052 - regression_loss: 1.4226 - classification_loss: 0.4827\n",
      "319/500 [==================>...........] - ETA: 3:06 - loss: 1.9073 - regression_loss: 1.4238 - classification_loss: 0.4835\n",
      "320/500 [==================>...........] - ETA: 3:05 - loss: 1.9079 - regression_loss: 1.4247 - classification_loss: 0.4831\n",
      "321/500 [==================>...........] - ETA: 3:04 - loss: 1.9054 - regression_loss: 1.4226 - classification_loss: 0.4828\n",
      "322/500 [==================>...........] - ETA: 3:03 - loss: 1.9040 - regression_loss: 1.4217 - classification_loss: 0.4823\n",
      "323/500 [==================>...........] - ETA: 3:02 - loss: 1.9030 - regression_loss: 1.4208 - classification_loss: 0.4822\n",
      "324/500 [==================>...........] - ETA: 3:01 - loss: 1.9055 - regression_loss: 1.4227 - classification_loss: 0.4828\n",
      "325/500 [==================>...........] - ETA: 3:00 - loss: 1.9064 - regression_loss: 1.4232 - classification_loss: 0.4832\n",
      "326/500 [==================>...........] - ETA: 2:59 - loss: 1.9047 - regression_loss: 1.4222 - classification_loss: 0.4825\n",
      "327/500 [==================>...........] - ETA: 2:58 - loss: 1.9024 - regression_loss: 1.4205 - classification_loss: 0.4819\n",
      "328/500 [==================>...........] - ETA: 2:57 - loss: 1.9015 - regression_loss: 1.4197 - classification_loss: 0.4818\n",
      "329/500 [==================>...........] - ETA: 2:56 - loss: 1.9023 - regression_loss: 1.4201 - classification_loss: 0.4822\n",
      "330/500 [==================>...........] - ETA: 2:55 - loss: 1.9022 - regression_loss: 1.4194 - classification_loss: 0.4828\n",
      "331/500 [==================>...........] - ETA: 2:54 - loss: 1.9051 - regression_loss: 1.4214 - classification_loss: 0.4837\n",
      "332/500 [==================>...........] - ETA: 2:53 - loss: 1.9045 - regression_loss: 1.4212 - classification_loss: 0.4833\n",
      "333/500 [==================>...........] - ETA: 2:52 - loss: 1.9052 - regression_loss: 1.4220 - classification_loss: 0.4832\n",
      "334/500 [===================>..........] - ETA: 2:51 - loss: 1.9041 - regression_loss: 1.4214 - classification_loss: 0.4827\n",
      "335/500 [===================>..........] - ETA: 2:50 - loss: 1.9053 - regression_loss: 1.4226 - classification_loss: 0.4827\n",
      "336/500 [===================>..........] - ETA: 2:49 - loss: 1.9049 - regression_loss: 1.4225 - classification_loss: 0.4823\n",
      "337/500 [===================>..........] - ETA: 2:48 - loss: 1.9049 - regression_loss: 1.4226 - classification_loss: 0.4822\n",
      "338/500 [===================>..........] - ETA: 2:47 - loss: 1.9038 - regression_loss: 1.4218 - classification_loss: 0.4819\n",
      "339/500 [===================>..........] - ETA: 2:46 - loss: 1.9060 - regression_loss: 1.4232 - classification_loss: 0.4827\n",
      "340/500 [===================>..........] - ETA: 2:45 - loss: 1.9093 - regression_loss: 1.4254 - classification_loss: 0.4839\n",
      "341/500 [===================>..........] - ETA: 2:44 - loss: 1.9077 - regression_loss: 1.4242 - classification_loss: 0.4835\n",
      "342/500 [===================>..........] - ETA: 2:43 - loss: 1.9071 - regression_loss: 1.4237 - classification_loss: 0.4834\n",
      "343/500 [===================>..........] - ETA: 2:42 - loss: 1.9097 - regression_loss: 1.4255 - classification_loss: 0.4842\n",
      "344/500 [===================>..........] - ETA: 2:41 - loss: 1.9090 - regression_loss: 1.4250 - classification_loss: 0.4839\n",
      "345/500 [===================>..........] - ETA: 2:40 - loss: 1.9096 - regression_loss: 1.4257 - classification_loss: 0.4839\n",
      "346/500 [===================>..........] - ETA: 2:39 - loss: 1.9111 - regression_loss: 1.4267 - classification_loss: 0.4844\n",
      "347/500 [===================>..........] - ETA: 2:38 - loss: 1.9094 - regression_loss: 1.4254 - classification_loss: 0.4840\n",
      "348/500 [===================>..........] - ETA: 2:36 - loss: 1.9111 - regression_loss: 1.4265 - classification_loss: 0.4846\n",
      "349/500 [===================>..........] - ETA: 2:35 - loss: 1.9123 - regression_loss: 1.4272 - classification_loss: 0.4851\n",
      "350/500 [====================>.........] - ETA: 2:34 - loss: 1.9109 - regression_loss: 1.4265 - classification_loss: 0.4844\n",
      "351/500 [====================>.........] - ETA: 2:33 - loss: 1.9117 - regression_loss: 1.4270 - classification_loss: 0.4846\n",
      "352/500 [====================>.........] - ETA: 2:32 - loss: 1.9143 - regression_loss: 1.4282 - classification_loss: 0.4861\n",
      "353/500 [====================>.........] - ETA: 2:31 - loss: 1.9127 - regression_loss: 1.4272 - classification_loss: 0.4856\n",
      "354/500 [====================>.........] - ETA: 2:30 - loss: 1.9124 - regression_loss: 1.4266 - classification_loss: 0.4858\n",
      "355/500 [====================>.........] - ETA: 2:29 - loss: 1.9112 - regression_loss: 1.4259 - classification_loss: 0.4854\n",
      "356/500 [====================>.........] - ETA: 2:28 - loss: 1.9134 - regression_loss: 1.4271 - classification_loss: 0.4863\n",
      "357/500 [====================>.........] - ETA: 2:27 - loss: 1.9120 - regression_loss: 1.4260 - classification_loss: 0.4861\n",
      "358/500 [====================>.........] - ETA: 2:26 - loss: 1.9116 - regression_loss: 1.4258 - classification_loss: 0.4858\n",
      "359/500 [====================>.........] - ETA: 2:25 - loss: 1.9130 - regression_loss: 1.4263 - classification_loss: 0.4866\n",
      "360/500 [====================>.........] - ETA: 2:24 - loss: 1.9139 - regression_loss: 1.4266 - classification_loss: 0.4873\n",
      "361/500 [====================>.........] - ETA: 2:23 - loss: 1.9149 - regression_loss: 1.4276 - classification_loss: 0.4872\n",
      "362/500 [====================>.........] - ETA: 2:22 - loss: 1.9176 - regression_loss: 1.4297 - classification_loss: 0.4879\n",
      "363/500 [====================>.........] - ETA: 2:21 - loss: 1.9169 - regression_loss: 1.4293 - classification_loss: 0.4876\n",
      "364/500 [====================>.........] - ETA: 2:20 - loss: 1.9152 - regression_loss: 1.4279 - classification_loss: 0.4872\n",
      "365/500 [====================>.........] - ETA: 2:19 - loss: 1.9152 - regression_loss: 1.4283 - classification_loss: 0.4869\n",
      "366/500 [====================>.........] - ETA: 2:18 - loss: 1.9163 - regression_loss: 1.4295 - classification_loss: 0.4869\n",
      "367/500 [=====================>........] - ETA: 2:17 - loss: 1.9178 - regression_loss: 1.4303 - classification_loss: 0.4875\n",
      "368/500 [=====================>........] - ETA: 2:16 - loss: 1.9160 - regression_loss: 1.4288 - classification_loss: 0.4872\n",
      "369/500 [=====================>........] - ETA: 2:15 - loss: 1.9178 - regression_loss: 1.4305 - classification_loss: 0.4873\n",
      "370/500 [=====================>........] - ETA: 2:14 - loss: 1.9194 - regression_loss: 1.4320 - classification_loss: 0.4873\n",
      "371/500 [=====================>........] - ETA: 2:13 - loss: 1.9176 - regression_loss: 1.4306 - classification_loss: 0.4871\n",
      "372/500 [=====================>........] - ETA: 2:11 - loss: 1.9180 - regression_loss: 1.4305 - classification_loss: 0.4875\n",
      "373/500 [=====================>........] - ETA: 2:11 - loss: 1.9169 - regression_loss: 1.4297 - classification_loss: 0.4872\n",
      "374/500 [=====================>........] - ETA: 2:10 - loss: 1.9167 - regression_loss: 1.4296 - classification_loss: 0.4871\n",
      "375/500 [=====================>........] - ETA: 2:08 - loss: 1.9171 - regression_loss: 1.4296 - classification_loss: 0.4874\n",
      "376/500 [=====================>........] - ETA: 2:07 - loss: 1.9155 - regression_loss: 1.4283 - classification_loss: 0.4872\n",
      "377/500 [=====================>........] - ETA: 2:06 - loss: 1.9159 - regression_loss: 1.4284 - classification_loss: 0.4875\n",
      "378/500 [=====================>........] - ETA: 2:05 - loss: 1.9172 - regression_loss: 1.4292 - classification_loss: 0.4880\n",
      "379/500 [=====================>........] - ETA: 2:04 - loss: 1.9169 - regression_loss: 1.4291 - classification_loss: 0.4877\n",
      "380/500 [=====================>........] - ETA: 2:03 - loss: 1.9169 - regression_loss: 1.4288 - classification_loss: 0.4881\n",
      "381/500 [=====================>........] - ETA: 2:02 - loss: 1.9171 - regression_loss: 1.4292 - classification_loss: 0.4879\n",
      "382/500 [=====================>........] - ETA: 2:01 - loss: 1.9158 - regression_loss: 1.4281 - classification_loss: 0.4877\n",
      "383/500 [=====================>........] - ETA: 2:00 - loss: 1.9189 - regression_loss: 1.4304 - classification_loss: 0.4885\n",
      "384/500 [======================>.......] - ETA: 1:59 - loss: 1.9199 - regression_loss: 1.4311 - classification_loss: 0.4888\n",
      "385/500 [======================>.......] - ETA: 1:58 - loss: 1.9210 - regression_loss: 1.4323 - classification_loss: 0.4887\n",
      "386/500 [======================>.......] - ETA: 1:57 - loss: 1.9209 - regression_loss: 1.4321 - classification_loss: 0.4889\n",
      "387/500 [======================>.......] - ETA: 1:56 - loss: 1.9203 - regression_loss: 1.4314 - classification_loss: 0.4888\n",
      "388/500 [======================>.......] - ETA: 1:55 - loss: 1.9212 - regression_loss: 1.4322 - classification_loss: 0.4890\n",
      "389/500 [======================>.......] - ETA: 1:54 - loss: 1.9220 - regression_loss: 1.4328 - classification_loss: 0.4892\n",
      "390/500 [======================>.......] - ETA: 1:53 - loss: 1.9226 - regression_loss: 1.4336 - classification_loss: 0.4890\n",
      "391/500 [======================>.......] - ETA: 1:52 - loss: 1.9220 - regression_loss: 1.4332 - classification_loss: 0.4888\n",
      "392/500 [======================>.......] - ETA: 1:51 - loss: 1.9224 - regression_loss: 1.4336 - classification_loss: 0.4888\n",
      "393/500 [======================>.......] - ETA: 1:50 - loss: 1.9231 - regression_loss: 1.4344 - classification_loss: 0.4887\n",
      "394/500 [======================>.......] - ETA: 1:49 - loss: 1.9233 - regression_loss: 1.4346 - classification_loss: 0.4887\n",
      "395/500 [======================>.......] - ETA: 1:48 - loss: 1.9251 - regression_loss: 1.4356 - classification_loss: 0.4894\n",
      "396/500 [======================>.......] - ETA: 1:47 - loss: 1.9256 - regression_loss: 1.4360 - classification_loss: 0.4896\n",
      "397/500 [======================>.......] - ETA: 1:46 - loss: 1.9239 - regression_loss: 1.4347 - classification_loss: 0.4892\n",
      "398/500 [======================>.......] - ETA: 1:45 - loss: 1.9228 - regression_loss: 1.4340 - classification_loss: 0.4888\n",
      "399/500 [======================>.......] - ETA: 1:44 - loss: 1.9232 - regression_loss: 1.4342 - classification_loss: 0.4890\n",
      "400/500 [=======================>......] - ETA: 1:43 - loss: 1.9215 - regression_loss: 1.4331 - classification_loss: 0.4884\n",
      "401/500 [=======================>......] - ETA: 1:42 - loss: 1.9213 - regression_loss: 1.4330 - classification_loss: 0.4884\n",
      "402/500 [=======================>......] - ETA: 1:40 - loss: 1.9198 - regression_loss: 1.4318 - classification_loss: 0.4880\n",
      "403/500 [=======================>......] - ETA: 1:39 - loss: 1.9184 - regression_loss: 1.4309 - classification_loss: 0.4874\n",
      "404/500 [=======================>......] - ETA: 1:38 - loss: 1.9194 - regression_loss: 1.4315 - classification_loss: 0.4879\n",
      "405/500 [=======================>......] - ETA: 1:37 - loss: 1.9179 - regression_loss: 1.4305 - classification_loss: 0.4874\n",
      "406/500 [=======================>......] - ETA: 1:36 - loss: 1.9180 - regression_loss: 1.4302 - classification_loss: 0.4878\n",
      "407/500 [=======================>......] - ETA: 1:35 - loss: 1.9171 - regression_loss: 1.4295 - classification_loss: 0.4876\n",
      "408/500 [=======================>......] - ETA: 1:34 - loss: 1.9158 - regression_loss: 1.4288 - classification_loss: 0.4870\n",
      "409/500 [=======================>......] - ETA: 1:33 - loss: 1.9156 - regression_loss: 1.4287 - classification_loss: 0.4869\n",
      "410/500 [=======================>......] - ETA: 1:32 - loss: 1.9150 - regression_loss: 1.4283 - classification_loss: 0.4868\n",
      "411/500 [=======================>......] - ETA: 1:31 - loss: 1.9143 - regression_loss: 1.4279 - classification_loss: 0.4864\n",
      "412/500 [=======================>......] - ETA: 1:30 - loss: 1.9134 - regression_loss: 1.4274 - classification_loss: 0.4859\n",
      "413/500 [=======================>......] - ETA: 1:29 - loss: 1.9137 - regression_loss: 1.4278 - classification_loss: 0.4858\n",
      "414/500 [=======================>......] - ETA: 1:28 - loss: 1.9136 - regression_loss: 1.4277 - classification_loss: 0.4860\n",
      "415/500 [=======================>......] - ETA: 1:27 - loss: 1.9129 - regression_loss: 1.4273 - classification_loss: 0.4856\n",
      "416/500 [=======================>......] - ETA: 1:26 - loss: 1.9160 - regression_loss: 1.4281 - classification_loss: 0.4879\n",
      "417/500 [========================>.....] - ETA: 1:25 - loss: 1.9168 - regression_loss: 1.4290 - classification_loss: 0.4879\n",
      "418/500 [========================>.....] - ETA: 1:24 - loss: 1.9185 - regression_loss: 1.4299 - classification_loss: 0.4886\n",
      "419/500 [========================>.....] - ETA: 1:23 - loss: 1.9175 - regression_loss: 1.4295 - classification_loss: 0.4880\n",
      "420/500 [========================>.....] - ETA: 1:22 - loss: 1.9152 - regression_loss: 1.4275 - classification_loss: 0.4877\n",
      "421/500 [========================>.....] - ETA: 1:21 - loss: 1.9151 - regression_loss: 1.4277 - classification_loss: 0.4873\n",
      "422/500 [========================>.....] - ETA: 1:20 - loss: 1.9140 - regression_loss: 1.4268 - classification_loss: 0.4872\n",
      "423/500 [========================>.....] - ETA: 1:19 - loss: 1.9161 - regression_loss: 1.4283 - classification_loss: 0.4878\n",
      "424/500 [========================>.....] - ETA: 1:18 - loss: 1.9168 - regression_loss: 1.4286 - classification_loss: 0.4882\n",
      "425/500 [========================>.....] - ETA: 1:17 - loss: 1.9160 - regression_loss: 1.4276 - classification_loss: 0.4883\n",
      "426/500 [========================>.....] - ETA: 1:16 - loss: 1.9156 - regression_loss: 1.4276 - classification_loss: 0.4880\n",
      "427/500 [========================>.....] - ETA: 1:15 - loss: 1.9167 - regression_loss: 1.4290 - classification_loss: 0.4877\n",
      "428/500 [========================>.....] - ETA: 1:14 - loss: 1.9189 - regression_loss: 1.4299 - classification_loss: 0.4890\n",
      "429/500 [========================>.....] - ETA: 1:13 - loss: 1.9190 - regression_loss: 1.4300 - classification_loss: 0.4890\n",
      "430/500 [========================>.....] - ETA: 1:12 - loss: 1.9196 - regression_loss: 1.4304 - classification_loss: 0.4892\n",
      "431/500 [========================>.....] - ETA: 1:11 - loss: 1.9208 - regression_loss: 1.4310 - classification_loss: 0.4897\n",
      "432/500 [========================>.....] - ETA: 1:10 - loss: 1.9194 - regression_loss: 1.4298 - classification_loss: 0.4896\n",
      "433/500 [========================>.....] - ETA: 1:09 - loss: 1.9213 - regression_loss: 1.4306 - classification_loss: 0.4907\n",
      "434/500 [=========================>....] - ETA: 1:08 - loss: 1.9197 - regression_loss: 1.4297 - classification_loss: 0.4900\n",
      "435/500 [=========================>....] - ETA: 1:07 - loss: 1.9202 - regression_loss: 1.4303 - classification_loss: 0.4899\n",
      "436/500 [=========================>....] - ETA: 1:06 - loss: 1.9202 - regression_loss: 1.4303 - classification_loss: 0.4898\n",
      "437/500 [=========================>....] - ETA: 1:05 - loss: 1.9193 - regression_loss: 1.4299 - classification_loss: 0.4895\n",
      "438/500 [=========================>....] - ETA: 1:04 - loss: 1.9184 - regression_loss: 1.4290 - classification_loss: 0.4894\n",
      "439/500 [=========================>....] - ETA: 1:02 - loss: 1.9161 - regression_loss: 1.4273 - classification_loss: 0.4888\n",
      "440/500 [=========================>....] - ETA: 1:01 - loss: 1.9160 - regression_loss: 1.4271 - classification_loss: 0.4888\n",
      "441/500 [=========================>....] - ETA: 1:00 - loss: 1.9141 - regression_loss: 1.4256 - classification_loss: 0.4885\n",
      "442/500 [=========================>....] - ETA: 59s - loss: 1.9149 - regression_loss: 1.4263 - classification_loss: 0.4885 \n",
      "443/500 [=========================>....] - ETA: 58s - loss: 1.9143 - regression_loss: 1.4256 - classification_loss: 0.4886\n",
      "444/500 [=========================>....] - ETA: 57s - loss: 1.9149 - regression_loss: 1.4262 - classification_loss: 0.4887\n",
      "445/500 [=========================>....] - ETA: 56s - loss: 1.9158 - regression_loss: 1.4269 - classification_loss: 0.4889\n",
      "446/500 [=========================>....] - ETA: 55s - loss: 1.9156 - regression_loss: 1.4270 - classification_loss: 0.4886\n",
      "447/500 [=========================>....] - ETA: 54s - loss: 1.9158 - regression_loss: 1.4269 - classification_loss: 0.4889\n",
      "448/500 [=========================>....] - ETA: 53s - loss: 1.9153 - regression_loss: 1.4262 - classification_loss: 0.4890\n",
      "449/500 [=========================>....] - ETA: 52s - loss: 1.9130 - regression_loss: 1.4245 - classification_loss: 0.4885\n",
      "450/500 [==========================>...] - ETA: 51s - loss: 1.9125 - regression_loss: 1.4242 - classification_loss: 0.4883\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 1.9152 - regression_loss: 1.4260 - classification_loss: 0.4893\n",
      "452/500 [==========================>...] - ETA: 49s - loss: 1.9150 - regression_loss: 1.4262 - classification_loss: 0.4889\n",
      "453/500 [==========================>...] - ETA: 48s - loss: 1.9148 - regression_loss: 1.4258 - classification_loss: 0.4889\n",
      "454/500 [==========================>...] - ETA: 47s - loss: 1.9145 - regression_loss: 1.4259 - classification_loss: 0.4885\n",
      "455/500 [==========================>...] - ETA: 46s - loss: 1.9143 - regression_loss: 1.4258 - classification_loss: 0.4886\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.9137 - regression_loss: 1.4255 - classification_loss: 0.4882\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.9142 - regression_loss: 1.4261 - classification_loss: 0.4881\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.9131 - regression_loss: 1.4254 - classification_loss: 0.4878\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.9127 - regression_loss: 1.4250 - classification_loss: 0.4877\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.9114 - regression_loss: 1.4241 - classification_loss: 0.4873\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 1.9104 - regression_loss: 1.4234 - classification_loss: 0.4870\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 1.9107 - regression_loss: 1.4236 - classification_loss: 0.4872\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 1.9111 - regression_loss: 1.4238 - classification_loss: 0.4873\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 1.9100 - regression_loss: 1.4231 - classification_loss: 0.4869\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 1.9100 - regression_loss: 1.4229 - classification_loss: 0.4872\n",
      "466/500 [==========================>...] - ETA: 35s - loss: 1.9103 - regression_loss: 1.4231 - classification_loss: 0.4872\n",
      "467/500 [===========================>..] - ETA: 34s - loss: 1.9126 - regression_loss: 1.4252 - classification_loss: 0.4874\n",
      "468/500 [===========================>..] - ETA: 33s - loss: 1.9121 - regression_loss: 1.4252 - classification_loss: 0.4869\n",
      "469/500 [===========================>..] - ETA: 32s - loss: 1.9106 - regression_loss: 1.4240 - classification_loss: 0.4866\n",
      "470/500 [===========================>..] - ETA: 30s - loss: 1.9125 - regression_loss: 1.4257 - classification_loss: 0.4868\n",
      "471/500 [===========================>..] - ETA: 29s - loss: 1.9126 - regression_loss: 1.4258 - classification_loss: 0.4868\n",
      "472/500 [===========================>..] - ETA: 28s - loss: 1.9140 - regression_loss: 1.4270 - classification_loss: 0.4870\n",
      "473/500 [===========================>..] - ETA: 27s - loss: 1.9134 - regression_loss: 1.4265 - classification_loss: 0.4870\n",
      "474/500 [===========================>..] - ETA: 26s - loss: 1.9142 - regression_loss: 1.4273 - classification_loss: 0.4869\n",
      "475/500 [===========================>..] - ETA: 25s - loss: 1.9142 - regression_loss: 1.4274 - classification_loss: 0.4868\n",
      "476/500 [===========================>..] - ETA: 24s - loss: 1.9134 - regression_loss: 1.4267 - classification_loss: 0.4867\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.9138 - regression_loss: 1.4270 - classification_loss: 0.4868\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.9139 - regression_loss: 1.4272 - classification_loss: 0.4867\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.9134 - regression_loss: 1.4267 - classification_loss: 0.4867\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.9119 - regression_loss: 1.4257 - classification_loss: 0.4862\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.9110 - regression_loss: 1.4250 - classification_loss: 0.4859\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.9110 - regression_loss: 1.4251 - classification_loss: 0.4859\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.9107 - regression_loss: 1.4250 - classification_loss: 0.4857\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.9098 - regression_loss: 1.4243 - classification_loss: 0.4855\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.9098 - regression_loss: 1.4240 - classification_loss: 0.4858\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.9101 - regression_loss: 1.4246 - classification_loss: 0.4855\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.9109 - regression_loss: 1.4253 - classification_loss: 0.4856\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.9105 - regression_loss: 1.4251 - classification_loss: 0.4854\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.9116 - regression_loss: 1.4258 - classification_loss: 0.4858\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.9108 - regression_loss: 1.4252 - classification_loss: 0.4857\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.9119 - regression_loss: 1.4263 - classification_loss: 0.4857 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.9132 - regression_loss: 1.4270 - classification_loss: 0.4861\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.9137 - regression_loss: 1.4275 - classification_loss: 0.4862\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.9133 - regression_loss: 1.4274 - classification_loss: 0.4859\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.9112 - regression_loss: 1.4259 - classification_loss: 0.4853\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.9101 - regression_loss: 1.4253 - classification_loss: 0.4849\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.9100 - regression_loss: 1.4248 - classification_loss: 0.4852\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.9094 - regression_loss: 1.4245 - classification_loss: 0.4850\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.9107 - regression_loss: 1.4252 - classification_loss: 0.4855\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9104 - regression_loss: 1.4252 - classification_loss: 0.4852\n",
      "Epoch 00045: saving model to ./snapshots\\resnet50_csv_45.h5\n",
      "\n",
      "500/500 [==============================] - 517s 1s/step - loss: 1.9104 - regression_loss: 1.4252 - classification_loss: 0.4852\n",
      "Epoch 46/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 1.7114 - regression_loss: 1.4237 - classification_loss: 0.2877\n",
      "  2/500 [..............................] - ETA: 4:06 - loss: 1.5224 - regression_loss: 1.2225 - classification_loss: 0.2999\n",
      "  3/500 [..............................] - ETA: 5:27 - loss: 1.5813 - regression_loss: 1.2393 - classification_loss: 0.3421\n",
      "  4/500 [..............................] - ETA: 6:19 - loss: 1.6380 - regression_loss: 1.2810 - classification_loss: 0.3570\n",
      "  5/500 [..............................] - ETA: 6:50 - loss: 1.7703 - regression_loss: 1.3253 - classification_loss: 0.4450\n",
      "  6/500 [..............................] - ETA: 7:11 - loss: 1.6242 - regression_loss: 1.2205 - classification_loss: 0.4038\n",
      "  7/500 [..............................] - ETA: 7:26 - loss: 1.7198 - regression_loss: 1.3091 - classification_loss: 0.4106\n",
      "  8/500 [..............................] - ETA: 7:31 - loss: 1.6583 - regression_loss: 1.2592 - classification_loss: 0.3992\n",
      "  9/500 [..............................] - ETA: 7:31 - loss: 1.7023 - regression_loss: 1.2689 - classification_loss: 0.4334\n",
      " 10/500 [..............................] - ETA: 7:35 - loss: 1.6819 - regression_loss: 1.2579 - classification_loss: 0.4240\n",
      " 11/500 [..............................] - ETA: 7:41 - loss: 1.6794 - regression_loss: 1.2642 - classification_loss: 0.4152\n",
      " 12/500 [..............................] - ETA: 7:42 - loss: 1.6830 - regression_loss: 1.2801 - classification_loss: 0.4029\n",
      " 13/500 [..............................] - ETA: 7:47 - loss: 1.7378 - regression_loss: 1.3065 - classification_loss: 0.4313\n",
      " 14/500 [..............................] - ETA: 7:40 - loss: 1.7617 - regression_loss: 1.3247 - classification_loss: 0.4370\n",
      " 15/500 [..............................] - ETA: 7:50 - loss: 1.7673 - regression_loss: 1.3168 - classification_loss: 0.4506\n",
      " 16/500 [..............................] - ETA: 7:53 - loss: 1.8093 - regression_loss: 1.3487 - classification_loss: 0.4607\n",
      " 17/500 [>.............................] - ETA: 7:52 - loss: 1.7825 - regression_loss: 1.3265 - classification_loss: 0.4561\n",
      " 18/500 [>.............................] - ETA: 7:51 - loss: 1.7558 - regression_loss: 1.3133 - classification_loss: 0.4426\n",
      " 19/500 [>.............................] - ETA: 7:51 - loss: 1.7718 - regression_loss: 1.3323 - classification_loss: 0.4395\n",
      " 20/500 [>.............................] - ETA: 7:50 - loss: 1.7449 - regression_loss: 1.3145 - classification_loss: 0.4304\n",
      " 21/500 [>.............................] - ETA: 7:51 - loss: 1.7138 - regression_loss: 1.2851 - classification_loss: 0.4287\n",
      " 22/500 [>.............................] - ETA: 7:51 - loss: 1.7139 - regression_loss: 1.2773 - classification_loss: 0.4366\n",
      " 23/500 [>.............................] - ETA: 7:51 - loss: 1.7312 - regression_loss: 1.2905 - classification_loss: 0.4407\n",
      " 24/500 [>.............................] - ETA: 7:50 - loss: 1.7462 - regression_loss: 1.2989 - classification_loss: 0.4473\n",
      " 25/500 [>.............................] - ETA: 7:49 - loss: 1.7478 - regression_loss: 1.3019 - classification_loss: 0.4459\n",
      " 26/500 [>.............................] - ETA: 7:45 - loss: 1.7266 - regression_loss: 1.2827 - classification_loss: 0.4439\n",
      " 27/500 [>.............................] - ETA: 7:46 - loss: 1.7119 - regression_loss: 1.2688 - classification_loss: 0.4430\n",
      " 28/500 [>.............................] - ETA: 7:50 - loss: 1.7613 - regression_loss: 1.3013 - classification_loss: 0.4600\n",
      " 29/500 [>.............................] - ETA: 7:45 - loss: 1.7684 - regression_loss: 1.3058 - classification_loss: 0.4626\n",
      " 30/500 [>.............................] - ETA: 7:43 - loss: 1.7687 - regression_loss: 1.3047 - classification_loss: 0.4640\n",
      " 31/500 [>.............................] - ETA: 7:44 - loss: 1.7792 - regression_loss: 1.3172 - classification_loss: 0.4620\n",
      " 32/500 [>.............................] - ETA: 7:45 - loss: 1.7726 - regression_loss: 1.3153 - classification_loss: 0.4573\n",
      " 33/500 [>.............................] - ETA: 7:44 - loss: 1.7722 - regression_loss: 1.3139 - classification_loss: 0.4583\n",
      " 34/500 [=>............................] - ETA: 7:44 - loss: 1.8028 - regression_loss: 1.3384 - classification_loss: 0.4644\n",
      " 35/500 [=>............................] - ETA: 7:45 - loss: 1.8250 - regression_loss: 1.3462 - classification_loss: 0.4788\n",
      " 36/500 [=>............................] - ETA: 7:45 - loss: 1.8088 - regression_loss: 1.3357 - classification_loss: 0.4732\n",
      " 37/500 [=>............................] - ETA: 7:43 - loss: 1.7894 - regression_loss: 1.3238 - classification_loss: 0.4656\n",
      " 38/500 [=>............................] - ETA: 7:43 - loss: 1.7820 - regression_loss: 1.3183 - classification_loss: 0.4637\n",
      " 39/500 [=>............................] - ETA: 7:43 - loss: 1.8018 - regression_loss: 1.3352 - classification_loss: 0.4665\n",
      " 40/500 [=>............................] - ETA: 7:42 - loss: 1.7871 - regression_loss: 1.3240 - classification_loss: 0.4632\n",
      " 41/500 [=>............................] - ETA: 7:41 - loss: 1.8114 - regression_loss: 1.3403 - classification_loss: 0.4711\n",
      " 42/500 [=>............................] - ETA: 7:41 - loss: 1.8110 - regression_loss: 1.3430 - classification_loss: 0.4680\n",
      " 43/500 [=>............................] - ETA: 7:40 - loss: 1.8148 - regression_loss: 1.3449 - classification_loss: 0.4699\n",
      " 44/500 [=>............................] - ETA: 7:37 - loss: 1.7967 - regression_loss: 1.3313 - classification_loss: 0.4654\n",
      " 45/500 [=>............................] - ETA: 7:37 - loss: 1.7967 - regression_loss: 1.3300 - classification_loss: 0.4666\n",
      " 46/500 [=>............................] - ETA: 7:36 - loss: 1.7907 - regression_loss: 1.3283 - classification_loss: 0.4624\n",
      " 47/500 [=>............................] - ETA: 7:36 - loss: 1.7885 - regression_loss: 1.3264 - classification_loss: 0.4621\n",
      " 48/500 [=>............................] - ETA: 7:35 - loss: 1.7735 - regression_loss: 1.3132 - classification_loss: 0.4602\n",
      " 49/500 [=>............................] - ETA: 7:35 - loss: 1.7637 - regression_loss: 1.3066 - classification_loss: 0.4571\n",
      " 50/500 [==>...........................] - ETA: 7:34 - loss: 1.7642 - regression_loss: 1.3077 - classification_loss: 0.4564\n",
      " 51/500 [==>...........................] - ETA: 7:33 - loss: 1.7755 - regression_loss: 1.3168 - classification_loss: 0.4587\n",
      " 52/500 [==>...........................] - ETA: 7:32 - loss: 1.7913 - regression_loss: 1.3309 - classification_loss: 0.4603\n",
      " 53/500 [==>...........................] - ETA: 7:31 - loss: 1.7991 - regression_loss: 1.3359 - classification_loss: 0.4633\n",
      " 54/500 [==>...........................] - ETA: 7:31 - loss: 1.8071 - regression_loss: 1.3423 - classification_loss: 0.4648\n",
      " 55/500 [==>...........................] - ETA: 7:30 - loss: 1.8069 - regression_loss: 1.3438 - classification_loss: 0.4631\n",
      " 56/500 [==>...........................] - ETA: 7:29 - loss: 1.8273 - regression_loss: 1.3594 - classification_loss: 0.4680\n",
      " 57/500 [==>...........................] - ETA: 7:28 - loss: 1.8327 - regression_loss: 1.3642 - classification_loss: 0.4685\n",
      " 58/500 [==>...........................] - ETA: 7:27 - loss: 1.8516 - regression_loss: 1.3779 - classification_loss: 0.4737\n",
      " 59/500 [==>...........................] - ETA: 7:26 - loss: 1.8619 - regression_loss: 1.3842 - classification_loss: 0.4777\n",
      " 60/500 [==>...........................] - ETA: 7:25 - loss: 1.8647 - regression_loss: 1.3822 - classification_loss: 0.4825\n",
      " 61/500 [==>...........................] - ETA: 7:23 - loss: 1.8815 - regression_loss: 1.3938 - classification_loss: 0.4877\n",
      " 62/500 [==>...........................] - ETA: 7:22 - loss: 1.8830 - regression_loss: 1.3962 - classification_loss: 0.4868\n",
      " 63/500 [==>...........................] - ETA: 7:22 - loss: 1.8794 - regression_loss: 1.3924 - classification_loss: 0.4870\n",
      " 64/500 [==>...........................] - ETA: 7:21 - loss: 1.8959 - regression_loss: 1.4044 - classification_loss: 0.4915\n",
      " 65/500 [==>...........................] - ETA: 7:20 - loss: 1.9115 - regression_loss: 1.4152 - classification_loss: 0.4963\n",
      " 66/500 [==>...........................] - ETA: 7:19 - loss: 1.9111 - regression_loss: 1.4127 - classification_loss: 0.4984\n",
      " 67/500 [===>..........................] - ETA: 7:18 - loss: 1.9112 - regression_loss: 1.4148 - classification_loss: 0.4963\n",
      " 68/500 [===>..........................] - ETA: 7:17 - loss: 1.9094 - regression_loss: 1.4155 - classification_loss: 0.4939\n",
      " 69/500 [===>..........................] - ETA: 7:17 - loss: 1.9101 - regression_loss: 1.4155 - classification_loss: 0.4946\n",
      " 70/500 [===>..........................] - ETA: 7:16 - loss: 1.9047 - regression_loss: 1.4113 - classification_loss: 0.4935\n",
      " 71/500 [===>..........................] - ETA: 7:15 - loss: 1.9003 - regression_loss: 1.4093 - classification_loss: 0.4910\n",
      " 72/500 [===>..........................] - ETA: 7:14 - loss: 1.8987 - regression_loss: 1.4086 - classification_loss: 0.4900\n",
      " 73/500 [===>..........................] - ETA: 7:14 - loss: 1.8933 - regression_loss: 1.4071 - classification_loss: 0.4862\n",
      " 74/500 [===>..........................] - ETA: 7:14 - loss: 1.8837 - regression_loss: 1.3988 - classification_loss: 0.4849\n",
      " 75/500 [===>..........................] - ETA: 7:13 - loss: 1.8854 - regression_loss: 1.3997 - classification_loss: 0.4857\n",
      " 76/500 [===>..........................] - ETA: 7:12 - loss: 1.8943 - regression_loss: 1.4080 - classification_loss: 0.4863\n",
      " 77/500 [===>..........................] - ETA: 7:10 - loss: 1.8936 - regression_loss: 1.4067 - classification_loss: 0.4869\n",
      " 78/500 [===>..........................] - ETA: 7:10 - loss: 1.9033 - regression_loss: 1.4127 - classification_loss: 0.4907\n",
      " 79/500 [===>..........................] - ETA: 7:09 - loss: 1.9111 - regression_loss: 1.4181 - classification_loss: 0.4930\n",
      " 80/500 [===>..........................] - ETA: 7:09 - loss: 1.9075 - regression_loss: 1.4146 - classification_loss: 0.4930\n",
      " 81/500 [===>..........................] - ETA: 7:07 - loss: 1.9022 - regression_loss: 1.4114 - classification_loss: 0.4908\n",
      " 82/500 [===>..........................] - ETA: 7:06 - loss: 1.9051 - regression_loss: 1.4118 - classification_loss: 0.4933\n",
      " 83/500 [===>..........................] - ETA: 7:06 - loss: 1.8994 - regression_loss: 1.4073 - classification_loss: 0.4921\n",
      " 84/500 [====>.........................] - ETA: 7:04 - loss: 1.9023 - regression_loss: 1.4087 - classification_loss: 0.4936\n",
      " 85/500 [====>.........................] - ETA: 7:04 - loss: 1.9041 - regression_loss: 1.4120 - classification_loss: 0.4921\n",
      " 86/500 [====>.........................] - ETA: 7:03 - loss: 1.8990 - regression_loss: 1.4086 - classification_loss: 0.4905\n",
      " 87/500 [====>.........................] - ETA: 7:03 - loss: 1.9017 - regression_loss: 1.4104 - classification_loss: 0.4912\n",
      " 88/500 [====>.........................] - ETA: 7:01 - loss: 1.8996 - regression_loss: 1.4091 - classification_loss: 0.4905\n",
      " 89/500 [====>.........................] - ETA: 7:00 - loss: 1.9015 - regression_loss: 1.4098 - classification_loss: 0.4917\n",
      " 90/500 [====>.........................] - ETA: 7:00 - loss: 1.8933 - regression_loss: 1.4045 - classification_loss: 0.4887\n",
      " 91/500 [====>.........................] - ETA: 6:59 - loss: 1.8928 - regression_loss: 1.4049 - classification_loss: 0.4879\n",
      " 92/500 [====>.........................] - ETA: 6:57 - loss: 1.8898 - regression_loss: 1.4021 - classification_loss: 0.4877\n",
      " 93/500 [====>.........................] - ETA: 6:57 - loss: 1.8936 - regression_loss: 1.4056 - classification_loss: 0.4880\n",
      " 94/500 [====>.........................] - ETA: 6:56 - loss: 1.8960 - regression_loss: 1.4085 - classification_loss: 0.4875\n",
      " 95/500 [====>.........................] - ETA: 6:54 - loss: 1.8926 - regression_loss: 1.4062 - classification_loss: 0.4865\n",
      " 96/500 [====>.........................] - ETA: 6:53 - loss: 1.8892 - regression_loss: 1.4039 - classification_loss: 0.4853\n",
      " 97/500 [====>.........................] - ETA: 6:52 - loss: 1.8831 - regression_loss: 1.3997 - classification_loss: 0.4834\n",
      " 98/500 [====>.........................] - ETA: 6:51 - loss: 1.8943 - regression_loss: 1.4074 - classification_loss: 0.4869\n",
      " 99/500 [====>.........................] - ETA: 6:50 - loss: 1.8887 - regression_loss: 1.4043 - classification_loss: 0.4843\n",
      "100/500 [=====>........................] - ETA: 6:49 - loss: 1.8852 - regression_loss: 1.4024 - classification_loss: 0.4827\n",
      "101/500 [=====>........................] - ETA: 6:48 - loss: 1.8855 - regression_loss: 1.4039 - classification_loss: 0.4816\n",
      "102/500 [=====>........................] - ETA: 6:48 - loss: 1.9012 - regression_loss: 1.4159 - classification_loss: 0.4853\n",
      "103/500 [=====>........................] - ETA: 6:47 - loss: 1.9040 - regression_loss: 1.4183 - classification_loss: 0.4857\n",
      "104/500 [=====>........................] - ETA: 6:45 - loss: 1.8971 - regression_loss: 1.4135 - classification_loss: 0.4835\n",
      "105/500 [=====>........................] - ETA: 6:44 - loss: 1.8947 - regression_loss: 1.4125 - classification_loss: 0.4823\n",
      "106/500 [=====>........................] - ETA: 6:44 - loss: 1.8937 - regression_loss: 1.4123 - classification_loss: 0.4814\n",
      "107/500 [=====>........................] - ETA: 6:42 - loss: 1.8949 - regression_loss: 1.4134 - classification_loss: 0.4815\n",
      "108/500 [=====>........................] - ETA: 6:42 - loss: 1.8953 - regression_loss: 1.4145 - classification_loss: 0.4808\n",
      "109/500 [=====>........................] - ETA: 6:41 - loss: 1.8945 - regression_loss: 1.4134 - classification_loss: 0.4811\n",
      "110/500 [=====>........................] - ETA: 6:40 - loss: 1.8879 - regression_loss: 1.4081 - classification_loss: 0.4799\n",
      "111/500 [=====>........................] - ETA: 6:39 - loss: 1.8886 - regression_loss: 1.4090 - classification_loss: 0.4796\n",
      "112/500 [=====>........................] - ETA: 6:38 - loss: 1.8797 - regression_loss: 1.4028 - classification_loss: 0.4769\n",
      "113/500 [=====>........................] - ETA: 6:36 - loss: 1.8794 - regression_loss: 1.4023 - classification_loss: 0.4771\n",
      "114/500 [=====>........................] - ETA: 6:35 - loss: 1.8783 - regression_loss: 1.3992 - classification_loss: 0.4791\n",
      "115/500 [=====>........................] - ETA: 6:34 - loss: 1.8750 - regression_loss: 1.3973 - classification_loss: 0.4777\n",
      "116/500 [=====>........................] - ETA: 6:33 - loss: 1.8751 - regression_loss: 1.3981 - classification_loss: 0.4770\n",
      "117/500 [======>.......................] - ETA: 6:32 - loss: 1.8791 - regression_loss: 1.4004 - classification_loss: 0.4787\n",
      "118/500 [======>.......................] - ETA: 6:31 - loss: 1.8786 - regression_loss: 1.3996 - classification_loss: 0.4790\n",
      "119/500 [======>.......................] - ETA: 6:30 - loss: 1.8844 - regression_loss: 1.4041 - classification_loss: 0.4803\n",
      "120/500 [======>.......................] - ETA: 6:29 - loss: 1.8838 - regression_loss: 1.4041 - classification_loss: 0.4797\n",
      "121/500 [======>.......................] - ETA: 6:28 - loss: 1.8809 - regression_loss: 1.4025 - classification_loss: 0.4784\n",
      "122/500 [======>.......................] - ETA: 6:27 - loss: 1.8865 - regression_loss: 1.4073 - classification_loss: 0.4792\n",
      "123/500 [======>.......................] - ETA: 6:26 - loss: 1.8890 - regression_loss: 1.4077 - classification_loss: 0.4813\n",
      "124/500 [======>.......................] - ETA: 6:25 - loss: 1.8922 - regression_loss: 1.4087 - classification_loss: 0.4834\n",
      "125/500 [======>.......................] - ETA: 6:24 - loss: 1.8927 - regression_loss: 1.4084 - classification_loss: 0.4843\n",
      "126/500 [======>.......................] - ETA: 6:23 - loss: 1.8902 - regression_loss: 1.4070 - classification_loss: 0.4833\n",
      "127/500 [======>.......................] - ETA: 6:21 - loss: 1.8864 - regression_loss: 1.4045 - classification_loss: 0.4819\n",
      "128/500 [======>.......................] - ETA: 6:20 - loss: 1.8826 - regression_loss: 1.4023 - classification_loss: 0.4803\n",
      "129/500 [======>.......................] - ETA: 6:20 - loss: 1.8776 - regression_loss: 1.3984 - classification_loss: 0.4792\n",
      "130/500 [======>.......................] - ETA: 6:18 - loss: 1.8783 - regression_loss: 1.3996 - classification_loss: 0.4787\n",
      "131/500 [======>.......................] - ETA: 6:18 - loss: 1.8827 - regression_loss: 1.4031 - classification_loss: 0.4796\n",
      "132/500 [======>.......................] - ETA: 6:17 - loss: 1.8900 - regression_loss: 1.4061 - classification_loss: 0.4839\n",
      "133/500 [======>.......................] - ETA: 6:16 - loss: 1.8951 - regression_loss: 1.4114 - classification_loss: 0.4837\n",
      "134/500 [=======>......................] - ETA: 6:15 - loss: 1.8944 - regression_loss: 1.4106 - classification_loss: 0.4839\n",
      "135/500 [=======>......................] - ETA: 6:14 - loss: 1.8894 - regression_loss: 1.4073 - classification_loss: 0.4821\n",
      "136/500 [=======>......................] - ETA: 6:13 - loss: 1.8868 - regression_loss: 1.4050 - classification_loss: 0.4818\n",
      "137/500 [=======>......................] - ETA: 6:12 - loss: 1.8900 - regression_loss: 1.4072 - classification_loss: 0.4828\n",
      "138/500 [=======>......................] - ETA: 6:11 - loss: 1.8863 - regression_loss: 1.4049 - classification_loss: 0.4815\n",
      "139/500 [=======>......................] - ETA: 6:10 - loss: 1.8899 - regression_loss: 1.4069 - classification_loss: 0.4830\n",
      "140/500 [=======>......................] - ETA: 6:09 - loss: 1.8923 - regression_loss: 1.4085 - classification_loss: 0.4838\n",
      "141/500 [=======>......................] - ETA: 6:08 - loss: 1.8932 - regression_loss: 1.4100 - classification_loss: 0.4832\n",
      "142/500 [=======>......................] - ETA: 6:07 - loss: 1.8916 - regression_loss: 1.4088 - classification_loss: 0.4828\n",
      "143/500 [=======>......................] - ETA: 6:06 - loss: 1.8929 - regression_loss: 1.4107 - classification_loss: 0.4822\n",
      "144/500 [=======>......................] - ETA: 6:05 - loss: 1.8925 - regression_loss: 1.4103 - classification_loss: 0.4822\n",
      "145/500 [=======>......................] - ETA: 6:03 - loss: 1.8884 - regression_loss: 1.4069 - classification_loss: 0.4815\n",
      "146/500 [=======>......................] - ETA: 6:03 - loss: 1.8960 - regression_loss: 1.4118 - classification_loss: 0.4841\n",
      "147/500 [=======>......................] - ETA: 6:01 - loss: 1.8966 - regression_loss: 1.4125 - classification_loss: 0.4841\n",
      "148/500 [=======>......................] - ETA: 6:00 - loss: 1.8988 - regression_loss: 1.4148 - classification_loss: 0.4840\n",
      "149/500 [=======>......................] - ETA: 5:59 - loss: 1.9013 - regression_loss: 1.4167 - classification_loss: 0.4845\n",
      "150/500 [========>.....................] - ETA: 5:59 - loss: 1.9074 - regression_loss: 1.4215 - classification_loss: 0.4859\n",
      "151/500 [========>.....................] - ETA: 5:57 - loss: 1.9020 - regression_loss: 1.4175 - classification_loss: 0.4846\n",
      "152/500 [========>.....................] - ETA: 5:56 - loss: 1.9035 - regression_loss: 1.4200 - classification_loss: 0.4835\n",
      "153/500 [========>.....................] - ETA: 5:55 - loss: 1.8990 - regression_loss: 1.4169 - classification_loss: 0.4821\n",
      "154/500 [========>.....................] - ETA: 5:55 - loss: 1.8945 - regression_loss: 1.4131 - classification_loss: 0.4814\n",
      "155/500 [========>.....................] - ETA: 5:53 - loss: 1.8959 - regression_loss: 1.4149 - classification_loss: 0.4810\n",
      "156/500 [========>.....................] - ETA: 5:52 - loss: 1.8997 - regression_loss: 1.4183 - classification_loss: 0.4813\n",
      "157/500 [========>.....................] - ETA: 5:51 - loss: 1.9016 - regression_loss: 1.4206 - classification_loss: 0.4811\n",
      "158/500 [========>.....................] - ETA: 5:51 - loss: 1.9010 - regression_loss: 1.4201 - classification_loss: 0.4809\n",
      "159/500 [========>.....................] - ETA: 5:49 - loss: 1.8954 - regression_loss: 1.4160 - classification_loss: 0.4795\n",
      "160/500 [========>.....................] - ETA: 5:48 - loss: 1.8956 - regression_loss: 1.4162 - classification_loss: 0.4794\n",
      "161/500 [========>.....................] - ETA: 5:47 - loss: 1.8918 - regression_loss: 1.4134 - classification_loss: 0.4785\n",
      "162/500 [========>.....................] - ETA: 5:46 - loss: 1.8955 - regression_loss: 1.4161 - classification_loss: 0.4794\n",
      "163/500 [========>.....................] - ETA: 5:45 - loss: 1.8979 - regression_loss: 1.4167 - classification_loss: 0.4812\n",
      "164/500 [========>.....................] - ETA: 5:44 - loss: 1.8979 - regression_loss: 1.4156 - classification_loss: 0.4823\n",
      "165/500 [========>.....................] - ETA: 5:43 - loss: 1.8944 - regression_loss: 1.4135 - classification_loss: 0.4809\n",
      "166/500 [========>.....................] - ETA: 5:42 - loss: 1.8924 - regression_loss: 1.4122 - classification_loss: 0.4802\n",
      "167/500 [=========>....................] - ETA: 5:41 - loss: 1.8976 - regression_loss: 1.4152 - classification_loss: 0.4825\n",
      "168/500 [=========>....................] - ETA: 5:40 - loss: 1.8974 - regression_loss: 1.4159 - classification_loss: 0.4814\n",
      "169/500 [=========>....................] - ETA: 5:39 - loss: 1.8996 - regression_loss: 1.4174 - classification_loss: 0.4822\n",
      "170/500 [=========>....................] - ETA: 5:37 - loss: 1.9005 - regression_loss: 1.4188 - classification_loss: 0.4817\n",
      "171/500 [=========>....................] - ETA: 5:37 - loss: 1.8959 - regression_loss: 1.4156 - classification_loss: 0.4803\n",
      "172/500 [=========>....................] - ETA: 5:36 - loss: 1.8923 - regression_loss: 1.4131 - classification_loss: 0.4791\n",
      "173/500 [=========>....................] - ETA: 5:35 - loss: 1.8979 - regression_loss: 1.4164 - classification_loss: 0.4815\n",
      "174/500 [=========>....................] - ETA: 5:34 - loss: 1.8995 - regression_loss: 1.4184 - classification_loss: 0.4810\n",
      "175/500 [=========>....................] - ETA: 5:33 - loss: 1.8986 - regression_loss: 1.4173 - classification_loss: 0.4813\n",
      "176/500 [=========>....................] - ETA: 5:31 - loss: 1.9029 - regression_loss: 1.4204 - classification_loss: 0.4825\n",
      "177/500 [=========>....................] - ETA: 5:30 - loss: 1.9012 - regression_loss: 1.4195 - classification_loss: 0.4817\n",
      "178/500 [=========>....................] - ETA: 5:30 - loss: 1.8997 - regression_loss: 1.4187 - classification_loss: 0.4809\n",
      "179/500 [=========>....................] - ETA: 5:29 - loss: 1.9029 - regression_loss: 1.4216 - classification_loss: 0.4813\n",
      "180/500 [=========>....................] - ETA: 5:28 - loss: 1.9014 - regression_loss: 1.4210 - classification_loss: 0.4804\n",
      "181/500 [=========>....................] - ETA: 5:27 - loss: 1.8955 - regression_loss: 1.4169 - classification_loss: 0.4786\n",
      "182/500 [=========>....................] - ETA: 5:26 - loss: 1.8933 - regression_loss: 1.4152 - classification_loss: 0.4782\n",
      "183/500 [=========>....................] - ETA: 5:25 - loss: 1.8909 - regression_loss: 1.4138 - classification_loss: 0.4771\n",
      "184/500 [==========>...................] - ETA: 5:23 - loss: 1.8882 - regression_loss: 1.4117 - classification_loss: 0.4765\n",
      "185/500 [==========>...................] - ETA: 5:22 - loss: 1.8883 - regression_loss: 1.4116 - classification_loss: 0.4767\n",
      "186/500 [==========>...................] - ETA: 5:21 - loss: 1.8857 - regression_loss: 1.4095 - classification_loss: 0.4762\n",
      "187/500 [==========>...................] - ETA: 5:20 - loss: 1.8893 - regression_loss: 1.4123 - classification_loss: 0.4770\n",
      "188/500 [==========>...................] - ETA: 5:19 - loss: 1.8888 - regression_loss: 1.4127 - classification_loss: 0.4760\n",
      "189/500 [==========>...................] - ETA: 5:19 - loss: 1.8864 - regression_loss: 1.4109 - classification_loss: 0.4756\n",
      "190/500 [==========>...................] - ETA: 5:18 - loss: 1.8924 - regression_loss: 1.4140 - classification_loss: 0.4784\n",
      "191/500 [==========>...................] - ETA: 5:17 - loss: 1.8960 - regression_loss: 1.4157 - classification_loss: 0.4803\n",
      "192/500 [==========>...................] - ETA: 5:16 - loss: 1.8986 - regression_loss: 1.4165 - classification_loss: 0.4821\n",
      "193/500 [==========>...................] - ETA: 5:15 - loss: 1.9003 - regression_loss: 1.4172 - classification_loss: 0.4831\n",
      "194/500 [==========>...................] - ETA: 5:14 - loss: 1.8987 - regression_loss: 1.4160 - classification_loss: 0.4827\n",
      "195/500 [==========>...................] - ETA: 5:13 - loss: 1.8999 - regression_loss: 1.4168 - classification_loss: 0.4830\n",
      "196/500 [==========>...................] - ETA: 5:12 - loss: 1.8995 - regression_loss: 1.4166 - classification_loss: 0.4829\n",
      "197/500 [==========>...................] - ETA: 5:11 - loss: 1.8986 - regression_loss: 1.4158 - classification_loss: 0.4828\n",
      "198/500 [==========>...................] - ETA: 5:10 - loss: 1.8998 - regression_loss: 1.4170 - classification_loss: 0.4828\n",
      "199/500 [==========>...................] - ETA: 5:09 - loss: 1.8980 - regression_loss: 1.4159 - classification_loss: 0.4821\n",
      "200/500 [===========>..................] - ETA: 5:07 - loss: 1.8948 - regression_loss: 1.4139 - classification_loss: 0.4809\n",
      "201/500 [===========>..................] - ETA: 5:06 - loss: 1.8922 - regression_loss: 1.4125 - classification_loss: 0.4798\n",
      "202/500 [===========>..................] - ETA: 5:05 - loss: 1.8921 - regression_loss: 1.4126 - classification_loss: 0.4795\n",
      "203/500 [===========>..................] - ETA: 5:04 - loss: 1.8905 - regression_loss: 1.4110 - classification_loss: 0.4795\n",
      "204/500 [===========>..................] - ETA: 5:03 - loss: 1.8874 - regression_loss: 1.4089 - classification_loss: 0.4785\n",
      "205/500 [===========>..................] - ETA: 5:02 - loss: 1.8940 - regression_loss: 1.4136 - classification_loss: 0.4803\n",
      "206/500 [===========>..................] - ETA: 5:01 - loss: 1.8961 - regression_loss: 1.4158 - classification_loss: 0.4804\n",
      "207/500 [===========>..................] - ETA: 5:00 - loss: 1.8945 - regression_loss: 1.4140 - classification_loss: 0.4805\n",
      "208/500 [===========>..................] - ETA: 4:59 - loss: 1.8928 - regression_loss: 1.4130 - classification_loss: 0.4798\n",
      "209/500 [===========>..................] - ETA: 4:58 - loss: 1.8935 - regression_loss: 1.4138 - classification_loss: 0.4797\n",
      "210/500 [===========>..................] - ETA: 4:57 - loss: 1.8926 - regression_loss: 1.4129 - classification_loss: 0.4797\n",
      "211/500 [===========>..................] - ETA: 4:56 - loss: 1.8954 - regression_loss: 1.4159 - classification_loss: 0.4796\n",
      "212/500 [===========>..................] - ETA: 4:55 - loss: 1.8954 - regression_loss: 1.4156 - classification_loss: 0.4797\n",
      "213/500 [===========>..................] - ETA: 4:54 - loss: 1.8945 - regression_loss: 1.4145 - classification_loss: 0.4800\n",
      "214/500 [===========>..................] - ETA: 4:53 - loss: 1.8918 - regression_loss: 1.4126 - classification_loss: 0.4792\n",
      "215/500 [===========>..................] - ETA: 4:52 - loss: 1.8925 - regression_loss: 1.4131 - classification_loss: 0.4794\n",
      "216/500 [===========>..................] - ETA: 4:51 - loss: 1.8931 - regression_loss: 1.4141 - classification_loss: 0.4790\n",
      "217/500 [============>.................] - ETA: 4:50 - loss: 1.8941 - regression_loss: 1.4149 - classification_loss: 0.4792\n",
      "218/500 [============>.................] - ETA: 4:49 - loss: 1.8931 - regression_loss: 1.4144 - classification_loss: 0.4787\n",
      "219/500 [============>.................] - ETA: 4:48 - loss: 1.8911 - regression_loss: 1.4129 - classification_loss: 0.4782\n",
      "220/500 [============>.................] - ETA: 4:47 - loss: 1.8957 - regression_loss: 1.4157 - classification_loss: 0.4800\n",
      "221/500 [============>.................] - ETA: 4:46 - loss: 1.8954 - regression_loss: 1.4161 - classification_loss: 0.4793\n",
      "222/500 [============>.................] - ETA: 4:44 - loss: 1.8922 - regression_loss: 1.4134 - classification_loss: 0.4787\n",
      "223/500 [============>.................] - ETA: 4:43 - loss: 1.8921 - regression_loss: 1.4141 - classification_loss: 0.4780\n",
      "224/500 [============>.................] - ETA: 4:42 - loss: 1.8901 - regression_loss: 1.4129 - classification_loss: 0.4772\n",
      "225/500 [============>.................] - ETA: 4:41 - loss: 1.8893 - regression_loss: 1.4124 - classification_loss: 0.4769\n",
      "226/500 [============>.................] - ETA: 4:40 - loss: 1.8916 - regression_loss: 1.4138 - classification_loss: 0.4777\n",
      "227/500 [============>.................] - ETA: 4:39 - loss: 1.8913 - regression_loss: 1.4136 - classification_loss: 0.4777\n",
      "228/500 [============>.................] - ETA: 4:38 - loss: 1.8913 - regression_loss: 1.4135 - classification_loss: 0.4778\n",
      "229/500 [============>.................] - ETA: 4:37 - loss: 1.8915 - regression_loss: 1.4138 - classification_loss: 0.4777\n",
      "230/500 [============>.................] - ETA: 4:36 - loss: 1.8951 - regression_loss: 1.4149 - classification_loss: 0.4802\n",
      "231/500 [============>.................] - ETA: 4:35 - loss: 1.8961 - regression_loss: 1.4163 - classification_loss: 0.4798\n",
      "232/500 [============>.................] - ETA: 4:34 - loss: 1.8965 - regression_loss: 1.4161 - classification_loss: 0.4804\n",
      "233/500 [============>.................] - ETA: 4:33 - loss: 1.8932 - regression_loss: 1.4131 - classification_loss: 0.4802\n",
      "234/500 [=============>................] - ETA: 4:32 - loss: 1.8943 - regression_loss: 1.4138 - classification_loss: 0.4805\n",
      "235/500 [=============>................] - ETA: 4:31 - loss: 1.8933 - regression_loss: 1.4127 - classification_loss: 0.4806\n",
      "236/500 [=============>................] - ETA: 4:30 - loss: 1.8934 - regression_loss: 1.4129 - classification_loss: 0.4805\n",
      "237/500 [=============>................] - ETA: 4:29 - loss: 1.8907 - regression_loss: 1.4109 - classification_loss: 0.4798\n",
      "238/500 [=============>................] - ETA: 4:28 - loss: 1.8898 - regression_loss: 1.4107 - classification_loss: 0.4791\n",
      "239/500 [=============>................] - ETA: 4:27 - loss: 1.8937 - regression_loss: 1.4132 - classification_loss: 0.4805\n",
      "240/500 [=============>................] - ETA: 4:26 - loss: 1.8979 - regression_loss: 1.4153 - classification_loss: 0.4826\n",
      "241/500 [=============>................] - ETA: 4:25 - loss: 1.8998 - regression_loss: 1.4165 - classification_loss: 0.4833\n",
      "242/500 [=============>................] - ETA: 4:24 - loss: 1.9027 - regression_loss: 1.4184 - classification_loss: 0.4843\n",
      "243/500 [=============>................] - ETA: 4:23 - loss: 1.9069 - regression_loss: 1.4209 - classification_loss: 0.4860\n",
      "244/500 [=============>................] - ETA: 4:22 - loss: 1.9078 - regression_loss: 1.4216 - classification_loss: 0.4863\n",
      "245/500 [=============>................] - ETA: 4:21 - loss: 1.9078 - regression_loss: 1.4220 - classification_loss: 0.4858\n",
      "246/500 [=============>................] - ETA: 4:20 - loss: 1.9110 - regression_loss: 1.4245 - classification_loss: 0.4865\n",
      "247/500 [=============>................] - ETA: 4:19 - loss: 1.9091 - regression_loss: 1.4231 - classification_loss: 0.4860\n",
      "248/500 [=============>................] - ETA: 4:20 - loss: 1.9084 - regression_loss: 1.4227 - classification_loss: 0.4857\n",
      "249/500 [=============>................] - ETA: 4:19 - loss: 1.9101 - regression_loss: 1.4247 - classification_loss: 0.4854\n",
      "250/500 [==============>...............] - ETA: 4:18 - loss: 1.9114 - regression_loss: 1.4257 - classification_loss: 0.4857\n",
      "251/500 [==============>...............] - ETA: 4:17 - loss: 1.9114 - regression_loss: 1.4259 - classification_loss: 0.4855\n",
      "252/500 [==============>...............] - ETA: 4:15 - loss: 1.9101 - regression_loss: 1.4254 - classification_loss: 0.4847\n",
      "253/500 [==============>...............] - ETA: 4:14 - loss: 1.9084 - regression_loss: 1.4242 - classification_loss: 0.4842\n",
      "254/500 [==============>...............] - ETA: 4:13 - loss: 1.9114 - regression_loss: 1.4264 - classification_loss: 0.4850\n",
      "255/500 [==============>...............] - ETA: 4:12 - loss: 1.9101 - regression_loss: 1.4251 - classification_loss: 0.4850\n",
      "256/500 [==============>...............] - ETA: 4:11 - loss: 1.9078 - regression_loss: 1.4234 - classification_loss: 0.4844\n",
      "257/500 [==============>...............] - ETA: 4:10 - loss: 1.9103 - regression_loss: 1.4250 - classification_loss: 0.4853\n",
      "258/500 [==============>...............] - ETA: 4:09 - loss: 1.9104 - regression_loss: 1.4252 - classification_loss: 0.4852\n",
      "259/500 [==============>...............] - ETA: 4:08 - loss: 1.9113 - regression_loss: 1.4264 - classification_loss: 0.4849\n",
      "260/500 [==============>...............] - ETA: 4:07 - loss: 1.9121 - regression_loss: 1.4268 - classification_loss: 0.4853\n",
      "261/500 [==============>...............] - ETA: 4:06 - loss: 1.9107 - regression_loss: 1.4259 - classification_loss: 0.4848\n",
      "262/500 [==============>...............] - ETA: 4:05 - loss: 1.9118 - regression_loss: 1.4268 - classification_loss: 0.4849\n",
      "263/500 [==============>...............] - ETA: 4:04 - loss: 1.9108 - regression_loss: 1.4265 - classification_loss: 0.4843\n",
      "264/500 [==============>...............] - ETA: 4:03 - loss: 1.9097 - regression_loss: 1.4256 - classification_loss: 0.4841\n",
      "265/500 [==============>...............] - ETA: 4:02 - loss: 1.9061 - regression_loss: 1.4228 - classification_loss: 0.4833\n",
      "266/500 [==============>...............] - ETA: 4:01 - loss: 1.9078 - regression_loss: 1.4245 - classification_loss: 0.4833\n",
      "267/500 [===============>..............] - ETA: 4:00 - loss: 1.9082 - regression_loss: 1.4246 - classification_loss: 0.4835\n",
      "268/500 [===============>..............] - ETA: 3:59 - loss: 1.9071 - regression_loss: 1.4242 - classification_loss: 0.4829\n",
      "269/500 [===============>..............] - ETA: 3:58 - loss: 1.9049 - regression_loss: 1.4227 - classification_loss: 0.4822\n",
      "270/500 [===============>..............] - ETA: 3:57 - loss: 1.9058 - regression_loss: 1.4235 - classification_loss: 0.4823\n",
      "271/500 [===============>..............] - ETA: 3:55 - loss: 1.9088 - regression_loss: 1.4257 - classification_loss: 0.4831\n",
      "272/500 [===============>..............] - ETA: 3:55 - loss: 1.9100 - regression_loss: 1.4271 - classification_loss: 0.4829\n",
      "273/500 [===============>..............] - ETA: 3:53 - loss: 1.9137 - regression_loss: 1.4296 - classification_loss: 0.4841\n",
      "274/500 [===============>..............] - ETA: 3:52 - loss: 1.9125 - regression_loss: 1.4288 - classification_loss: 0.4837\n",
      "275/500 [===============>..............] - ETA: 3:51 - loss: 1.9135 - regression_loss: 1.4299 - classification_loss: 0.4836\n",
      "276/500 [===============>..............] - ETA: 3:50 - loss: 1.9129 - regression_loss: 1.4288 - classification_loss: 0.4841\n",
      "277/500 [===============>..............] - ETA: 3:49 - loss: 1.9139 - regression_loss: 1.4294 - classification_loss: 0.4844\n",
      "278/500 [===============>..............] - ETA: 3:48 - loss: 1.9140 - regression_loss: 1.4298 - classification_loss: 0.4842\n",
      "279/500 [===============>..............] - ETA: 3:47 - loss: 1.9147 - regression_loss: 1.4304 - classification_loss: 0.4843\n",
      "280/500 [===============>..............] - ETA: 3:46 - loss: 1.9136 - regression_loss: 1.4298 - classification_loss: 0.4838\n",
      "281/500 [===============>..............] - ETA: 3:45 - loss: 1.9152 - regression_loss: 1.4305 - classification_loss: 0.4846\n",
      "282/500 [===============>..............] - ETA: 3:44 - loss: 1.9114 - regression_loss: 1.4279 - classification_loss: 0.4835\n",
      "283/500 [===============>..............] - ETA: 3:43 - loss: 1.9128 - regression_loss: 1.4296 - classification_loss: 0.4831\n",
      "284/500 [================>.............] - ETA: 3:42 - loss: 1.9129 - regression_loss: 1.4295 - classification_loss: 0.4834\n",
      "285/500 [================>.............] - ETA: 3:41 - loss: 1.9154 - regression_loss: 1.4308 - classification_loss: 0.4846\n",
      "286/500 [================>.............] - ETA: 3:40 - loss: 1.9183 - regression_loss: 1.4336 - classification_loss: 0.4848\n",
      "287/500 [================>.............] - ETA: 3:39 - loss: 1.9184 - regression_loss: 1.4332 - classification_loss: 0.4852\n",
      "288/500 [================>.............] - ETA: 3:38 - loss: 1.9210 - regression_loss: 1.4355 - classification_loss: 0.4855\n",
      "289/500 [================>.............] - ETA: 3:37 - loss: 1.9191 - regression_loss: 1.4343 - classification_loss: 0.4848\n",
      "290/500 [================>.............] - ETA: 3:36 - loss: 1.9220 - regression_loss: 1.4364 - classification_loss: 0.4856\n",
      "291/500 [================>.............] - ETA: 3:35 - loss: 1.9207 - regression_loss: 1.4352 - classification_loss: 0.4855\n",
      "292/500 [================>.............] - ETA: 3:34 - loss: 1.9199 - regression_loss: 1.4344 - classification_loss: 0.4855\n",
      "293/500 [================>.............] - ETA: 3:33 - loss: 1.9208 - regression_loss: 1.4352 - classification_loss: 0.4856\n",
      "294/500 [================>.............] - ETA: 3:32 - loss: 1.9241 - regression_loss: 1.4373 - classification_loss: 0.4868\n",
      "295/500 [================>.............] - ETA: 3:31 - loss: 1.9228 - regression_loss: 1.4361 - classification_loss: 0.4867\n",
      "296/500 [================>.............] - ETA: 3:30 - loss: 1.9200 - regression_loss: 1.4339 - classification_loss: 0.4860\n",
      "297/500 [================>.............] - ETA: 3:29 - loss: 1.9181 - regression_loss: 1.4323 - classification_loss: 0.4858\n",
      "298/500 [================>.............] - ETA: 3:28 - loss: 1.9170 - regression_loss: 1.4316 - classification_loss: 0.4854\n",
      "299/500 [================>.............] - ETA: 3:27 - loss: 1.9143 - regression_loss: 1.4296 - classification_loss: 0.4847\n",
      "300/500 [=================>............] - ETA: 3:25 - loss: 1.9168 - regression_loss: 1.4312 - classification_loss: 0.4856\n",
      "301/500 [=================>............] - ETA: 3:24 - loss: 1.9194 - regression_loss: 1.4333 - classification_loss: 0.4861\n",
      "302/500 [=================>............] - ETA: 3:23 - loss: 1.9186 - regression_loss: 1.4329 - classification_loss: 0.4858\n",
      "303/500 [=================>............] - ETA: 3:22 - loss: 1.9169 - regression_loss: 1.4316 - classification_loss: 0.4853\n",
      "304/500 [=================>............] - ETA: 3:21 - loss: 1.9166 - regression_loss: 1.4317 - classification_loss: 0.4849\n",
      "305/500 [=================>............] - ETA: 3:20 - loss: 1.9155 - regression_loss: 1.4313 - classification_loss: 0.4842\n",
      "306/500 [=================>............] - ETA: 3:19 - loss: 1.9172 - regression_loss: 1.4326 - classification_loss: 0.4846\n",
      "307/500 [=================>............] - ETA: 3:18 - loss: 1.9158 - regression_loss: 1.4316 - classification_loss: 0.4842\n",
      "308/500 [=================>............] - ETA: 3:17 - loss: 1.9140 - regression_loss: 1.4304 - classification_loss: 0.4836\n",
      "309/500 [=================>............] - ETA: 3:16 - loss: 1.9126 - regression_loss: 1.4294 - classification_loss: 0.4833\n",
      "310/500 [=================>............] - ETA: 3:15 - loss: 1.9137 - regression_loss: 1.4304 - classification_loss: 0.4833\n",
      "311/500 [=================>............] - ETA: 3:14 - loss: 1.9146 - regression_loss: 1.4315 - classification_loss: 0.4831\n",
      "312/500 [=================>............] - ETA: 3:13 - loss: 1.9134 - regression_loss: 1.4308 - classification_loss: 0.4826\n",
      "313/500 [=================>............] - ETA: 3:12 - loss: 1.9122 - regression_loss: 1.4299 - classification_loss: 0.4823\n",
      "314/500 [=================>............] - ETA: 3:11 - loss: 1.9128 - regression_loss: 1.4303 - classification_loss: 0.4824\n",
      "315/500 [=================>............] - ETA: 3:10 - loss: 1.9113 - regression_loss: 1.4292 - classification_loss: 0.4821\n",
      "316/500 [=================>............] - ETA: 3:09 - loss: 1.9118 - regression_loss: 1.4291 - classification_loss: 0.4827\n",
      "317/500 [==================>...........] - ETA: 3:08 - loss: 1.9110 - regression_loss: 1.4283 - classification_loss: 0.4827\n",
      "318/500 [==================>...........] - ETA: 3:07 - loss: 1.9113 - regression_loss: 1.4286 - classification_loss: 0.4828\n",
      "319/500 [==================>...........] - ETA: 3:06 - loss: 1.9148 - regression_loss: 1.4313 - classification_loss: 0.4835\n",
      "320/500 [==================>...........] - ETA: 3:05 - loss: 1.9137 - regression_loss: 1.4303 - classification_loss: 0.4835\n",
      "321/500 [==================>...........] - ETA: 3:04 - loss: 1.9123 - regression_loss: 1.4293 - classification_loss: 0.4830\n",
      "322/500 [==================>...........] - ETA: 3:03 - loss: 1.9112 - regression_loss: 1.4287 - classification_loss: 0.4824\n",
      "323/500 [==================>...........] - ETA: 3:02 - loss: 1.9128 - regression_loss: 1.4298 - classification_loss: 0.4830\n",
      "324/500 [==================>...........] - ETA: 3:01 - loss: 1.9116 - regression_loss: 1.4289 - classification_loss: 0.4827\n",
      "325/500 [==================>...........] - ETA: 3:00 - loss: 1.9110 - regression_loss: 1.4287 - classification_loss: 0.4823\n",
      "326/500 [==================>...........] - ETA: 2:59 - loss: 1.9108 - regression_loss: 1.4282 - classification_loss: 0.4827\n",
      "327/500 [==================>...........] - ETA: 2:58 - loss: 1.9092 - regression_loss: 1.4269 - classification_loss: 0.4823\n",
      "328/500 [==================>...........] - ETA: 2:57 - loss: 1.9072 - regression_loss: 1.4252 - classification_loss: 0.4821\n",
      "329/500 [==================>...........] - ETA: 2:56 - loss: 1.9099 - regression_loss: 1.4266 - classification_loss: 0.4833\n",
      "330/500 [==================>...........] - ETA: 2:55 - loss: 1.9078 - regression_loss: 1.4252 - classification_loss: 0.4825\n",
      "331/500 [==================>...........] - ETA: 2:54 - loss: 1.9082 - regression_loss: 1.4255 - classification_loss: 0.4827\n",
      "332/500 [==================>...........] - ETA: 2:53 - loss: 1.9112 - regression_loss: 1.4273 - classification_loss: 0.4839\n",
      "333/500 [==================>...........] - ETA: 2:52 - loss: 1.9112 - regression_loss: 1.4274 - classification_loss: 0.4838\n",
      "334/500 [===================>..........] - ETA: 2:51 - loss: 1.9084 - regression_loss: 1.4252 - classification_loss: 0.4833\n",
      "335/500 [===================>..........] - ETA: 2:49 - loss: 1.9082 - regression_loss: 1.4249 - classification_loss: 0.4833\n",
      "336/500 [===================>..........] - ETA: 2:48 - loss: 1.9076 - regression_loss: 1.4247 - classification_loss: 0.4829\n",
      "337/500 [===================>..........] - ETA: 2:47 - loss: 1.9080 - regression_loss: 1.4250 - classification_loss: 0.4831\n",
      "338/500 [===================>..........] - ETA: 2:46 - loss: 1.9082 - regression_loss: 1.4251 - classification_loss: 0.4832\n",
      "339/500 [===================>..........] - ETA: 2:45 - loss: 1.9070 - regression_loss: 1.4243 - classification_loss: 0.4827\n",
      "340/500 [===================>..........] - ETA: 2:44 - loss: 1.9078 - regression_loss: 1.4246 - classification_loss: 0.4832\n",
      "341/500 [===================>..........] - ETA: 2:43 - loss: 1.9075 - regression_loss: 1.4243 - classification_loss: 0.4832\n",
      "342/500 [===================>..........] - ETA: 2:43 - loss: 1.9043 - regression_loss: 1.4218 - classification_loss: 0.4826\n",
      "343/500 [===================>..........] - ETA: 2:42 - loss: 1.9034 - regression_loss: 1.4213 - classification_loss: 0.4821\n",
      "344/500 [===================>..........] - ETA: 2:41 - loss: 1.9043 - regression_loss: 1.4217 - classification_loss: 0.4826\n",
      "345/500 [===================>..........] - ETA: 2:40 - loss: 1.9028 - regression_loss: 1.4207 - classification_loss: 0.4821\n",
      "346/500 [===================>..........] - ETA: 2:39 - loss: 1.9032 - regression_loss: 1.4211 - classification_loss: 0.4821\n",
      "347/500 [===================>..........] - ETA: 2:38 - loss: 1.9039 - regression_loss: 1.4218 - classification_loss: 0.4821\n",
      "348/500 [===================>..........] - ETA: 2:37 - loss: 1.9035 - regression_loss: 1.4219 - classification_loss: 0.4816\n",
      "349/500 [===================>..........] - ETA: 2:36 - loss: 1.9030 - regression_loss: 1.4213 - classification_loss: 0.4817\n",
      "350/500 [====================>.........] - ETA: 2:35 - loss: 1.9010 - regression_loss: 1.4200 - classification_loss: 0.4810\n",
      "351/500 [====================>.........] - ETA: 2:34 - loss: 1.9001 - regression_loss: 1.4196 - classification_loss: 0.4805\n",
      "352/500 [====================>.........] - ETA: 2:33 - loss: 1.9025 - regression_loss: 1.4208 - classification_loss: 0.4817\n",
      "353/500 [====================>.........] - ETA: 2:32 - loss: 1.9028 - regression_loss: 1.4213 - classification_loss: 0.4815\n",
      "354/500 [====================>.........] - ETA: 2:31 - loss: 1.9044 - regression_loss: 1.4229 - classification_loss: 0.4815\n",
      "355/500 [====================>.........] - ETA: 2:30 - loss: 1.9016 - regression_loss: 1.4207 - classification_loss: 0.4809\n",
      "356/500 [====================>.........] - ETA: 2:29 - loss: 1.9022 - regression_loss: 1.4212 - classification_loss: 0.4810\n",
      "357/500 [====================>.........] - ETA: 2:28 - loss: 1.9031 - regression_loss: 1.4219 - classification_loss: 0.4811\n",
      "358/500 [====================>.........] - ETA: 2:27 - loss: 1.9044 - regression_loss: 1.4231 - classification_loss: 0.4813\n",
      "359/500 [====================>.........] - ETA: 2:26 - loss: 1.9030 - regression_loss: 1.4219 - classification_loss: 0.4811\n",
      "360/500 [====================>.........] - ETA: 2:25 - loss: 1.9020 - regression_loss: 1.4213 - classification_loss: 0.4807\n",
      "361/500 [====================>.........] - ETA: 2:24 - loss: 1.9024 - regression_loss: 1.4215 - classification_loss: 0.4810\n",
      "362/500 [====================>.........] - ETA: 2:23 - loss: 1.9045 - regression_loss: 1.4227 - classification_loss: 0.4818\n",
      "363/500 [====================>.........] - ETA: 2:22 - loss: 1.9047 - regression_loss: 1.4227 - classification_loss: 0.4820\n",
      "364/500 [====================>.........] - ETA: 2:20 - loss: 1.9028 - regression_loss: 1.4211 - classification_loss: 0.4817\n",
      "365/500 [====================>.........] - ETA: 2:19 - loss: 1.9026 - regression_loss: 1.4209 - classification_loss: 0.4817\n",
      "366/500 [====================>.........] - ETA: 2:18 - loss: 1.9008 - regression_loss: 1.4198 - classification_loss: 0.4811\n",
      "367/500 [=====================>........] - ETA: 2:17 - loss: 1.9045 - regression_loss: 1.4221 - classification_loss: 0.4824\n",
      "368/500 [=====================>........] - ETA: 2:16 - loss: 1.9045 - regression_loss: 1.4222 - classification_loss: 0.4823\n",
      "369/500 [=====================>........] - ETA: 2:15 - loss: 1.9029 - regression_loss: 1.4208 - classification_loss: 0.4821\n",
      "370/500 [=====================>........] - ETA: 2:14 - loss: 1.9015 - regression_loss: 1.4198 - classification_loss: 0.4817\n",
      "371/500 [=====================>........] - ETA: 2:13 - loss: 1.9010 - regression_loss: 1.4198 - classification_loss: 0.4812\n",
      "372/500 [=====================>........] - ETA: 2:12 - loss: 1.9027 - regression_loss: 1.4209 - classification_loss: 0.4819\n",
      "373/500 [=====================>........] - ETA: 2:11 - loss: 1.9004 - regression_loss: 1.4190 - classification_loss: 0.4814\n",
      "374/500 [=====================>........] - ETA: 2:10 - loss: 1.8986 - regression_loss: 1.4177 - classification_loss: 0.4809\n",
      "375/500 [=====================>........] - ETA: 2:09 - loss: 1.8981 - regression_loss: 1.4174 - classification_loss: 0.4807\n",
      "376/500 [=====================>........] - ETA: 2:08 - loss: 1.8970 - regression_loss: 1.4165 - classification_loss: 0.4805\n",
      "377/500 [=====================>........] - ETA: 2:07 - loss: 1.8982 - regression_loss: 1.4178 - classification_loss: 0.4804\n",
      "378/500 [=====================>........] - ETA: 2:06 - loss: 1.9009 - regression_loss: 1.4190 - classification_loss: 0.4819\n",
      "379/500 [=====================>........] - ETA: 2:05 - loss: 1.9035 - regression_loss: 1.4204 - classification_loss: 0.4831\n",
      "380/500 [=====================>........] - ETA: 2:04 - loss: 1.9014 - regression_loss: 1.4191 - classification_loss: 0.4823\n",
      "381/500 [=====================>........] - ETA: 2:03 - loss: 1.8995 - regression_loss: 1.4177 - classification_loss: 0.4818\n",
      "382/500 [=====================>........] - ETA: 2:02 - loss: 1.9010 - regression_loss: 1.4190 - classification_loss: 0.4821\n",
      "383/500 [=====================>........] - ETA: 2:01 - loss: 1.9006 - regression_loss: 1.4187 - classification_loss: 0.4819\n",
      "384/500 [======================>.......] - ETA: 2:00 - loss: 1.9008 - regression_loss: 1.4187 - classification_loss: 0.4821\n",
      "385/500 [======================>.......] - ETA: 1:59 - loss: 1.9005 - regression_loss: 1.4188 - classification_loss: 0.4817\n",
      "386/500 [======================>.......] - ETA: 1:58 - loss: 1.9020 - regression_loss: 1.4197 - classification_loss: 0.4823\n",
      "387/500 [======================>.......] - ETA: 1:57 - loss: 1.9020 - regression_loss: 1.4197 - classification_loss: 0.4823\n",
      "388/500 [======================>.......] - ETA: 1:56 - loss: 1.9020 - regression_loss: 1.4197 - classification_loss: 0.4823\n",
      "389/500 [======================>.......] - ETA: 1:54 - loss: 1.9024 - regression_loss: 1.4203 - classification_loss: 0.4822\n",
      "390/500 [======================>.......] - ETA: 1:53 - loss: 1.9022 - regression_loss: 1.4201 - classification_loss: 0.4821\n",
      "391/500 [======================>.......] - ETA: 1:52 - loss: 1.9019 - regression_loss: 1.4199 - classification_loss: 0.4819\n",
      "392/500 [======================>.......] - ETA: 1:51 - loss: 1.8995 - regression_loss: 1.4183 - classification_loss: 0.4812\n",
      "393/500 [======================>.......] - ETA: 1:50 - loss: 1.9003 - regression_loss: 1.4191 - classification_loss: 0.4812\n",
      "394/500 [======================>.......] - ETA: 1:49 - loss: 1.8993 - regression_loss: 1.4185 - classification_loss: 0.4809\n",
      "395/500 [======================>.......] - ETA: 1:48 - loss: 1.8997 - regression_loss: 1.4186 - classification_loss: 0.4811\n",
      "396/500 [======================>.......] - ETA: 1:47 - loss: 1.8993 - regression_loss: 1.4185 - classification_loss: 0.4808\n",
      "397/500 [======================>.......] - ETA: 1:46 - loss: 1.8989 - regression_loss: 1.4185 - classification_loss: 0.4803\n",
      "398/500 [======================>.......] - ETA: 1:45 - loss: 1.8987 - regression_loss: 1.4186 - classification_loss: 0.4800\n",
      "399/500 [======================>.......] - ETA: 1:44 - loss: 1.8973 - regression_loss: 1.4177 - classification_loss: 0.4796\n",
      "400/500 [=======================>......] - ETA: 1:43 - loss: 1.8988 - regression_loss: 1.4191 - classification_loss: 0.4798\n",
      "401/500 [=======================>......] - ETA: 1:42 - loss: 1.8992 - regression_loss: 1.4195 - classification_loss: 0.4797\n",
      "402/500 [=======================>......] - ETA: 1:41 - loss: 1.8985 - regression_loss: 1.4189 - classification_loss: 0.4796\n",
      "403/500 [=======================>......] - ETA: 1:40 - loss: 1.8988 - regression_loss: 1.4191 - classification_loss: 0.4797\n",
      "404/500 [=======================>......] - ETA: 1:39 - loss: 1.8980 - regression_loss: 1.4186 - classification_loss: 0.4794\n",
      "405/500 [=======================>......] - ETA: 1:38 - loss: 1.8984 - regression_loss: 1.4191 - classification_loss: 0.4794\n",
      "406/500 [=======================>......] - ETA: 1:37 - loss: 1.8977 - regression_loss: 1.4182 - classification_loss: 0.4796\n",
      "407/500 [=======================>......] - ETA: 1:36 - loss: 1.8983 - regression_loss: 1.4187 - classification_loss: 0.4796\n",
      "408/500 [=======================>......] - ETA: 1:35 - loss: 1.9002 - regression_loss: 1.4200 - classification_loss: 0.4801\n",
      "409/500 [=======================>......] - ETA: 1:34 - loss: 1.8985 - regression_loss: 1.4190 - classification_loss: 0.4795\n",
      "410/500 [=======================>......] - ETA: 1:33 - loss: 1.8993 - regression_loss: 1.4193 - classification_loss: 0.4799\n",
      "411/500 [=======================>......] - ETA: 1:32 - loss: 1.8992 - regression_loss: 1.4196 - classification_loss: 0.4796\n",
      "412/500 [=======================>......] - ETA: 1:31 - loss: 1.8998 - regression_loss: 1.4198 - classification_loss: 0.4801\n",
      "413/500 [=======================>......] - ETA: 1:30 - loss: 1.8992 - regression_loss: 1.4193 - classification_loss: 0.4800\n",
      "414/500 [=======================>......] - ETA: 1:29 - loss: 1.8990 - regression_loss: 1.4193 - classification_loss: 0.4797\n",
      "415/500 [=======================>......] - ETA: 1:28 - loss: 1.9000 - regression_loss: 1.4200 - classification_loss: 0.4800\n",
      "416/500 [=======================>......] - ETA: 1:27 - loss: 1.8998 - regression_loss: 1.4200 - classification_loss: 0.4798\n",
      "417/500 [========================>.....] - ETA: 1:25 - loss: 1.8994 - regression_loss: 1.4197 - classification_loss: 0.4796\n",
      "418/500 [========================>.....] - ETA: 1:24 - loss: 1.9016 - regression_loss: 1.4214 - classification_loss: 0.4803\n",
      "419/500 [========================>.....] - ETA: 1:23 - loss: 1.9031 - regression_loss: 1.4227 - classification_loss: 0.4804\n",
      "420/500 [========================>.....] - ETA: 1:22 - loss: 1.9036 - regression_loss: 1.4232 - classification_loss: 0.4804\n",
      "421/500 [========================>.....] - ETA: 1:21 - loss: 1.9035 - regression_loss: 1.4233 - classification_loss: 0.4801\n",
      "422/500 [========================>.....] - ETA: 1:20 - loss: 1.9020 - regression_loss: 1.4222 - classification_loss: 0.4798\n",
      "423/500 [========================>.....] - ETA: 1:19 - loss: 1.9008 - regression_loss: 1.4212 - classification_loss: 0.4796\n",
      "424/500 [========================>.....] - ETA: 1:18 - loss: 1.9022 - regression_loss: 1.4221 - classification_loss: 0.4801\n",
      "425/500 [========================>.....] - ETA: 1:17 - loss: 1.9028 - regression_loss: 1.4221 - classification_loss: 0.4807\n",
      "426/500 [========================>.....] - ETA: 1:16 - loss: 1.9041 - regression_loss: 1.4229 - classification_loss: 0.4812\n",
      "427/500 [========================>.....] - ETA: 1:15 - loss: 1.9033 - regression_loss: 1.4222 - classification_loss: 0.4811\n",
      "428/500 [========================>.....] - ETA: 1:14 - loss: 1.9025 - regression_loss: 1.4215 - classification_loss: 0.4810\n",
      "429/500 [========================>.....] - ETA: 1:13 - loss: 1.9018 - regression_loss: 1.4211 - classification_loss: 0.4807\n",
      "430/500 [========================>.....] - ETA: 1:12 - loss: 1.9016 - regression_loss: 1.4209 - classification_loss: 0.4806\n",
      "431/500 [========================>.....] - ETA: 1:11 - loss: 1.9010 - regression_loss: 1.4207 - classification_loss: 0.4803\n",
      "432/500 [========================>.....] - ETA: 1:10 - loss: 1.9000 - regression_loss: 1.4199 - classification_loss: 0.4800\n",
      "433/500 [========================>.....] - ETA: 1:09 - loss: 1.9008 - regression_loss: 1.4211 - classification_loss: 0.4798\n",
      "434/500 [=========================>....] - ETA: 1:08 - loss: 1.9026 - regression_loss: 1.4229 - classification_loss: 0.4796\n",
      "435/500 [=========================>....] - ETA: 1:07 - loss: 1.9014 - regression_loss: 1.4222 - classification_loss: 0.4792\n",
      "436/500 [=========================>....] - ETA: 1:06 - loss: 1.9007 - regression_loss: 1.4214 - classification_loss: 0.4792\n",
      "437/500 [=========================>....] - ETA: 1:05 - loss: 1.9024 - regression_loss: 1.4229 - classification_loss: 0.4795\n",
      "438/500 [=========================>....] - ETA: 1:04 - loss: 1.9020 - regression_loss: 1.4223 - classification_loss: 0.4797\n",
      "439/500 [=========================>....] - ETA: 1:03 - loss: 1.9016 - regression_loss: 1.4220 - classification_loss: 0.4796\n",
      "440/500 [=========================>....] - ETA: 1:02 - loss: 1.9022 - regression_loss: 1.4224 - classification_loss: 0.4798\n",
      "441/500 [=========================>....] - ETA: 1:01 - loss: 1.9038 - regression_loss: 1.4230 - classification_loss: 0.4807\n",
      "442/500 [=========================>....] - ETA: 1:00 - loss: 1.9043 - regression_loss: 1.4237 - classification_loss: 0.4807\n",
      "443/500 [=========================>....] - ETA: 59s - loss: 1.9030 - regression_loss: 1.4228 - classification_loss: 0.4803 \n",
      "444/500 [=========================>....] - ETA: 58s - loss: 1.9044 - regression_loss: 1.4236 - classification_loss: 0.4808\n",
      "445/500 [=========================>....] - ETA: 57s - loss: 1.9051 - regression_loss: 1.4242 - classification_loss: 0.4808\n",
      "446/500 [=========================>....] - ETA: 55s - loss: 1.9073 - regression_loss: 1.4261 - classification_loss: 0.4813\n",
      "447/500 [=========================>....] - ETA: 54s - loss: 1.9071 - regression_loss: 1.4262 - classification_loss: 0.4810\n",
      "448/500 [=========================>....] - ETA: 53s - loss: 1.9106 - regression_loss: 1.4283 - classification_loss: 0.4823\n",
      "449/500 [=========================>....] - ETA: 52s - loss: 1.9092 - regression_loss: 1.4272 - classification_loss: 0.4821\n",
      "450/500 [==========================>...] - ETA: 51s - loss: 1.9098 - regression_loss: 1.4279 - classification_loss: 0.4819\n",
      "451/500 [==========================>...] - ETA: 50s - loss: 1.9102 - regression_loss: 1.4280 - classification_loss: 0.4822\n",
      "452/500 [==========================>...] - ETA: 50s - loss: 1.9118 - regression_loss: 1.4286 - classification_loss: 0.4832\n",
      "453/500 [==========================>...] - ETA: 49s - loss: 1.9114 - regression_loss: 1.4287 - classification_loss: 0.4827\n",
      "454/500 [==========================>...] - ETA: 48s - loss: 1.9103 - regression_loss: 1.4280 - classification_loss: 0.4823\n",
      "455/500 [==========================>...] - ETA: 47s - loss: 1.9110 - regression_loss: 1.4287 - classification_loss: 0.4824\n",
      "456/500 [==========================>...] - ETA: 45s - loss: 1.9137 - regression_loss: 1.4305 - classification_loss: 0.4832\n",
      "457/500 [==========================>...] - ETA: 44s - loss: 1.9130 - regression_loss: 1.4298 - classification_loss: 0.4832\n",
      "458/500 [==========================>...] - ETA: 43s - loss: 1.9128 - regression_loss: 1.4297 - classification_loss: 0.4831\n",
      "459/500 [==========================>...] - ETA: 42s - loss: 1.9142 - regression_loss: 1.4307 - classification_loss: 0.4835\n",
      "460/500 [==========================>...] - ETA: 41s - loss: 1.9144 - regression_loss: 1.4308 - classification_loss: 0.4837\n",
      "461/500 [==========================>...] - ETA: 40s - loss: 1.9133 - regression_loss: 1.4300 - classification_loss: 0.4832\n",
      "462/500 [==========================>...] - ETA: 39s - loss: 1.9119 - regression_loss: 1.4292 - classification_loss: 0.4827\n",
      "463/500 [==========================>...] - ETA: 38s - loss: 1.9108 - regression_loss: 1.4287 - classification_loss: 0.4821\n",
      "464/500 [==========================>...] - ETA: 37s - loss: 1.9125 - regression_loss: 1.4301 - classification_loss: 0.4823\n",
      "465/500 [==========================>...] - ETA: 36s - loss: 1.9125 - regression_loss: 1.4304 - classification_loss: 0.4821\n",
      "466/500 [==========================>...] - ETA: 35s - loss: 1.9107 - regression_loss: 1.4290 - classification_loss: 0.4817\n",
      "467/500 [===========================>..] - ETA: 34s - loss: 1.9117 - regression_loss: 1.4296 - classification_loss: 0.4821\n",
      "468/500 [===========================>..] - ETA: 33s - loss: 1.9118 - regression_loss: 1.4294 - classification_loss: 0.4825\n",
      "469/500 [===========================>..] - ETA: 32s - loss: 1.9126 - regression_loss: 1.4302 - classification_loss: 0.4824\n",
      "470/500 [===========================>..] - ETA: 31s - loss: 1.9109 - regression_loss: 1.4290 - classification_loss: 0.4819\n",
      "471/500 [===========================>..] - ETA: 30s - loss: 1.9105 - regression_loss: 1.4288 - classification_loss: 0.4817\n",
      "472/500 [===========================>..] - ETA: 29s - loss: 1.9100 - regression_loss: 1.4287 - classification_loss: 0.4813\n",
      "473/500 [===========================>..] - ETA: 28s - loss: 1.9083 - regression_loss: 1.4277 - classification_loss: 0.4806\n",
      "474/500 [===========================>..] - ETA: 27s - loss: 1.9085 - regression_loss: 1.4278 - classification_loss: 0.4807\n",
      "475/500 [===========================>..] - ETA: 26s - loss: 1.9069 - regression_loss: 1.4265 - classification_loss: 0.4803\n",
      "476/500 [===========================>..] - ETA: 25s - loss: 1.9059 - regression_loss: 1.4257 - classification_loss: 0.4802\n",
      "477/500 [===========================>..] - ETA: 23s - loss: 1.9063 - regression_loss: 1.4261 - classification_loss: 0.4801\n",
      "478/500 [===========================>..] - ETA: 22s - loss: 1.9060 - regression_loss: 1.4261 - classification_loss: 0.4800\n",
      "479/500 [===========================>..] - ETA: 21s - loss: 1.9072 - regression_loss: 1.4271 - classification_loss: 0.4802\n",
      "480/500 [===========================>..] - ETA: 20s - loss: 1.9063 - regression_loss: 1.4266 - classification_loss: 0.4797\n",
      "481/500 [===========================>..] - ETA: 19s - loss: 1.9072 - regression_loss: 1.4275 - classification_loss: 0.4798\n",
      "482/500 [===========================>..] - ETA: 18s - loss: 1.9078 - regression_loss: 1.4283 - classification_loss: 0.4796\n",
      "483/500 [===========================>..] - ETA: 17s - loss: 1.9085 - regression_loss: 1.4290 - classification_loss: 0.4794\n",
      "484/500 [============================>.] - ETA: 16s - loss: 1.9084 - regression_loss: 1.4291 - classification_loss: 0.4793\n",
      "485/500 [============================>.] - ETA: 15s - loss: 1.9101 - regression_loss: 1.4303 - classification_loss: 0.4797\n",
      "486/500 [============================>.] - ETA: 14s - loss: 1.9124 - regression_loss: 1.4317 - classification_loss: 0.4807\n",
      "487/500 [============================>.] - ETA: 13s - loss: 1.9125 - regression_loss: 1.4319 - classification_loss: 0.4805\n",
      "488/500 [============================>.] - ETA: 12s - loss: 1.9131 - regression_loss: 1.4324 - classification_loss: 0.4806\n",
      "489/500 [============================>.] - ETA: 11s - loss: 1.9143 - regression_loss: 1.4333 - classification_loss: 0.4811\n",
      "490/500 [============================>.] - ETA: 10s - loss: 1.9142 - regression_loss: 1.4333 - classification_loss: 0.4808\n",
      "491/500 [============================>.] - ETA: 9s - loss: 1.9152 - regression_loss: 1.4342 - classification_loss: 0.4810 \n",
      "492/500 [============================>.] - ETA: 8s - loss: 1.9155 - regression_loss: 1.4342 - classification_loss: 0.4813\n",
      "493/500 [============================>.] - ETA: 7s - loss: 1.9144 - regression_loss: 1.4334 - classification_loss: 0.4810\n",
      "494/500 [============================>.] - ETA: 6s - loss: 1.9153 - regression_loss: 1.4340 - classification_loss: 0.4813\n",
      "495/500 [============================>.] - ETA: 5s - loss: 1.9166 - regression_loss: 1.4348 - classification_loss: 0.4817\n",
      "496/500 [============================>.] - ETA: 4s - loss: 1.9153 - regression_loss: 1.4337 - classification_loss: 0.4816\n",
      "497/500 [============================>.] - ETA: 3s - loss: 1.9144 - regression_loss: 1.4331 - classification_loss: 0.4813\n",
      "498/500 [============================>.] - ETA: 2s - loss: 1.9140 - regression_loss: 1.4329 - classification_loss: 0.4811\n",
      "499/500 [============================>.] - ETA: 1s - loss: 1.9140 - regression_loss: 1.4328 - classification_loss: 0.4811\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9153 - regression_loss: 1.4339 - classification_loss: 0.4813\n",
      "Epoch 00046: saving model to ./snapshots\\resnet50_csv_46.h5\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 9.999999424161285e-20.\n",
      "\n",
      "500/500 [==============================] - 522s 1s/step - loss: 1.9153 - regression_loss: 1.4339 - classification_loss: 0.4813\n",
      "Epoch 47/50\n",
      "\n",
      "  1/500 [..............................] - ETA: 0s - loss: 2.3768 - regression_loss: 1.6935 - classification_loss: 0.6833\n",
      "  2/500 [..............................] - ETA: 4:04 - loss: 2.3567 - regression_loss: 1.6430 - classification_loss: 0.7137\n",
      "  3/500 [..............................] - ETA: 5:25 - loss: 2.2749 - regression_loss: 1.6437 - classification_loss: 0.6312\n",
      "  4/500 [..............................] - ETA: 6:04 - loss: 2.1724 - regression_loss: 1.5676 - classification_loss: 0.6047\n",
      "  5/500 [..............................] - ETA: 6:27 - loss: 2.1412 - regression_loss: 1.5311 - classification_loss: 0.6101\n",
      "  6/500 [..............................] - ETA: 6:59 - loss: 2.2408 - regression_loss: 1.6099 - classification_loss: 0.6309\n",
      "  7/500 [..............................] - ETA: 7:15 - loss: 2.2703 - regression_loss: 1.6199 - classification_loss: 0.6503\n",
      "  8/500 [..............................] - ETA: 7:20 - loss: 2.1957 - regression_loss: 1.5830 - classification_loss: 0.6128\n",
      "  9/500 [..............................] - ETA: 7:25 - loss: 2.1602 - regression_loss: 1.5715 - classification_loss: 0.5886\n",
      " 10/500 [..............................] - ETA: 7:32 - loss: 2.0901 - regression_loss: 1.5198 - classification_loss: 0.5703\n",
      " 11/500 [..............................] - ETA: 7:33 - loss: 2.0320 - regression_loss: 1.4754 - classification_loss: 0.5565\n",
      " 12/500 [..............................] - ETA: 7:39 - loss: 2.0146 - regression_loss: 1.4733 - classification_loss: 0.5414\n",
      " 13/500 [..............................] - ETA: 7:39 - loss: 1.9513 - regression_loss: 1.4260 - classification_loss: 0.5254\n",
      " 14/500 [..............................] - ETA: 7:43 - loss: 2.0003 - regression_loss: 1.4750 - classification_loss: 0.5252\n",
      " 15/500 [..............................] - ETA: 7:37 - loss: 1.9715 - regression_loss: 1.4554 - classification_loss: 0.5161\n",
      " 16/500 [..............................] - ETA: 7:42 - loss: 1.9921 - regression_loss: 1.4720 - classification_loss: 0.5201\n",
      " 17/500 [>.............................] - ETA: 7:44 - loss: 1.9749 - regression_loss: 1.4644 - classification_loss: 0.5105\n",
      " 18/500 [>.............................] - ETA: 7:43 - loss: 2.0199 - regression_loss: 1.4955 - classification_loss: 0.5244\n",
      " 19/500 [>.............................] - ETA: 7:43 - loss: 2.0440 - regression_loss: 1.5236 - classification_loss: 0.5204\n",
      " 20/500 [>.............................] - ETA: 7:42 - loss: 1.9973 - regression_loss: 1.4927 - classification_loss: 0.5046\n",
      " 21/500 [>.............................] - ETA: 7:37 - loss: 1.9625 - regression_loss: 1.4635 - classification_loss: 0.4990\n",
      " 22/500 [>.............................] - ETA: 7:42 - loss: 1.9637 - regression_loss: 1.4697 - classification_loss: 0.4940\n",
      " 23/500 [>.............................] - ETA: 7:43 - loss: 1.9639 - regression_loss: 1.4731 - classification_loss: 0.4907\n",
      " 24/500 [>.............................] - ETA: 7:38 - loss: 1.9554 - regression_loss: 1.4667 - classification_loss: 0.4887\n",
      " 25/500 [>.............................] - ETA: 7:43 - loss: 1.9347 - regression_loss: 1.4528 - classification_loss: 0.4819\n",
      " 26/500 [>.............................] - ETA: 7:43 - loss: 1.9583 - regression_loss: 1.4673 - classification_loss: 0.4909\n",
      " 27/500 [>.............................] - ETA: 7:44 - loss: 1.9673 - regression_loss: 1.4679 - classification_loss: 0.4994\n",
      " 28/500 [>.............................] - ETA: 7:43 - loss: 1.9517 - regression_loss: 1.4563 - classification_loss: 0.4953\n",
      " 29/500 [>.............................] - ETA: 7:44 - loss: 1.9317 - regression_loss: 1.4369 - classification_loss: 0.4948\n",
      " 30/500 [>.............................] - ETA: 7:44 - loss: 1.9114 - regression_loss: 1.4215 - classification_loss: 0.4899\n",
      " 31/500 [>.............................] - ETA: 7:45 - loss: 1.8970 - regression_loss: 1.4115 - classification_loss: 0.4855\n",
      " 32/500 [>.............................] - ETA: 7:45 - loss: 1.9012 - regression_loss: 1.4153 - classification_loss: 0.4859\n",
      " 33/500 [>.............................] - ETA: 7:44 - loss: 1.9143 - regression_loss: 1.4197 - classification_loss: 0.4946\n",
      " 34/500 [=>............................] - ETA: 7:45 - loss: 1.9323 - regression_loss: 1.4306 - classification_loss: 0.5017\n",
      " 35/500 [=>............................] - ETA: 7:44 - loss: 1.9390 - regression_loss: 1.4323 - classification_loss: 0.5067\n",
      " 36/500 [=>............................] - ETA: 7:42 - loss: 1.9367 - regression_loss: 1.4330 - classification_loss: 0.5036\n",
      " 37/500 [=>............................] - ETA: 7:43 - loss: 1.9547 - regression_loss: 1.4401 - classification_loss: 0.5146\n",
      " 38/500 [=>............................] - ETA: 7:43 - loss: 1.9716 - regression_loss: 1.4478 - classification_loss: 0.5238\n",
      " 39/500 [=>............................] - ETA: 7:43 - loss: 1.9659 - regression_loss: 1.4453 - classification_loss: 0.5206\n",
      " 40/500 [=>............................] - ETA: 7:43 - loss: 1.9461 - regression_loss: 1.4328 - classification_loss: 0.5134\n",
      " 41/500 [=>............................] - ETA: 7:41 - loss: 1.9513 - regression_loss: 1.4395 - classification_loss: 0.5118\n",
      " 42/500 [=>............................] - ETA: 7:40 - loss: 1.9435 - regression_loss: 1.4363 - classification_loss: 0.5072\n",
      " 43/500 [=>............................] - ETA: 7:39 - loss: 1.9167 - regression_loss: 1.4174 - classification_loss: 0.4992\n",
      " 44/500 [=>............................] - ETA: 7:35 - loss: 1.9050 - regression_loss: 1.4118 - classification_loss: 0.4932\n",
      " 45/500 [=>............................] - ETA: 7:36 - loss: 1.8981 - regression_loss: 1.4051 - classification_loss: 0.4931\n",
      " 46/500 [=>............................] - ETA: 7:35 - loss: 1.8980 - regression_loss: 1.4039 - classification_loss: 0.4941\n",
      " 47/500 [=>............................] - ETA: 7:34 - loss: 1.8986 - regression_loss: 1.4031 - classification_loss: 0.4955\n",
      " 48/500 [=>............................] - ETA: 7:33 - loss: 1.9013 - regression_loss: 1.4057 - classification_loss: 0.4955\n",
      " 49/500 [=>............................] - ETA: 7:33 - loss: 1.9015 - regression_loss: 1.4068 - classification_loss: 0.4947\n",
      " 50/500 [==>...........................] - ETA: 7:33 - loss: 1.9101 - regression_loss: 1.4117 - classification_loss: 0.4984\n",
      " 51/500 [==>...........................] - ETA: 7:32 - loss: 1.9019 - regression_loss: 1.4071 - classification_loss: 0.4948\n",
      " 52/500 [==>...........................] - ETA: 7:31 - loss: 1.8995 - regression_loss: 1.4065 - classification_loss: 0.4930\n",
      " 53/500 [==>...........................] - ETA: 7:31 - loss: 1.9236 - regression_loss: 1.4131 - classification_loss: 0.5105\n",
      " 54/500 [==>...........................] - ETA: 7:30 - loss: 1.9343 - regression_loss: 1.4191 - classification_loss: 0.5153\n",
      " 55/500 [==>...........................] - ETA: 7:28 - loss: 1.9355 - regression_loss: 1.4204 - classification_loss: 0.5151\n",
      " 56/500 [==>...........................] - ETA: 7:27 - loss: 1.9493 - regression_loss: 1.4276 - classification_loss: 0.5217\n",
      " 57/500 [==>...........................] - ETA: 7:26 - loss: 1.9688 - regression_loss: 1.4402 - classification_loss: 0.5287\n",
      " 58/500 [==>...........................] - ETA: 7:26 - loss: 1.9832 - regression_loss: 1.4490 - classification_loss: 0.5342\n",
      " 59/500 [==>...........................] - ETA: 7:25 - loss: 1.9843 - regression_loss: 1.4511 - classification_loss: 0.5331\n",
      " 60/500 [==>...........................] - ETA: 7:24 - loss: 1.9868 - regression_loss: 1.4534 - classification_loss: 0.5335\n",
      " 61/500 [==>...........................] - ETA: 7:23 - loss: 1.9838 - regression_loss: 1.4543 - classification_loss: 0.5295\n",
      " 62/500 [==>...........................] - ETA: 7:22 - loss: 1.9809 - regression_loss: 1.4538 - classification_loss: 0.5271\n",
      " 63/500 [==>...........................] - ETA: 7:22 - loss: 1.9955 - regression_loss: 1.4626 - classification_loss: 0.5328\n",
      " 64/500 [==>...........................] - ETA: 7:21 - loss: 2.0028 - regression_loss: 1.4712 - classification_loss: 0.5317\n",
      " 65/500 [==>...........................] - ETA: 7:20 - loss: 1.9969 - regression_loss: 1.4683 - classification_loss: 0.5286\n",
      " 66/500 [==>...........................] - ETA: 7:18 - loss: 1.9870 - regression_loss: 1.4598 - classification_loss: 0.5271\n",
      " 67/500 [===>..........................] - ETA: 7:18 - loss: 1.9812 - regression_loss: 1.4570 - classification_loss: 0.5242\n",
      " 68/500 [===>..........................] - ETA: 7:17 - loss: 1.9937 - regression_loss: 1.4663 - classification_loss: 0.5274\n",
      " 69/500 [===>..........................] - ETA: 7:14 - loss: 1.9952 - regression_loss: 1.4667 - classification_loss: 0.5285\n",
      " 70/500 [===>..........................] - ETA: 7:15 - loss: 2.0070 - regression_loss: 1.4760 - classification_loss: 0.5310\n",
      " 71/500 [===>..........................] - ETA: 7:13 - loss: 2.0006 - regression_loss: 1.4736 - classification_loss: 0.5270\n",
      " 72/500 [===>..........................] - ETA: 7:13 - loss: 1.9997 - regression_loss: 1.4743 - classification_loss: 0.5254\n",
      " 73/500 [===>..........................] - ETA: 7:12 - loss: 2.0075 - regression_loss: 1.4804 - classification_loss: 0.5271\n",
      " 74/500 [===>..........................] - ETA: 7:12 - loss: 1.9969 - regression_loss: 1.4734 - classification_loss: 0.5235\n",
      " 75/500 [===>..........................] - ETA: 7:11 - loss: 1.9979 - regression_loss: 1.4758 - classification_loss: 0.5221\n",
      " 76/500 [===>..........................] - ETA: 7:10 - loss: 2.0115 - regression_loss: 1.4868 - classification_loss: 0.5247\n",
      " 77/500 [===>..........................] - ETA: 7:09 - loss: 2.0131 - regression_loss: 1.4903 - classification_loss: 0.5228\n",
      " 78/500 [===>..........................] - ETA: 7:08 - loss: 2.0038 - regression_loss: 1.4847 - classification_loss: 0.5190\n",
      " 79/500 [===>..........................] - ETA: 7:07 - loss: 2.0065 - regression_loss: 1.4837 - classification_loss: 0.5227\n",
      " 80/500 [===>..........................] - ETA: 7:06 - loss: 2.0099 - regression_loss: 1.4872 - classification_loss: 0.5228\n",
      " 81/500 [===>..........................] - ETA: 7:05 - loss: 2.0042 - regression_loss: 1.4833 - classification_loss: 0.5209\n",
      " 82/500 [===>..........................] - ETA: 7:05 - loss: 2.0067 - regression_loss: 1.4842 - classification_loss: 0.5225\n",
      " 83/500 [===>..........................] - ETA: 7:03 - loss: 1.9954 - regression_loss: 1.4755 - classification_loss: 0.5198\n",
      " 84/500 [====>.........................] - ETA: 7:03 - loss: 2.0059 - regression_loss: 1.4824 - classification_loss: 0.5235\n",
      " 85/500 [====>.........................] - ETA: 7:02 - loss: 2.0090 - regression_loss: 1.4866 - classification_loss: 0.5224\n",
      " 86/500 [====>.........................] - ETA: 7:01 - loss: 2.0071 - regression_loss: 1.4842 - classification_loss: 0.5229\n",
      " 87/500 [====>.........................] - ETA: 7:00 - loss: 2.0117 - regression_loss: 1.4881 - classification_loss: 0.5236\n",
      " 88/500 [====>.........................] - ETA: 6:59 - loss: 2.0147 - regression_loss: 1.4902 - classification_loss: 0.5245\n",
      " 89/500 [====>.........................] - ETA: 6:59 - loss: 2.0038 - regression_loss: 1.4826 - classification_loss: 0.5212\n",
      " 90/500 [====>.........................] - ETA: 6:58 - loss: 1.9960 - regression_loss: 1.4759 - classification_loss: 0.5202\n",
      " 91/500 [====>.........................] - ETA: 6:57 - loss: 1.9922 - regression_loss: 1.4741 - classification_loss: 0.5181\n",
      " 92/500 [====>.........................] - ETA: 6:56 - loss: 2.0055 - regression_loss: 1.4826 - classification_loss: 0.5229\n",
      " 93/500 [====>.........................] - ETA: 6:55 - loss: 2.0017 - regression_loss: 1.4805 - classification_loss: 0.5213\n",
      " 94/500 [====>.........................] - ETA: 6:54 - loss: 1.9945 - regression_loss: 1.4764 - classification_loss: 0.5181\n",
      " 95/500 [====>.........................] - ETA: 6:53 - loss: 1.9905 - regression_loss: 1.4737 - classification_loss: 0.5168\n",
      " 96/500 [====>.........................] - ETA: 6:52 - loss: 1.9969 - regression_loss: 1.4784 - classification_loss: 0.5185\n",
      " 97/500 [====>.........................] - ETA: 6:51 - loss: 1.9922 - regression_loss: 1.4762 - classification_loss: 0.5160\n",
      " 98/500 [====>.........................] - ETA: 6:49 - loss: 1.9864 - regression_loss: 1.4732 - classification_loss: 0.5132\n",
      " 99/500 [====>.........................] - ETA: 6:48 - loss: 1.9857 - regression_loss: 1.4714 - classification_loss: 0.5143\n",
      "100/500 [=====>........................] - ETA: 6:47 - loss: 1.9788 - regression_loss: 1.4669 - classification_loss: 0.5119\n",
      "101/500 [=====>........................] - ETA: 6:46 - loss: 1.9812 - regression_loss: 1.4700 - classification_loss: 0.5112\n",
      "102/500 [=====>........................] - ETA: 6:45 - loss: 1.9795 - regression_loss: 1.4677 - classification_loss: 0.5117\n",
      "103/500 [=====>........................] - ETA: 6:45 - loss: 1.9875 - regression_loss: 1.4731 - classification_loss: 0.5144\n",
      "104/500 [=====>........................] - ETA: 6:44 - loss: 1.9935 - regression_loss: 1.4763 - classification_loss: 0.5172\n",
      "105/500 [=====>........................] - ETA: 6:43 - loss: 1.9966 - regression_loss: 1.4781 - classification_loss: 0.5186\n",
      "106/500 [=====>........................] - ETA: 6:42 - loss: 2.0077 - regression_loss: 1.4867 - classification_loss: 0.5210\n",
      "107/500 [=====>........................] - ETA: 6:41 - loss: 2.0030 - regression_loss: 1.4835 - classification_loss: 0.5195\n",
      "108/500 [=====>........................] - ETA: 6:40 - loss: 2.0021 - regression_loss: 1.4843 - classification_loss: 0.5179\n",
      "109/500 [=====>........................] - ETA: 6:39 - loss: 2.0060 - regression_loss: 1.4861 - classification_loss: 0.5199\n",
      "110/500 [=====>........................] - ETA: 6:39 - loss: 1.9972 - regression_loss: 1.4793 - classification_loss: 0.5179\n",
      "111/500 [=====>........................] - ETA: 6:38 - loss: 2.0025 - regression_loss: 1.4821 - classification_loss: 0.5204\n",
      "112/500 [=====>........................] - ETA: 6:36 - loss: 2.0111 - regression_loss: 1.4867 - classification_loss: 0.5244\n",
      "113/500 [=====>........................] - ETA: 6:35 - loss: 2.0046 - regression_loss: 1.4820 - classification_loss: 0.5226\n",
      "114/500 [=====>........................] - ETA: 6:34 - loss: 2.0042 - regression_loss: 1.4832 - classification_loss: 0.5210\n",
      "115/500 [=====>........................] - ETA: 6:33 - loss: 2.0010 - regression_loss: 1.4813 - classification_loss: 0.5196\n",
      "116/500 [=====>........................] - ETA: 6:33 - loss: 1.9946 - regression_loss: 1.4770 - classification_loss: 0.5176\n",
      "117/500 [======>.......................] - ETA: 6:32 - loss: 1.9891 - regression_loss: 1.4740 - classification_loss: 0.5151\n",
      "118/500 [======>.......................] - ETA: 6:31 - loss: 1.9842 - regression_loss: 1.4704 - classification_loss: 0.5138\n",
      "119/500 [======>.......................] - ETA: 6:30 - loss: 1.9890 - regression_loss: 1.4743 - classification_loss: 0.5147\n",
      "120/500 [======>.......................] - ETA: 6:29 - loss: 1.9888 - regression_loss: 1.4737 - classification_loss: 0.5151\n",
      "121/500 [======>.......................] - ETA: 6:28 - loss: 1.9870 - regression_loss: 1.4730 - classification_loss: 0.5140\n",
      "122/500 [======>.......................] - ETA: 6:27 - loss: 1.9866 - regression_loss: 1.4722 - classification_loss: 0.5143\n",
      "123/500 [======>.......................] - ETA: 6:26 - loss: 1.9883 - regression_loss: 1.4741 - classification_loss: 0.5142\n",
      "124/500 [======>.......................] - ETA: 6:24 - loss: 1.9875 - regression_loss: 1.4735 - classification_loss: 0.5141\n",
      "125/500 [======>.......................] - ETA: 6:24 - loss: 1.9890 - regression_loss: 1.4754 - classification_loss: 0.5137\n",
      "126/500 [======>.......................] - ETA: 6:22 - loss: 1.9814 - regression_loss: 1.4692 - classification_loss: 0.5122\n",
      "127/500 [======>.......................] - ETA: 6:22 - loss: 1.9788 - regression_loss: 1.4661 - classification_loss: 0.5127\n",
      "128/500 [======>.......................] - ETA: 6:21 - loss: 1.9715 - regression_loss: 1.4608 - classification_loss: 0.5107\n",
      "129/500 [======>.......................] - ETA: 6:20 - loss: 1.9687 - regression_loss: 1.4586 - classification_loss: 0.5101\n",
      "130/500 [======>.......................] - ETA: 6:19 - loss: 1.9696 - regression_loss: 1.4599 - classification_loss: 0.5097\n",
      "131/500 [======>.......................] - ETA: 6:18 - loss: 1.9668 - regression_loss: 1.4579 - classification_loss: 0.5089\n",
      "132/500 [======>.......................] - ETA: 6:17 - loss: 1.9644 - regression_loss: 1.4562 - classification_loss: 0.5083\n",
      "133/500 [======>.......................] - ETA: 6:16 - loss: 1.9606 - regression_loss: 1.4525 - classification_loss: 0.5081\n",
      "134/500 [=======>......................] - ETA: 6:15 - loss: 1.9587 - regression_loss: 1.4509 - classification_loss: 0.5078\n",
      "135/500 [=======>......................] - ETA: 6:14 - loss: 1.9621 - regression_loss: 1.4526 - classification_loss: 0.5095\n",
      "136/500 [=======>......................] - ETA: 6:13 - loss: 1.9636 - regression_loss: 1.4538 - classification_loss: 0.5098\n",
      "137/500 [=======>......................] - ETA: 6:12 - loss: 1.9608 - regression_loss: 1.4521 - classification_loss: 0.5087\n",
      "138/500 [=======>......................] - ETA: 6:11 - loss: 1.9586 - regression_loss: 1.4509 - classification_loss: 0.5077\n",
      "139/500 [=======>......................] - ETA: 6:10 - loss: 1.9550 - regression_loss: 1.4485 - classification_loss: 0.5064\n",
      "140/500 [=======>......................] - ETA: 6:09 - loss: 1.9543 - regression_loss: 1.4485 - classification_loss: 0.5057\n",
      "141/500 [=======>......................] - ETA: 6:08 - loss: 1.9582 - regression_loss: 1.4525 - classification_loss: 0.5057\n",
      "142/500 [=======>......................] - ETA: 6:07 - loss: 1.9577 - regression_loss: 1.4519 - classification_loss: 0.5058\n",
      "143/500 [=======>......................] - ETA: 6:06 - loss: 1.9558 - regression_loss: 1.4513 - classification_loss: 0.5044\n",
      "144/500 [=======>......................] - ETA: 6:05 - loss: 1.9562 - regression_loss: 1.4516 - classification_loss: 0.5046\n",
      "145/500 [=======>......................] - ETA: 6:03 - loss: 1.9485 - regression_loss: 1.4463 - classification_loss: 0.5021\n",
      "146/500 [=======>......................] - ETA: 6:03 - loss: 1.9447 - regression_loss: 1.4443 - classification_loss: 0.5005\n",
      "147/500 [=======>......................] - ETA: 6:02 - loss: 1.9440 - regression_loss: 1.4443 - classification_loss: 0.4997\n",
      "148/500 [=======>......................] - ETA: 6:01 - loss: 1.9406 - regression_loss: 1.4411 - classification_loss: 0.4994\n",
      "149/500 [=======>......................] - ETA: 6:00 - loss: 1.9395 - regression_loss: 1.4401 - classification_loss: 0.4994\n",
      "150/500 [========>.....................] - ETA: 5:59 - loss: 1.9472 - regression_loss: 1.4464 - classification_loss: 0.5008\n",
      "151/500 [========>.....................] - ETA: 5:57 - loss: 1.9448 - regression_loss: 1.4446 - classification_loss: 0.5001\n",
      "152/500 [========>.....................] - ETA: 5:56 - loss: 1.9380 - regression_loss: 1.4393 - classification_loss: 0.4987\n",
      "153/500 [========>.....................] - ETA: 5:55 - loss: 1.9352 - regression_loss: 1.4375 - classification_loss: 0.4977\n",
      "154/500 [========>.....................] - ETA: 5:54 - loss: 1.9373 - regression_loss: 1.4379 - classification_loss: 0.4994\n",
      "155/500 [========>.....................] - ETA: 5:54 - loss: 1.9436 - regression_loss: 1.4429 - classification_loss: 0.5007\n",
      "156/500 [========>.....................] - ETA: 5:52 - loss: 1.9385 - regression_loss: 1.4394 - classification_loss: 0.4991\n",
      "157/500 [========>.....................] - ETA: 5:51 - loss: 1.9342 - regression_loss: 1.4364 - classification_loss: 0.4977\n",
      "158/500 [========>.....................] - ETA: 5:50 - loss: 1.9361 - regression_loss: 1.4377 - classification_loss: 0.4983\n",
      "159/500 [========>.....................] - ETA: 5:50 - loss: 1.9354 - regression_loss: 1.4369 - classification_loss: 0.4984\n",
      "160/500 [========>.....................] - ETA: 5:49 - loss: 1.9413 - regression_loss: 1.4405 - classification_loss: 0.5007\n",
      "161/500 [========>.....................] - ETA: 5:47 - loss: 1.9407 - regression_loss: 1.4386 - classification_loss: 0.5020\n",
      "162/500 [========>.....................] - ETA: 5:47 - loss: 1.9392 - regression_loss: 1.4370 - classification_loss: 0.5022\n",
      "163/500 [========>.....................] - ETA: 5:45 - loss: 1.9396 - regression_loss: 1.4358 - classification_loss: 0.5038\n",
      "164/500 [========>.....................] - ETA: 5:44 - loss: 1.9365 - regression_loss: 1.4341 - classification_loss: 0.5024\n",
      "165/500 [========>.....................] - ETA: 5:44 - loss: 1.9358 - regression_loss: 1.4340 - classification_loss: 0.5017\n",
      "166/500 [========>.....................] - ETA: 5:42 - loss: 1.9333 - regression_loss: 1.4327 - classification_loss: 0.5006\n",
      "167/500 [=========>....................] - ETA: 5:42 - loss: 1.9291 - regression_loss: 1.4294 - classification_loss: 0.4997\n",
      "168/500 [=========>....................] - ETA: 5:40 - loss: 1.9273 - regression_loss: 1.4277 - classification_loss: 0.4995\n",
      "169/500 [=========>....................] - ETA: 5:39 - loss: 1.9286 - regression_loss: 1.4298 - classification_loss: 0.4988\n",
      "170/500 [=========>....................] - ETA: 5:38 - loss: 1.9267 - regression_loss: 1.4286 - classification_loss: 0.4981\n",
      "171/500 [=========>....................] - ETA: 5:37 - loss: 1.9221 - regression_loss: 1.4255 - classification_loss: 0.4966\n",
      "172/500 [=========>....................] - ETA: 5:36 - loss: 1.9202 - regression_loss: 1.4243 - classification_loss: 0.4959\n",
      "173/500 [=========>....................] - ETA: 5:35 - loss: 1.9308 - regression_loss: 1.4310 - classification_loss: 0.4998\n",
      "174/500 [=========>....................] - ETA: 5:34 - loss: 1.9300 - regression_loss: 1.4303 - classification_loss: 0.4997\n",
      "175/500 [=========>....................] - ETA: 5:33 - loss: 1.9362 - regression_loss: 1.4342 - classification_loss: 0.5020\n",
      "176/500 [=========>....................] - ETA: 5:32 - loss: 1.9396 - regression_loss: 1.4370 - classification_loss: 0.5025\n",
      "177/500 [=========>....................] - ETA: 5:31 - loss: 1.9336 - regression_loss: 1.4326 - classification_loss: 0.5009\n",
      "178/500 [=========>....................] - ETA: 5:30 - loss: 1.9338 - regression_loss: 1.4323 - classification_loss: 0.5016\n",
      "179/500 [=========>....................] - ETA: 5:29 - loss: 1.9360 - regression_loss: 1.4343 - classification_loss: 0.5017\n",
      "180/500 [=========>....................] - ETA: 5:28 - loss: 1.9330 - regression_loss: 1.4326 - classification_loss: 0.5004\n",
      "181/500 [=========>....................] - ETA: 5:27 - loss: 1.9324 - regression_loss: 1.4327 - classification_loss: 0.4997\n",
      "182/500 [=========>....................] - ETA: 5:26 - loss: 1.9317 - regression_loss: 1.4322 - classification_loss: 0.4995\n",
      "183/500 [=========>....................] - ETA: 5:25 - loss: 1.9308 - regression_loss: 1.4316 - classification_loss: 0.4991\n",
      "184/500 [==========>...................] - ETA: 5:24 - loss: 1.9320 - regression_loss: 1.4325 - classification_loss: 0.4995\n",
      "185/500 [==========>...................] - ETA: 5:23 - loss: 1.9294 - regression_loss: 1.4307 - classification_loss: 0.4987\n",
      "186/500 [==========>...................] - ETA: 5:22 - loss: 1.9296 - regression_loss: 1.4308 - classification_loss: 0.4988\n",
      "187/500 [==========>...................] - ETA: 5:21 - loss: 1.9261 - regression_loss: 1.4279 - classification_loss: 0.4981\n",
      "188/500 [==========>...................] - ETA: 5:20 - loss: 1.9293 - regression_loss: 1.4300 - classification_loss: 0.4993\n",
      "189/500 [==========>...................] - ETA: 5:19 - loss: 1.9257 - regression_loss: 1.4273 - classification_loss: 0.4984\n",
      "190/500 [==========>...................] - ETA: 5:18 - loss: 1.9241 - regression_loss: 1.4259 - classification_loss: 0.4982\n",
      "191/500 [==========>...................] - ETA: 5:17 - loss: 1.9204 - regression_loss: 1.4232 - classification_loss: 0.4971\n",
      "192/500 [==========>...................] - ETA: 5:16 - loss: 1.9190 - regression_loss: 1.4225 - classification_loss: 0.4965\n",
      "193/500 [==========>...................] - ETA: 5:15 - loss: 1.9196 - regression_loss: 1.4227 - classification_loss: 0.4969\n",
      "194/500 [==========>...................] - ETA: 5:14 - loss: 1.9167 - regression_loss: 1.4206 - classification_loss: 0.4961\n",
      "195/500 [==========>...................] - ETA: 5:13 - loss: 1.9168 - regression_loss: 1.4209 - classification_loss: 0.4958\n",
      "196/500 [==========>...................] - ETA: 5:12 - loss: 1.9144 - regression_loss: 1.4194 - classification_loss: 0.4950\n",
      "197/500 [==========>...................] - ETA: 5:11 - loss: 1.9163 - regression_loss: 1.4204 - classification_loss: 0.4959\n",
      "198/500 [==========>...................] - ETA: 5:10 - loss: 1.9148 - regression_loss: 1.4199 - classification_loss: 0.4949\n",
      "199/500 [==========>...................] - ETA: 5:09 - loss: 1.9163 - regression_loss: 1.4212 - classification_loss: 0.4951\n",
      "200/500 [===========>..................] - ETA: 5:08 - loss: 1.9167 - regression_loss: 1.4211 - classification_loss: 0.4957\n",
      "201/500 [===========>..................] - ETA: 5:07 - loss: 1.9173 - regression_loss: 1.4204 - classification_loss: 0.4969\n",
      "202/500 [===========>..................] - ETA: 5:06 - loss: 1.9215 - regression_loss: 1.4235 - classification_loss: 0.4980\n",
      "203/500 [===========>..................] - ETA: 5:05 - loss: 1.9205 - regression_loss: 1.4229 - classification_loss: 0.4976\n",
      "204/500 [===========>..................] - ETA: 5:04 - loss: 1.9218 - regression_loss: 1.4243 - classification_loss: 0.4975\n",
      "205/500 [===========>..................] - ETA: 5:03 - loss: 1.9213 - regression_loss: 1.4244 - classification_loss: 0.4969\n",
      "206/500 [===========>..................] - ETA: 5:02 - loss: 1.9268 - regression_loss: 1.4282 - classification_loss: 0.4986\n",
      "207/500 [===========>..................] - ETA: 5:01 - loss: 1.9263 - regression_loss: 1.4276 - classification_loss: 0.4987\n",
      "208/500 [===========>..................] - ETA: 5:00 - loss: 1.9322 - regression_loss: 1.4311 - classification_loss: 0.5010\n",
      "209/500 [===========>..................] - ETA: 4:59 - loss: 1.9307 - regression_loss: 1.4295 - classification_loss: 0.5012\n",
      "210/500 [===========>..................] - ETA: 4:58 - loss: 1.9291 - regression_loss: 1.4276 - classification_loss: 0.5015\n",
      "211/500 [===========>..................] - ETA: 4:57 - loss: 1.9277 - regression_loss: 1.4265 - classification_loss: 0.5012\n",
      "212/500 [===========>..................] - ETA: 4:56 - loss: 1.9236 - regression_loss: 1.4239 - classification_loss: 0.4997\n",
      "213/500 [===========>..................] - ETA: 4:55 - loss: 1.9240 - regression_loss: 1.4248 - classification_loss: 0.4992\n",
      "214/500 [===========>..................] - ETA: 4:54 - loss: 1.9283 - regression_loss: 1.4276 - classification_loss: 0.5007\n",
      "215/500 [===========>..................] - ETA: 4:53 - loss: 1.9298 - regression_loss: 1.4272 - classification_loss: 0.5026\n",
      "216/500 [===========>..................] - ETA: 4:52 - loss: 1.9297 - regression_loss: 1.4276 - classification_loss: 0.5021\n",
      "217/500 [============>.................] - ETA: 4:51 - loss: 1.9312 - regression_loss: 1.4288 - classification_loss: 0.5024\n",
      "218/500 [============>.................] - ETA: 4:50 - loss: 1.9285 - regression_loss: 1.4265 - classification_loss: 0.5020\n",
      "219/500 [============>.................] - ETA: 4:49 - loss: 1.9270 - regression_loss: 1.4259 - classification_loss: 0.5011\n",
      "220/500 [============>.................] - ETA: 4:48 - loss: 1.9287 - regression_loss: 1.4273 - classification_loss: 0.5014\n",
      "221/500 [============>.................] - ETA: 4:47 - loss: 1.9266 - regression_loss: 1.4251 - classification_loss: 0.5016\n",
      "222/500 [============>.................] - ETA: 4:46 - loss: 1.9253 - regression_loss: 1.4242 - classification_loss: 0.5011\n",
      "223/500 [============>.................] - ETA: 4:45 - loss: 1.9240 - regression_loss: 1.4234 - classification_loss: 0.5006\n",
      "224/500 [============>.................] - ETA: 4:43 - loss: 1.9239 - regression_loss: 1.4231 - classification_loss: 0.5008\n",
      "225/500 [============>.................] - ETA: 4:42 - loss: 1.9227 - regression_loss: 1.4223 - classification_loss: 0.5004\n",
      "226/500 [============>.................] - ETA: 4:41 - loss: 1.9213 - regression_loss: 1.4216 - classification_loss: 0.4998\n",
      "227/500 [============>.................] - ETA: 4:40 - loss: 1.9192 - regression_loss: 1.4200 - classification_loss: 0.4992\n",
      "228/500 [============>.................] - ETA: 4:39 - loss: 1.9206 - regression_loss: 1.4212 - classification_loss: 0.4995\n",
      "229/500 [============>.................] - ETA: 4:38 - loss: 1.9168 - regression_loss: 1.4186 - classification_loss: 0.4982\n",
      "230/500 [============>.................] - ETA: 4:37 - loss: 1.9176 - regression_loss: 1.4195 - classification_loss: 0.4980\n",
      "231/500 [============>.................] - ETA: 4:36 - loss: 1.9198 - regression_loss: 1.4219 - classification_loss: 0.4980\n",
      "232/500 [============>.................] - ETA: 4:35 - loss: 1.9197 - regression_loss: 1.4223 - classification_loss: 0.4974\n",
      "233/500 [============>.................] - ETA: 4:34 - loss: 1.9237 - regression_loss: 1.4261 - classification_loss: 0.4976\n",
      "234/500 [=============>................] - ETA: 4:33 - loss: 1.9249 - regression_loss: 1.4273 - classification_loss: 0.4976\n",
      "235/500 [=============>................] - ETA: 4:32 - loss: 1.9237 - regression_loss: 1.4264 - classification_loss: 0.4972\n",
      "236/500 [=============>................] - ETA: 4:31 - loss: 1.9222 - regression_loss: 1.4258 - classification_loss: 0.4963\n",
      "237/500 [=============>................] - ETA: 4:30 - loss: 1.9220 - regression_loss: 1.4256 - classification_loss: 0.4964\n",
      "238/500 [=============>................] - ETA: 4:29 - loss: 1.9201 - regression_loss: 1.4242 - classification_loss: 0.4959\n",
      "239/500 [=============>................] - ETA: 4:28 - loss: 1.9168 - regression_loss: 1.4218 - classification_loss: 0.4950\n",
      "240/500 [=============>................] - ETA: 4:27 - loss: 1.9198 - regression_loss: 1.4237 - classification_loss: 0.4961\n",
      "241/500 [=============>................] - ETA: 4:26 - loss: 1.9205 - regression_loss: 1.4234 - classification_loss: 0.4971\n",
      "242/500 [=============>................] - ETA: 4:25 - loss: 1.9193 - regression_loss: 1.4229 - classification_loss: 0.4964\n",
      "243/500 [=============>................] - ETA: 4:24 - loss: 1.9166 - regression_loss: 1.4211 - classification_loss: 0.4955\n",
      "244/500 [=============>................] - ETA: 4:23 - loss: 1.9175 - regression_loss: 1.4217 - classification_loss: 0.4958\n",
      "245/500 [=============>................] - ETA: 4:22 - loss: 1.9154 - regression_loss: 1.4202 - classification_loss: 0.4952\n",
      "246/500 [=============>................] - ETA: 4:21 - loss: 1.9161 - regression_loss: 1.4205 - classification_loss: 0.4956\n",
      "247/500 [=============>................] - ETA: 4:20 - loss: 1.9145 - regression_loss: 1.4189 - classification_loss: 0.4956\n",
      "248/500 [=============>................] - ETA: 4:19 - loss: 1.9161 - regression_loss: 1.4204 - classification_loss: 0.4956\n",
      "249/500 [=============>................] - ETA: 4:17 - loss: 1.9150 - regression_loss: 1.4202 - classification_loss: 0.4948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initiate training for 50 epochs\n",
    "!python train.py \\\n",
    "    --tensorboard-dir logs \\\n",
    "    --freeze-backbone \\\n",
    "    --random-transform \\\n",
    "    --weights {PRETRAINED_MODEL} \\\n",
    "    --batch-size 5 \\\n",
    "    --steps 500 \\\n",
    "    --epochs 50 \\\n",
    "    --gpu 0\\\n",
    "    --multi-gpu 1 \\\n",
    "    csv annotations.csv classes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%ls snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'snapshots\\\\resnet50_csv_50.h5'"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in last checkpoint\n",
    "model_path = os.path.join(\"snapshots\", sorted(os.listdir(\"snapshots\"), reverse=True)[0])\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify what backbone (type of architecture) was used for the object detection\n",
    "model = models.load_model(model_path, backbone_name=\"resnet50\")\n",
    "# Convert model for Keras\n",
    "model = models.convert_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Bed',\n",
       " 1: 'Swimming pool',\n",
       " 2: 'Stairs',\n",
       " 3: 'Chair',\n",
       " 4: 'Lamp',\n",
       " 5: 'Couch',\n",
       " 6: 'Television',\n",
       " 7: 'Kitchen & dining room table',\n",
       " 8: 'Billiard table',\n",
       " 9: 'Fireplace',\n",
       " 10: 'Toilet',\n",
       " 11: 'Sink',\n",
       " 12: 'Bathtub',\n",
       " 13: 'Refrigerator',\n",
       " 14: 'Gas stove'}"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the label classes\n",
    "labels_to_names = pd.read_csv(CLASSES_FILE, header=None).T.loc[0].to_dict()\n",
    "labels_to_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function #1: Get predictions from model\n",
    "def predict(image):\n",
    "\n",
    "    image = preprocess_image(image.copy())\n",
    "    # Re-size and preprocess the image\n",
    "    image, scale = resize_image(image)\n",
    "    # Add additional dimension to image tensor\n",
    "    boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))\n",
    "    # Rescale detected boxes based on the resized iamge scale\n",
    "    boxes /= scale\n",
    "    # Return all predictions\n",
    "    return boxes, scores, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function #2: Draw detected boxes on the image\n",
    "# Set minimum threshold score\n",
    "THRES_SCORE = 0.3\n",
    "\n",
    "def draw_detections(image, boxes, scores, labels):\n",
    "\n",
    "    for box, score, label in zip(boxes[0], scores[0], labels[0]):\n",
    "        if score < THRES_SCORE:\n",
    "            break\n",
    "\n",
    "    color = label_color(label)\n",
    "\n",
    "    b = box.astype(int)\n",
    "    draw_box(image, b, color=color)\n",
    "\n",
    "    caption = \"{} {:.3f}\".format(labels_to_names[label], score)\n",
    "    draw_caption(image, b, caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function #3: Show detected objects along with predictions\n",
    "def show_detected_objects(image_row):\n",
    "\n",
    "    img_path = image_row.image_name\n",
    "    image = read_image_bgr(img_path)\n",
    "    boxes, scores, labels = predict(image)\n",
    "    draw = image.copy()\n",
    "    draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n",
    "    true_box = [image_row.x_min, image_row.y_min, image_row.x_max, image_row.y_max]\n",
    "\n",
    "    draw_box(draw, true_box, color=(255, 255, 0))\n",
    "    draw_detections(draw, boxes, scores, labels)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.imshow(draw)\n",
    "    plt.imsave('predicted_image.jpg', draw)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>x_min</th>\n",
       "      <th>y_min</th>\n",
       "      <th>x_max</th>\n",
       "      <th>y_max</th>\n",
       "      <th>class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000325b47e09c6aa.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>469</td>\n",
       "      <td>128</td>\n",
       "      <td>741</td>\n",
       "      <td>Chair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000325b47e09c6aa.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>546</td>\n",
       "      <td>183</td>\n",
       "      <td>960</td>\n",
       "      <td>Chair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00061ea456165593.jpg</td>\n",
       "      <td>196</td>\n",
       "      <td>530</td>\n",
       "      <td>481</td>\n",
       "      <td>761</td>\n",
       "      <td>Sink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00061ea456165593.jpg</td>\n",
       "      <td>604</td>\n",
       "      <td>536</td>\n",
       "      <td>875</td>\n",
       "      <td>700</td>\n",
       "      <td>Sink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000698b6a00772ac.jpg</td>\n",
       "      <td>463</td>\n",
       "      <td>393</td>\n",
       "      <td>1023</td>\n",
       "      <td>584</td>\n",
       "      <td>Bathtub</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             image_name  x_min  y_min  x_max  y_max class_name\n",
       "0  000325b47e09c6aa.jpg      0    469    128    741      Chair\n",
       "1  000325b47e09c6aa.jpg      0    546    183    960      Chair\n",
       "2  00061ea456165593.jpg    196    530    481    761       Sink\n",
       "3  00061ea456165593.jpg    604    536    875    700       Sink\n",
       "4  000698b6a00772ac.jpg    463    393   1023    584    Bathtub"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Will be running detections on the test images\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorda\\Documents\\Capstone\\keras-retinanet\\keras_retinanet\\bin\\test\n"
     ]
    }
   ],
   "source": [
    "cd test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAADnCAYAAABBu67aAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAEAAElEQVR4nOz93a5t2bLnB/0iWmu9jzHnWpm5v84+Zx8fl0EuF5RkLMtYyCAh8QY8A1zyEEhwgeR7XgC44B244QIhbmy4AAHCFJhynXK5zv7IXGvOMXrvrUVwEdH6GCv3zixqUxISyrFPnrXW/Bij99Zbi/jHP/4RIe7OT6+fXj+9fnr99Pr//qX/v76An14/vX56/fT6/5fXTwb1p9dPr59eP73+Fb1+Mqg/vX56/fT66fWv6PWTQf3p9dPrp9dPr39Fr58M6k+vn14/vX56/St61R/75r//r734AgwR/tCd77bBui70u9Guwv3TnX/j48pXzcDhzYXf7p1La3y9Fl4vzm/f4J/edsqyYrtDGXw0+G//zQVXQUdH18o4OmupNFW2bWOMQRFFVREtuICJoxi1KPet4y5cWqNo57pWGM44jNoKVYV929ndkdaQVvg0lHccFG7fvWMdWms48Hkf3E1wURQohLdxd5ZawAaG06ryYfz7LP+n/yXb8h9hdBwwgcOc+2GYSyygOCAIEF+Jf/9JXYWfP/QnvjHfBfyPfvtP/tL3viPxPhJ/TmGHiHC+nfjj5773m19eyR+//5/Wifgf//N7l+oOKvPvf/wD56U93cIffbDE2uSdISLne821+lNL+0Pilvmzj/f549cfXYr8i57Bv5ySJp7L43mBnH91ebr9H7oH/7HPfN5//vTH3LOcH+R//JNPv/L4nohQVVCcQuXr/l/n//LNf5f99T+iFeHni/D3fnbhdQEx8MNAnNKUeqnoqpTS+O3vd/5fv33DDH7+emXxjo+DuiwUFbobNoxixnpd+Nmvv2G5KiKd/bbx/oed2+fOdjhDBCkFbwVfK29b5/22M4YwDmMcg6oFUWU3570PtmEstaC5/LejYwhrhVYKww1DOQz+d//o2z/50H/UoN7NMTNEFRHNJY/FMAdHGA7DnGrGWiqrCGbGsIKJ4lgsuhuKgBTMOrf7wRBYgSYjjNewMKSAloo4WHeGd7o51oRSHG2F1gq4UCu0VjAGTmw4d8JYt4L2joojBSpOc8Fw1lLDxpTK7sa6KmrO7RiMYYgWSmmMMdgs7hVzvMN+GKX85/z2L/57bHxic6er8t3u/O13G/uuuMZnCoqKInGDcYEIJhZmwKfBBBdFEMz9YQDE0fn9uZpPm/1L2zJ/ahqF/IbHgSxF6GPEvfiXh0Uk3/OLXwZ9OpcO6HnYzwvEzk9XBEfk8T4/7AjiI4pEkGRmcV0i8dzDPMYzItbK83/zXc0dxWm1xL31gTkUEVDl6IPxxS09GRMHEX0YzVwrEygIVZVhhvlc86f1dnJXP56LpNGL/ReG/XSkHmfoj+//2YHlmubvnv/M72peezgNcj0e96KxwWLvS3zn/MTTOOc+ns8k78n9cR/zPc3B3M6ftXRSZg7ujKd9VlX4aq2sdK7yNf/et/+rfLZQxalVWNZKVY8zUMBMsCFs7zvLKMhLoazwi4+N4spSwIZjtTIQNhdEAigtBV6vlaoGx4GPznjfKUAtQrc4S8MdLYopIHGmxzawPqgaK1gwLiq4wlKU1oR7dz7dB4cVhjvLItQmLKIcLlj/o0d5vn7UoB7mtFLy4eRB0lh9F6EjbAhb7A4K8FIL3/XB+76zLo1aFE0M4T6AigEHigyjI1QXRCQNRzzAUuLhVQmEOu47Yxd0UY57GOClFi5rodQ4dNv7ARYPv2OUVlEbFIfLUpEhWDeMwtKcwaCLsHenu7EWpV0an247/YgjYyaYjtggqmgpgbTjuwzrbP3A25KbdmBquATKdSwO37AvjNC0XV+8XP8IFRUSlYhiNqZ7YgK6acDcHwdM8sxYnnpReTImI5yjPg7V/PnnQ3xep8yD/0Bb02DE71safUWwfA/HJxqeSPEZgYYFj/dRQ4Bh4/y8eN/HJyJ+/rx73E8Y93yzRNdaAgFpid+RsApPhuu8q/P5PaO0eZ1hdBS3jovg/viNtGZfIF7Nd5Xz9h4GzNIMzs9x8/n44zPdzz1/RglP6ytP/xbRh1PJr00MLqJ57c8vO9/74Qc9gc28g/MnHqg8/BHifhpSSeciOpH7XCtHVcPQYaxieVYEUbgUZUln3XFqEWpt7IehpbHbzn0zlnJQxfm4CmqK+MBL4RDl8za4d6Op0gSW0rCj891vv6WWgrhhPqiXK9JBRsfN6WYsuRdchG0Yn/dBK5XdjDGcVmAtsFZhN2c7Isp0UbSEkV1bRRhgg7UuHMP4odePGtQisUmqKn7YeQjl3BiFrcNeBEeoDq0qq8+NKohYHgTFZUdYGGbcB6yjc6fABksVtAptWeh7IIWRnlbcc2Mb3jWRJWgFWUCaUChcXdj6AYdhFoe9oIg7a6vQhE+fb3Hdw3CFMQZjO/AiLEtlKQ3tzqcx8pBbuGtik2AjNjOJrswQF2w4brGZLPerSCALwTF5IJSJluCBEqZVi8P7MFolkY4LaCnx2YnA3e08SJpGRp7e3/SBIlSUIuD6pww5iXLiPczsNELn788j+oxsnxCOiSMnsp7X9vhzosEHy5BIS+I+1cNxu0wMJed7z7UK2yInCpwQvJsRezxc8nFYOLJpYPK6vkCa+bvP0XoAxELRfHYl7vxE6RMlTmP2hG59osO5oM5peCSRZ9iqJ+d0erJcz/M+n76We09VERFGH+f3zO3hD+Tx9S9fea02UbMwRB4Q+1yT6arzLWXeRuyASSP4+fOPfaAayHARDXAEqCT48ohWv3vfAgDVwpKocbhxOBFBjXmmK2oFtwBYmNNaYfcOPii1MBT6bhxuFInPKs2xPti7RTRrEh5X/HGGEmG45T5V4TCoNZ7L236wo5gUUEF9cGmKWmdobNzqzhg/tNb/AoNapeDjiI3lxjBnoUICfsPpDt0EKRGaX8rCR6kMM6pEOHCGf+n5HCit0hZn7wNpgmHsJugYZ9iOCH04/TgYJgwCdRQVWnFqA18Krs6x7dCdWjQWowSii9Cvc/t8p19XdnMOh79920GE17rQaqNUuNRC3zeqOB8uyj5guDAsHkIVUPe4p3geNAEdxpCBu5yBL+4ZAPvT4ScN8TOazIPpnmjmcWDPDfx0eOVEB09ozwSXCUcTyREHdIbxJhH+uFk6pyezOtGU2ReH+fHy07Dp0/enoTOm89AEkxPLxvWeBvE0oo9LP5GY+OOazoN8ftLT9T7+bu6oyGnkvyRCPK91Gr1EcBl6uyeVMp/Fs6GVec1yfn1+3vM1Sq5xWlkgKRGRQM8ZeU1XF886H9nT/Xner/iTkz0R5bPjejLgjxDlxyjc05ietziNu8H3o6HzoEKiZXKfnW5ixluoR8QGRi1Oq4ImHTA/d7iwu/B5d6Q69ThQ6Wng4rm5OmtrWAc1Z3Sn7/0ZQFNa4WUtXFrh47Uxjs62CdY7fTi4UodS785xgFFAnVIVlTi/GIgNLjWfgUEVEC3gcLfBrgUXQbVgRyDmilPM2IZiHr8r/sO5/B81qBcB18JAKFoRP2K5y8CloRJoTV1ZiiDuccEG5oVWC4cFlwLBhC0u7OIgyvKycMFpS4bm5shwSiu4G4ZgbvghiApFFDfQBV6+urAulW3fGMPptwPp0LQh4pg4VgTVQI/33rGtouZIhc8dvvu88XEd/Pxa+apVxCLEV5yGxQEthd0dL8K1CEWMesTB7QguhUrHPAyKSz8TB5L3PTmt8IMeSJuHMfoyVHtGZrmv3c5NqiL0iWDnz4o//eozf+cnZNV58DSiiecwXkQwwlkgk5+0E7EKiWzzHp+g2Im+4qODc3P3J7T1ZBzFT374pGGn8TW+F3I/G+a5GPGLOs+4pBEVfRj38/0f9xj+6gmB+eN6PI3k5G/dgyJy9PH1XGvJ64jnG4dKn8lUPKO4RHU8Geb83BPbPRmzh7/x833I6FAQxGLfuVvkJSRDfOCxcb40jn9khOdKPvzBgwaYjvcJP1v+TfWxZucujuOb6x9nTDUitQkEDoPPByjOLz4UPn7zQpXc+x6BnnVDGNz7wWEj94FjIyKv4YnC9x1B6JeFUgruQTN2F1wEG7AfglvHRtxcqXGNA9i7sR/OfRtAQ1TY+4FqpYrTEXYLgsRdoRtFCkUccY29NAalaDhv+XKtn18/alDdBKtK74ZLRaqkZ5b0phqpoKLJ1hSO7iyl0M24HTtDW26auIgx4qEOM1BlkYK4U8RpDYaAVg3UkyivNgUL41oWoa1CuzaO7tzfPELyUbBjQIufnwhtONCWRASVWp12qfzFN1eOEeEAw5Hu9L4FqnRjrRWx4FKkgo3BqrCoUIuy4/T9wBSkBCKZyaOZoIBM6swdKDMckkjquVNFzk0+N7agib4e/NzJdRJJkxkuTpT2/P15hI3g3HQezPN/z4FmIuKnA2M+ebrH4XOJ9bc0eipxeMySk5tIXOcvTFT79EHCmXDR/LtOLtc8Tu+J0p9+T+W0M8/XxaQQzHHxpy9b2nw5P+85jZQu5TQqZ6KNcARFhDIdD8E6PuBdUiNppB9rLpg4w/2Lizf/3nrnZ33BS8+vJT9pfxRBPM7PdGAP4ziTdQ8HRP79C+AqD2PpeCaX+OL6I0Z6+j13Somkau891C+iuBgPZ+Whxsl94FJwIqE9GHy4Nn75iwu//NXKqrknPFBh74PuzhjhiCUR4LBYx2MY+zE4+qAP5/0Y7N/dHhGeOW6DkU6xj+BFzaB0oXrhUmpsR3OO7uyjY7mZigxMPT/XWTQS6jbXwyOCNkvawGF043sr+8XrRw3qd/uOWvAjZv3kEsOgxsbajoPfy0CAInFRlxbc44sUpD4STppn4ejwT39/57YrX10aX39sKJHJ+/z5zn3f0FqoVcAcOwwfcWjXtbHWC7dd+Lu/+5a3N6NoiYdpRtUdrYoriMb1fLis+Oj0saGXgvjg64tyfFyQYVw8jCryCNH7iARQH4ODQCzv3fCaXt0FPMIph/DSaUjC1TyhVE+oOUP7ecb9gSAmqnInrI3FwRJ58JjTUMznOakCRzCZzi7eOE1cGO5EDYpnkuY8ZY/PPtHzI2Sb5i1uwTOTzBMSyd0gPA60PD5/co7PxuTZ8H//5U/GyB+/cq7xDIkf8JjTKDwQbyy3PhsszVRMXoynMS2nbY5fNvcUYQQF5UnDTMP18DAPyubx5zRQ8oVhm2s5vYs//zmvbxrPSbk8OTfHUQezhwhMntbR3TMyenpWT+s5nftMbtmDWDod3sPxPUyFnhcHkPkTf1AaJ5een+vAPgZJzHFZlZfXxl9/feWvPi58bKHIKdpQV3xYUFUaBhCP2zeLNR+EMzq6cT+cbYRBVITrUhF3unccY7inssjp3Rkj92+Jz3Tgsio//+bCfY/rNNeIwghk3fJhmjj7GCBKa43RO4Mw+tJzP/8JxcZ8/ahB1VZZm7EW4bDC52FpKWJLFoWqhZdLhdyAKoIpLLWxLCW8bXo0FTDCUP1ud27j4Lefdz7eGmuFpSpjxMLI3pEjMog2lG0bAeXvd95757IqxyEcWuiqFFWKFnYPVGkSSaJqzv3tTnGoq7CsBY7BcdtZeqeUAsO4jU4rFRS8CN1HosiSaLpw68ZuxnrvfCBQZ1UN8NOE2+AM1R7GM8LRQigXIHhhy90/PaEx8+GO+3gEcPJAOHGuHXU5jb/lQZiPeBrxx2FKY6qhtSgqZ/pBJDbwec3f873ijzP1oNQsD5GfofDkAKdllURvJJUQ7yMnijwRojvUkqjA4clQn2Hl0zU9h6jyvbDrvIa5CpIOL4/76ZbOLLszmMj08R7TIT1z3g8PkNIkpsF+oLRnxPlH3OSkITIpcnK+8mWS6wEX5UTB4Ig6qhHq/in52fPzIdc3jEPey5P/mV+aUjpJ7+/P38v94+4hHZsOwJ3hCR7OJGNc67JUymKoKarwYSlQlY+L83EB6UcgSnFu7wd2dAqCa4T3M+l2XRrbtkWoLUIRsKpsNgKB7p3FQj3kEpI2d6GURmuCrTOYCJpQPGjJpTofXhf24fThHL3HnjFhdD+jRjNn7wcuEZ30UdiHY92BcFz2/ef79PpRg/rVRbjWSsXYXPnDHqFHHMSZBHHWBZZSaGjqApValUt1diSTOLFJQ2KhvF4XXpaQy7wPj/BUhLUtXF6UVguiEVKO4azD8Z5vUwyK82Fdgw7IayqUODA1SH+bIdEw8MgWFnW0wIeXeACqLfRubiytoBIyIEnZ1iSgxwj9aXfYDsUcbim1Wqi4FmSMuVvPHS4SWXlVoZQSYbIfmD2MHRlO2EyGuJ+ZZT1DVk+E9WTyhFhxz0M0ra5PSsYRSV4Th1Q1PFMMeXrOg3qiK4+N+sVLOLnIiWnjy/E+PhEMGb4iFKaD4aHiz3WBODBF9cEUADP8zcs4Denj955+bv79yXCQ33MeaBQ/V+GBJNEvLNH8LE0qYviXyHgmguRErX5+3b8wzN9bSx7I9YxYnpdVJsp/vgc/Ha6cxKcnWn98rjwZtqdPAwnJ3QPFS3459tE8vl+qTHxuhz96zb0i5w/IuaZLFV4uhWIgRwjoejf2+8F9b/TdGPctEkT94P19B4frpaHiKXPrtFrxQsjdxmD0wVDh0Mq+d/ru7O8Hv3vbkaIcDsMdKQWKPigTc7QWREJ1dMr4fCAuFFXEhWVpoVW2g+V6QVQYw7h45TwaxFqNY2A9FSgpJf1Trx9HqG7YEMwGpQqVQI8qSo8sBsfu3N8PTAUrwlpS3D3JZ0+ZDw+soTiMg1UrkzNcVGku+HC6DApGlfAMZs7Hlyu9DBDhIsq1wHXVCA/GoJSK5oHWAq5x3ZgzVCi1UmtBi1MK2OqIVCgNO6D0lGAUR5oEb6ca5Ln7GZLsJnziA29FIrm1bSwotYQ8ayZyyHPjGX4c5ox9D1QSmahYG5lJoBManOhtrqOc2JWUq+VK2gN5eS7yNMhhT+207ZNnDfAmJwj7Pv/2pRF4OlBJW0zPPc9tvE/5Huf3PVRN0qNzXfwhr3ooIR557+eqqZmlPq/xYR1xgttaF02ODFyUFuoytjEwkUD00zDlOpzXkO/5sGN2GjJsOg0/JVjzvp4rvOYznA7hVEM8xdQPLaw/ePH8yFAPPBC2nP89EHbc9ZeG+jRyJwXwfBX5GQ4RPeaGxM/1tflhj9X+4rnPz9FZOpTr8uyQVZ2lKBULYDI8uUe4VbjtO35U1ALdje7gwrpW1qUhlmvrimrJopqK+ghVSltQF4Ybb7cN62ClIEO5H5EAHsM4UiLo3Tj6oLaGpPxvuKFa6L1TRFlqCRqTzq0PhjnXzSkCXZxKRMsC1KXycil0E+63jerCon+mbGrgLKqnpqyVQv8eKojITiil0q1TGAwFlcLoxpECWimJyCQSWNveOYqgbiGBIrmibvSdlDwEH3IMZzQ4zGjqrCJcu/GCsB+dPgZSO8MMLTW43dERoJVAy9INKTulwvW6oO7YiKqoYxuU3Sg4sgp2RPYQLWkQAwFfaqVYSKTuCN+sK5/ayn7sDOsMf2xLgVNCJBJSr9M+aRZKIE+b1B/AxUN3OBNKc7UDcesD8ZWHmNvzvZ7OBSHj0kTIUa2lOA9d2tPh4WHoHgb86YKe/vpkjeNL1s/D90BuU54UBur7VH6oFxypJbnLmUDyM/x7CkTPD48kX2Zb1alaWFvNcDq4r8tScIPjnvU8QUynY3xCk2LggURPBPc9eDa1myIpD5v/CTyH+o97/94//KEkeNAUz+bryXA9ff0LA40yE3jG0/V/8XHzM2LNn9UFz+v35XN6usfpcPPfz38/OWDiLMQyzvtxSpEwZHtHEvoWgSbCWkoU5wigztqERRu1FPwY7D3Il+VS0GIRybUFs041523A2J39cL57P7ChrC1kk6hSSmEfnT2BnlA5zOmHUdHzbDQXzAt9GIf1yPQfg+6KE1xtsH9R+KIMYHBdCtf2Sj9G3F9Wev7Q60cNKjhLU5pJZuThlh7EfSTZH/mcY4wwSKnlcokwWrXGuZd5sEZyNoEAVB8Pr4hQJMLi5opYZ5EsKx1ZbGQjSk8B2TslER3m0HskGkyQFO9K81woOI4DK85razA6mLFtG/u9s7jSlpYIVti6cYxBrQ3EGd24s7PbYNw2BOfincu1sV8r7+L8G3//zn91cXYLw1XTKBR1aglPL7lJnzOJwaXNdXjiF5kGz/P/5IHk/Mvn5N//0nloRn6jx/qfYWscN/vCaIVsaP77PP6ncXtEGee/5WH2HucukPUMmeTZQuS9TCqj1Ih6xuQ2/fuf/j3E+vTpjiM+qBpyvpmYmaqGfWZsXSYem57jgfJPaDA/xyIklMgsO1Eu/Pj+fIuJZOdLsvh5JoGe1+oZ+f2xMbTn63k2xEwZmFNqwec6PT+bCDmeUD95XXEvfPHTcQ3mlkmueChfItwvXyKaMrXcYSL0Dv/7/1j4u39OKlEiKeTOKaJfFGqrXGvFu2FjQEor3ZzbHhn+InB9aaEpd8O95DOQKN0egXyxwWVtfPc+uN13MGNdV5ZF6R7acPFBdzA1llq5lJKFNxVFuI0xLzjK6WtUZfXuHIPI6LtjgaJQMaQoo4+8BqBAP2mvP379eMgvhVoba8JimfZepuhEQXtkzPLxDOCwkERcJAxpFTjcHhtYMmuZspNuxtYFXFkqaYjgbR/UFodhHEcg0Bpcpic6UwLdDHOu6wvixmFRq+tEOCA7lDWQ8TiM+21nSQ9XVaHV8KJLlE+KA3agqmhxVOMaMU21QFZLmVFxbHR+8xvnf/w/vfOf/RPnvsX6FRFUs1pqHjJ5oMLnI/cE+8+/S/79PGN/4hl9ieP8j35uRp2PL37vzZ8/4Puh5ffM1+Ni/XsXIzx/dvzLzs9/XJ1/8Rbf/6ufP/+n7vT76/DlNTxrVp/f4o/f6UuHMA3g+f4SwbAgX9Syn5/6uMEffc3bnm5Avve9L57JpGn+6K39dDCz8OHH9sB8qz9Gpt/7uXPpvjQM33/veJfxQKj5tv/6v+78L/7nlf/wf1KoIlxKQRlR6JJVZq+XwtEGIsbRHekGUhCEMQZLq7wulWWtrJcLe9+53e9oNUo/KKWlkD4MalPl44siUrkdxm0/2N0Z+8FSCXmURxJNRpSlLzV0wurhRA4DssZ/SvSGR0XmMaKYoNSFpsHB4kpLCylDeNVGw7n/wPrCvwiheuhGVRuMwSW5lFM+I/MhCsMGRScCCt7TNcOVmThJVchTmxWOMb7knAyKOjCoJTLj2zCGG+6DYcp7d67FoQoqzrsUvrvf+VlZuBKifkcwS7H9EPoepaQuWV2tSqkFGRE2tiK01Nl2m+gaqo44DkUYRGns2hbeEcyVfeu4d2TA/eb8D/77hX/0jyOj/tKU66pcl9CuzkYRonG/U2wvpDjaMhM+ObIZopLhvk6kFI1mThmQTPQUXzB/0gkaMCu4ngFT8r1nBdGp3fTw3pCa3Cex+ml58mxNoyURWj3qCwLpimZhBpIJgzBUknKz0EJmfbqE0iGKOeLNY0/oGc56cvgANiz1pwlLMwNu7ozs55Cl/CeytInk84BJhoTP8a8RvKyK0M3StD543OkVH5ynMLnqqdOYS/Mw2frQ+cqTMfOJAB9rIK6nUZzhveLUGtWH4yyAiE+Rp2s5TX9+77wrfzj1uP/8HXnsGeazfnoT4ZHRF5JOEPgf/o92WpvOWimiSFYe1Xw3Ow5oI5LLYtRagsarBTP4cF1RN1QG437jtu1ILZSkgPqwkBe6RWMjLRx0WjUoSl2vbN2w3gM8DWdM2aFHzkL3zrUKSymYC8NLFAGM6CswCP07xfAi1LZQtCDWmUUfH64XVCvD76xNaR400Q+9ftygCmx957VpkM/Rzy4MljweYCuKWmwsBRiZnBEy1Rie1lQeXaqIpFAhQ32Epkp1w0fwGUtT9FLZxhHGz6JO933rUYXlBa3wrsa3w6hHZ1kDzlsqAtyBGlrW4UKpGl1pWgMNnal1AQ2+p6zRF0BLiJnXNUouPn3a+Py+54nIDkndcI/FD+MDxw7bFsdBzalSqAhanS5gQyhTepYINowHkYDSPJSn8TIcxUxPK6ae2c10VIgwfDz0l6556J8QiJFawkcS5ZGln+fOs1AgnOZ48sQPfo4nzi2/l1xkOAZwH5kIIpJ7EvcR4ugwVvaw5XHY07t4anhkmjt5luETXGnak4c2cxqVaUizv8I0qvJwDp7Y2cSSGookYRhzP516OHCh53tICWWLycNIn7phj6TcyKucjsWI52RP6/ggGhLHuzCmaM5nqJ5ONPG2EsmckVpLZDqtL3zc97D0l70jZhrFn5yMy1y7h6b14QTCsdSSpj0derSvkHMfqEREWbSgfrDktaWwhu4wamVRYVFHqzAsuq2ZD2yPHIsPo66V2sI4Q+yRoYJWqNUpR6pURILJsig8GMwKPQkD3yKiLBJfm6uNG000lAMCtQbCFjIXcwxEjSIpD8OoqvQj6L8dWCos5YfN5r8AoRpmQc/GudU8rJocQzz+KsG1MrKKJzfXIJt7ZBJBpJwcnns85OVJgyZpcEse2KYTwUZSpVJi66nyaRt82gdelfK6sFOj6YOGYLtbZ/KSLiGhEIsHEQ/D6Pmfu1JFg9ZwEI1Kje0w9n5g5ny+bbgrimHjeGxdkbymNBxn4iEynr0b3jQPgZ6bPlMdj1DqKTEgBII6y/WT24tDlI3tsqrDE/Lo96p5xMMQzY9gtguUhxkNY0aeSg1UKY+T+Uh+PJ7ZuTXcT2PesymMnkguLNDz/UT2UiMZmPyHS6z1NIpiWduVa0WGfPGz4UhSyht0VuFEiwm64wC5nlTDRI1FQZgJMM++t4q64ANMopkMiUDMJtpPjfCMl/N+RxaCzATe5BBjzTRr5eP3u1voOSGUGe6nwY7rLCEPPEt346OGg/vs1RbRlhaYWXs/nx1nJHM+e5/OKFHuEz86jedswiAZ7Zx9DZLSEzl/OJyCBz56cOrREg8R3ELjXGetqihG5dv3Tm/w2pRrVSpxzlYRalkYZbqyToWs449+HJZJJsVZKuylgB0YHfOg24okZsMZElVbF1W8d5obzQsy7ET6rRQ2iz2lEiDDxhm7hCM2TwfkdIusv5uhkRFF9M80qDeLBIscTjNhQ+kDxp5ZLomQ4D6O4FtLpccTPfsQugdiHIcnGju7ZjIQ3i1KOgVlIXtZ5gYtRPOFuOGANEpnbWGA9zHovWP3jRVYVIERRlJAJJJeYrFYEbooY0Dfoy9jFc0qI0OkUaXx/n7j/b5x7IZSaU345c8/0NrCvu384b9wkCite964MOU0oQ9kblzPwzDDJ7EsuZQvYmefGzsR6yAcmgiBGvNnbHZSkgfO/KLyBrJjVMql8PNzXLJO2z0Q4nMy5NQ2zfd95LWnJjUokzSSeQ9NorDjYfSjf+4zQpbs9cosEU2kpRLStGgdHiJ2EiU/KsdmYk2/CKsnNTAd08lUnuG1RAcwAAuHp2mIAukZuJ5IC8BLvFdROTW+0XjmoQNevDBK5gByuUT0rPt+UDbxLAcWSBk5C1fMgrZRCX2ymSUNEVTVSVekOsHMOWZ/1vz3iYDdcS9fGPUALDPz/+Bwh5z2Lp7pU0LqpAoSvZqDJ7Xy2KvkWsf6qSqlODIMoXB4aLQ/7cZWBgdO+ebCUuM82LFj7ix+UETPKqTRB2UL6WKpBU16YDfYu3PfO5/ed3rKEKuP6CGCgnUohV3ivZoKS1NWjeS1uuMqDODT0TnMMSksGj1DNLtSSY2GLmFN4yx6H7iGXnoqZMaf277vdzcQMf4g4QlEjLfd4Ljz8rKgwNIW3sfgQAI+p4FRc7SHaHdH+Xy/cVmF61KQEr1Q3xGEyj6cwwZuxocWaoJZT64SvRbHOJBSGAZ9GJdSuNTQtl0vFcZgkUBBE+16htATnRYxaqvZ2CE2uUiIFn1E9lFFWGpFLlA+FNa1IWVQS7QV3G7jPMQiwn0MmipuYVw0EXmcPT+RmItQShhGZjPg0449WtvZRGgyQ98pZfI4/B6cm53ht59SH8nIwSeUmCjy/JyShmYGgAI+zlBv3lP+RnJ736uClxJm7UngH1IufdAB58nL63ssQvQ+wBNVlEd/0HmgnayoskCvZrmumvzphNxZPOAlbfNIoF/OsHvyllEfTwT7M8wVpXuUC897CXmwoc8sf/KMngY9jLUlcZDPPJ2Pn4UZWYuW3HPxR4sDLRrFIxmVlTIRqD7KmZ8DeNUM02dV0HQm8e+exnUiq+gDmgYYj/LORL7mgbyicXauSxph4FTTWZY9kzv9XDMPt3cSDNmn0jwiyVt33Du/tuiY35bCL3524ZdfXfhwiQbTM4EsGUH0vJYRxDc2OodEpFN6dMh3M27bwW6pLLIADHVGRe6MHk3onXDQQ+Boha2PyJO0xs0Gb2YMUZqWEzb03vGshLJHW5h4VkXw8PjsozA6nIqRP/H6UYP6vkcIFpSfn5nql8vCdan84dvPGAXLMFezEUTGf49ySFH6AcvHSreD4wj08X4cgWYFXqry1p3vuvFhqbworALqA3fl89vG8nIJhDICp0kpDB9c1FkShm/5gLL84kTIYWscXQRdC1rnRpv9DZ3j6BxHpdaGqLJeCsuqHMfg2A/e3g62vafxJBrV9uB8ZmXTGYWlF59avbO5hc41majvSQGQ4S2iZ8gZ4Z2eTSeMEcg9Q+nZMnDaQZHn0sb8uSfOU+S0EUyj/qwzfK6cMfH5hmHozCIhmU3Gp9Yxjmk4EjBcRlAQEGslMDOS8TN6rg8632cEK+CBcPGJxOO52ZkMyFBb4uqNgaBnWavKw4hqGgNnhmya6zCLbyfmf2g48YfBqRqVfxO+PlMeUHIN8hvJWxuzD+yD/9REyp7rKRrrEst+iq3y7HyZ7ZeUGYpPDe4jmHBCv810REEOZ5cm4RDL78WaD5/qmkTeJ9aP67X5NX8UG0we+kyISSSZPBH3pCpUicZE2bz8qsJa4Ndr41dNqJZNjkugUlcoWqAQSU2P5u2jH2xHKALMAzytFdZ25ZsPzrZH5aTKlGAZYyjdYRnGNix7f8D9iEi6NMFLGMqVZDpGx8fALaNMS0fpCVDEadEmL/p52GAfclZV/tDrRw3qX33VaCXgbsuEhplyEF1a9rWi2jhGp/cn1JPJltkETDBkURZNvdeeo0AkwpUiwCiMpfG7+87yNvjltfDziyRnonTA9oO1BVf6ee+4GkNheztYVVLAHki6SPCiSxGaW9ZtG20Pz1dqQSQEwIzsFiWV2+eNZV3pNqjdaJvi3dgPeNsisWVSgnDvlk2mc6xIbtGoTFKcQbfIPg4PvZzHPIakNjgPPAiuGq3LTpSUakJ/HGafWe8Za5KNUabudv6sJOJ6QowPwXYm1TwOubsj9qBbSHog0LHmYc0wPznAeMzhKEwtMdbkcSMcTpIPJMKu2U+0INFgIhUX58fm5zIdc97IPLz5bmEGM+T1GXpmCebIwzCfhZPJ0SzH1efrkf70Gc4shAhO1OlGOK86JyVoRgp+8o8+o5W0a6IpJvf42qywmr1pg499OISpcnjsg3zOaSXNIpSe7QSZSHU66LSwZ52ZPqKSaYBJZUBhJg157KvzE/Xps2N95xsNnGFCT/Qr8mWBwboUrus1orytoO5Id/a3zrgflBfFLSIIT4PkBlqEcmlBXVj0KxAbLN1BBuYCh9Fa4ZuXym07OEoAkaKB2PYe7fk88zR9BJViIrylrrQVjyb5DptHoRA9DLN5CbVR/u44/FR0XF8rawMtC+u6RBHQET0Ffuj1owb1ZxdlqcJShbVGR6duxu/uzt996lzbwq9/fuHYNvYtBdAZZi9NWVpUFRjC4crhxs3h+tWFVQjvQBDZpRY+9c6nDT53gwq/+tVHrtpwyVlI5tSqDBpDo9a/lhA9SxXIRg5zsw6MUYQtrYwQI1vMgN1CuAvZfi/lWcPR0XOjRcaPbJSy9Uwa7HEQb/5Ion1xGHw23ob7MLZx8Pk+oo+AhDGZkieejNgj+54gUEJ+BPwxyszuP5OrnWDy8V7z3/l+k8NkhuCJpCbiEcloZCbZHgY2UxQnsT+rnJA42GIzOcPD7J1/PCKBSDKQ0aKf16ieYdxEydjpCIoE9+V5+nVSHJ4mQKbn8CeENxNhnFDvrFpLlcHAovhD9GGY/XENmh2/nMcBOx0mk5sGzmqc+L7KRIGci//MVMjcI6dDnGsUazqR47y16ajDKiZ/mtZ73p7O65Dv/X7+f5fHZ6joCWTO/Zp3liKeWE7l0cPAJvqfkVA+ZUm5nxAt5PaBHHu8aTqk92Nn88JSYiCQ6KB5ySgjHPa+dd7vHbODpRReLwWR1L73QVsKZRvUbeAjR7HEBoUjDtq6VFTCOFKMriCtctvuvC4h8hcpfD46owplDSPvopFzIRU/R66tOMulhirB9SzKGd3o/c80qM2jxl2R3HxC9iUJFskOvCsr0SChJO/XWjQ3WVrMKekibAa/e7vjZlwqfH2JBZh1OVIL91tIIroJe4e1NV7XigmPElMptLpwfQm5VinphG0gJbKlSxVqkUzkREMWKTBbsglOk5hkOg9PJCayy39u0KIS3pbpzWNz7vrKpyLUa+X9iJ6JGzWNzkwicaIWN+c4jOGRmQ4VnDwdxIkhLY3GQ1TuIhw9BDmlZLg8eZ5pUP2BWM5j9pSpPZM2qXcUnrO18SxVpuFNHlUe2VPRuG9Vyaqj+eE8JXTCUcTfLWdhCSKPKMUzC6uZLJxFDxpQLmiOvJAosJjKiTAbpyHP954eI0Lnx2ryZJxJpDjR5MiFc4lVfLS9mzREhOQC8aeHhGYi5Yn3Z5QQzW6mMgEiQpeTt40LTKc0oSGhMvjS4oejd2aCKLXcpxGbagcBl+8ZxEcHsUkVuc9eEWf+L77/1AR83oeIPlFWXyLeZwroVAL4JAsMp3C7dXzbqOPR9Ux1UiB5H/n8F9FIUgvZtDyUNkeP5PNSNdtwFhyjimDHYGwGI5JWw5xRQ1lwDGffOmtt4AZ9ROPqdaHb4DgcaXGOR+/YfrB3CwZqZiInfz8GTSQmnA5DB4gU7vcdt04rGrTFn4tQt/3AS8GPGJuhKb5WFVoR3u7O59vB4kYdyVGX4G+sD9QaJsZRAqFiIRX59H6AdQohrlaV0KhaIJghzj6c2y2G8dGie82WzWbF4OPaKDUOxDEGez9NM7XCpRUuRc4xB/LaQv+pMfvmgw7UjNv9oEhLp9FREe63g8PAS4n+rkUZ4nTrCEr1zrsKv/x65a2/8vn2hpeUw2Riq2po3CgZvrtyqYJoKidEM5ubBr7omZyKNm3l3JACfLgUfvFVi8OT4bQkgjLTzCyPx4GxRJQSLRHn0SUPJLnRx4S8zNlfDxglqSX0s8tO6F1nxjqMdOokyTTNRIMu2Xh86kKfUPe8DjgTOQ9/IEiJLD+nTXygVeVh8F0fmftZJOEnWk9uOmmJqFrTL4opRNK1iWTT7jRyTpajkJrIkiG5cZbByuMg+pmnkTTUuQgA9pBCzYWdxt/zPUOKPN/XcTFUylmUwUxuxv970hvnvx1cZgnCAwmfKFrCcEm22R9zZtj5PPxMCkquhzqPxGegggjz572mOXV3xj4YI5ztSE5SMFTAh7DUlaVa8Nh5/3304E1R1qVkci7Te/1gdI2mRe6RMBKnLCUNX+hMa2uIhNRviLIfGypKyz4iw7OhvSmmxuWlUpbK+zZ4v2+PPEVk+xAThjhVw8bt951t7HSHVcPgDmIO1Z9lULWEYNlVGBISDYh9tyyFt1tnHBKNT4hYoRObsQ/j6AelKl1j9HMkNGB34z6c4k7NEOwQ40j3pu4c5vyTP3zi63sg3eHROs+DcKHvnVtRCkq3MbX2lFLom1OlcymwlDAe+l55uRbWRalFgE5R4eJGxVjbwjgG+zFg6+x7COWlCqW2zC6OMCDHRsFZfEOrU66Nj1dB5H6iTs+dOKVatejZ7NZHaNpEJRJaceSoJe4lGjQIVQrdI7K5LsrPPtbgwKyGwSSuJ5pva5L7YWwj4aUP9JZ8Zppg8uSfPTM9WcrZrxwkdLszxJ8GkNioyAPlTNsXiGoeQjmN51T5WdIXpxFicsTJ46Zx0DQYY1hy0PEew6Y200KYnw7ZPMPyGaJ66nRPlJbJpXzJ7FEr0YlMiEw8ZwVbGExNQ1wokUfLp1tL6po1DHPhwUC6ciL3ObRwIsFpneIYlTO5IU+/PzyMVpGpWphqj7MvV6yx5Xua5v0J5iO55OchzzxUIImwJwQ/k1Lz/03bkr871w8PSdfwyCHMlYxqvViTSLz5GUGIEI2MqtCasvcdleCkZUQjooGz7xvLuuSIaccPp4/YRCqkpMzRlslCC870NpwPRVmrYlXZt41jGC+XShFnsx6KFI33qVq4rMqyKKUopYYcq98P9ttxgg4tEs2daokafjMWLef0ZpXoDfJDrx+feuqDpWhQ4gIUxSlsbuxHVAhViY5RIpI9RCOcNYnu4I3C4cYiIaGyEs+zSVRGLVKiU9MZnsaz7e68daPt0I8RGzwn/IXhMT4dMfdpWNYRV2HR1MoSA/ZuqVtrNljXFo1ccA4T1qXyel3xMbJNXjBi16WiRaOZdKvRqEEkNgPjnHHVRNj7iCTXVOTMIyIntRfTRvEcCjAlU54hxzRkqS1MQbggUErMHCdF35lUMRw0S2gn8hA7D8jc8kIcsNlNSRL9hBRJoUQhRiRF8kjn9NFozhvv9ujdEEjGjEgqZUgJRFLslHPFT6vPxEhekdYTLTHDZ53nWb4IpydymnDIzU+d50zKOJZ12mmInFM6FA1H9BTUz2h7zEorDwMd3dNKVkd58t9JPyT8c+lp7NLu+HHSJScS9dBen49/8uEzeSSxZ8os3yabakskUkui5ukUQgcLlOCMy+zY5IG4pmG1qFJB05KHH31wsidJEeFJJDzlAYhzJ5yD/OYDNZktC6czzNPhxmTLOQ1qrGvhIZgzjyZDtSn3frD3zqLKy1rBDK0LmxtG5XK5MEZnu+04Chp7VpUwaj3m1okRM+dQihg2OPdo5AQM3GhLpZjACJqslGjCE06/g8O1NbAY8icIUko4UxtYHxEx1zCixYjxSppO1v/M9n0lN0AjPIULHIAOOTVorURoO1LyU1tDiyAMWlYouXuoBHIhFokBgIpxUcPSUHaf9eWRbS0SNfZrdqkpRR8ZvSh9QQv0e3SDGT1QjWqEzyM3z1Ir7h0s7qOU4GiEmCyw7Z1+7PF9cZa1cJFGJ05rbPasymgr2JoJncdh8ax1nRtKckOO7jE2xaIcr9Ro0hLEVmQaScI72oNFaHoMi8OS3eznpFI3eWTyAZvQPEXfY6IZz2uwkbyupwF86A7jUCQmmQjN5sEmoWeg1PNDPectzaRW0gqqUSqIB/p40BexdiqcmW6ZiozUXM4QfGbqMwY+5Urz2U/EG7QHCPVM1g0bj8gg1UIz62X+GB1jiaLEY8jivCfLW50gaxokzzcLmiOf8PBTpjUyAeX+yOjbqfN8JKgMj1LltG0+RsiO5lM4ecl0KB45CVOfKrWUmOn55KLFY7xDzMHKIgXy5yTig1C/hNtWZqifjk9OHXvepzPHvkyVQidLSOe6zpfPUeogHq55JiVjtHqs+7Eb98NYLjVHn8RnzHlq0geXVmlX5f19w3vPs12D7lFBpdK3AzzkTLJEMtGGUFuL4oDjwEYPnellxd+2eLYEd74fdiZV+7Fz7J3DBqMIWiJicKK14O4jlD7yUH0M87Pn8p9lUDPOiRvSCP3FUi8oBfFO0Sjr6yM8fJNAtFVDGTCGs6fURkZA/j4PpijXUjKUgNvcXT4NWIZgGKUETxjzneJ/rSil1pC1eIxxJjlBISqnLtW5Vii6siq0AqIRttEP3t/eQsKhEVLNSQQqFhUWwxmjs1wX2qXGBpz6tlo49nFWuATPF5xpzEGfhk9P3uxxwJ60fxYIZB/RDCZmCFkkeYjM7KxRH2koPMNcQTIMDpQQibFEs+Kpv81AfpYOMxHoAyme8qnJn+XPTOQ4jWoc1jR0ImdVUIRDoIRsxi2SMlPL+ii61XzPNO6EAyh50CdvfG5aJ6VO02BPIz6lTh5eVcgx8NP68Ji6MDliCQ57Gq162m47sfBMoc3/PEPPibonDznf44uYIC2rP/76oCN4hNKTaw+pT2ySc1/k70iiaE9AEs4yFL+e9zl6VpB5cMGe6zUpvmmgpwElUbtIhuuJZud6nf0O5ASq5333kbmRpGrIs9nKc+9aPz856EFDa0ljXDBT9u6Mw6BkbOTw9umdy8vKerlQFwspk1tIJUXpbrQi3GxEpzcpeO777sboRqsNDtiHsUpNG2NUjSisSEzrGKMnxRQJrrY+ZIHjGMjaECkh13JhUcFL5HhGjz3UH4jkj14/PvWUvBgNbk1NaBTEj1OGoyLUGsho744Po4mgPby7QXol2D1K+roZt65RMy+KjTA+tQhIZwKHkRA0mlOHyHcfRtFKbZUhUbK2GwwLCuFMqrjRtNCIg/P1ZeHlQ2VjQ1AoylLXqLA4oqOUmaEjqrLqUigiWeNduFwvDO9s941+3xHg5brwh83Bsgn3PHAiGRKPcyMH8k6Bv0wkm1IWz8xyntU4jIHobSJDSyRhEeLP0DikTBEiucg5VVYyY64+kzMPOyEeBDwuMSQtU19KGMghCQ4zpJ+SojkyRCm4j2x4bBF2aVy8+DykibJGRBImqRbJ+5aEXOIDJKp6QkYzs8+cyaXZwSzObBhek3A47o4mFxlVPYHGHU6HAw/DP0eGqOqDu53vLelkZkLQ7alEM4PnuZaZ6Z3zloQZ3seam4DrY9hcRNzpmqaBlTA8OpUaHiBi1vSX5MBFJnKc8jWywsmfqJB4jkZQE8M8igkgQQ3c9keI+1wWPDP64eZmsuxp3+Z+HB5gZBr8Ih4JpQy1XQTNZgtDQFR5D70h3gff2eAmJXMIO62V8IVe2L7b0ffowL/dI+vP3A0Ki3m2+XT23qP/cmkZucTePyz2y3acLHCUcNvAKGhp9OFhN9SQIhQPKdTZFEiNVjUaJkkqWMTx8jSNQ/7MESiHday0HI0bWsEoatXT+6lCrcFb+ujcj53mjaVkdjn5EAMOc1xCmL+JRDf+7QhpikBPGQ0yJxlGc5HDHdP4/W6RVVRVTGLi6cvLhbfPbww3DokO7GsetqpKk8gcmxq9D1QqItFOrNaKsCEjUWmFbkrvAeZCE+m8vd1gxKz6kGgAY1A9RMEzjp6HNM7PFNKnhUoDMY1VHlEmh/nQSj5Q4JSsKY/Mc4yEyFZ3FpMbJ7c5PJqyaB5kJaKGGebysKsnouEBUk7j4OksPTPCDzIjUMF0FPNW5m2eifK04F48DbMw6/mfZ1WJRPmplGkA0wBbcn3CKT+KP2YfiUSfxpkMsVzzDKEeXOxzZcv8uzy4RRfJsBV8Uih4UhLzuWmuynx2fq7JrDSbvvzs7MQj0prUxiwtTosfsqBslCIQ/GJJadtcU4m5W5yIXPDyRDcQ+5wz0kiUO+KcHu7ce3zofX84IjOj1ooNy3HggCZH7ZZ9BmJCqdmkPCJaFHVKcS61ULqBTv48FsGXgqwVq5Xlw4Ja5+gH3cBV4yztR3CUNRJPMgYD5zhg346zRLXWwiBGTx/DuQ/YD6EVCxVNDvGbEzNqt1NiuB2d2/s76/3geqkpxyocfUZ5RiW1uqUGnUmnRXVOSLxcQAMle0aOP/T6cQ61ljg8rhSPN9vG8TiIeA68MsYY9B5Iz80gZ7JohmndsvlrbkgD7u7cj2iRVZ5C1Zl8keEUD36o9859HwRpLRx7NIFd1gUbnRUSXYRkKLoxwXb00LveNl7rgolFO7/R6aGWpLaK20G9LOzbhovwvg0GUGoYsHKEdm1ZYyCfk42uPcP0Fhsp5XckIGVKfM4wNDBG1pZ7Hhh9lCUGhDzRpxNt9Ixo4ac6G2YpXSxq4y1QyDHbu2XoqIkSJk6L/rCEBnIiMiJMN5sOUplBrRBhqZ+XnwaMcDaBZNPipTGZ2r7ikuJwHmG7TiTE4890Do8Z92GoTmkUc10VSz3u1MWGDCdHEp8/m+0BHczSvAV8zA+VSS0m8uPkMVVSPpUHLOC5YnQyzRbAIp/RKalPmYOdn0E6kWmAMyEy704VukcfJY+nc/izSuELlxcFF+5P6DQM9OR0gze2dGZTReBY0ZNKUDh1UJqIl4wYPdffZCYCY5+0Eg3YDzOWtQbAGfakWX50/zok+OGatubDtXL5eOFnHxZ+9qHRSmX0hYGkWsc5tj1GDGmMKSqtcowD6xpD+aaIPwt/1Ab1CrIN6mGMAfdjZ3SDcYBHY/kyOiW6STNMeN/g8z4obzFTavYD0QIfLwtrrRxH5/1+RHcp3SMZXStjGCady7LSSmO/7/zIBJQfN6j33TikUJUQy2ulj3Hq2Azhcx+Aso8IXRqVS9FMBkViqMqcGx4bflbiNI0LFjNqiZHTJQ/prCRyd+jOizZqVe4Ohzgug2VZo3nBEbyJzpkwRc8OMkcu4NKdK5HsEhNu70f0EHVjKVHE4E0xjWoLaYL1fsq0GqGjZRi3I7R2xzDMBqrh+TiPgDMFe3OExBw7LBIJAs/Q/fx5mcEpTGlLmYalhNY2uKcRKMTKiUSOw1IlEDnbyCVl+8AZis2Ql1RL5OtRP+UPZEgahKzD9xMxp4GIjE48SxHQmCapKTgPhyI8QnyySc087PP94j2rlEfrOvN0LnmtiRCrlOxepqdFngd76MP4QPQYDc5xypJC74g80KwQjVosoy8SDePRG4Az6PAsZpidrizJ2hlhAKTELttLIpF4md+dvKxC9Co4r3RGMjzWPGwjPrtuMbWtmjTYo5Y8kkc5iC6+8KAfHKJWUKeMNeiAXLvpSM6mKyd6lgQ3nHzhWT6rCf7zolWFy1pYlpj9ttTCMhaKwgd1Phbngx0sIbmhZlZ+qQGKrEVr0OhdmqNPqlCWir3EyOf7dqBa2cfgAJZLY10HlZhEfLGXE6Wbj0C0GZGuolzXxst1jb4AFjp2ejRv2bvx7X2nsmM9Rrt0dxjGJxmYbnQ3tAivq3BpcBzObdt+wGL+C7tNDVSiUWtTodWEQzjdhO7K7z7t9KgjpXtImDhmP9TwmrVGQqkV4RidEF+FgHYmccopyHxo7Q43DosGEOaDY8QYgwOhLnFMjvvB/f3g2HPburG0mJi4XnKMgim7B2WwNOVwYdtiNpR5ZBtfa+Hj1w3XFkaoRNMFUeEYg81DmmVH57ZFjf7ne5Dt+oAfWWYopxg9TfgDb3gcw1mZEu30MtRnIsaZAAkjNHd8q4XaUmc6Ai3tw9m9I6mVw4VS8vdOQ5EcYgrs54GWhNIhMSqnpnAmvuJMp8nP8DkvNgzejHHNn+RXE6FnAi0R0bDZH1/OTPU0K4mFwyGciDEcYgY4zJn2cTkzFPf5qWeoGesfhnskqgtZ0DSY4ewiIZKdnibg9DBc83l4Gqwq0WgnbVZe04N7jcTRdDh+XuOJZifGz2dPhu1n+khC8zsR+tlQZ6LioacBM+/n84AHvXSupgdVoed1PR74sMFhkeyZH30/OtGVy0+e2cleu0d/0FY2zuRnrL3RirIu8KqKeqGJYrfYw8WMa1Feq7I4Se8NhnVYSlTKEX09tm2H244LlFa4LAOrjXsfvN8HPjqf397Zu1GWC7tZTDWWOCkf1srHS0VGFBSVotFnuSp3jHIpESWPirVoHt4tZoaR+Rk8qsWOI0pe9zE4utHMWFpMXNiP6Csitf2AxfwXGNT12sAia780QYumFs+gF1Q666q8vEbn/aNbSp/AGuwS6LSUglTl2ir3m7Msyi+/vlK8s/eclV0KN+v4MXB1ZAjLsvD6UcKzSQ9ifIsN/nKprC14rasubPdOT57nWoXroqxNczBocCr96FyWhhSFdYHSEFLz6s62xWaNDDRQg7fp6mfYKqVgLcjtbz93rhfl0kIQnK47DtcpNwrjptnib753TFWV5M78DKEkuZvI7ucpJwacLq2wLAET3DW6bu2Fb3tn6x0Rx8bstcopwJ+IcSLRyUIUUcqZyIvoAPdZAxBlt5kpm1RG6JXm1wJaVQntoBBJzImGqyYF4nFYxS3VH8rMOE+ufExe2B/O5WEwH3pXf9KLetokJxIWlmoNLTyqYJysLCNpID/t2eRWnUhIhBOaXb08nXuqUtKUej7j4FqmsZmtC6fBTzSc1E3Y3eQYeCgDOO8RCs7sRzs517j8ifHj2RRRougwR8swx914GmxOKdR87sEPP13LLE8lEOqkeNJGM6amOREyRC7DyOgxL6moouYhARuD3Tq+Z9+L3UIBs8dY5zGEo3cogmphfVlxN3bfWOsSvZS7837bY+NehNvhfHo/6Lvw+RDed6F/Pti9J3uhFAZfXYRxabx6DMS0otylcHNnG5aNVIQ+IrioOTppCbTH4Y/uVZdaub6s3O4H3313Y3TnFx8qaw0YeO/zbP8ZBvW6KFiUb65rJnBy14zPBzfprK3welUaoZ3cfVBFWJdyln+IxsGVQ2jSaQIfL5Uqzj6U4mF0ywG/v/WQSVH5uFR+9XWlZica96hrx+C6tjiwLTBgPwbbHl40BnTFjKhaZ/OKCDVaia9RI4SrpYBnE+vzQIcBC8SX01ZVKGWwrIXtt6/856rc3bltB9chvFpLYxxoJ6pCyLxreaAWSFT0QFanDInM4EuEjXOCQR5RWoFVs/NWxsTFo2cj3SlSotIjdZDdZug8DXZ8bwLLaDTSz5pxxTOrbudhC94wROsxnz37PhlnhtVmRZFkFyCBRkQkrcjJ2ak6r2vjgxQW0eDGgW/3O7du7KkHjRBYTgslopR8nh6iyqiI6z34YAzL6QvXZWFpQZXUUjKh8mjdF8J+T8F/wOEQ5k1WMtBIzOUKGmeMuS5phmyGIrlnpjHT2RYvM/CngGkmpWaS8rEX5rOV2fyB6UKFB6UwDVkqD+b7uiDZU1aePI4nX6Hy6LE6+97OeXCehnR+VGip49N9Ri06BRszclJglpZG0kwseNhaoycxhZPuq/OWLA7F8hJnxMYB3kCE67Xw+vKKuPDp2xt+VLRVuhTcDg4X3rrxPpT3YZSmXNc1Wv+Z0rcNNaPvhreo7Ny78GkffEod7Mta2cfB0Q9eW+UqDVG494P394O3I7rOicPrRfj6GOG4joGMge2SjZkaazqnH3r9qEFdshSy1UB7KtERe3JqIplZ7gP3EZnCrK+2PWZfozHHyXtHRnQrSlYYEacRVSPFhQWlSiHrhsA7xeBaG1rrOQiwANeqgaiSa2oa5bBjWP6MId0QLyHytwO8YHtEUG4Wg7nWBayjHk1ZopgBFuClloxoAwXUZeHyYeXzTfltEX7z8yufbWc/DvaMrIKbCgmLToSQp8Tdkey689jLXx6iObRtuEQYArGJI2mJlyiTrESj3ioSWfzqrGtBtHD0HlyYlrPiJlBpPA8zO8Xik2jo/riKebA1oeocESFCjFa2eeWes4EcPYyXpSIl30skNIDy+ByDoEv2d2riInfnPga32Vcg/zzr1jLEDDCcRkoISZIFwn9Zo+rF+mCtK9++7WzHoC71NG4q+uS4SAVKysrmaHOZ2DQQXKtxkKKFniSKn6G9nGqX51ldpHQnjE6O/PlesmrKwITZrAQemtYHUuQLmmYmonJ9jFQkzOcwnWTuqXRi00N1TwnX3HdPa3Gi2/NrwiwQ0NnNLJ/f5FMhgVKJisHrUuEQHI3kXiWccC1oCTa3rEGPjZwHVdfKshQu68L97c59u+MU3Hs4AuL3lq9eWbRy2Q5UYSmG7A7dsKbQg5Y8cA4FG2Be+HzbOYrw5p2+jyhCqgU5wml8uu14W/I+orDmn/3h4A+3jZ99+ADduBTlsjQq0bpvULD+ZxpUOyybfRANQnrHdqNbNlqVkGFs9x59I0cYVF8kOI5h0RKr5Q2PjnvMGioIdt/j5yzQwKkaFMXFuB2d7S600lmvV7Z9xzTQi9keCakORx/ZrSZCvVbgemkgMRVRpESyq/fYQO4x2ro627ZzrbEM1gfSYhPIiOqbVsOx2NbxO7zfb9y//YzgfL0MvrpU7odwWWftdd5FhgWTB1U3OoZLOZNGqpmggnNUdSXnLgGzrDQSWaFfn5NZrXNynZXIMKuGcD0inOhZMMPu2dmJ7C1Q8vDFaUnUMpFsGk/7IvQG9xifbSZ4Ij6L+j8uo7A2oCnmytBAQEWmNCxnk3nqVkdywe4s7lyN09BAFjycBmN+eWqbs4l4rnNVoe9OaYWXpbDf72zb4NM2ooQyNbznyJKUADr2VEAQpmbE8maZsGRFVxglJ1B5eQp7Z8l0ETm1v9MpzmiulKj3LypJjciJOKE8OmDpHK8xEW/Wvs9Eo86MkJ0dsSZgDhVEtJ3UIievKxrIfZsUgUhSAw9HPqOXWdk2uWudtA5kMu1Bo4RBdcwPdO6zRbGsjgxjHM3GvWaVmziUjHLVU79+sG3Gfduz2GGOeSkneEOg24Efof8ectCGRDpHHCpZGRkO33okppe1sZtzy3NfKex9ZPs9ZZhShnFtlfW6UK4rf/h843ZEbX+h8/qyclkU68rt/Z3dDq6X9Qdt5o93mxoj0KgLK4F8jm6R/e2xMQ+MIUFIS04v9O7QSsDmzHi2ViPratCJkSdLlojODOoxjjxwGZZ1p3dCZ+2RtbWR1sWDZC9rCR6QHAfRI1O4rg0tjm0d6RMpxZAu8WgMvS4C1rlkd//i0eavSNAcxYLgn23DfDjePdp64VSLsSvXVan1Set4RnMp+cimG+KSmzXDLZnJp0CclqJz0YeGcu762XtSEnnMsDzTCRSRnKlluBY2c44jRo2I5owkjURgE88Kkjw0JfXEGlSHnkmt2Hg2MzHoeagcPROTSqe06G17KYWv1sLmIeR1D2s+J1HNJM2wbDWXhQhmEf7HHKSJ9EpKz5IXTHRkFmsZs9aCZtgZ9P2gHxsvL431cmUbR+wZL8wSTScQks0wGE+NZd5XIuRhD84xxN1Ps4Sck/PtswoQwW2kHvRRLjzla9NJTU7UzdOPJeKd6NADlNSSon6CMz+vIZ1tlJJmAkgUiuO98+Gy5j6I6GEiWOa9PB/wtMgPYgGQoNeiUu/hOIBs5CPpx4J3viw1KBY11AwtUcyyVGFZo5fxsJD52YDt2CJxvBa4LDBg6wejO8va6CN0nlYqchuM7nx6/xxgw0rsEemoK8GgxpodFRRjyxLkb67CX3+48u228WYwhsTYkwQpqhUYqDvLGFSMpVZ++WGl65XDOlKUl9eVuihbH6gWWuHs5P+nXj9eeqqE9MYG96NHvTxwFIcemePWCh/WBemDnpVBw40yQsPYSkVtDtgNT9vN+fa28aFAsdC75b5hNjq2NJJNI8SMDjWBWIoKNQ21CkgJoTMOVSqXS3STWZYCTRh7x+79LJWbHnqMzlKjIsotK7WI6gnJufKh0YzWgK3VHMnQEuU1ZGxUJSYGnEc2D848JIlGqszwW85y0tnIWIhZQ6egP5M8UfabUhBCVxijh4lx3upIhTKEJWHnfYP7Fq0J+5ii+mhIs1b4qoVKotXGqsJLVX7xsfAPfv3KX37zig3j/XYPXatrKC1aw1y59w0RpS4X/pN/9gf+0Xcb7oXDnKKF4cK//ZtX/lv/zq9RWUMj6IMxOpSW/VGik/oYwnbf2Y/BdsDt1vl8H/zh88Z93xkuSKmYkyMojP0IaumyNIoW/q9/+4lPHV4XYdOFrYcF05LFHZpdnSwdMTEVIZJZJTnQlBSRCC1DZU+HVxOl2TS02VA7SkEjqRZjlGfjkagcdNGsMQij6W5ny8akKx/ZeiblwMlHB2hIKkCC/tj3wZFOpgi0UqAoRz9wH3xdC4tGExObySoA76lmkKTUgpuelWIqelZMRUXf/Ax5KCge7gAhVSdJdxybUZvkWGeQKrTLiuEc985+dHyE419bwdTxI5oXilkM+esBxnY3WKMoyBzGNpASqPYArCzRai/pp0ULXgrvx+BtN16XwqLOS83GQzfnblCLEzPRIsLGLRxXYoc+4ixdLwXZjjjN/eCg4DZYSkRmp/bvX9agtmyyJbnYQ53SKqU41SA0TIYcFiL8bNYww/ZBYbsPvrkWFnH2DIdjblBc1zE6InUGNpnMCSSzFOX1UhNJWIRMyW1dWwmRefWobDJFRoq4h0Viagkexy6FXkGH0zJjLyUkFzLjyQyJQ88WPGy71Jw6UOh7xweMfXB/3+IQ2IjmI+a0kbzjybElZzr7CzxVSrlbcEoip8fMSOvknaekaV6eoNnVXiPDm6WFUXkUlImqRP06kVX1+T0cLAeXWSA8lSgZvBT4zWvjP/i3fsl/7e//nNcPle/eOr/7/Tt/+LTxtoVmLyiIyt4XbBxc1oV/fl3QzzvD9dTPmw9+/s3K3/uHv6EsryGqlCM6ekmBWqNstAyEHX/bsbsjWpHe8d9v3H7/xn3fMRUuH15AJXh6i0bjAlwvK/sQ/mf/6/8b/9Hf3sPBVUXGDiL8coVffHihD+dtCxXEMZSe87nGiGRcSiQxCU56ouCzc3kWHLhEu8mCZxOVcI4VKDVCXjzoosMCfIQVjiKTYCjKWah1lq35NKck/eUnfynAwMKAS8GHcK2NbAMTn1+DQvh0H1ByEOSMBtK4488JUM7PxZ9LS8OZpNLugU6fbHL82qNwAIf9Phj7gYzO8vMrteVst+F8e9upsWnpPY1fLSiV/Tjon2+8XFt2j6p4d/p98N47mjX6/Qga46U0mkAXZ8eREhNAhg0upfBawQ+jrZWfvaw0j/LV3QqHBXI9Dksn4czx5UMiWsA05JH7zpURDXlc2LfBasn5lqAyj7MJ/b+kQZ3qAJXgK4rlQ1A/O9n3IYwjhnSZRjeneA0GytvRqU1YLwuaBmCGKzYCBUoK3aOmOULHqBc2rks5dZxqM3IZlFay1jmUCGWA26CkARnHTn290JaC4/SywzYoIzLW7Xph+OC47QFIAFPnGI4PBynoHlq78OBhxM7hYGkBPZHsfD3q0adO8VEBM0O0Sf+JTCWAn+F7LHzKgtzju27M6qnofq6Bam2+YxjSeQjGCMMDyrpUmgo+RnBSI8p8xWNS0WUR/uY3H/h7f/2Blw8XUKEu8OHjynBhXaMiLRRciklMoa114Z/+7hPFs1mEJVeosFwb8nqB5ZKKgnFqHaV3dOxwqfjyGntGHbziS0W78OJO2QpSG229IBmCdhtcgiinbzvv73vUdLeC9UG3Hvrj4vy7v3nh3/ubr2EY370ffH7b+fa9835EM2JkoR+D98P4vHfeD0fLikjhd7eNdxO6S1TUefYhSOWFEGvgwGYBFDwNUy1yRgRriKHOSaMOSFjexxmb/OzcgzFeOJ2TU6TGzK6HNINxzqGKxkSPZtopb5tif/+SJ5U8VyfCShphJtTk3JczlyFPlyonqg4HL6eN0Kq09UqXxvu7cTX4u8+dt8udr18ajeBPXZxjdN5vGc3eDr5+iSZH9M79fnCMqKC8ENHnUpxdHexAEFaZahRnlSgOeG3KquBLTdoM3o/oSvfWB/sYXwxodAkKzAZndechoS3v7lSP5k7eHR9CJ597jfLaua7/0gZ1ZgaHpWDfPWQSFrKYs/FGU0SNcXSKtiTko0VfvSova4lkSck8pcMsmZMznI8sHUT2UMzY++C+df7qZyt3g9ttj3HQRH1vSZqgMSldzYqPznHsvH/X+fDxSlsa0mqGn0ERUHMo2Cj0I3D1IWSvV2Mfjjdh2SOMs2GoVErVs++l1BotxpKLOzchkczwGiW74YHl1PzN5+GQ2dmHoD97p5xGteikDh7NL5yoUhtZ3un5SyqRWR6H83k7GJl8qspZ9756IJdCUC2/+vrCv/Hrj3x8bZgbOpxVoK0LL18Hkp9qgSKCLIXy8oKtlb/97W/53/7tt9kHoVBag9b56ldfI7/6C1iuCAN8i3U/Nvz3n5DjHT7+Brl+oPS/w+/vMARpF/iqIqJc9oaPR4O7UgTREtzrcQOdnYwc5eDu0EenJq3w2pSvrrAfih0DXWLy7r7vKDXGg1eh+gEbVHGQaDp+wzgGeBd6H1SJ5t+BrqL+/nUVXtrK7/fOP+sHI0tjVYLTXdT56rJQCI3ssHoGKksVlpINtEfMH9t71KIPMwZOH8RAS9KRykPOFm30lFKjkUdF0nQHK+7Sp90DUcxHFl/w1PhkRlNylj1H9B7RixDtKrVkhymXNOqzPCP2+5AQz79vxm9vG3Y0/sYcr42Xry/86hcfKA7HcdAPi1JOBBlO74PvDqPO53xpiBYuEnRdMacuX3HbBre9M7YRYf4wmGNStHAM482Mbe/cjtnoJBqu9+wTcWmKukXjaQS3mDEVM9vyzCqoKWIlaswkBnF2j4SwyES0fy6HaqE9630wMhGkicACH3qUb+amt6HsHpUFqoNV4atrUAQqEUZPVah5HIgmRALIwzDWE90F8f/53rntBVkbc9KNlhj6VUr0Ez16JhdEU7NYqEthHxvf/eEzH19eWK8LtS6McUTiYe+U2ii1ch8b93tneFbJVEVrpbQQiWNZcZPGTfUhJUGVcYww1jwc+llpg2SKIoXgiQp0zIyr4BrZzTNxIp7caaavfPKuhPGeaEaCT53t7EQADWpmT666EJMhwVkIGvHSlNdF+IuvV/71X3/kV1+trLVQ3CJsrhVdFbtWVEvsJhUoilwqfr3AWrl89UoT4VDoNbLYTQvrVx/g4y+jEcK4EZ1PHO9xj/7xV+iv/x3wDf/97zDvlLqcGiuX4LJERkQLOCIVUaUV5ThgXRdeSjy/piP5y4LUissejdHdGPedfmx0g30/2G4HrlHujBT2e2ccYXDcHK/KojHywv3AxcIAqnGp0YS4j051aNpQHdHMRMK4qEZP2aUo1xYKhHhGiQR98NW18lIFs+iMdNuddzduu3G4ce+Dvg8Og91ChjaEpF6inr6I8rFVWpVUIfjZrSz6GPBEMQXyGu6nVjZ3aUZacja1CZ41jGnV2K8zKegkJZVUgDZFSmTLrcamXEpFVfjqqvzV1yu/ulZG7xy1YNeFfT8iX6Chzd7MUMlcSJZ9O9nXwZyXSwC6boO+WXDtw+j7wI6REr2IxFYWLkfMmJod+I/sB+Cjo1qj74UPhsYZ6jmNdbYuLDVacIoaY5aoIogZZeTvjHMB/+UManVy5k6hZyNGzUYmUqO/YO8Wo5inLMdTwqLQFzhqZrIPo5RCrdFEQbWg4qE5kxa84OzwQnisYYVuwtv9oElNYxWZ5+Mw9rc7w5WtG0ePZIGLoa0BRtXG9haufrtvMcYgM+S6exysJqyXlbau2JDozBXkbnA/WZ5oQiTK3LIvJ6eGsLtSehr7p7D+LP+bqJXglhHLDC4nYlXh1AlGZdEJVZNLDVc0AaudByKSCybOeJJs1dkdKZFIFWEV53WBX3xo/PU3L/yX/+IDv/rZhbJU6tJol3AkU0ZVDqB7SqOyhNMPMEHvB/t2RyQQGxIRyCpQ6Pj+OXjf/R2RPUKIz59wHP2bfw//6t+HT/9xyMK+/iocx+e34MELOA0pl5M/l+WKlAZmlHUN+dG3WygItNLkYCuFJtFd68OlUuoFLR0tlaIRthcqfUDfLZzhADycVW1hDF5bIPKbRIn166IxW6yGFG3bI/xuWOw3LQgVkz1oHpEc8Gi0jLcDdQ7WApfiLJL6WItr8dvB0Tubw9s2eO9Gt0gI7prGVIQh0UZvIaipSjRiFylZzeoPSdlsspM433O/yanWCAQaQClAgo2RqhTOPTr1uZJSv/n1pURSaqlyVkxJP1CcV5yXY8Cndzzf0w18P6IwY2kUFY77naolk7Rh7K10qrbs9BTFwC9VKMvC+71zRTErHIPg3gssGkBnDGPfO6NbaLLHOBNiAOYtxrhkEqp7RBDuMTNubcrLJc55R+k9HFgUh8S62p87pG82qtgtyGHRkGgsVtjEMQ0pU+8BYuKCnXvSA3U4S/ck7C1HskZw8r5bAhdnM2PBczxKILQhZIcqZd+d3Q+8RlXQdji/3TZs69Gyz8L+kRNHqyhLTi68H8qn20b5XAGjaGRb0VApXK6FX/36QqvCfb+zb7Dvnf1+D/lUdl9CYix2qQ9v/7YbrUTTkpZE9Qz7nSC1p+h7jv04+1iKQwkeR9KonmNIyPr/pAnm/+b3zLMPKZzjWdSVSsEse9gyJ8L6aUwvRXm9KK8vja8/FH72sfB6CYqmVEWXEskYrZxlmfud0TtaBawGlVEEXLnfD3DNRuCRlGwlR00fWxZw94DFtzdkH8g33yA/+++A/ByX/wP+zTfoGPCHb+Ozf/YVYobfesCyfg+Mf1nDqEaLchxF/D3RTei9igi1wCLCxw+NulbaTblcrqjC/Wh023jfD4YFGhRJqVgqA1qRzAQbtUhoW18qFacJrEUZ2mJkhjb++RjIkLNfgssjrL8UqKOfYEMVLrWwFKF4GFiyHBIzxmEcLuzd2Y9wsJsL730wJOO2HHnuOotogtOFLB0m+P2gRgM1TooNvkxMxWQLTmXDnBbxLNELKimqscIhBUGlEUJy3DZWjEWjWtI8suZKcJDWJqAI5InDvnX6Eft5753dD5ZlodtsONRR2zCUwwetCK8vK4PBfttAoizd94HLEXuzRvVcq4WyFt7MGAzqUmIMtWrkfkoWi5jQDW4jpqSOYQxxmhaWAm7GMYJ6EYvmRFu3MLA/YjN/3KCWEYdEhVJq8I6zgsFGaPyATVOIYRYlfQqtVVyFbXS6ZwbfCvcjPaaPkCBIDNlyHGo0N4inFQHwNpzPe9T7X0p44e3Y2XPEapcow+wOvTsMx946W4ZXBsjS6JoevMSjDVG68d47/8XffQdCjHq2udlajjwms/kRR2kHfz8YBr+7O1eBVSs+V3KCytSSkggSjRHVJZNas0mJCWcl05StTFokOucn0symNO7ZoUoEdMymVsjZ6CPF0VjqNT0ThnHPRQKlX6rx4epcmlFL/Iwn8R4nqYQSYhrTWiJ0bwK1pWg8Qj5NKZIQfFV7XZG64twQLvF76x1fL8iv/htY+w3qHb/8JfILxf/wj0E+IW0NY66daO3jIDWuab0Gkv3Vvwkf/iHIV7x8/k/5+L/5P1N+97fgxuIRuraqfHhtvLw0+q0m/zhozWmrsFDZ95hg2aqe0rsugyJGk1ivsobMbllyCoUZTeFyaWiNsRzLvaKdmEEEOULGWEV5VWctimmMW7GiXIrSZrOEouiqlDFlhVmZRRjTYxibwWaS7pMIPRVoqVNVMMupEacMikxqSjrxyds/FKelRBXg9NPT8eeRPI31FPkHuzXOMStTS60OqxZqCafRjzwGHkHJ7kHUvVwuVIxaBvdbZz/CdlRdGZ6hO7E/W1sRi3FHVWKK2nbv9CPomVYr3jeqxdj5Mgp+H2hVyuuFXhZum/HbP7yfo44kNbPrpSHi1Fo5Cny671RteI85eRtHaIldIjoYlp0a5USy/Lk61A8fr6hDk0KtEdKBxiIfznfboBe4vlx4qVEVtY8Qz18vLbSTMqLrVCKu29tA3gbXS+MXHxbcO1UDUaKFb/d3fCIQgZfXha9fBVplKSCj40r2SKyhEyVQ47Z3RneqltCh1ugutbQSTU14cCblJOdjR/lwLhGvpocuWRGTTTNmeK1w6y/8QeD394Pf73dag7/u9UwYjMyCTr40AMgjcz83N4kmIQ0vJz31hEdzplB2Lppt/5CB2wCpyT/NavRoAtFHdPwqHn9Xga1HOGSjIsQwwhCCl0wkZpiR6BlX/NKQ0aAIXhQqyWdn/fbkLEpJTty4XK74ekWagq/YeEN4geUr5Kt/gPgFlw1p/1aoFY7/J7TX9DDRDlL6jrMh7MCKf31F/uY/wD/+N1G+AVe0fcVf/OZXyP/xn4BG9VBjYanCh9fGcilc19AO70RIOkugqwZq0wZtUaCgwWbQlpR2Jc/tI5MR0QofShS3XNeIi0txkCRsRFETXovyYSG1sKHNHMNoqpl7iP1hSQdoKYnyiaboamfpqAjRQSp5e0kjJRJIsDPbJuqpxiKNyISjIa2b5zf50CekGvtq7rn59fl52fYvJX149AB+WRuLHKgatVSYSgSIiDThsZTIjqtHYyTx6L97dAk9qsfARAk9WgwkVKgleiUjDfeOrjHXzg8Di/zGpcWcqmHG64dXjqL4iGjRegwLVXH27txvnfHtwWDw8vFKXSrbAZeV0L++R8VlFZCibOZ83nbuB1TP4Z1V2cYPY9QfNajffT5wF4oOLkuhNaiLYEdn26PLuFm0AFsAxogqIndsCw9VmlBXpUpUQDUxijjVjWsJ9yk4JbuALCJUhN2D+VE6LdFkeNvBGAP1CD6qwro2bvc7Is7hg1Wdl6WxLmFMaw0POlS5HZnAGUYjFANLiQa3x4jxK2YxFG6qocwGMpswi9MWeCvCX3xz5c12dhv03MljhufmyQXmppybPPfcVLI9N8wQQqtn+TuP0RQP7lR8hnIJt01P6xuR2+TNOL8okvQNjg0NrtgLS6l89bLw+toSmc6TJLCusKzwOrDeeczJGrAsUVShc40iC38phXVZWF8+IuvXuFTEC3L/p1AW5PXv4ctf58FdgV8j/n/HbSClwqVEfOU9amtp0fBieUX+4h/Ch38X4WexepJFF37EupTkptUptfLy8oHL5UJfN27bHcVj36lAK6FndcNKOMB9xFA2y6af5sawnvK9JZy45zPJ+njVaHLMdGXpCdVnUirGHEtJ+dQIDa1ZINmYumrRUk8crcpisHqOz4YYtT6yLNdzb0gmT7Wczzb2SGyS4D1T45zacBcmgfowfNOCEjkN8kszIRqdyAZT6BVlEXLKszSTzLE3nRFF+mFYinJZKpcWvK/tO14iqVirsqQvvtlM3WbS1yOx95i24DGptEj28wj9d6ht4LoW2OcU4xbNk4ZRsOg/4iF79Br0ydveORy2twN9D73we92D/7V49lJihLvZQHShVmXfo2dqQ6Ja8wdeP94P9dPOnmHDUqLtWmk76h1NCctw4fefD0YTSpKprTp9dMaIWuqvvrrys6vSMK6SvTMN6P0cvTDoSM3Wb6kkMIfXy8rLxbnvB2bQeyyCm7Df7oFfLmv0Ksx4vdWGHkciu4qNzjZ22ssrfTsiTO0jGrNYtLyzWmF0RnaBMrJRigAWQnnxDL/fNxT4ugyu6phWvr48svyTezrNaXJZc4SJT1afqTWV03Q++nAGanVKCM9nqWXSISrBv3pqgyXf5WzlJufHP5IRk2ZI9BCHwlhbJBkjrFe4vsJf/AK++Tpo3b7DcSD7gXx+DznTCA68pC5TS7RFa0uhvb7C8isoC/QD7Nu4jpe/Rvh63imw4Mt/CfnZP8E//eNAY6MjQ+DqSF1D/Pyzv4Bv/j7INzzmhILbJ+63Gy3vrZXGpRKz4Kd+uFZaid4NSym8rsp2GMMLcsRwyDGiGbd7pH4tkxCSrRX7mLOYOJUGnsLwk+MenJMiRKJy7loKZUlOxyPqObt6mcdM+W2wtKjVEg3Ovo2Qy5WUzZURTcJPPatxcvuabx9tEz110FPaNGWEubtUU/MNZ6lrkqgnOg3ylSfF9OnIdUoBmHtuBM+cxnXglJq9OhIA1NSR9xEgiSy5bUuhRs0OWw/ErirUmnOePOiNqVjwMpvtSDQYyhnlAweJJtBZUkg/NoZnt7aeCgyMPSVpuDBGzL06Djs78DeB15Tivd03tgFaGksF88r9GNyOHg2y/xyDqioUqVQVrjUmhbpG9r+1xs2iPGvKO8RCApX4PULbDLXBwzPNjWgeJamiMEYI3PsIo5gPtwuMvaNSuSxriJpHDa52H/RssGE9Nv/1eqH3PRMroUeFARaEebWCj4MhxFjr4fjuHNvBoYNWGsgIUXQ/kCJcP1xxNAaD3XvqJuIhTXF3Hz1kUExtX3jszJUQvVfSSAon/ySQM4WC8K+5CUP3Cur6qFxJo6jiuGqgOuREpDPtIMy/O7OVWzSjiaRKlCs6l0uh1RpbIDrORBMLKiwX5OUVXl/ypHQYYexoH+D2jt9uj8Fl7kgRWoHL1Wm/+A1c/jLD9Xfwr5Bywduv+XLAmYD+a/Dhv4L3T8h4h1HxviJ6A+mwNOTrv8HLL/MwD87GiPvvuL/fiaruOS7HaDLwHsMUe+9E+9vC0kpMCkU5PHnN4yDIkSwnhWiO7pIVNNCtc2BclgUkeONaou/srK2fXaU0E1O1FqKPjadeW3LECKFQUIDQKdfFWVpjxzhMQijvcR2LhzRuji8Rj/aNRUNxMLtfwczA+8njB5ubBQWuDwWKyGlQ5zkP4Oon5x568UkXkLvvMS1LNEf/6IzmlKoVWg16uMkT9x7N5pFCrQ2772AHas6iAq3mgL3Q+46jR0loLYys7OuZyKuZbDYfiAWSh+DCo22lBN1lwt47b+8HL+t6DvrUGe6dziKGIlacS4l9sh3GbTvoFBYBbR4jYCS0w8efG/JD9LO8FLjWOOybWZx2iaDUxNh6Z6kVCVtDG4ZUpbWWc3+ilVzNll4xp0hYStRS11qQEd5j6lBnxWY/jPdP9+xJCrVp6h2FtpYIf0agCjFHXbnfd8pLY2k10Jh1fK1sdoBGvbEaxNaMxYlpoRG2NRV0RD/HBWGIc4ggpQby7j03bYypRWKzMg8WWZv/lHzKpxelsGYnsa2amVMVTKKio2jUOs9nMBMFolG7f/YrTfSZ5FpkmS3QnyYKjqYpwkXgpcBLcz5eK69ri8Shh3i9jY7oks5IAxZsW0DgHMLmpcDlGpyYx9emZlFUWES4vjTqz/9tvP6biP8W/LeIHlB+juhfPB3MSmD2BuU3yOUXyJ1Yl7rjukNd8K9+CZefBR/Ht2RjRcAY93/O9n5jiIZ8q8QaFAz6HnpagdoarRnrWrEeCdVLLRgFlYEuQu1CuTvvvUcz5RFNUzQNSClKVWhqLCpIDScykHDsFtWD0YyG1BKXlNZmJwuZE00JxCqZeSbCaxWLkerkmBOPPV3wSM7k7zsPdYeU+Kzu0fNVk0+1yb1Ph5e/c5ajpjE5BwxOh/9U9TdzDP5MP2UHMJVo4KL42fzHcVoLvrTUEk1OejY/GQdealCFfZyfjwm29ZDi1TCm4bQqtQWnjBTGCAPqFtSII2jmXUTCeO+9Rx9ViXLgl5eVz583jv1gvazZyyMSfBrZvOjp69Hgu2gUILhFy88ijpRohakC11U4hnLf/0yEujksI7quuEsQw+aMgH/Jl5Zo2ZaGMPqJVkiN17CR1VCgVZARo0R2ixKuJZPKsWGFpSslh+dJKgbUhWNMctEZDNbUw5am9CpYP3i/b8GZqNBtcNsjA/m6rmiLNn95QqKyh4pbP1vMbccBYrQ1eJNWozF2WS7c9ncUC636NjdX0H2jj+jNmQYwtKCRZTWEI8jNOCCplRse3cVHluOKlehMZNHVPuaPRyJqPCEO0KzUcEKjmXhUokHMREvF5SwvLR5t/5ZFuC6Fj5fCZY3P28fgGMpVWjJZBczwLRCivN+Qo8N6hcsLQaQ3dLkipSBiiCy0HMvbLl+j178EfgbyArUBK8jXwIdcn/pwMgDyDVz/Pr7+FSJfhQHqb6CXlOPdwN9jE/GCEx2LfPs7xqaYxPDGkqL1pTjLGofiogdbdglaZDBqYe9Qx84hcaCutfDhpXGvA95CpjVGcLO1FhaPZNxalYtm2alGG3TjPQ2mAAPLCG6Zmk0mN50RjEcGOUoxcxYXkZ13i/3gIx5vnwR4iYNaUfZ4wARDE8E+xEhvF42STkCjqWP0fkj5XvEYljm1ykC0HMx/ioRyY/byVUmaScopt5LMFVRVLq3EviXCc8djtpzD273z29/fubTGYdHG7xgH4sJSa/QgEOf2fnB7P4CSyVWPZvPNWT9U2qUG5fXmsA9kOItqqF/0kQA+zGI8dVtiTcV5fbnw86+y6EZArdAt+jRn0Bj7NwHQkaCq1tAjd8+uYcPO8U3xuX8mh7qn/fExtY2hD91RKC1EzRhNoiLER9RZz2YQ5jndUyLW9AGW5Z/dBqNXaAUpSYJXpd4OaobEboFQ60VZ1oX3+w3b4uB6A0q0BpuC53vvrLXQigbCcKErbDr46sPKPo7YNO40d+iDpTbMOzpIdCZIzSmUCr4fVK9cryujWTTp2KLyx0ZkF6agepq8udzR1N0fPQicswFHcHdkCzooxbGRJQ0S6IipYEqkcR6E2VOUNNQz3A9IgaWcbXLR0Y4wGgGvWULZSuE4Bvd758PLEkkdDVKe/Y58dnhf4fYWF3qNQgdeXmG5glSmRmHugaWV4GN9y4N+AfkL4BvgCrTk6iZDN8PTV6T8g0Tt1/h+nSb3b3H7TxD/LqgJPiNawWBsf4fvkyvXFJ4Lr+vC9XqhuiP7PdZ9qbSq7Gq0Bvsweu9ghf2+R/tFC2MwG6JUgdel4D2UJWsRWmaNVWJOmWhERT37Unjy25pC87kPPO87KIHoNJaU4pmLmPtiJiXLswBfEmHGtmfR0BerB3c7xfGeeyAGGcwKrbkrg8M39/N6DDiHYk4jc+6zDPkJQ/NgrzlR6RjCW/ZH7gPGsfMLc3ZTFhfe7jv7iCbdo0cjd+WI6Eic+9Y5jlBI+LxK26ilcumNj77Szbi/HdgxKA6XZcWH0a3jUmit8HJdMA/6TSCbTD+02rHmnf3Y+bAsvJQwjH1EsvoYcDsGFKWOACHR41lZ1blKdC2b89d+6PXjlVIRkNE8Ouu3EtMph0czZE+NGBat9WSWDmZI64QcJFqK5gxGV8RjENYxJOqkRVK6wSlKJjOTTs7rcePaWvQlLCX+q1EDvVmPmUJZLVQl+NUSNTtsx6B3jYzniKYrBUe9U1pDUYZ3mgu6NLQ17v2gSxQybG/3QJEl3kOzf6pqdMj3LBgAMvQ/W6OchpTkMi3/bdZRaZxNJnzWIPMIsZzkKHPzWxrMfJN5TDxDocmdxfvP5tRyblTN0CXaCAqjw+3eo3RXPEJ480gk3Sw4zNtbWOqkZTwdh6MMi+YdomFo1mIsY8Pv3yJfkZ96zf8ma/z9l0eMzGvezDS1Chzg3yEjegG4DsQP3BSxjt0+Y31nTkadyZV1qdRWqBjlKFGcMiqtFWqJYpOugq4L1kNu18dBkRo8HSHgX4pyVUHWmnI6gkc2R0Z0okdL8JPqZ7mjJNXC046Aie5mT4ysHBpJoZ1TcKNiMLbMo3x58p8l6OPI7k9zK3OiQaDds4msAp1EvrE57FSUZOh/Xufjz0iqzojLz/3JuZ/jk7s43Qf3I/sUu5yTei8vC19/80otsB2DbT848lp8zOSpcVkrL5CZ/aBPtm2jD+UP753v7tHkB58JOacce9zLGFjKLj/2GjxnGWwjyq3f3t6437bT0YgLX2kJPbtHBAzCPozP+2Dz6KK3qPBSC0uWDF91UGzw2ir9GOGIf9Bm/shrqUITO5svVAnvqT6iC7pPzipkUJqlbiohYl9bC74yT/kQp5SGSlSEHF6pHtrR7j1aYyVHEzFIVE6MbPBgPZJdtYb0QouAjZjtTUgdfI85Q2WtIdnS8FBj8oBmmAZyrjlWRUsg4Pt2RJllz+5K0bwSXBn9YOwe7eeY4vrMimcnndh+QeZH1UxkXWO4ohBNVDTLAIMHmuNVnHRGTETgaSPOeD/K8M62fHlpc5PPA+APgz4BoDnsFhVl5tkDNKdYMgX5yfshngdciHHIWco1g3RtWCKxQyIkKhrVP61VXl6+Qa+/TgMiZCXF06GVp3/Jk7eX84/I3TnwBxi/DwmVWYRKfsR62xt2uwFRwed0ipQ80CmNF0NKobrR9OCyCFxqlPZajcKJbXBkB5m+D5wok3ypJWZi4dRWgmsjhOpjdKpWfInaoRjrkaoAD9xXp1HVB6cZSSU5o40Iy/cTSc3ld6K944Hk5N8sHa1E5OQSPOOUbjAiOaRydvWKOVixdx/BjD/W17MXbypQ4nlxIuIQgsXzCHwTX584UkRopdLWRiuhulGE6musnxqvZbAshWur9DWy6nPcuvVBqdFHlxFz6FyU3Yz71tiOyG0jI6quMqkSkWdcx7AwsuB4K4w6IwRnXQAvLO2S1KNldKwYMavKh58JuutSKGbY6Fjy702i5Hy3LI4ZRjHn8sMR/48b1JKezghupUyelGhksue/m8BFoynEQUDtWpRacvKoRLPYw4SDIK07zj2f00VDnO8emc0wNjFB9d4Ht+CpM9vpjHtMDtBaOGQ2VMgeAB4NNUKbV5ILUsYBLMrgiM5PUUcb7ew8RllHwiyMk2Ymdths+hyzcvYRpbgO3PaD2sZZoTKNyJQ+xc6cpYFO8J9kOPXg1uZJmgZU5TFaehqh2EDhFOwJxYS9nTMP5jpF3jrSP4EAp7xmjrUwCSdWvUbH90xOxG6PCixpLSqXENCGrJdQQdcFysjMbrTfKxLNqr/6xW/Ql785r/v/o1cieP/ixzvOQPQjiOJjB9nzGuO02X3Hezjv0IFGB7RlyoNKQZbQi67d8NfGUuXsWuV9MJpz9RKZ/c0iCemRHFxaUCS1Vo4eI5Uva8NNsaIh1cvQfbZ2jHUuVC1UddBwOt08JYKSCNYfGs6UAE2HeobdeRaGCsziC+Jwlyonn2njjHOYpXNzLaJBdZTRZgfgk2ePUc6TSrKgu9Kxqswm14Gcq06qyjn5gWGsUlBGABMEjVib2jurRbtMtGAMqpSISFVTrjU4PA1mN3RRLkucs+iJUNBSmUzxnNJrGH0Ech0DtqNHsrmWE4wM88z2z6kKscTWI3/hMrtoRcOYYzi3rXOMC33fMbPz2ULIKIc/yuh/6PXjWX6P1mJ3oPY4xHN47Vph1zjEIUKW1MpliRZQbNCyeevWB+/vO5+3mEnVDb7rG0uP0rqaZOHWJTdX3MynrfO7dxKFBDpeiqPHYNgWvS2LcBydGHmtyN14yY7HuzqtVm4DSgddIkQ9HD73LR4UmVSrBUqMlR3mHHuUb15KwVxzTDb0PRJGtMo2omfqtB0j0Uj1YMAiApsmn9O05vJm+Jco1BJRPj0CnYFdopueyFRmsjg/b4Z+PiZNYlNpg3tUn1xq4VpCIRG18EQzkVluerYlE84U97oEX3q5Rpep5TWSUYvRStRIVwAdVIXrh5/hcvnRbfW0wX7k3wr8AvSbWLviMCvouGPlP4fr/yNqvwfgBYkWVayrUl5eEUJMrl1Dq6pCXQrb2Lnu2Zv3WihLjNJoOmj3nbUre2buS2vJZY+sZCoxa0uc24jmHcNIPjUSqK5k5Z9nwpOzd8PwUHqExiCqD0UDtHTzaJrMlGJNNBlnyxIclGJctKDEhIRodJYdvXyuXbb6EzlHdXuaVEh5X4rqUU3/nh27zuhmyuKyH6hxVvoJTqAUx/aUnpXZDi+qFUlKY460GYyoVpOHu8AcOyz62Y6OrAu9G/f7waoNHz0GLqrSWjyryxq0XN8O3t4PxvseM8xaOxG12ch7TO1qJpKkgpZQCDjZ6EmUIXG27nvn82fwY2dtFamVnkoe0k7s+/aDO/pHDeo3r/XcsAehfVtrDOyjRhs1uRtoiW5FYqg5dQithUQhvG9shNfrhV4Oyi1u9ro2ygjJR2jygwqwE6OCibITIUIkHqJv6nUpLK2xeEg0jt7Y9kFPsfuOcLNEbsdBqcprK6wF3GNQ156ifjdPkBhlhMEPB9853HgTwWXgRVBtHLfgD297XGOxHp4YwvOezCiPUF5gDt3zrA7DH0UE6lGVwhNnOjsFxesB32ZfAZmJDIvKHs9BbMElxu92POfKZ/PnNvWnYcRLicYoiOG2B67VGp/nAy+K1Pao3z9GtBq0TuhcJ5qI+edtaV9oHH/0NZHpvD8X4AZ8B74ngI94yKm5AgX8AvpL2vXnwWG7TZwFGg2u9fUas82qwg6yN8S3KD21zlqJ6ZWLolY5zGkfGsuq3LbO+9bpPXSZvu+h49WoCBtidDG8T0pGTh5TPPj0pl8mexSShM1sf5ny+9wHHpVG1gdHH2wdbl3ZcuwQFgqCpQorc5xOrL645zgcwhtnQslkTpDlnGr74FDl3FUzwTn3DElLDMn7d6Lj0vmcwglEcckAj/OtaMxsE6GZ0m8dLzBK9JYoa8XNed92WmvUWikoh+3sveOuiPRwEMMpZpTD6LszSsj3xnawvx+UVjn6we19cBxx//2+sY+OO0lDxfglvNMuhXZZA532wvunG33vFM9RLtfC5eVCXRc+/eENHwevH6J8+bBBbUpdatiNY/nBLf2jBvWvPi7cu7GPGD/ycll4WZQlCW952/m7Twdrg59/vXApFv0bD+PS4LKkFCIbTpRaWEz5Z9+9Mw74+euVNXlXkehF+Pu78d39UwranY+vhb/+9SsQIxGO1BBd18L1WhEfFClRcbEPjp4jawUuLbWvNWRSl6qIdoYP6loYS80a+/DINoyOoFppNeQoIj0DpQVB0bpwK6/8HriPgyEhuzqkZAhuPLdHg4lEJ9fpj8N/bumJXyXpg8kjxieHEc6QP42VeZoy5zS6D+7VzrAxWvdlRyU4ywTxpCeUaNmXulm8RwKqNsjOPKghdDg2XN6jb+Xxzr7fmYUDWsLZlR9pHPGnX5GciD8H8Aew/wzGJzg+gd1BUs8rjtevoP0aZGHffxcaUJk6S1Axlum8luXkgqWvyHbHbneKOssa5FXJuTB9ixLLIsGdrlVYakyn8A6qJWaOKRz0zJ1FDbnPgWYe/G0RaBqyJu8n4467Jb0Cs//m3Bye5a54JG2GkR3kI1ehko5TFG/x3KRkaaaPR78OefDmJY2tWzhhyTltZ/9ceLAEGc30c4/A2QQVOGYiSTyRa3yIpyPvfUBz1A80FTLdLXhHbSwSsi/cKFrph3HfNuw46G7UZY3SUvdo7h08SvSqdY1WeyOc9rYf2PuOSZQK9wQtojFKZdsHt/tOVePShLUWluXC5dKyaAfkGBHZdRhH54jQjrsax2G8rCuvH67BTfeBlI565Fuq/rDZ/HEO1S3+w1mK0mSgfoROdKSXFKGVwlfXlcqOb4aP6L0vCi/XhquzlsK6VpZj0LLGvZghttMcLi+N1w+Nqs5/WjRCJ5zXpfKXP7tSW0gwvns70NH5uEYybHKCw6G1wkhdWdU4GJrCZ/Oo9Cke002LanQCEkJcbLEJBgPRQZNAAzHSxTOLWVB1KM7nIvzlNy9YiU5UHy4PLDnlQNN4gjKbSZMb2iwPa/IxMpNbM0k10WwwITH/yKJsd6Kdx7C3uDYzOQ3tFJPP3gHR9T046qPH9SnREEVKJgINQgRpUTVVl5CTqUTFCwW84xR0PE389KyZzmz39wP5H37lzT2tXHx5Xo/B9hmOb6HfQ1Xy+nO8/QLxn+WvBCso4jGtFWMtNfjAtoK04F7NKPvCch9YV4YOmkY3snJzvBvve8e6cxzRMqnV6IsqNdvCZQOactSsCoxECyIZxkbzmiZC0+yjG6sZ1UgeZZMDCciYmf6QTkUhgFSloqwDDoH9sJByEfTBzENJcbTAGDHIbvYiDgcMatNR8ZiTFd73QdSSSFUe/4o/pgOIPy2RbVE95VYhxYv+CLIE32wCtj8RVlpZr4XLtaF9nH0Wjlp57x22mDPmJbruFwyvMe5mgoPeIyk9FDANGq4PanFeXleObFo+TKDAZV1Yq7NtB2ZO71Emf9t23DrXtaEeyfZyKdGfwSudmNjxedvZDuPrjwutwfh/c/YvP7YtSZof9jNz97X23hFxzrmPzMpHZWVWVlU32RJRFFokW+SAICQIAgRBIDTUkGMN9L9oIAkCONKAkABJBCkIlESQEptUsZrsZpP9qs56ZHVWPu7rPCL23mstdzfTwHztiKzuSnbfqMrMe8+JEydib1/mZp99jxYNorqSSh7I3F98wn81sb92lh7xC8Ud3yrrdaN7SMLOLfhll6Xy1VdPFN+A6AR9q0EQl4ZLp6HYOXFpPThkrfH47om7EiPNtq1sW+JqE4z0VPfE5Wnl+u6J+7sJEWhto7hRKui2jjdZqFu9veklOZMkZI2iLEVZu9FwXCN40JIGJIFQ18qUM71VEiEpnKaJVPIIxOtYi62jJofHM+LOw8AvV2sUezGc3zAbHw/bIFGN273jN411yHuHYkVkQAawKxDS6ChddhzWb53N7s6DD1NhJyK+PSKZfXyV8JUlUmstNshBBti13/tTNoLpbt+P47kh+QCiQVnyndmQI8XSApsUJ16nXseD/LJQ/pM//vFj6QStaMRS6AFkHj+IIKdfQ+6/D/oN8BOWDgxPo9CpS6i2DodMyhnKhEseHa4hy8R8L+Fpu2ykotA75VHCH7NXujWaxnY9axTrVBIpDayc8eN3R84Ns8G6EMdjo0MuofePqzimFUWGS1o8M2nAA9YDq49Y7RhV50lIBk2MFYkk1wEpoEP2LEKyMaGM98DH2dvpY+4jGdTl5t8ge2c8zltcyDuN6fljV/zZuPy7GzoYIvtMlFOMy9YDguqADfs+8bBhKB4a+U4fCy2oDGMdDz7qXDIfPjxhLV4v6yPeZBs+pYV4f1qj9zYm4jsSznlr47wLtdpwvhLefHQkJQ0IQCVSA5YNtWAa2BbMlnnWiNtWoZmwbTF9TIcCGs/VtsRWvGosBPXwNTvU1SKGZHeST5rwniLyYJgs7gqDVDvTKaOmJA8lxXEq3B1ntnXBe2w769KCe+eR3XIoMVqRErkoa/WY0SWihp+ulccvr/TasJLpNfS/6jK020NGNnKoZHydZCEhFQnFgwNNFFMZuFwwHYtH9304zFhVvIZf1CQ5xodeMYsAa7UeEzFjfG6N2mJs6cueXLrf8jx3pCI3rG3c/aMLHXioSlDAxsdu9JtUI+As52FM8dzlMmCAvRfdE2P3cX6QU/ZUERidC4QjEztOG/wB9i3vEHLj1pAmg9UUD5/v+LCHUiV58HHNe1BYWmO5vEetQZp/1dH6Cz4M/AK+sNsjuYTOnjwh+T6+//6L6Lifvhjd14A/cgLr5KPicywUIPwZvCWYJuQoFO2kNCCBWjnUjTevoxC+ex/0ttJTXDYS70dWRVVp1llrcCr7YJOMHSoFBy2cUkQW7z1q0uftNDg5hVFHWNwxOMHxu0WD/peTUNvGojJofvv7PF6mARPcDHvkGZON3cMOpthtGrmdS9nxecbCa9RXgqHzS1tRdr+I/e98hq7MA4bTIX1OkmIigMEIMGwLvLm2aGJEjQpBX+w99igpYaWwNmgmwybRw6NDgo0hOPMkSCkcpomUoV7acJqMpauaDmmqkxKkbBxLIk8Z90It5dmPw2JT06vTpcOslHkGGjnBYY5crd4bYmG/6N6ZUmLOX7OgWot4B/MdzI5xxLHIrtaJn1+uIMKraSLnSltCjZQISeaHd2fKyOLBjGM5UNKCUrmbJl7PzlQcyxOGcWnr7WCIh/v+IRWkdlqcweEfGcelb3YLOWvhJzYYKH10AQGsH6WwqfNUF1LJ43sM9ZDmiFQGg6KhGxboW6WvG6gOZUiA39MU0rTDKZF0Zm4FdGNM+mNEGoXMghfnY7wV9oO7n+4Y9wyLsW/QtCJyIR6k2mxsaeMwm9rgoMaW2cayLjrgCG67nwgsUOEuCR9Pyl2BQ84UKSSZ8K542wnOcZOL5oFf6+gOnyVbovmG4e7Ls+YwoYFzNaNt6zMo99/28ecmffZXxwYGmjPCm/gt7WCVfv1jksVDX7/4DN9sbJLjvRE6U9EI/EsePNruAamUOcZsCQpUX3qY7+TEqwellGCKeBcuV8OJxWdshWPcba1hBjlPqLaAGgaX09VwjY7sviSkx3tvKQqbEnsEcyWN85k0eNXH0zToUJHcudaQOZemrFujtshV6vtCU4ituoGPKWc3N9mlz92Dj2rmN/u/UG/ZOJ/jkvfBI91x1dvbMv5tcED99r4GVHFZGohwPERTY6RnooiDu1LrkJym0Uz0WO4yXgtNUU9Op5liwlN3bA35rff4WYrAPCemUwpGz9W4PPVhSSmjxwhc18zRntgujTKFp7L1znyaSFnworRrjdenx++ZtgHX9DBsmRN5zlhb8M0iJnwoq/KUbpzzf9LHryyoMsiviNObs7lx6ACKWWOaJkpOXFvIwNgq0p00wsPcA8tQ4mFPxGFTGTxIKk7ocK+X4STah2sQvKADjf/0uFWCcBwMAm8y3rxwOK8WsEOaQtomZvHnNF7w4xT+qNKgb0YfuTWu0K1RDjM5xTavtxhT8uCV2jjIe3Ez92Fbl2DeO8C9Bx03OjGu9HE806CQ3Mbq59P3zGW9dQURtrdZqNOiaASVa49SiWITi7g6aFXx0HWKJu4PmY+mxCeHzMOknEqYrVSz8EdAgtalCSmx3Q8ebUV0DWxQlTBjMIKv1XAqm62Y9OAXW4QlttZvP/0/VVGNgzY60gzyGvQR4UP8cT1CWoEFfEG2a9gJ9ka7tEHsHt2bO1mdw7FEhHU2qI6vHbcVTePhM4s/XxfqFiR93JnLgbuD044F8c62daacKfMUQZVmJBMOkvGcSblRZsGro30HqzsTiUmfi55lpaeR0CqxqApGTw+7yAyHu4l5qAq3FiyUhHGUziEH/r25Uj1w/6KJRKRzuvmIpfFxLsYZG/Q533nROjrQMfGnJL90uQdKtRuYj+mJuNziMh9PgUdBrl1ZNdEXG7BUx5tz5/C4NbbLFqyHHMYnaCz+JAnTYUJF+HBd0aRD1CWsHo1cXRo+pOGnu5kyTaAx8rcOrUUUUAsvwoikt/AEsVD6sG4VFaNMOtzpArNdLSzvd8/V3pw0QgY7xuSga6X1jqcc35lEo9XbgAj+go9fbd83isLN3qsF2V52d+4B3MU0ayglKEnemTQFgZdM2rXnDuhGHhQLTeBjtdS8ozg5ZVTbeNBHMJlKxB50i4JNdL9JCiY1sMgsYcfnoa7ysYHs3XCPqFvv8Wb6sGnqFp1kHniW5xjpkgithQWYpYwNI+XePDaa14p1Z12MnAXNTilpn4xjDHe5jVdIFK19pHLZF0tRDOOA6/CzjPHfBi6nIiFhHzw609CNx0cc8CDyDzjVoitZahinJBGyde6z8GqOScEN+jBUdtPo4rxF8XLFy9BZTgeEsKzD8/Bv0NsiI8jqiiWnOtQmIyjxn66O/vJJ21+QV6AfEYuka0AQ0z1Y/P0qJ1y+Qup7rI7XTAvoErszL8zTjB+PgTf2NRZseoLU8aUhIydIJQw5rTVyyczHI3hkHolklqkhSUjDjMGJzCklYWW/EHVwjQO6KQiHJJHnpYpKgpKwpFzWGmGSO0/YjKU5m40RmkhUuK6NbYcU2ugy0QFvyPOWXp6ho7i+YraL4224yXhEx+fv8IgISUJKjjBECfFne98xer3diX0Q421/5Mc53sxZ1m2IXzruwmQzR8APiXbKQavqjloULttZCUsYzuuIkBecNGXSYaJMKTi5jG+jNS41dhkpgGQ2g9pi+Z01456Cjujj59LAeOdUqG40n2kELk4JkYABXRxsYj4cObdGs4a68niOi8dGVlxSEInpQr5ujLR7R1OmDSpQvJCOdiOnFJQVogPboRc3oVfDNTEdMymD1QoWURw3Vxu4BX95a0wlk4G12816ziXckLaR7hjxIgNAH65M06lASsEpXRtSnZRjdJIeNCrzuK1iWZCpa2xzGbLYcijoIbHWjtcWIgYLorVrAPuyn1SL0TZ4g0YeG+aUdxX/838YXWVs6vfFwovtPGNLzj5SxWtju9JrFNFB5Y3Ps4BeZDwIz2NyyPN2UjhE0VxqLI4eM7yeEzrn6BgGHhqmykOY4A3ShE8ZTRlyxEkIBhLmyJIDnfWBccUAYNTk1AKuGfkV5hEvP/bJyXn+OUQKzgx+HFWvQXrA/QGx4TqVDiAX6nYdk8cwCRkL1DQfh8rLIFVIY0logswFaQ3EKKIDhzRyCbx2OmRef3TisBiX83obe1cslof7mSh5KKNGXtr4HsSNeZo4HBNaJrpBtU7vDbdwPnJT6MYeZFfXytad1Zxl6ywNqo1CZsFg6WZsHmFxu4l11jArCpXY82u+B0K+pNGlPR5F0sBu4/+EiIJPKahJASXF1/cdjx3TmQwHMyEMvO8PIQhoNnwyVJmZSEn4+M2R0689jKVb+CY3C+CjjlC8II9EMc27raaE9879a2FrkYhQt8a2Nra+c3/Dhc5SLEM37/QejIAgNzRyCrMeoQYXdzizzUXQ7ogplbAfTcAiK5sbHeWpGcu7x4CKPIqze3T098eZVw93f+GZ/tUYag9cyiASThnyNYtOcX9jzTprm0gaY2RwKcdNmoNOk1xJyfEpMy1h8lxd8V5JU+AoYVotJA/zAwg3m8WI2OJu4ZloymUNrPAwB47Ut0a9brGemRKShTLH2L+tEeyVcygjEBnuSg4aYPm1Cx+eVibJTGP7H0v04BpOOd+02i57R5UCO7RGdnlRFoj/Hpt4DYxiUFteFBKPCyoNGdhuOrHjWrfIjfE54jGS7ConG3LG2PpCMiV1KKIU1TB81sBRRRyjhd2ZlmGLJ/EzpBnmkQPlCSnDF5UoIHE952EEM0hZPRIgvfegDrXOpWd6u/JspOE3yPVFGf0nnDR5voFw4AhyBDboC64zkl7F101HqGesd3of4ufdWIaEqCHW8G1hJMcFVmgbpIzc30Ep+PWKPK3xvloUWBclHeD+kJietnhYawuZskKrHZEShjJbmBtnlG08cE01nKAmIRcBenRjDjSLHPktzJENYqE0CBHWY7Jam7NsTrW4sLuNc4LQUbrJjVW2G0EPktGQ4I67m4AUzIwi8TrEimG35COWkwN73BdkRZUdveiD/1sG7NPpN4+KrHCfPbi5DIlrSkwjlkXrysEqU0lsAkhEsm/VYzlcwl1uTgMOTINLOZoEIyA8J9HaxLJttFpCAJTC5Fssltu9O2vdWNZG10gIyMIQvDCew/jSFQPpeIriqxo2jzZl1J2TpOhoLfYW1uWGRftmnLeFr871L6yZ/60Y6uCk0A2uhKFGJpF6PPppjIyrC0eDhpJN2CzRV+dAqIDmFAXVxMgSS5/LZvSjopJpbaNRWSyK1S55MxKrJbhWZAqweKvG4xqelZNKdMtmbNWjYymDdkEbBOVx4Juz1hryPU0UTRzujlCEt+8u1BZFpK6NXMI9q9kwUhl0lr7WmymJjegjHwu7fbmyo6gDvh8j8jM5iX382rtt8ej+xsMlEgd/LOyfjU9GgTdvt6+8b7j3Qq04pym6sSyxHHk9F16dEnNJZBncO+sYoSyTKeFlyGwGHxbrgVW6juKq0TalBiU6u+pBAbMmFGnUqmzXC9q/hHTHbczg9j+/XE7F/9yvjRGau/F7E+5n3BeEeyQ94KeP4PESfruL0TfHdgTEOq7BYnAMtRBbRKc/QZriDhRgJMPqVWENpoLkEgVYlWIZvXS81ggjlLj8clLaurG2xtb7bQkU8Fj8+5Scu+NEWyrujZwmLkulNxvhB0JtERcdC6eO5oTaKILc3vihJx+YfY+s+UheiPe+jfdh2k/bWCCHuDX2EjkpmsZYP6iFJqOYMoyaPcQDxu54Nn7fIiG0uXJt9bZsVZxDSUw50dbG1sezYD0u/tqw3sLbozrNKlkyft3wGh0y4mzDZEJTYJTeYSrTcMaP8+iAtpVZFEisyzaeyXjlsybmu8J0jLigh8MBdadL+Cgvl43Y+zyLiESf5agh51VqrUxqEdk9TJBqq9B7xEdXH1S0r4mhnt1jgyrhrh+YQ7wYWwtsZDOj4jzWiiTjOqa0yWIDqE8bJQUJeC4hc7ysRifxYet8tUJ2G8ZOlZVCd42DpXBpjXdLZfbGXfaIQnGnV2EhsSG0NVzW28j0OYqTU+V0KqzNOK8tTJMG6Xws7EA3ZjMOp5mtQ+/Ch3WNjnDTIRiI6/y6bdxPyiFluoY582NzrksjT0bpo2sd6pLAqkbh/aWi4TcaVYzwu03heBBGlxqL1V1LPbBkZ5hLp18ybthNMpwA1z9+feB+VrQ3DqIcs1OwiAB3HY5Hw9RYLDDTlGNZkwb00AlndTEkWei2Vx8nJuGthlSSkOeKKWVzLo8f4PK3YqMuR3z4oD737n4rnvH9R0fy/Ds6Pv81yD3kGZErLoOk74D+AWiMeGvbaB7Ybh9shOnuHj/chfDKBJ/nyKmS6Bq9Klom/Dc+xr7730fefsD+q/+YdP0QFB0vSM7kY6I9BYtgX3wdpszdYQ7zZ7fbGxvNVWAzU8lMOTEdgmp1WRqKMJXC2mpQiIY7/9Y7KWfSNNGolGrUNB7Y/cIyG8XVnvOghllJs8BZzRnUHnDXQU/em5NQOu6ta79BTXtxHBezjvM4tvkyOgeVWKamF81OJLeGyMPM6TV65SwB2Z2OJ1JKLMtGXcMtLB1SjPYlUbfOtlRcQtJZDsq6VnrrtOrkNLFcG5pgmqIhm0rIn9WFp2vFsrIsK13h8PqIibC2ylSVdl3RlKnV2K4ruw1M6y3iaXKE8JkZkjKbO62FsdLm0EXxQQdDg9zfW8jvDzeu+D9jQZ3vp5vP4ZRhKjOHEje4urM1QfoVr2AlIwdhFuhbhwTTlIfpSXDsLh3Egi3gbqyuvKuGtDXG1uT05LQUt6Sa0nIkHM6HifkYi658yGyERA13qiZq0vA97Y33F+ftWsM4WYTLuoGGXVv4ThpOBXVSq8yXbWQNZaqApggZK1lBE4izeJg4rDSaxNb9qTbsulKakJbnUrETpfcUg71kyF72bm41e7Lk3uU+F5hbtYz3MzBbYJfJ+qCxuEPrcTCMRCmZT1+dUDekV7Q522VlWxfOrmSZMZviaxoB4E/7sq7jNLR3fEgykQSTE+7ghtgojt6g6SDEG0rimsL3kvPncPpDSA+g9yB3hFv/6YZJ3n7OvZ8fdo3PHwJkJH17L1cxGssX6PQp5M9p1m7EfpdwiRIMPUyozPi8IFIgnwYoGcu3PQcLzfDpv4Z8+hH+5Z/Bn/zdqOfrgqTOfDowHyvbNTwOck4DG70nXzb8s4x5BR+L0xTFbhJB+4LkQra4uEIZV1Ad5HM0Qg5L4jApnohQOAnYwDToVmko5/pNZWdBzx8UqfDBjSiaHsrO21ZePXDY5uBt31/obbmkg2mixJkPGACS7+qr0Si7DugvPC1EjGawrBZOcqJB7+uNTBu+ByG9nWYNUsYGW9+QrHiKlAXNkVdX5jIYJ2AmRF78yKTSwC4PJeTgtTZyjvO61OjOt9ro14KlFJPA1dCt4r6gkvBuMWT5SFc2o1YfcSuCWfgHeB2c4hQso7rWsayT8PxwSDl6/69VUA85KAath4Fs0nDHziWhHpV+KjpkaInDVIJSMPT+h7lQ0l5QoyM7SKJb4mePF+a58PohxjDFxwZu4n17jO2lxLh2f1d4mJUy6cDOQlM7HwpgTCP/pzaj1hg/xA2b4k14dTeHCGj4eMZYHXaEkdZo0UOJ8qAj32dwD2VgU1nTyMVStvTAlpTp1QkpnYSRpyiYIn7j4iWP238vHy9KblBTeNGpjhFob9ocuEVMj27CBy1mQLI3u9Zuu01bYFqffnTkNIP0jeVceevGednw1TkkG5vcgXGiQYDfXYZI4xIR/LIiarFhJ4OU3aYWN6c1Z+vh14k0Sk9YM7y+RbZ3MCtoGZhviAhEdlnE/no08GuYkusD4eqv7Gu4+FEDP1QcpcD0bXh1wQ8PoWEfr0utHZ+VcneHHB7wVGKx1caFgA/j89F5nx/xVvH5Ff6934X+VRSpxw/wdKboxief3LNcV+pqXJeFXJTTwxHyRPe3N2qRoiRRigqHUphzJs93bNsF90jlbGuk5WYZOfVUDrNSUmEzx329FTUZ00nW4YQmUFww1eHLoLgr3Z5jfdxezEKjaLYeLILQr3m8HuOCZoz/eSw5Y/k47nLZz1TAIeYxfMV7EO//tjlTAekWS1lNSEu4w7osHHrn7nQCzyy6RZROGnqtlJlSJOTMhwmuK5tDShmXHIVTFS2xuFKVsU+LfULZ4LJstGY3Exqz2PdsW2W+TYChVSsKuzTXHaqDeUYHrNLc95+MaWSB7a9mllBfikLOu7LwaxTU83mLLbkJniBZuBNVHKUF561Fp3FZNt71MB6Zi9IT2HXjMCXKFHrxuSilVQ4+zKprQ7bIhcoKGcN7Yx6bc0XAlLJF1DFbvHiJALTFDNl6qIkcDhZb3p5DMz1NmXnOaIobuOPU0T3mVEg3HmcUfdVQgy1rHdhMaLLN8/OCytuuHCDT43t1Q6qPRcFeQDUeWhlLhADzfqmo7h+/1JftnYjFiLrjPIGNxcLPrA+qjA7BRVx26hqKNDceHo5kCl/Wx8CtVZkIfXrOsVwyg5RLjMI9tLWe5Fnt1Vu4ZYSTbzyM2xUnw9Yix8cVtwgy603DMLhe8fXniD3C9B7JryG9AnnAfQIOo3hfoX8J1z+B88+hJTzNeJqQ6SNk/iakN7jeIToDiXAs/QH+6td58y++Q/7d//x5l+WJ7MM9az4GvSVf8T4hUkM+JluIF6YZv7tHZAaU8s3/DtL+CD58FYVlLKpSq5ymmW0BLTm+tipLXdlaLPnCRGbHLp3jDIeDIskQq5SUmRyoK9Oh4Fvneq1YaFFBZSwYo4jZYGyMtzYwdZUwARrFbsen48/Z82vAWGa6024XLsP/09j3qbscliGU2d3PRHahVLhfMehae9pE1tgnxMAitM3x1shZmKeZpAUToaG02pnnykevDyxzYXm80tow006Bz2rJYUY/Fcw8LhYNnu5hCoN6ayMoL2UKQt+MKSmnKSMFUkpsKbrq7CMOZkBi1aI+CHrDzo/zAVnb2Py3gS0L59aHMsuHib5R5gjrKy+ojq1/zYK69rihUkrxl4/xvVssjJCQZJoTtI/kI9oZUlHUOsWiYLkbJWW2daXXMJWlGaw96E45XmQvQu4gI5HSAO8VW2KjLKJMJX4vJYmBT1NgqK3BSCRNJZF7Y7IIK0ulkLKiWTAVeltZlnaDwSJSyTFR6gbWnLtjQqeCaGzyN+tUg/phC2eqy8bhECYM6zn+XmEsh27GKNEF7p3xqI7xj2PRIaNj2LHU26feOokdEtBxkxJg/c4rBEwaronL1vnjn77lw2XllIW2Ncwzx8k5KtwdSvBsB21Fx+1PNbyuYZPkWzxgLWSf5oa2invg5i4BdWw14qXdPHBvEdpW8cczkj8LXLZ8AfNrOLxG8keQHwICEAW/Qn+P1w9w+QKermg33JawWDLB+4aXCb1/hZ/eIKc3+P0/j5ffxsxYO7QW5B7bu3lNuE5QL3HpYphsYfQiAscDnk7w/X8Dmb4TY+zh+/Dm+/j1PXIosCW8WsAhfUZ7R3r4UpgJT+tKhHc+2wdGA+zMk1KOhTQdeNM65QDrVxewPlyTfHR/AffUZqy1B51odFlhjLxzQON9Xke3OQ+KlPugEFs8Skmf8XtDaOMSji5zdJ4vplXfz99NlfpcxKM+6634xgQ18FNlvO/Bze4tGhnzxrx304dCngvzpAgVhkE3QwmYJOwkpbZYw7WgM5UULlWSE12VZsbaKsdU8OsaptlbJRncjQw0LcMnYTWwUHemaPkAqIMDHlTLPQ6lo2LYUG1Ztds0uXstlKzcHye09rHUDkFA/doFddASEp1MEN1xUHOsd1LK0ZYTN2cYv2sUHYMiKYLsRjQsKqSUo9h4gP2zRgZm8vBSNY8sKyHebCccbQ5puCQR2/qSnGkq5CSDxmT0JiSdAbvdvq12vDdsa/hcsOOEp9ClP54rvcdYfXd3ADrX9YrKgd6cpVVUKknh7v7I1oVr7bS1M7PTloLy0of8ccdCnwsiv/Tre/Hc9dT759rogEXG2/piocXoDnYaSBTZPjiBYQiy43S1CZ/99Ikf/+zKlJ1jVu418Sophykgj2qNrYW1otUNliukOWgo0kEruCGth4IKgebBwbSI8+2tUXujDcls10j7tNbhcgm8o0ww1wDQ2jt8+gpJHw2df3uByy6B1SJgC2wrePhFSLsg56+w9z9F0Pi6+T+k2zf4r//9/5S6WVjkpZE5VArp+BrmT2Kr7wlvX6LX9zAJVl7BNONd0Vd/DZf8rGdPEzJgCnyCLZYg9vEPkPOC/dE/5FgKMp3QrzbQD4haqHc8wvoEoZSCzgWZlLuPTvR327hoE8vWWPszPGUeeGRrcUnKiDHpFkm65nLLhGrdnil7w4rPfL9ch6jCXhbQWKj6eJYCv99XgDKgr1/uhG/qvr1mDM+IOKfjEt6nJoPl2jBzsgvVe2DPHsUrxncNQn4SjoeCWPBLJUHBQIXHa2W9dg6DO9yHOEU6EZsuMaIXzdQ1OO0Zxpgfz80eGhrFOr5n6xGtzSD8iwb/ug43PBOoKFsNXmzWHKpBDzP9uWQmlO41JkNR1g7n+jUL6utDCef6ZrRqPK6NuRTuZ8dTRjWs/UL1MbKyd97qWMogwnGaqNmC8ihhaqKqiIcWeso6yNEGSZizIhryMenhd5iT0HZTj9rjsHRh3fpIEoV1jTjalONwVQdJBTRFFEhXbOsRPe3wYTMeL87WDH1cOc1TYFepDuccJ0lC3JjWaywfxJmrcyAWCC5CTpmkwd3xgdMwOrt91b+T85HdZIQbDzU+e7S3xAXho3XeuaeBAEb+/K2DHRV3J1zvS6ple5bdTQmWkrBDJpGZs7L0eH0K43vpiZF1PVyJxjfX6/hfY8SBhlKqLtDqiKuJTbMS1JjWFDuvqG0wH4Z6zYA5frw0YkzqFVoL3K873irKFouj2sE2dKuwrrAt9LZFrLgJXleu20/54qefD+eu0e0RJPLls59yfPNfInrC5gOqBT9+C07fQQ5/DRNBrYIc46EUEK8gX8BDpV8dWSb4K/8qnP4K+vFfxT77W/CT/w16NFLOlOwgPdRNIxbc3HAtHIqiUwaNFE3zSvghCdaE1p3L1lhbjdRNi/G6e0Skm+2vbACX5rtyLgoptwgbC7XbmF72i3u323Nz6kj/xfowWLkBriBB4RKLIrljtUg4d42yyy9p+P1ZrVVH1Je40Jdgw5bJmcy5vG/odOFxFjQZkyhTLlErtoogNA0osW+x4NIsnMoBmSV4szlj3ahrXOjXtXE5V9ydoyjSYv6LxNh4HpQUHWQNFoSqxN0ug1tuzjLMupsHde26xtg/7biJysDkG+feydlpOE/N2ExZ7esW1DnRu9NTYi3R1qPjDZNE946rh1oiQEL2dMZYITQMZyoFqJh3Kg10aC/2Ls1kqKcMiCgNZLeq2/mWSmT0tcj5EfDeors8HaIdN6Ptqp9g0FBrRSRFLo/teTaCNOepOe+WiqJMKMvK842cgzycEFoXnpaQCub8TCeR7vSthWnIuPn3TKkbnrXP+QF5xXi4twT88s77tqQahFQfYgEFZFeVSfznJYIQRZrh5wpr71xbKEBwZcWpqdPKiI5eN+RuGsF6QcTeCypTAp3Df3RriDpSN7AKRZE8syfd9hb2c3080N0Nrz06XomHGACriB0ClywV6NGF2vA/SDkWUdXwbY3RrnfYrrBsWG3Uy0K7LixNWNYzq0w8XdfRgcWizjyzNXj34894qKE+sknR4x3z8YR++nPk1wQ5/hDnkxvdTLzB9iN8+wyuG/rBkfKA//r/HKbfjiUKJ46vEumb30Ne33F8twE/QUyiqR/vZEnO3Snhh2Ocg95imZsTiYp6H2QGp7mwtDA1Nw8lUTUPnqoZ1XQsUjSKpwfuvS9WxsL6RaHbl5rxc3VGXLmPRaKP50oYstlfFl48//pQJOro/tzHa/XsMeGiLA7X1m4RPpHDZhwcPqyd81fLzWg6uXCYM4cUDKFaK1szPCnL2tiacXE4aUQOHQ5zXDyXytPThm0ewogaF8hJM9mjg82HebxencuyQRcmzbgMa0FRGjqmZ6OlTO2dWgO+FJmG3Ds+kiTIiRa6nWASmLE6NHH8VwgBf/VSamtjKTjMJ0ZIloy44Qjv2gvHwEU8NuMzodhJMiJsd/VRUtAWKYIDQtgxzCRRxHIKlx6IDm6rFkT1PCzr8gj6E0G0472GrVbJ1C1YCXfzzN1p4nK+sFxrgPJbJZUDKjOajNOxkDVzf5g4FaEtlbdLZ3WiE/PnF0kQ+kDs+xYxstJDtucpFmLAC8ceH+bOL3T7t1tfdt4Ku+FK4DpxyHnx5+Mf46LCd47gGPs1upbOvo2NeO/WwwYNicugSmSmW/eI113DfMJmyLYh9QprgPYyzzF/tg41ip8va1xQNeGnhGjBag/szIaTkXlIP91g2+BwBEtQDZEtimltAQGEGUPwJFNCtEU33CvS7RZl7WulXhfqsnE5bzw+rrSecBIkpVYdBSMWHN2iuT2/vfC2Vj6/bDydFz5U53Cc+PbHR9589Nd5+N5vk+9eIw+/iSVH5owvP0Yfv4gN/8Xxb/0Ql9NQun3A3v4tyuk1/ObvwHf+ZabPVlz/biwvhoIOgaRGEsdyIc0HWM/kqXB3OnK6eEwHm7E0IbGy+2FYM+p4iVp3qkVWfB+Lr2qhnZeknDxOVZiO73SjMdFwOz4BQ3vAUvHrzxhrFNOXBj08c5N51vYr+/M/tO/jmcxZORwTTCPVYRTd+TAFA+f1gXR/DEy4OcsS/gTntjJnhZLopYR0NmeSOdU775eVD4uScxih1Bb0yG4xOXkKaXb1TiJqwjQgL5XopNbaWb0OQUzADcXi2Wy904FrbdRqoQz0aLLMG56EkgppNwSScKVykUi/VUG/bof6bNsXN6F2wzRx1Ug6FNJQHOk+vyISz0vGyR4O8+ESFXrdPB2YJyNLDezIxuIJJ+VEKhO5N1Q3RCIQ7bIGkD/lEjQRghzsbkNlFfG/uyGKepCoH14dePho4nJt+NbZWosbpoVp8P2xcB7ONEvdbedGptTI4VODh1mZRWg1YITAqeIQWTM2N+4tTnGMXMau795r6W3cH2CVyHD1yUNBJUPSq6NLTUN9I+HafmslXG9FGOe5U/FogcOYJZrDnqLYtpiAWLszVWcdxVWTMkm0td56uEq1Bss5gs3H3ytri7/vpAO0qrj1KN7ESOkDFlit4bXCRSAbrG1IQIGyBE9mTBcRqjfkl3WBdYGLYds2WqpEno5EiujElx8an314QkrmWJynzVhbqL2SQq+GFWc5LzzWJ54eG09PVz6cO1924cuHI/cn5aM/vPK9k/Dp9B/F93A6Ig8zbpXlulK4J5/OyOWncDTsZ38d+/F/xfLUubv7Dfz0r1B+/UeI/j9GiYHRtIefZ/Vb+bEWclDNyvH+wEkn+tJ4fQCOjWMzTAJbTW+fyI8rT5fKtm0xHZpTLUQAeUxxachpuwdrxcfkAgx+aix194UmhIR8T8HYz6So/FKnGo/6i5bVueGn8cvjkvdY6sxilJzCQrILU07cjbwyUefVR3fknNhqLOPcgqbo1tlqp7uiCsfDHBDCVsMIxi3CEMf5jnodz/auUkqqsWBK8Sx5r0wlUw4TtRptY4h9eohzxsc0TThCmafwZB2BobV3mufBDHC23tEcvs4pCX3wrbfasfbiNfpnKag7z00GNzN7PNDVHPdOdqhVGfaUcXtZp4jSiVAzgLZV5KBhYGFGGjTDZo55xjzMTOZDhslJzW84nuGRhphyODt1CcDex8Gy0OMioBjZUmw4hxJjPil3x4JPytQT126x/XNB6Wxr5ak3snbmsSXNxHZxL06hyHTmFFhvT+NlyyloQ7KPTnEq90F/5/q5xHZe+hjLfIdW+42HuuOfUSSdyN7RX/r86Cj85nZjNpT1PsxpbO9U/KbTZlCravfbsrBGSFEIAjoMb8D4u9caHeJWo+hljbYvHHsRGema1sKJR8LKERn6c3fa1lB1tLXA/ETQVGIZlBpCg1ZBp/0gQK34tlAvnbZ5BLCtV7ZWR9yx8tX7hcenxuFhAjp1KIgYyq9uTmvw4bFC3vhwNaw5RYNxos14/7jy+PZnfHR/4NNPEiKF9vMz5IxlQR7e4A/3+HlDzv8I/+Ifwi/+HltTzk9PHPXTAAC+9bvcHQ+YNjIbm4bbWpIU3gyt4mL0rXM+1+BLAiqVnDqzZo6UAHBL5mgdJuHuOHF+2pj1ibfnRjMdZP0dchPmKcFY4NYeBimMznKPO3HsRqd67qdenD3Zx/lgevzS7L8X0L3DZYffYlmzQ41Fg9vatk7fnNoTS6uk7vz88yvv/B2vjhM6sOCIeRmc4e6c11BaPbxq3B8Tp3liu66oR1RRb4M+qcNByqEOY2stwjTFoshFEYtVtEmitU5dK80c5BCsgaSDoaARPOrclGetOcu2B1wGBNMJ1k9SYuHcg64YoYdf02D6usWz1L0HHUSVJrG9EzJL6yyDUF4Jt5xC0HBWhJIyKPEmGCzLxnoeBiYp3ozqibV3DkBXHTncTmK03AjzPEUsr8UCwoazvI3NZ9ZYdrWBtXZr9CY8vevUq0aGuaYbmb6ncBnPbmQ3IKOuqDmzBKn/Ogj083BXum6doplGsA4A2rA4y6LP1BPZOW+7m6sMSem+cxJuPq+3DtNunaaOkWwPXwuz6Y57GR2E7+1uiId892mNSy6eG7kRtdOgqOw55A6Y7zlZStsarMCkWCno4tA2fIptM61CzZF6Wub4iTbDNmhdiYyp/SEs1GqczytdhJQnVMKVKedwLBNt8f13B0bnu3sPWvTyjY21b1yWyuNl47I2zpeNpTlaZtw7a5WYBEbLFE1I2C9+8X7hi77wdKksLS7o45T4aJIwM3m68AdfPqJfTBzmQs/OdJh5fbpn/sF3sd/8LeSUcAx/usAP/1VO33mP8v9B51cgwvTJD/jkN/8S8uP/nMX2FblzSso8Z2jjIqqN63njfG5cl43tGnSr2huPl8676mwQF5yD9M5hgtd3BTEN5Y+EEUuXQTb3kXLrAb2pxPkHxnJsjPUehRUEvQF/0a3d4CSFm6fEjuvveCvPvx68ZbnBCuLRDSN9GHxr4JUlOM5fPW6c10fuD4WHKaw3O0pvwXo1FdbeEFU+XDe++4075qTBae2d+nhlXfqQ8go6TaPDDdtPq43tch2LUYHuVGEssJWMcZomXGHZDFs7rrGE9W4h2kkxFZo7abAqNquct8aKUvLEkGNSNDKzmoes/WsV1CZB9LYmsYXsnYowSTiND+1LbA/HmxLm0cZiHVrHc2J2IbsgXdi0xAJp3DYfrGIu1M34wIYsnadbLk1cp09r44tLFLLADA2xiKsNzXncYN6N3nx0q5FuuHm4+3er0c4npzwcmTVzSBMzlWnY2WUZ+JAIqcdYkNRjFC+FVqHVFoWdiIjRpDRxhq9KHEkfN7oE/uRi40EYnebOMRufuxfiUG49u1YxMK6sgwryottws9u/71tdJA12gMYyacjmXAKqWMw4eVxCy9ZZNg+csxGJlVnp1zP9Wpk+eaALpB4XmB5mSCm4qq3TlzXMPgxsPIh9HM7ajNKiaLsOXNUcZaQluAYly4KVwbCzszZ+Jldak4gEJy5ZSZ3DJCRPCBvL4rFU6OHhuRt6uArXDm0V3p+Nd7UhrXPMCpo5FqW68AsRfvblmb52au+8Psz8xt3CXy4/4iMX/OEBe/sWmiDJaduZcisw4PmOh1/7dZr/dboExu+M+OjW4LLhW6U+brSlcb1uLEtjuXbW5jwtC+/Onc+uzmMNTX/JiZw0zI69h6zTja314NtKvFR3+9Z/X2LebqXbzRbd5RjXiwas4vL8/e/Y/n4mU/CpnpekPqSm7F9fuG3/w8UlKGUOojnMVtSQNNzPpkKXzOMSrnCqinfnYsql9kHUT+hu16k53Lw8os5Pb474WNRdzxe2ZUU1oznodMnDayOlxGXZcE+s5mzaePVwDANukXALW4PquPYWxicjh66PTDyTxKUZazcOhyn48s358NS4XlZyLri1EK0Ad/NfvJX6lQX1k1cZRcMYYPAScpbgf2o8/KcOH5YryYRP7idOk9O94i5MScmJkT4anWuaE73DdBl0l2OhJefJjDSkHZvJ4AfEW392+DBwnoACBFqnTIOGIcTCBh9Ba5BbDb/UkiglnL2ThIpCHGRrdOukIjRvaNkLtDGpjw34MK5GKJpZ10ZpihJRIL04PsfXn14lRM6jM9w9pvaxm6HSYCzhZNjuya072Bf/Ls+FMml4bDIw6lu42r55VR0rXBnPUBT67tHRqIctWiVGl80jrPBaG9dNuS4ry/WKtzW60HWF3lFNYbjRNTDPuxNeErJWtFboxnY506phIsO71fEha2TY/NmABrzH1xoi1BBm1U5dFlRz0FF7RBF3D7hla5XaKykLb17dczzNnK9n2rVhYlQL2kvvYbWmRBTMbmmoUjBvYcPmjld4t3SuPdRC03ykW2atF5bWef+48qdfrfztL9/zgz/4Y7770Ss+fiiIHNhQ/NWB+zcTb66V6Y0Aje3pHUbGrYJEBwZEkX7cqNeF69PKdt2oW+dyXWk1+Nw+Op6woYN13Ti3UADdn8KPNk9GHkyK69pZGbRCCt3Dw6GboZpHMZRxPIJG1nwUNxkToXI7mTqajgG43iCqPWByX6hikPOeh2U36MkGtIUPr9ZBnVtq5wjc3R2oeuLxw5nLVrk/TkGBco/7W527OdF853mOBW2Lc1Cy8ubjeyQJ53ewnMPbuO3YVjfwl3E9UeizStgV1o2trTSEMmWmQ+LghZQKbZy1bamsK6wI1eC81HBjM8i1MXfC2g+jyXjPvAcX+usU1GQRC5xLwoaO9X7OgZ1YD0cWImVQmjOJcJRwknIN1/J5UkrWIMziSHL6XmRxDkW5n9N40JyUhEvv5MeMaCO3jpQTH72eyEkHwBzd6lxSKGHyMDAwZ11KpEQSh/MwFaYSS5GkIZ3TFJ1o0cjr3jOZ9kgIleG1KEEJcwmn+7u7KYrN+zvOfyJ8/PERmRuTOg/HOGw6JLPGnsI5iFMW/9zHwd2J/fuSYH+Let8Pd4DjYXjTMRS834omL9gE4VHrg5M6Dvyuyyay3I3wl1w256qJpRndOq0pTOMxq33gtlA/LLx9d0HnA0feMx2esNYjzfN44u37M9caSyr3cF0PPuigAQ14xi3SE7oEV3NPSGnV+PDhQlJhKjk62IGF50k4pBk5KnOORYK1maczPL1fuKzw4XELtoVHkXBzeo2C01qjbRtrWwOfxaluPK2V2vNYQGz41lAzMonjIaNzeE58fl15X79i+kUiB9jG/PoV3/7hD3jzuOLfNtbHn/HTP/w7rE2RZhG3gnJI4eh+fVzwuoWWvhpP543zpdIl4zmFPrxA9vBUnaYywuiiEyRF4dckkBNWQqkjY+JwD4d8kWE2zgt7OoDhKeEWSiFERjRO6PGT6oi25mVbOgpUYK0u8b8hWR1c8fF5IW0dEuiA5GMBfbtIndMhsx0PnNcVZ+GYI/jQrJEPMzlnrpeV6TBgElkjSaJ3rsvGnIX5LkL52rrReyikIKS5BtQWS+WiCYyIN9kqglOmmUKYpygJzGjrlbaFpFaakR1aUtwrSUZ+2xYKylmVqcQOoA1RgbmFnePXKajnS8d9i5RBiQeGSnCzamXpsPizw/zTpSJbA+tITmhWLtfOXISH08RpShFj3EP/jhnL04VjK7fOhRKMvjxGEFewVklb/HCbG5KFDBTvHOYwa3EZ30dJNASVznEWSlb6CPhDQvc75xQ46BhrbGBwe25MGLXEeKElzHl9AJ6iicUyS4I3D4k8FbIb96UC4cFoso/y+5+RF6DVczewdwbxa4Peoi8fDLkV18DF9k3/WDvcEhOCs9qHcsv37gJngNi3XnnrztKMrTuuSiolTCe64TWPMVxYV+WzLy+UyZjPa0wkJry+y5xeKY9vjdU2eks0RhidOdsKb99XrtctfhYPdc2hTCSdQu43iuy6xWKxzIWcw0slj1ekTDPHnCi2kWsDwhPi+OrEIgcel6ewV0Nxb/H+CJSSef3xHfUSxsBl7eSccY+ETVXhw7mzXDaua8Vr6L9PRSgK98fE4f6Oul556p3aneVqLMsv+Bzn4x//53z3h3+Zz//2f8iP/uRnsZTEkK5DXRNpma3W4U8r9Nq4Xja2atg0MM+UKRPYulCHhjynRG/G42WlWSxKLrVx7QFdtXGemjhdRgifxUbc/1zTdJMsS3hY7Hg98Mxj7juRf+9uA2dXkXEebUhgYyy3/RTFjT6UXB6XrEaCwMNxCDiskaQxZ6GO10ZFSGJMKaDBy7Kyeuf13R2HSSMwUSCR0S4sjy2c/V1pa8N7LKDltnd4ZsS4+6A1JeYpR0KCxILUWuRN1S2sAekEdOGBPetNtJJYRpKCiOO9x85wmug2nNRUQ9X2dQpqI2Jqtx7ttHUheyMrtG6s3amEXKvhnFuLED6PzPBDKiRNCB3xRNtgbY0aExKOs21KlciKiZyXUEzk2+YSrBl9bfQq4bRNRC8ENaSTNgke3xaYmoowFeHhkyPZYgu5LlsssFLGJuU4Crfb82FykUihBLRWZHN0SqR5Go+5kcUxbyjwMIWK0mpH++CmqgJOItzGdZhHG3E54LsRxV5N4wbcg8XwYDgM9fEYy/ZelIGvyu1fdkbI3kmERVuMXzK4hC6+C53YUJbauG4Gm2C1YtdKOxs/OzfO2xIGxDrx1YeGpIXDMqNiZIHzuXL6YHz+1Tk8ZIkOtQ9fzOvWePvllZQ2mjrFHSmJ+xmmUjlqRrMzTwdyKZzXM9uagiamwV3eu17vlSJOkQkQUjOyOXcJflICRpLBg7ZROA6z8tvfe8Ux3fP0dOX900ophdqj6ygl0805nxcul5Wnc2PdlPdPG09PV/qlcX9dWC4rTwtsCusa3OZ1+4K/9//89/j2K+UP/8P/L+dLbH3dxzLE+g3G6Q7SW3irmgS112KS8O4sa+W6NJp1colt9XLdOG+xU+gOmxvXSkR/jPcTGakZY/m0X50vifcIsXvy5xE9ukh5ca8/Y/V6u3yjkAyDwBtO9TLlU0cBCne3kICn5PQW5P3IqQp4L2chp1jo5DyShr1ySCmmiSw8vLrj4TTFY2AMlWDQwaw6/ewofdzz8X3uPJpUhJSjgaq9Uw7TTXhjNSKAUkkcHw5gictn78GjOw+aYgtMPwWuvjRjyims/nosxDRFjHerdqMx1voSr/5nKKjXrXPMe88Qj/fmxlKHXljC/nzPgLJxkLo5rTYcY54ygnNpYXhRzTDPEYInUCXRhbGpT0wpuGVKj5tSAqPpFhZhTbi9WSUHWV8kUZfOsjQ0TdRWqVcjW2eeFRs8dUfwHgTTSo+LYOtxK49+MKWBpQqBnawbU2tIAi1KKkIpsdFVCx09slOYGFzB+DBJGC1kpy+2qElldJRjQeWBDT/rpnhBuH6Bw44HaDeXdtlXrrfTjvU9/X3gYKPg+At5HuKBpS6VbeuwwdP7ld/7oy/42Ra+tFNOZJSSE6c51GveG5PAw93K55c1zL15Zih0nM16cPjYSF3RPDEhiFe8BwtkyoUjlbVX6nJFbBQMoOYjW+8sdSMXwZNTpWNkaq0YES/SWx8BfYZKQjzFA0LjgPHmOHFMifvjCTdjq5Wn80ZKkUP26jRRl4XzecWZ2Dbj6emCX4x8TJT5Dq7GVhslO8d85KPjAS4X7O1XJIk4Dtu36gncjcaKFuc0n1gfL2xrp45nodaKeaKvxttL5YvN+WBGGw5n768bl62PPDOLrrT2wRKJJkM0XNDEJShvQ/8f58NvR2ZX6OkLw5T4tL3THKXJ94K8//swNd/P8H5jj+chTYmcw8UpGDYJPNMJGXHvYSzi3WhbI0lCJXYwGyFMSMHz4niYmbMweR9LtPhZugy6HWMZKz2MInrguD4WzWTQKaEOJYNqJBUHCWakg0yJNCfaYuQ53bKrujtaDc0T5xpFctsl7Ulj/zMerCZho3goGhhw+5oFdTNhIgK53MME4eoWqZs7NUd1jAPjzR6epd7Dn1LMbzxW9Vi0xIJrx1tCgdQIfuruJv78JvoNvEfi95PEKD+rU3snDzcrHcA4g8JRa3SefSwqShKKOPOU0BzdgqaJHlIMpPeIkFUZ29ZMF+P+1YnD3cRaVzQlauKGYXrr4+uMhURAmaSbRlqHGub2I90s926/Nsax5wXVIFxpjJOjWgL70mX/42Gq3H2P+Y0H7OahOhYMZsP5XcOI2DVGyGszlrWxnjesReT3V4+VPB1Q7cypc8yd3o25BJ3NFe5RxBN0QS36aTOPh6BBu1ZeH5zv/Pa3uPvB99AvPuf6k1/Q68rxm7/Gww++D9/6l8hPjzz9R/8Oosbp+9+J16YmPvzpP+LpvPDq9YHjaaaROV82ni4XjCCQP57XyLQycK9jDBR6dd5+9sTjl/C0bjgJ75VWQ6vgVHRKzNOEt866gXmlDqu6ci/k+cDkcJgbzQ4hsR0X+bWufPVnP+fydKWjozsH83iQL0vj52+v3B+Ux3cL57PztBpdEimH7LhaY+vO+0vjy9bZrLKtxsWMpQUW28zYzEjuFAkGSheG4MPBdJge+1hkyoszFJzctVrQinb4aTQKEMV/NzkxeS7E+yF1Ak+1/VyNS/+wNzITWMlc3CMipHcOKjSrwRiwhtUN8UQe8JQDc4miWVul9A25QiNxPc6s5+BpSjN6jRgXXRrzrGTxobxMwSNXxZuwXBdyypQp05eGJOMwF1IZfq6Dq43ANBdKiSWYeiyaa48awvAUmUohVNDxd8jA0N06WQjGyZ/n7P7TFlTHqb0xyzQMTzziA4pG/k3zCCZLRktxE+Y0+KAWxIzYKAa3s2hokrMrScdW3jubj4zxcQn5bdzghvWYRQhat3iXxWLppKqUXFBLWOusmw/wPQQIVhu7y/2uIqmbkXp0lUnDdUAwppKHKmL0eO6UQyYfZpIqJSUubaH1Og6m0WsdxX5s2G0Uy6HHtxsW6s+d7IsO4Lb9R2HgQnvhlYGbCSMO2H5ZkrsXVfEhkx2FNKkio8MPsr/TkLhZh2N7CDSEx/PK2y+feDobXI3LeUMXQSZhTbCmKOL3HsEkJGerldqERqdLvG/ZQ5HVN+dyXvjBN1/xvX/zf4l+96/Bj/8DLv+3f4d3X1Tu/+q/gP73/ldI+oS0veP+7/8eIo989D/7X9OOv0PqC/xH/3vkP/kP+OSv/U85/cY/T//57zH9wd/nusYG+dWvf8q3Ll/St6/YGmw4k4Gr8mkWHjp88bhx1I1f/+6vsXz1OX/67sJnZ2eh8o0y8/FpwUi8O3e+2MKUZfJMKQpz53JtXN15XQpFoNfK52vnD5vz2Vf/Kb94/8S1Rb8XD1JcWH/05cb/6W8/Mskj784hkvjnPsp881RIubBtlcsi1BrUtcdlY3NofWTSj653HeRz1cAffVhpigbZPaaCXRmYSao0tbjEXajI7XJ/6WqWxkXuAwZI+xkak89+ptz37lTG+B+S8kMpZGl4yRwPIcoJ1p1wf0h84+MHLj9KfPz6CPd3tC4stVFb2wkFweRRRVJiTmFEv20Vk3RTKXXzcIkzx5aA2e5SoaTMtVXaQL1aM6YSoZPbEumtr18bcxJabchFOLVosPpqyDWeB03QtsZmzmOL1zZpYmuhvnOCiVCSckwTzTsCTOqIf80OVfzZJcmIMXTWgY8Q/LyuyrnuhHQNdQJ+G5s1S1ANxdkQtKTYMg7lQreBMY7bcHf23+kdAX6HqqpHxY5faxaRyFNHPVRMvTi9V3bKzqiiYb5MuFmlKWhSzZS+VVQirK9oMAFMCCu6RKSmTsJ6vbA+duZj5u54wqfINrIOac7YMFHY8agd8ez4WHAND0YYC679kI8u15+78sEP/yW+4Hg3bmYyxu1HC8njDU97+fmju+/hCBb1PJYCCqgJIp3LuvHzL4X1GnZ9R3fWdaP3RJsS5M5ZayhHBVYNtcy7q9MI93Uzx4b62InNs/cF+fIfAE9sf/P3+eqzJ7ZqSDogOiE4/cNP2D77BcePBbFGSvegR/JHHzN99/vM/4N/Cz9+g/ytH8L6v+P0i7cc/vJvcf8//rf4rd/797n//b/P2w8TWKaqcrDGX/roDb/1SaH5le/91qd847e/Rf8DmPI7PvzxI/Wy8a9944FPv/UxvjZ+/x99wT/47D3fSCd+/dWMSKcl+PvV+dYr53/yL/wmh/crT5+949/9+Vf8/Lzwg9/8NpM6f+PnX0Y0DtFBCsJ5afxRX+lmLEvlo6LwZiL1gHqWrbFuwjoikrtBdQnnox6b++BaR6nO4yLeqUyhGosz1EbBU7m9+LF32CeeGy76fCb8hkHuM6LcOtLbSdvPKnqbfsydjFPglgyQiyMWAYGHUvj41ZGP7yY2FT55cyB9dAiV4Ghe9ukq/MqNlFPkzSUBM7oV6t2e9Bqy7rbHQ5sFCV+EJIUyHOv23zMPh//WhM8/rETiaxii+IdHuo3k4LHwVdXgDKfwuphKcHO8RYJwMw9YwzvXWpEeJtNiHbe/eCv1KwvqpEEDwgUd3Dnv4dJvu9A2ZzLCpLEocnd0RGyoRjJhydF9SYKeBmaofVA+EoeSKNKY08g2l1D36OgozX0sjCKjyojIgmWpTET2TPgrQm7xYlkP9cOEjoWXc3+cePPxkXRSUkms28rbL55YnjbcQlbpreNiTCPmtteKeIpN9JxJU+JwPEQ3Wjv5IPTWaG38XBKHzU3jlhndaRRKHwU0Dus+2vuQ07nv6Zk8j+yj4Q2j4DQOt9zmMBcfmOleiOM1zylMbHprN4ZBGJlE8OC6bbSeebw4XylgzikL33s4cG3CuRkNQv2yhgEIc6J5Z1mVZbOgH1kotfaFGwDd+MlPriz/9r/LfJh4/9WFbsJJNn79j/8byu/8Z6DfpP6t/zNvf/oTkFe8+Zv/R+Qb/zFUYfsb/xlYQbggQPcNkUh6La8nvGR0ApWC04fKqoEk9DAhpxOncuHufsLrGds68zST9Yn7KfPwr3wH/R/+68g/+IdM/5cP8Eewnjrveo8NdHd8bfzOp6/49NfinGzScYuz+p1vnvhqa5jX0H9bsEgyiRVhGganSuebp4mjCtt4Dc+XznV1uoW6zp1h20csJwnFVHgY75vxGL9V5RaCt1fNwE+f1Xc2JicbzI99kRNf/vky3yPg93PxnDThO3Z0+2fZWxuBkkaxM2hbbM/zWCjaVnn39pHeje1pIU9XpgBcqR2unlhr+BMcp4RaY5OhkBxGQa3Hwvc0hfHK46WSU0aSDANui1rjSsOZDiEt7usWfxeRbdXdwmMCEInmjcFWCD8AJeWEiUVGXHdsmH/bLhAa0MuyGO26xZTQ7eZv8s9cUOO9Ggodi7+waqie9i7KLa4z9dBVzDmxZx9ll4icJhERu3EjaoZpBpeN2Jwbx6yccsg/SUGRSuNwMxzRY5PtI41QQ6njwYlFw4DaBrabUyKrcSiZ3juZRO5O6sZdKRzuJuT1xGnOPL2vPL5bWS8VM2M+TQiF9bzFYSnC/Ucn0kGCjjFuqO67z5NC19thjZd79BG+Y1gvlgZE/xr/sCtexrZ/L44875t8cG9fdrVxnw2AHkIZNdS6OSdKj8LrSdHdaNiB8cA+ts7bq3CPcUDJ6qQpRtMNuK4bzYmcHxGOByVNQvdIaRALg3A3xcfCTwjH+smM2jo/+ekHSinsEthWG9e/86fMh38bKQcuv/8n/PzzC6KJN/+/v4f2/xrpztNX76l39/S//n+AH/4u6c/+BvUnn9EvF9a/+wfc2f+WX/ytH/O0CK6GDb7vZsrP3595//6OfqlsP7lSvqic32/hpWmNnpynD+/RP/y7bD/+Oe8en5gOE08GtizMAtKUDeeP/vgd/s75xfsL/2ip/GILtsTf/KMv+cOvruNnVlwqDR0YfmyXxZ37SfnW/URWuFxWmimPm3DZQhl188IdblVhajOofPbsXJYk3ewZy8hG253GzH1nKd++jo1icjtELz5uk5Dvx284Sw0oalSgm8zSXvxZ0T1yeuDyBnQbG/7CVTtbr9Cdz98uPMzOXe601ljduXji6VqpW0de3dElRvqt1tGbhWDmNMPDfGJdIw0jFefpfOW8hsjhbp5Zl8piRjko94dCe4qMulKUqWQ6idqia80pTFlUI/err0GnautGzRqqrixIiW7ZWx+CJmJpua146zQPI/M3Hz18vYJ6bU7STr2uwRd0WKwhZpThwuICS3fqzY0lSsnVADMubYsFT1KyCGWKJVeS+HdcaJa41gh9U3G8bdThQmVAR9kMtmZ0BZHIHZQUuGldO2HkAt1ig1c0DY19HtzGaN8uH67UunE6zuS76BKmQ6JMQl8Th8NEmlJAFxYjfZrj18qssBpW+zjoAT00U/raR7cRg/8NI91PtQxZ7u5UNbqMWGXJzeDi9gTsVKkbVrA/DEMS6PvjPEa0YfEn6pzK8Jw0i+9nmEkXj67fRPnQQc4VGWPRKcFsykGNac6U44RJ8GrVheNhQotiStieaVCGqr3IQMJR6xyJTe+qwpR35Uo4259/9paH39vQ+cDy8w+s1Xn8sPGz7UtS66TjxFPrlLxS/4vfI//49yFNXJ+utKp8+LN33FXhz/70p6ytgwliYdDjCk9r4/1T46l2/uRnH/g4zSwSRhxWK00Lf/+/es/hD3+ftioflsQ3j8qlJppUqoRi79dfHThX5/e/eKJ6Z61wl5XjcebLp5VffLhStKCSh2ghZtnkzuSK03ldhPu0L5jCmGaz4YBkcgv1yyOmuo33VwgXpujOuTlKaXQKt6bTu7+wi4xzkAbscvOLYL+U/Tb57UtLBkwRrm36DDOJgBvqz1AfBN4a0EY841kFlYRZPI/VOofTgSpCBTRHOseUMt06x+7020VgkBUlfHci9yrH4nvpfFkvweUtiaLGXQlp6rVGlErOytRjd7AtFZqTSphOL7XSXIYwIpbNeU4c5vCobb2jm1GXhSbKdmlcW2UuE7MEJUskcbkaH55W6lYxE0SVV6+OvH6Yv15BTSW2a3s6hQqQ08Ak9xtLUCrVM9feuG6CktgGzmKaaBLKpiSdrcYDsAz+58U679eFrE4uEyWFu3YbZHgnpKjn5pHtUpyDw7p13hP5U9aiS90MlmZcWyOrM6lz2CpTDnXXU/XYFl478+xMV6UUjWXW0mMcIeSnvQXJ+ODGgc5mF159fKBuK+sShiAtFc7blculcpp8P2l0DwvDGJmUXaIX4/qLQ04omtyF5GEsLKPr2FHk22Z24Fw7ZUp0HPoUYXCxAAu87DANddH++T3Qhww3K8WnrKwYVxM+35QDTlk66UOnzEGNmTTsy7LAXBqlJA4l02rjZ5eN6inMOIY01sQ4JuGjKaGinKfMr33rFa+K8tVnT/ih8PD6Hp0VkvDw+oHfEufh9YlXP/wUobH99InLF04pQjrBfviyTBxPV3Q+oXcFswPGB/B4fwNxqSSZyRlevSoULbS1kb0jRbifCylN3B0Lsxg+gb1ySj5y3homISb49PXMsRS+fHfhy/OCJ+X4yYH5UPjqfOXLDy2K47ZRh9dDVkXVuE/KfQrjnTeTcG0bQgqeqTlVlC6JirFajLdZg+2CDY6y7BinDNgloKRD1hEzJMMqcCydMMTkBqOaBCPA5blBje4sXE2fm9dxLkcCgOzdq2r4Ev3SHR8ijCwxhexwlHlHJZygjgfh009OfJaUuWSSg5RgI+QOp6nQXDkvG2Y9Av3MKEXZNufpWlEzTkV5v1SUcOafvNATJAn46rItKOPGIQIDZ41J0pPgtQ3DlljOmYQaTbOOdF2lNlgrlIcpvn+cOYc5S+vO5bpwvYawowkwCaVEysj6/unrFdRXB+UwIpUDwdvz6hNFR/Koxsbs7VONUKu7A5MqXeLWy0liwZNfvIkCae3I+w3VxOuHA8dsTIcS5O4M6/vGL54aneCepkPi7iHBsGwwgeuu08wxMvceeExXWJuzuLO6UCxyq8RGMNcGfjXyk5JS2P5ZiwA61sru0akipKvBhyv5K+X0eUHM6e9Wand+9nbFSgVTjoO/modeem8PbmmT8ueWA74vo/YDK//Y678/ODeS9Uv4gB2PHfioB668O7EljY3ubuK0p3KSouOvoanlqTu/OMfSaVdmibabbBYfKbMyTFqGh+Zj2yJBVhmBbRkT+NH7lX/njx4Rd04n4bdJ/MYr5dufzHzyzSOnj+6DdCzCsRrfPWeoDZUN66Gn/um7hTkrc5rQ98LVhbePG+/fLhwPCfv5wn/xkydWq5yy8M9/cuDv/OLKI8bFlZ6cb7w6cp8yvjRwpzx15M2RH37vIw7fSKhl2tuNt2/PfDgK1QrzofDqYeL1qwPdhJ8c4eFRsJTZ1s7Ww2ilpBhNw7rQaLUz5Xg2Dlk5FpjTRCmhp792oZpwMeNDMx67877Bhy2oa/sy1sfCMA8VVBgkh2w4qQwZ8oB+dlxonBzTfTEZJyMwVEYX+vyxwz6BvT8vRVWemdD7FCVDWr4zbmJaHJ2vEwISccRsxKwrj4+XwChdWK4V25yUhSzClIPOdc1wKokiiavH6vZ83Xi8Ng5FuJ+Uu+MRb41JBatb2G4Oo55mnUMKH+S5KNJbnM0pkeeMbKGOzFMZRPyNRsid21rZrp3L2tA5c3woI3QvIdZp7qxrcNrNopuWDMc5c5wyxaH8E57Vf6qCOqswq4f6SQdp3WNxVASmrJh3as4cUiUjFDd0aIaTQtHgsp7KFNSGXtEcVJt5YOtvDgfuckVLEPY1xXi1a4fdjSklXt3NMbIODXEuoWpoNZZCkYQsvA7YFVFnmhKaBg9WjEjtebbes4F5qg82wHitEjL+OezGgpcagLQcShzKMiNlo3Rj1h4b1j5AfHHME05sd1+cZm7b/efSuW+inovsKJv7cmAPLPylP6gEzxcfnMwRsz2oTrEMk4GRPRPw945ZNQ1qzADwROJ1M0YYG7cHN1wgZWSIBfVKCN8Dk0HMxvmiGl+8O1M75Eflv3zf+DR1fufNgW89KK/uZg5TCnI1AUvUS+Vaja158EUtUhj+i588cunC29V5u1YetxQ/J3FRmijfmjK/+/EDf/b+yvuW+G9++sRPP6w8ZHgzZT46zhyT8Pl55eTO8Zj4VjqxPT7xeF2BjKtQrfN0vvD55cL0LpNy4bo2Nne22rlsEZ5OLiznyrIuYSU3YkpA6AgfWkSVzEl4asFrliHhXM1ZDSqJqzpWwloyMggCK9ynkuZhBLIzXoIbHWOn+LBi9GhyNLREY1Qf3Mn+fOZ2D9OIg95LrjxbSjK645spSigW92lq//zeQ9SzNaN7iiWqBtOjN+dyaWx6BXOSgbed+jUgiOvKqWQ+ebijAsu2UWuldmWr4dmrCvenzKt55vIUUfWtK+tmLBtcLc5tUR0OcoaIhapJ9klMmKYcdUbALEF3tssWBXUJJgGHUIG2HlEoNGXdOnVpEeOuMWkeSuGYo2lSkbGw+hoFlR7JmirKlBOFBB4bxF3B0ayz1PDutAbFnYLz4VqDalESJQut9vAlnUKHv52vSI/t3+XDlVw6WoQ6OlRfdlA+7sTr48J1igwptx4KipyQLGDgLTZ0dRCIk0pQMjwMPUoJ3IUkWM5UEbatcb4s4VLkkNXQcVtv5qMQx008SeIgKd48hFWFU0m8r43WO23bPRJ1FKChQtExRsmuP+b27z5wXRkrVPEXmOvtw0fhCsaFEMyHXc5qbkgK+llvgY05jO5mPJBjIbXjZeaRBhmy4B1fi783umnYrRj3PycSajNVx72NTTNDwx8JuTq09ebONmIClqVzFeP8oZIZOJwOkxqVkTkXZHUFSgoZquN0a6xd+WxpvKuDhLbjeaNb/rOnyv/1j77iyR1F+aoJX3wVpP+O83BceMjCwRv/4q+/5qeL0b5YeP0AUy48LcIfffHIT7668tScrQq5ZMoUyqupKPMhM81hlLyJ0ZLz0WkiV+HL84aXRCqJZdt4dLgYaBs0J14ULUm4Ol0NSUHlm25nJZJDhSFEGWeDneY2RBr7NOMD33T9JX/w4bAfgo/dhEeHAOE2ITIidcaFucMLUZzixo4LeRTTIRbRQf/rzkj63dkrI0VDlZRiUlHv7CZ3vcefXa2Fbac71cOsR9JEq42tRwLEfYlE2u1asbUjmqjNeXtuvK+CTQXFgvIIHGUQLFWwNlgnY6xvdYnJLPTodI2MvGYWVE53lmuM+HVpeBfq0qHHue50jvM88sCisVsHletrFdScEjmFQ8yydTZrceAlKENp6Oalv9hI98ZhytSUuSwrognNmbrFwqroNBRLgekYsLVOFQMbLvWbITWEXz4e9G0z2uah827x0FcxvIwH0oVJFPVYjok3Uo4xI3chbeNQZ4FcyVNCehTmrXnQWnqNboXYbmtOzFMBM54ujZqcQxHKOd6otqxc1oU5FdouRFB5Ju7q7sA0fk90rFefFVDyYtzf1wh7QdsLatBl4j9APJTuYEEfcQsjCBJhzO3BWQ2RBC8eo+CIio5Fxv4UCvgNTpDRuUOXZ6s3xkOkFg/HzhUEQ1zGRRZPp7gH7pcLy+jYqUbyYTyDMGnk88TiIBRc6sPzlhAJuEHXwtveOY94mGi1wlPheMj0OfPz60rKBSymF9HgFNZaed87LSV+66OJ7z4Y33gl/PC3X8EGP/rxe/7450/8yWcXzi2ScJfWQxl1Uda10ZuRi/Dxxw988jBxzMrrhyMlJz5cDazzxSX4vLV7LEp1h2miO5TRRYqHJ6iNheQYCsIPYRTQJOn5HVNBNFySRJ9TTVc63XcJqQ6NuQxM1YcE3F6cq7jonrvTZ753/L+MEf55UgqO+O0AxdEeeXJInK+GUltDUUoSDvNEQlkAU6WN2pG8k91ZEbbWw0g7TbgbtRqXtbIMG8a6bXx4MnqCg4zwQlfO3bmao2akBHULmqXrcFKrFRtuVjhUwjBHZbAvNIUasHY8C9M0sTRjbbCtTtskrCt7H4teH7sXJbkNCSwsw2vhaxXU69LQ4tFh0qMTdI+bZ3C5JCnTWCQNyiK1VY45eGQpC0VDEVU1HNXxzDwMYJ1IAjio41JIDiYRwKVDH7+Pl9mDW5qzcCjKXSlM94lDGTSrnEJKedlIJXF3UKYcmUznS4sXX5StjZhehVevT+DG5VJ5fKxc10r32MBeW+faItLFVPA+NL3TjJnzeNkCD57k5uIfiajPSig8/h7dr5y9ARV9XgIw5H9++yMvsK8IOkxJY3PpwySDAAOSEq74O+6lxLhCmO96zOLPRfr2jT0vwHRsfvfPiW7lNg+Om3LvpuXWMfsg9iNDcePxuo6bkK011mZsDufRsew4fJJGktHxCrcHe+ch7yNtT8bq4bCkGuTsJGAoWwuRhpkhLcxPHMO9hhxYEhl4czfxl37tnv/u9+/5wfc/Zt2Mv/sP3/Lf/Pgtjy3x1CfWajwtjQ9ro+PkKS50EeF8NS6/+MDbD5ljEe6PM6+OhTcPhS/PC5cqWE9ca6UPSaczssn29/i2Xd99HZ4xdBnPgQMqfRDYn1HteJ+j87dRIn3IS6cwQ6Pa8/QTj+dzYsTtWh346e1sebyvNqhT6gFVxTnpN9Pu+NzItDpMKRTG5jwuoX1XBc/CQ3fup9glbMMoR9WZxbGc6AplPtC2znVroZ1PSrV4FvKIJ2mS8CmztEZHuJhTDjOn3tk61Iuh2kM0ZEZdR2NUQt0kafgAKBzmMIa35tStB9XiMLE24937K4spRsK3xoRzSvEzqxMZd2NngCREM6mEk9hf9PGrtfwIxXyQywOA7xAqJx3YjsSDmLKG+oMcGmR1DsccRbPvyoIhexTGGzfUE0C3yDjv3enZ6Rbyq0mE7zwon75KfHRKvJ6PnI5KmQghQImb+bxsPJ03tq3jtYErnbhd1uY0E7q1UdDhMM+4Gct1w1plq30owKC44lX4sFUWd8qU2UP3ukURMI8smjKHVnnvKG0vSjteKdFp7XppGR2YeBTFqCZ+W//H8ipMBfflQSwQ4mi7x9IAdjXY3hnvY9tQVI2ihz/bm92K3Xi4dwwMhtnEKKSBW8utu9rHzPEZe7+F60759vEtZPa9MypkG0RzF8wiFqaPEVThFja3U3ii83l+/XZxgzrMifG6xFZ893x1E9zCvi5w34QbTAeluPEbryb+8jeO/Mvff+CjuyM/+/l7ns6NH/3siT9953x+vnDeGr07l62z1M48FabWYqGaA+vdamexSDQtjxv3c+aj+yPffn3gMHV+8n7FSYEF99FheyxP47w/36U2Lqe9vvl4vYe+IsAPj/fSRrdoPcpjG1Z/kkAkjXGcMffvS6YX9/btkn4WD+yVN5zyRzF/bk7HxLJ/Wry/gjMl5aSJSRNbgqlEVLNbp4uzWkfWBdxpdKpXtGv4RnQQN1SN5pHOy9hrnGZhyjme/V45b5XrEv6nJonVImDScLZm5KScjplDcbx1aheWrXJeOocpx4ieAy58GFBQq851bVxXQ914vCw8Xho2H+lWOdCZJiGhNG/hjpUz3QNysBrFeOuN/pKc+89SUCUp3Yy6+bBWS4GTiYD028N30wcbwdfKOZYNvQfNxoa2uDvJfFjOyfPo4U6eQuEgIsxzDv7ZufKD13f87rcmPj4Jc4nx87KusMWI2E1Yu7HW4Pb1amEUnTrb2nk4ztS1cq1t6PYTWRVZQza3LWEM0hq02uktQgWXBk2FRSLeRAHvnd4aU9kAJ+dhIZaglBjV4lDugNZAoQbOs+ObjJ87cqBGcWTnpD7DJ9yK8PPoNsri7cDfOojRTTCWFgyDbBmtiLJjpQNzumE0o5ANPas7uOp4CF/auDm78MZsF3bEzxB/z869HXjd+P25PItxd6qLoOxwoNgOiMjAhUMSuzMpQ+Eyfg4N3mHK4/uzXR00rP9SSGu/ccj8lW9M/HPfe8UPvzHxzdOR69L52//wS35+dj77cOEf/OyJ9xtcRhxPkkQ1xTQKj6HUGljxcZ7Y6nb7+ZobT1unP125mzOneeZbD8ZxM65deXepXBssPcLofODqt+50XG4xBTzLkh2Gau5ZGsrtPAVrpiS9yaqThpWejAnG+4u7aced/lzxvn0jPDN3doz8mZtKvJ+298MM1ocjLSKAMnBKnekwkXLhbk58+tERbSfeZ+WTTx7QV5+MaWbsWIZRSVIl51jwNPcx6ofirdtwS3MLQxWJKWirw0OidlSU4yFRJofuQY/rhdqMa2+ca8XX6NI/uwS+a12otWOtUiL2lzJMWmxbeX038ToLNpI9rCTerzXiVSwu7+7BAtBS/sKa+asxVKIwtP3Qpx44UPMg+IrTvUUrLLBTmmKrLMH1EgkCdmx3yEhIuNrw6DSYJPFwUkqeb2/el9ewKbuThl+hrY6UsRke5iTHPNOt4TqcpNg7oviHMtIKDl1gS4G/DG8BMR9ZWS/0ws2Q7kiLLqy7UzFcexThsWnfHWyyZvYk0qT+516955H55h4lzx0Do6Cm9OKB2g/02GT4Dnv47pA+PBkZ4JvLGMts0LMc8cA0ozt5IXsd5eCl5n9fQNyK6uhc9iyilx/7yLpTc4IH+6LEG+zAxv4zQ8T9ji+A+l58g5Il5qA2MM+dIrZfIPIMMQw4CYki0PuQJMd3i3sYwdzlxG+8nvk3fucN//rvfsr9IfOPfvQz/pMfX/iDnz7xxWWlauazdwvvr6ApbiVNEc1dspBNySmSKuZ5wtsGqhyPmbo2hBRQikZsxto70lfujyNvaVmpZVe87e/WM3YiTkwMe9c/ftadbWJEoxCv5jjL41KL0fM5fiQWh87t3ZXo+BPPlpo77nor2Pul6mN8GgV0t6HEx0Xl+/P8XKSLOG2rw+VqYOu9kVQ4aEK3NiInHKsV37abreU0ZZIbkeYrqPWRQWagiSk5h7JHRzslT3EWVAeb0akt4zZ8QBS6dKQ7Vg1PJWS7rY8Fbbji7akHilCXlckzJ1GoG6ZCRVlW525KTAm2DkfNzEflo3sNZWCK1IVLM841ooT+oo9fLT0dGIVoUI9iVA3agCYN8XwKHC8Ni77uztpGvjbQrNMA78ZU4usEJ90iE57IoRESRYS1Guu60ltBzelr5eldvEklO5OOADqHlCv5lDnNJcK7moUT1K5jrh7WdASu6N2x5FTfwu3elE7Em7Wt3w5i5FQFeT+hZEkgYfN1yhN7pZyyYskROn3QeeKw77vdMbi7k3OoYuou2XOLBdEoyPvN7R436wA06S0Oe5Sq6E50FBLREAPsMkPfm4/9w8ccoILTb+Pl8+9xmxJuD/6YTUUY22HYuxknnIxuX0Ico5M8bgqzXWUSz6pKfL9BKBdQY/fiNB8dlqTb9xQX0yjRYwkQ30sU34j2HRSw8c/qcZnMJfEvfbfwv/ir3+V3fvCGdx8e+ff+k5/yX//pez67Go/XSjVnniNaZqzOw2UqhR4/qVKmsdjAxwI2U7dGuZs4HDPqid4qOnjHrXe21TjOhTfHzGEq+LuVdev0lKjeRuzX6ALt+cXfp4Zd7RQd+YAEbC9ycZb3GJs9KC8RxjuOYK5j0nnx5v6SVmpAT6NbvX2MX9O9yZEXXhPyonMVhvVmorfQ4psPTwkDq8blw0Z9qhSZ6N1Z3l/xfkY1sdVOKZkpO7230YgoddtCiZhTNA4i8cQolHm6nczeGr06dQ3/D1cZzRwhQDAjzT7EOJFqar3xZla+9dGRU0nUdaMdA2wK3X7BJS7Ffj8zTcpUooBbU2qv9B78dEmZRufjU8TX1Jdg9Z/7+NW0qSy8vxgF5WCDT2Y6uG+wdqOKhNyzh/b3gpDGxnjPodmlbNbt5tOoNvA7r3xojcd3lZqhEptAkwkhiNGtdhaCY1olcKysQK9MrdHOKynHgdtqJWm4srctCvWzzjy0Jy4+NvABEcThFVKKLthFMVWmTXhcgxaVRdjMeNyM2Rv3RHiZo9CEXiM6JL3ApLSHC5NKeB74sAq8pZTelC67MoobXelmZHEb8u0W0xKbpwbsZsAxUu5f+5nisnMTQ0oXj9iuiBEYdBcZo77sT96ovDZGRBnTCPuSZe+uBuATKbRDNjy6YSciSBQZXUfnRvIlMOUo0P78AItEsRw/047pye0CCg5sUNOCd7sSeUJ/9ZtH/s1/4Q2//Z2ZH/34S/7vv/dn/OjLiLO+1MiUaiJog6SZkgIjFQkDdBmm5qdjoQ/ByJQIcn5XbKucjlNAFSVs3pIqU55wa2x1Q/OBuwzfflVYezh4ZXlx2SG3119cIt7n5U04OslEmHbEO2+3E+Av/um2YHTG6z5Ao/3CM0GNAaP48036Yrl4ey9+qRhHaQ8f4oCrhBB2lJGmu1/MrTUg4y5sa6Nk5dUxAMDWwHu8n601kibIoayr1XDrHKaZXBJ122Br5FLIIrg6q28x6u/TsCtiDXcLkv+I7pZuHKYMzelLDec4Op/eJ7750QnpK+0astLTNIcrVT5gXXk8r9RqqCY0w+GYySnx9LjgokjWEGyUjG3G1o1WjfkvblD/W0b+HBkrV+9cwuyHAKeFY4mOy7aGaWNxRzRxbY1YQhhejZISc0pkTWytkRtcpXGtC2cLo47F4LEqG4HfLb0PYZByPGVeTVCvV9xD9iq9RTGSEek+khL3pUyfA//1FNxUgdGRxOFLeY/PEOwA01SY5vB5TQdl9c6XT4Z90ahkzOMme6rOh21lys69QC7K1gMrrvtoo6EM690CMhnLom6ODxw3ww0CiS4hDnHIeUeJGw+AqhLBfzGKBi6mmKdx9sfozF7kGF+TG3a2Q2e31IDxLO0P5W2zbwODYX/45dblxMUYtK9fGmHZF1PxEN/EBGOMvRmED7x4313LbeHCYCGMIiGMz9MbBHj7fm7YLVF8NLrff/mbE/+j3zhynO/5m3/vLf/vv/0ZP/rQqCiTDFmvZsQ7IlCmwmYVTeEaX1vD1chEOONxyvReUTo5ZU6HTO+K9cZ8KJg70zTTeovEgKwDxtqYjjNJ4bsfFa7tyrbsi70XM/X4sMEN7TfoJN5P23f5MjBt34lPz1/hRnsi3Pz3btI1Flj9htc/s0L2ryPjPb9xZAegbTtWvb93PiYJ8eEaJ0wHGRh2QC+RdBvn+7xsmFZeOTQCBjskRbQ8J6fKsAQUp5xmUlG2Lbr45k7OwCDbry08GA4l8XCXubs/cLnG9FsJJs6UQL3xMAn3p5k5GXMRDnNCU2MxJx0mDGXZYNkMZGGr8Pb9xtO1htxc4e5uZsrKeq2sDrU3TlPm/gGkO5elkZJyd/iaI//DsfDqVAgl7FACjQdbUzwEkybSpNRzZV2du8PEaQoCdmsh+0MFU6HohGZBc2aa7hGv2KVzRfiqO1hDSiy+kngY5hbhzcczOU/MOjKdthxGvD3GtITj3aPQdKFJ/HOaw7v1cJjw3JmGE808ZTRF7vt2XcjHCZphy8a2bpimyBta2vB6jZgFSQXvQe51wt1oE8WK4HN0g8cpUXJQdg4pc/OPksAJ8xhrkwYf9KX57+2/ozUcdnvjgI/O3iWqjO+FcCwsdvbEMzYmO2rwbPc2PvbjcGtaXjQoMnT5ty6I/XsZ3TE2FlDxF77cKL9EHHRcFPqiiOzwgoxiz+heuRWT54k4en+5dVQ7/rf/hSG2EP7Kx3f8pU+P5NPMn/zsK/7GH7/nHz11TJxZYsRrqaOuJBtR5kluunTxThYjpRCvJDOKCsdpZtvWSJtInbvTzPW6gHTmw4StldPdTLMe3NckYTwsnUMRvjNlzB9Yf/7E+20o7GT//p81+W77jDJewx2bHlPdfgE+X2HP753foKBYDDsaXWV/XiAGrBTv10sK1y7k2Ef8/b27YbrD6nHnqCoM39GAqTSFt7H1jkrh7uGOt09PPJ4X7i0SW49z4liGWx1h+I5FsZQUPgDJ4gIQjx5jnnLsKhx8g9U0OONTQjSRmiLrlbsjfO/TAycxfKuhZm6B505TZpoLiDBppm7w1Rcrf/r2A/Mx8endzOVaeXdecZSppDDueVp4EmetwnlzWu/czfB0CZv3tQvOxscPu2ThH//4lQX1/jTzpiy4CWk+0NzYaixojnNhSs48haP41oTrslCmzKtjvOjrNpY3JX64nIQIZWjMkjlmQSTGMjnOJGnoFAmKokpelOtifHi8cDwmXn9SmDJ0zyRJFMI04jglpASE3lejAr3GrSoJREvkX5jhrcMSG8en6lyXCA0rZrRL48NivJXGUoPHNs2xHCtF0ZQjvvoSN/dj7eQpoZOgKahhRljXSVNWKskD8/JhLhEFa+iyNXBKHVv2XVWF7Qd8l50OOk3f2YpjZCQ2v76P1Psjt3dxGkXoz7M8EjvvNB7G3YLRbqM8I5alviiqu2oqgKvbn91xzr3gjYey4SFhRYc1gt1+L4p1v5WH+BpDMx5smuhqfTztsjt07R1WXE6i4Z0wFeHLD4/87POFP3tcI7te4u/FjVyDQ60yQuNyhAGmYROp5nTrCGFRiFXmaUY147UjZPDOJ29OXC6X6OwOE90ac8nkXAiHnjAJkRQ5SN/9ZKb7kX/wiyuPTRm5pZgPnuT+Pu1VTQhC/j457OfgttQcE8BN9/s8iQRZe8AGZiNdQUc3a6OviWIdPGB2kCku5TEdDLLeAHScmzxgYL0jSCvI9GbRHbtzOsDd4Y6n97EBb8M8WzWztcZmDTfFakAl6PATFkGsk0fkvKjEuckJWgtnupwwhOvS2baFb3ysfP+bD+TeaJcNkUhG7VkxEZ4W+NlXF5Ia93eFpcIXHzaWCt/8xoGHeaRQHDpSCnMScA0jFIsJqbXGm9OR05xYW+fz88rTpqh1/vEn6vnjV2OoZqSS2SrhPWrBA2vWyMD9/Uw2Y62VXiNz/XptXMNtmOu6IgiHA6BK94ZriJWSt/AEBp4uC1+ycZgSU48YEvdGq/DBnc/PncPVSFrAKpd1w5twVOXNrEyz0krn9cev8NV4/Gxlwzm+PnJ/ymyPT+Qt8q2onSJh7eeu/Oza+farie5nPiyVt2d4LIp0IecIeptypoyUzSkrs57gcyGVwmFKWG30a43i1cfSyRyvRoIb7hQmEkHSrvZCdinDkYqghMg+rg1l0A075fYcjSIdaNfeibzsTqNSjT+xj/S+02D2ArkvAm617XmIH4qu5yK4j9qjY9oL6v7Vdgx03yIDDEpPfOx4+v532cBld+7sWMb4GO0ZyafsXdZzxxY5S8Z9Vj69Ex4OibcX5+fnjqlGt5Lje+vNmIqS+8CWcyenxJyDsqNiHA6JWqMj6UmjSaDy8HBkO1dySQzrZ77x0T3npzOaC7UFLNHXjdevj/S6hj8sinljzsZvfuPIdW38wReV6+iQbch1d+ji+bWNSSYq5cBO/blo7tCK7V2svwhshFvxE4+kUbPAQMOyMd7dxECb9PmSjD8/knL3r/F8IG7FXpSANyCSIHw0LWbUy8LdceIbb+5YkzKPzLZr7axbo5qRJJEETnMe+HHHybjD4XggZaH3HlJVc3rrmBtrH9v53Pn0oHzn43vSsrBdt2B8qKKHicNBWZeN959Hrtf3vnvPm9f3/PTzhT95955vvjlyn5xGRUwiPLE7aR4m4T3O4P1hYk7G4QA5Wai7ajQtp6Ny1K9Jm3r3tIY7zhphburKcYpsqMdlpbZGIug8T1VZG7y/NFKLA7LVoLNcW7i9WDgvUCS6pPdbdF+hrBCkGVuvI51TuTboVD6syqNVpsPGRwV8c1QL2o20NUQyXyzw1J94U2bePVbe9Uw/n/nht458bISSyDINjXwYM1ozHjfhm22jYGQSV3eUTHfjujW+3DY8ZaaslAyHkrhbGwdzfO1oGR4CNdqJoyrfuM8cknI/CSUnJo1DDE41uG6Nx9V5XI0qsPsD7LgWe4cwbn8GfvjM+3wm9e/46770iD9ut2WV46O7GG/qixFa9pFb9gdxL5HRT0bTIrc/tq+p5c8V40AJ4u8LxoINS7Q9/pjxsMZFu2ci7d+vwPDjdDztf2908rcHGwkpJsZBEt84zXzvtfKd10fOy8LbS+WpdlQTafCVVWAdNIlu/eZKP08T69qpW3yfOTslJ87XPriPU4y0bnz0MLPWgHNa66Sj8tGre67XhdNxCvy8K3VtPNw/UNcFzEn5wGWt3B0zf/nbd2ztzD/4YmHrUSy78wLn5IY9x5vQb3P47TMcAjaKl8KGTDve3rHIG7lHRsRpezUi8y9e9yRDmixDuuyRvdYJI5ZYdnJbPAa+LXQJumFKcQmZQE6ZuUzQGjll3BPXS+VQKohwfyxIUdbWWauRcxTZY8mUktmsB649+MutdTYZxj49UkiXCtd15duvMr/9a0eyR4hfquHv4QKLK1+968zHR76dD6goS1dkdg6T4L2y1Y03J+Gj+8Q0ZxCjzM7r10dAyLtkVaNz7d25Ag+njKpRm/DtNxNrC67z4cVy9Z+poH5+rjwujcfVaC325Zt17g6FjI4Oc18wxNZ86cZTEyYVkHwja/dxEyc0DD10b6Ak8qIMREMq2dpQwHjc9N0UXCEVVEOJEoXC+KbGVvxaKxMHWnM2H2YdDAlZcxKFRxHOtvBQEsUz79eNhZD7ZU+oNpxKlgDQHWGpzrU1ZHUOWTmk0Px/C8LYZWls1ehrRxX+6g/f8JdeDzPiEk7rmveNccS4bL3z2WPj//V3vuBHX3REMpmIyd6zqW4He2Bf+1KXm4Z7lBmRm5HFS6h0/4yX2Fs0mHJ73V/qvfHYuIvIze4PXuB5CC/+2tt4H3903z2PsZ69mxzdsD+Pq/vfdeu8xi8EBj4+zxnjKzsYO364+Mkv7ny4LuTXd7S+sfbG09ZoYgMjDZzfupFTFOs2ClbvPXwAZhlOQ52UClOOBdfSemCFvaBFOR4yqm3EZgiX64VPP34VvNStctQDT+frCJi0wFdbJSXhTsJg+aMH5a/8+gk350fvVi79RWEbrWrUsAH97K/Bn3utd1f9WC4pexSOjTF1tLlATFOzQBlfVzW4oDFxpSFNDQrk01Z5vIYlpe1dsFtsfGP8CLn3XCKM0BdaNy7VRhQr9N4Q6Vy3hfseqaXpRksV5mmOWCHNrEtlbQ2ZJmpqrD0uiGV7Phy1dda68f1vnvjd7x4Ra2yrYdVxAmqbJufDdeVy6dzdzxyPR754W/njzz7w5pOgRS1r4zgVvn1XmIjon5w1di8eQqKisVwTHNES+VNzRDuZxXP7SoXrdeMwz/B1Q/q++BC+gdug0ijG2qF5482cSDmMw7AoGFMakkyH7EqRwCFTirEg8nEI7qGEDjnpMNYYHYuIBOlteIi6hdQ1OUx0Dqq8mjPn3bk72huah5x0eVzDUKE3HqaJg4ajTx0b59eHmZMZXYXVnVcPM8k7tkaH9frhiAOTwOrwunokvHo4TgX/L4pFt8a1Go+LI2nDHV7Pyps3mflQYiOqRpDB42E2E05eWNcrD/NMqxeMTle/dcl55/zyPLXv4VLuiYhw+3MdDC/q3YvaFwT86IZUnr/eM29xFGNnGNHsLevzJvP2cIvESD6++nMOvLIrpgLvk5e1l9touSu3xjfh5r+EwY5PuhUU8dFju+Pah3uTYClc3l8fM90ai3VWdzTHazfNAS2s1plyCqFD2FsFnmsbd3NirRHP0Vpkrt8dFG3Cuhpt7XDMNFt5/XDH+XIlpwPr8sT58sSnn7wiT05rwtFnHs8L7kbJhdo35iKUFKZAmoT7Q+Wf+94919b58YeVlaCyZX++MPoOiezvue/wzP7u7jAPz7fuKHga9BB2rql4LD+LyBD6Bl2rW3SMOLQei87r2oJHPVCe24psHBgfYpGShRkoqiweMfE+WADpGLr32uL7u6wNTyttvMfn5cr5SmTTDc6r9oZMiepwvla23tkaJO385W8e+d7DzHe+eUC3xnrdeFqFn7/tIJXvf+fA6b4wN8gfVg5lwrZKssq3PzoyHxqZTq0RLZ4k6HLuwrr2kKinTJmm8HVOMRkkUbZlo2N0HZaCKHkSUpqYD4XT6fgX1sxfWVAvm1F7HG7zTh/v9sWc7HB3mEgC1VqMG+r0FmNk7x1NsYFPw3HKWr9tbpXIrk/xlHB/KBwn2AzWleHkXeg9bPW+9Trz5iGRrpVfwzkcC6iQaJxbJU+Fydv/n7M/a7Isu/I7sd/a0znnDj6ER2RGDkABhapCVZEqqtkku0W+tKklGSWTyUz9INOLzPQd9KRPoo8h05vM1GpjU+w2k9Rso5FSszgUCpVIAJmREeHDHc6wJz2sfa47SoViMx2WQCLcw/36PeesvdZ//Qee5ow4i62J7RAay8ipXn+OhNCRqYwus913XG2DXti8cLVRTDSL4Rzh8HHCnsGWtjGvWY10Y3v2cyESeX9OGKsY83mKDAlImf3VDue2xGXBh4CxmtI4TolhHrD+noggtZCr0Txe6mVcXTvUCz66Dur1GQNduaaVdTwul5qom13lmT5PKUrAXwWhq/xxpe3UUi980fpy5F8/1+4B8xslUy6LE3Mh47Q+uq7frxWEVsdXBkJZt/Y8d6alsRwucR3oIiZVPdCcKex7z3UQ9dMUtfrTXHWVZ6pnUGy0tdLeU3VzKjnhg6N36ldxGheGbsNmMDibCThOTXa4CwFHYdtbxBis9MzTwvE08vqTW+bziDeG8TyT84xzvmVsRYZhAzUqjotlazN/9OU18usnfvmUOCf1UCilXgrPykdeh4e6wjCXw2vlHj+vRqS98+vksZ6Aq9JqnWgka2TNelliVpvFWCtiW54bUFGbQbIhS0JFJ047ulZAe2+46S2pwM3VhqvrASOJGnfknxuurzfU/Y5aNZWUqvJeax1iKs7qRBOzOkvNm8BhmnEx8yc/uOaPPhG6qOGVOSZyEp4OZw5HB2HCdAPWGL56P3OsC79jO3IUXG/40aeGWh2d94r1mogPqngyRrAusKTE+XgiZ029WO+T3uvk5juPWKPLUWNYxomUClc7+OmPvmcEiuKez9SYWopKuER4WDL+vPB644mom5SzGognKPC+ZDVyTVV9JY3RG9+KJqG+6gO2s5wn7TYfz5GHKfE0q03XbmvUYcrBdgh4b6lpUbWQU3LwMRdOFr68HTClAJHdxvPqekfnDPE4cUTwvbDvBqiKj+RguXIWYyqhC7Dr1JB2SpyPC+MMS0toXDuyXNWsJtbm+ZkzVmBZEq4VlFwrxjqlldVKWia8hZvrAWMdc0oYv3A6J3WKqqKLmcuyhsu/XLoVeF4CtaZkVQsha+GDVWZo1rG+Wd5VVi7os9rNXIpV+yENB230T/2ZbTFy6SjbA3rZBV8WUKq5rlVxckO9mMGoCuiFAqc+d88XIxaeey9ZMeHL76NfW2rlvCRMzWw6YesCwRUOU6JahxEtwBvfSP+1YDEXP99CwtqCd45pijhr2HaeVNQR6zQuDKFj2zm6UElVOJ0S5z4Q/MTN1Y5pHNkMA2A5PE70/Ynbm57xOHFzs+XjwwO1wjAElumEobLpew6nCWMDNR3ZG8NPX/f0duEXjzNjFpKoOOKSpnkRVwjPq6Z6uQ9WDJsVctV3CCnP98nLKeU5MYJLWnHRIfDCPRV0mll9BFZn/vVA9S0DzrXPOyMMrqq72zzx/r6wxEhnA/tSOU8L3aYwOEvw3aWznueZzjh671WuXQtLqBwm7Vjf3jn+5qcOVyCVmXmCeS743vLq9RXHuFCtLrfGZYZcubvu8UPViPIUqXlWX4AI1Vjlq5eME0+tmZo1vNPfXuMCLTVZ7Q2lZB4fj/qe1YLUzM12w/Wba0odubke2JjvWVBrO7FKzS+wuSaFAx5OMx4hOMWEUtG9XaxV1UdGVVUlF50KxdB3jr6zVKvpjzmpG/o308yYlLZQimY/xah914IlLoXcF/r9lrzMzN7gHJicues9m40+ONc3e0oydEPb9l7tlCZkLbUI4zhTsif4jpwS908LJWU2RtVbj+fMYU6cquEwJVZb3tI6xalUXNM6p2LUEOZ5h8CcCiZY9vvQlGWZvutwXljmiKka72DsRGcdyKzj9xrTXbkE7q2lZh3lVm0SzQR6rW/rlvaCw72oxJe1zvo1bZRfIzbEgCm0xvYye2vne/kez0WW9vPy+oCz1th1nK+XOnDBTdcD+fJ4tnF//dvyXETXBVa5dMONBYDeE8kYgjW82QpiA4VZ7RatV/NqZ3R8TbkV8YKxym2kFPpgyEm9PkPnsbMyK5al8vg44d9s2PWBN2L58DRxfx7Z7zbElBm6QKWwGQKPMXI6ztzse/a7DV0vTNOZaUlsrgYkLioXshrNMY0F7zvSOLNzjh9eWbzt+cV95JRATG2y5Np8C9wKrFzGeH3vV1ZFu6bPeMzlvX2+Lyq1Phdk3UNp11dowJHQKFf6NeripQDfM28LrGlG143al1uzVMWoV/I8MyWgJLpYef8Y6Zi56S3eZeaYmFOiE8toFj7WglQd96d5ZrMx/K0vB3544+mDkMbCdIZ3H0aiFH786pbxEHl3OPKDtzvdsUjHD19llnlhORuym9AMN1pHaog5UUrG+4BYq8GAxjCeFuZpZmO8Rvi0has3FusC1jg2feGzT2642hrSVBnPkUDh8PQ9M6XWC6Yjp55ypnUhVWAsle/GhVeDYRcsHgNx1aqrc7Z1OoKJEcZSmJbI05K5nyL3p8y8VE04XTvhKm0fIYhYai2c5sRp9Lx7emLo4KZzXFnhbqf0Bo4zeTEUK1AsNc+kYonW8nBKTIsjlcLWw95DTZk8FWYR/u39iE+eP+g9U56YF9X1N+YXscJC49S1bqtcyoshJrVqM1Vf/2mqTJLAekLfYS2MS+Hw7kycEuM5kqvjeIrUpFvrbLRom/YAPKO06wj93G3ow/SsSFqxxvpi7F4XG5e/uRbTNihe5KAN+jSXwr0S7dvY2WIv10KNoAwJ0cPxuUvWTkNePLiXhvtFR1oaQ0Eol0q8Pv61/V6lfTuMuRRSfdA1TsdYw9Za7rZW0yhbsXHWqne4taSmtLvAHhYcFsmagOlFPXtt19G5tRO0nOaMfJwJbyzbQTGzdx8m5qlwKJE3d3tqmem8o1s852nk6djx6Sc3GBv55M0N794fSKXiQoCccL6jM4H3j48YY1hKBmMIXvji2jEEy9f3Cx/OhlyyevYWXbysXcyLQaVdo2d9mtBkzO135cXQ8VKY8XyI6bct0KavZ/w1v7jRVvioNOaBNc+YtxpYNw9WBZRYUiWLuUSrV6peQ2MwTvOaSPpe5wJjgnlKPE2Jfsj8pz+544dXFutVcZlJpFI5L5Ht9YBxnmUZcQ46VyCrJ6n3BjGdclVrVaGEqJRYnHK8jVWH/5yUxuicJ3SBbuPxwTQzIUFEseTxPBO84cvX19xslVN7Op1wzio8Zn972fzrO9R6uaaXi7OOcvoJyxTh4JSPVkqTeooaMVcq55hYqsVitSNqN8TjWDgsmZy175K6brOlLUTUiaY26OC0aGxxj6MvqmUXA30XqHMhZKHmTExRzREqnK3w1UPkl48LnRX+8O2Gz7eOeDoQwoBL8HnnmEoiiXoSxJSUp2iEXhx5XhiT4nqWgjXPnXvKiaUWYlWnq1rh8bSwKzMxB3JRl5yvfvXEhw8zVxvddL6/f+BwpslilYhdRP0QYq2UEhv5WQ+wteQIcjHSQHQTu2r1S32mXmmlanQl3TbBxV6vGYQL1Ja0sOqi16f3wiLIQJtGam0PCFa7HIQLPiDP3dGq3FllkyLrUM+LLovLjXUx7biU4IYBllVZtJYNNZh2Al0LTUvLchnnwLRMMMWkTdFkgGy0m159XwuFrvfE04JBif7WW5YMywQPYyY8zIRXgf3G4V1HLsI4jkyjZ79zlFjY9IEYJw6nkf3YcXc74L3j6XBkXmaGbU+aR4KrpBrx3pCLELrAeJ7YbrYcTiOv946+69l9mPjmUDikwkhjzZTn6WMti/qe1stIpEfBC08AefncroqrFbbTeGtd7TWf1fU6XK5Ju17rn7erYoy+nri+Nv3uekhkfUYTVdkqRuitgZwpxRBj1Ky4JCyxMOfM05IZp8wPXjv+4x/f8MVdj0ik1I6PHyaWcWKeFYbY9J4SI8FUPn+95WbXtZ9byCVd7jEjyiaqTamVc1ZJuoFu02FaBl1OcP/xDFYYOocBgivsto79TcfdVuiDo3eJZVRKV2d1sUwzqP9tH39tQS0vi9zl7V2fO2ElXp8WYRuEzhu6Whi8ZWMa6J2VTjJHoRqDbwU5lxdAO9IeypWjqB1MNc+pnUXAZnjjPDsLc9VUxb0FJ2qc4KxS7mbUdmuujmOaOS2J6A1pSVjTUUOHkcLOG373k57HU2Q+6eYfqfgQ6DvPOMemLjE4Kp1TVyM/ae+XMJyLMOVK196d01x4Kok5JqZxYpoq56Mmeoox9Jsed06aQW9pp/H6sEjLB2qdZ16tMdY/K5drIWshkmf54Gpe/XwwrZfsJffz+eFcH5cVf/1N0Hb92pfwQW6v8/lrlBC+FuTmnNSI+mtnub7m1an+r+peV77qhTf74j4TtECrZ59h3ysX+piTGkojiFU3MqW+aNdl2jKvpqIZQsBCYd/1dDFTSXQhMMVE8JZSVGa4FHg6jTjXsek7jAinXBiPJ3abLRUYBk+pG47HM/NUqDh8l7je9zzeT8ROs75C70lU3ry64tv3H9XLwTqcg2HjeXg6Y6vn2gP7nu0C98vMcSnE1XimHZpSTXtvy0vwtH2O5iqm98rzQ/z83Oqor1RCWTnOrQi3YeQC9ej30OPRSsZaIVIoUYtULbZlL6l50uCsms+0Be4QLBIcthpizORYOC+VKRbmceExGt7s4H/6t+744XWAVBmT4au/eOKb7w58+faalCa64Oh7R62Fvu+4ygqRLWnN3jJ4p2nJORVNajCWXESlqsZT4ogmjBVy0ud51wfEwbY33F55rjaOYIW4JE4xEYxhmiaW+QK8kEsFUdn7b/v46zvUF//+0p0GFOheH9ySVcLZ7QacSew7w3WwpFxVZYDwuBROUTeTRtbt83MQXK2/+XO9N7zeWs5jIlU1FTYFXFbDlWgsUSzRes6xIKIRz9MYSUVIaWEyhX2xfGYrW+fZGqHERFoyc6nY4AhUrnrPN6cj1TgMhjjNDFvPRgzeFLZeKWDBGryFRjAAgTkvYCveGYRMrvB4zIyL49p4gstsh8C8jMSlcHqauN72dD387MNIqpmCdqL14n7xvLPXEanpvauszKVLl7wWLaBFgHAZw82LYmhWZdWLu+HlFb10PutCi0upvRS/S5dXf/Mb5NYRV2hUJz2Y9HuuVX09MLngwWuxv/zr892lXgf1+QEX0S7JkrkdAk4KsRosFlu1kDpnUG9UpSrVhhNXMQ2CMqRYkEE7kCVmjAvkXFhyxhl1e+87nbbu72fKteVm13F9s2EcT8zjwvXNnpQjfec4PFZOh4nxuudqF9huBp4OkWlJ7IdAyZq0GjY9Hx6Dbvx7T86F62EDS+U0RkxviXEEBN9Zdt5xf0o8xeb+L+p9qrBQg2zgBST3fDpdrvAFemnMm/V+unSlOlFkyiWC53K15FJrsVhMMdSoFCKR5kXRYATvhRvrlUGQ1PdiMJBFsX7vAmlJPB1HnpaEpTA44advd3y271jmCZaOh6eFbz6cCYNnGDyGTK6RFNV8KTcuvDcNeMv6T2p0J7E6taScGwyWoKrrvgRD12t0EimzGHUbe33j2TYv2zhHzscz1nl9D6wj1gXve8ZpBizOB4z/nuYoUtdOtL64eC+3w/Vy055jwkyLal2DpVjNxjE141qHRy0kNPaAZg6x5of/5U7GVHWqyU2PvrGFfh9YTOVAJVZLWgofP8ycY8aJ4/NXA4+HEVssOUEtEZcrV8Uw5AhRu2ZrPWPKpDb2xWopVb1ah8Hj+0B/5TgcMzcbT4/HGfBWsabBe+zB8MnNwFW5Rt6PODMBVTG8IpxOmfraEnr4wQ+uub3rGM+Rmg1iLctR6SBrB9EsTWnpJpSyspekFZyXneLzvwgrzrjiqi/sSOR5g2sa6N56uOcnpl3ntaitJsZ6JpdnY5X6LCNdRbAXvqI+nZeHF7Rwv8ywquuyqjWaYkT9VMtzIWD9jqId6joh1arquliFzgmvdwFaYRGjIYVqdO7JSRNdc5NmIJZa1X0/eKP4+aILplJGqihX2le15Ks1s914XFLi/of7meMp8sWnG673O3JRYjhiqKWyGXrO55Hx1HF1Fei2Fj8YToeJod+xxIWtH7Be+dgueMpciEtkrhOvb3cYeWJcMp/e7bg/zLw/Tgw10G8H4uHM+ylqCGStzbW/2VO2eOr1gv9Gt//iMFzvkUv0ym+cXqJLZx3gLwedgiNyWdbkklmy0LWlzpIzKWtxHURwZDVCMeoTG7qOxXuNNVky0xiJUYMzd35gGxKf7yxlXsgZPrx/5OmcmVLikzc7vIPihL4zOmrXSsoJ44RqSlNY6+vNpZCXjIhChKELYBSrXZbEkmZ6a7kKPXe3PRbhfNBle+8gTjOrpaYf+pYdZXFG2FvP+TgholOOCZWSl99aM/96g+lW4NYT7eLaXlYYYMU6lU50WjLBwDkmgii5vpbmXPNytESxMCtKy6I9RC/14XMR3j2ecVLZd5br3tDZQrCO09NMTAK951fjyCkr6f/Nqx5n4Dyquio4i3jNsHkylum0cHU7sLeVMFhC9RxO58ZTrQydx/eG3W3H0zKpZ0ALG+iM8mHVXdxSa8XmTCAjRWMh7RFuCQAA9oJJREFU9LUXxBZKWljGkc3tgB88N7cbliUTs+Orr98Dld12g5NZWRGt8IlIi5nhkpOe2ydVjroaLz/jklLb7X857J47w5UGY8Q+5wu1sXoth5exuj2SzxDAizIuAvV5KtEKvBLRnw/c9S+snq68+POLbwD6nv7mIVGf/9u0hUtjPVD19XsjfHYFdztLSRFTCkbUcqTIcwE2xlKzHujiDDUL07Iw9INuF3PBieKwc66sDUdFhQAk2PSO/nrLr7878qdfPyHO8OXNht3WkpeRzeaGQz7TbTrOx8w4RZal0vcdm97xdD9yHmdKX/FZZZq7jeOQC+IdJRVO54UQPDc3W+r9E301LBamPvBwinjgD+567EPlV0+ZVCypRKJkrHfK4WyLpuckzhW6qS1Fo/1Ze5fLZcp/nkRq5dlPor3fshp916JmMs6y5HihQuZSmZdK33m8aGLHkhPLNNPVSk2JWg2P50kt8opOCqZWpnHixgApsRRNvig1MfQ9b28z11uNnJkWNVsyBkrT3os12Kry1JQz1YL1rjVnhTwnbUaK0uh2XrgaOl7f7bjqVXZaUsaUTD/0CgNU0cDA2pJua8J6UY9cEziPehAYhPNhYlm+rznK5TKs09lLTPW5s/yNyybqa1iqurEbZ4ix0BvLLIpnBTEYH4hpYVXWyMUt/Nld6TgX+mCJ2tAyGCV1W6OJit5Z9sGpJC0L50V1xc5muo2wufJ8GBd+fb9grGNbkpLqQ2VMEWcD1ntsPxCmTK3CXCJ2UfOSJRnGJbLUSFwq1pbW7czsc+H+cWS7U/zp4jVaC9SE7wwuOKwzLT4FcsoYE7QDqAXnXMu5yr9xc6sZtVzGuMs012z6ijxfD3gufqz45Ys/u+zJ24ZjNcxYu9ELJrsWt8uI3jxQ1+/wl3HQl93nSxwXLkuC5wNSX49pxV8PDU13sIa/9PPXG+4ZMjDNau+mt7zdWjobiHHGukIlqyzUGPUxK1mpL4BrrIFUMlNW/G7je47niaH3dMFBgpLU+clNmWHokSJsdx33j5kxC8PGc5gif/rVPV98uuHz1z15iThTlbPaGcZx5vHhiddvrtn2HUNYeHwasW7LMmecnfjis1f8+fwt3lkeniIJx9ff3PP553d0fUda4NVuw7QcWSxUKQwm8rc+22LriV8fIVtdAsVUsFaxxdK8G/S9flZbyYtruXawRk8nLlaIKN68fqX6ED9fN4AueLZDwGSYYiaVSqxCNIaSImWuGOOYc6YuhataOR4jhzjxcVw4L1qsAEpKFCn8cLdlHCNPh1mj2b1l13m2+w3WCEsq4AUTLGLUdEicw1rbhB8GqUkNj3JmNdHunbDdBLaDwwssU0SMEEpiOiWN7C4Z51Vebp3Ge/duYBoT0zRhvYYfivGcxkTMmb237LuAlMT5eP6ttfLfgaE+E6vXxw7RjfJKo1nD5arRcdi0v5OrRktbMcQmlQwIx5iZ0yqDbCNpw9TWArI+lAUVFyy5kDL43tL3HqlaaDGK19QkjGTmOXHdez7de/rBUjzUfsMvnjLX3nHtA3HJpMFTq+FwzqrtTjOpFE7zhHeOOkdmBLFqUktWBVfNerP6oumtD6eIDW1UrxnQbXLoAmIDuQjn00zJKp+dzwvjPEEuHA8nHp/OuoTSN1IL4NqdshKyW077+h4pCr0+MZdl0nOhe3kF5cUfNLWXaTdj62hXByiNfjLP11xal1nR4k5jY8jaia93yIvvvY70L17Ec2LAi4f0Ai9wkeyvL75UNUw2ot6e0u4v74RtJ2zaCDhNWkSLQCxZY2raMquW2nKRKs45yqxc1cMY6W72xPHMcYq8HjwdllkKYnUpcb3t2brC9X7Ln3/1DUWMdqudZ0mRn389cjwnfvylYbsx7DYdVMvp6ZFxHBnnHh8cPhjK2fD0tCBFu6zOLLy53TKdEl+nhLGeaap8/esHfvLFNQ/jgZurPYUNfDhRrSeYzO2rQKpw+otHzlWTF2LSDboxQlN/t+27/t4a610u12Plr673zvM+SuObqbV9j3bbrA0TovZ2KapNn7d4Z5hMJQlEMqHr8NYRolBNByKcU+KxRk5LZi4FNYSpLNkwWCimsMyGD9+NXO8Mu8Grs9McKUavZQgO55yKRijEeSSi1DlED8HgDaE3mj01J4IP7HcdwcJ4npjnGeccscXDVzRUk1oRB9brYRKnzPEwY6wnBI8Ex3GOHE+zciIG5YlbY+jC9/RDLS8emfX51KehsvIgjRi1qmv2XN45YtYE0s6AFY2ByFUuqYeK5RisCNk04v9fGlWBi8FyKpU5FaYkmJgJRghGqcmzVHYdvO46Xu88Gy+4VBhQntzOOX646ahjxAq8Gy0HK+xNhSrMSybVRFoqaRG8seQsxJKJuRCrmuoa9PWqk3nr4LxFTKKKUqAAxFiWxfCrbx6RKnQd1HTik9dXbLcDT8cDmEBuMjwjmqAqLbeqladWB81zN8nL5vEvFaz1Y8W6L598PrBahVQddZujfzMGo/LSadoZg7Oacz40I45TKcTUEqjWZSLPmKdFmqXpM4Pj+bquY+YqmW0E8xeGH6ZF5kBzQ2odsjGV4Cydd1hbqTWzpEx1gdVDtVCaSbEGvRkr1KiHT06CMZZxnjlNM8EH7o8j+ytP8OoLkKoh9I4QCq+uN6RFyUu6qHIMzoJbiKnycKr8y7/4wO9/+Ya7nXCzd5TUM54z43Hm9vaW4B1D3/Hx/glvtmx6r/6fRri77bm73vDwFLm9Hfjm/ZlpSrx6tSOlyJefbRBZOJ6FvnN8edvzye0Vj4eJf/FdxFshpUopCWOc8kIvePGq9G3Lqr/8ULfDrLBeO1GnKdbJhufiW8EazXGrVfPovVe3rOAs11tP13turgcG35NjZj4P2F8I260ne4cNwpTU2jNVCFkYJBKCNiUfP87YYuikYEtl5zuwlpQS52UhLRMVjSXqhx6xSsmUUigxqaG7AWrEdk4nn0ljlaYlYowDCVQrLNOi+XjiOR1PWJcRpx4QFUOsQu/BUJjOI9Z2BN8R5xHvOuZpolZht/ue9n1/1ceKuV0QsMtGuhKcmkMvS2HKha1pzlJOkAIl18vfXjuaC/JTNfjv+eR8vripwikJUza4CILTKONSEWe4ubJ8eu3ZOeFwzjyeZtx2SyqVw/GMxEq0ltlavjlm0v2BP7r2vOkspIJxitGlvJCN5bREIlpfbNUHTnFTGLNavBmB697S+dUKRH+TmDNpgXGC8zhT8ZyOZw7Hme2wod92Ci1EKLS0AZQKYwBWkwtEgfD2Hl+yol64r7+Ujr5A0J4/LgVWoRKKAvi1Nk/M9eBqj5NpRdyJwZmKk8xVMPzB9YbeCz9/nPh2LBxjudDeiqjayiF6cP62+0bWjCNdJhmjHgnJKARh2pNuLuOotO5KOc7WaMfceaHWpJ6ZTqWqypXXDro2OMUYQYxS8OIy47pASpbjOPPqaiDFhfv7mS8+3bEJntNcGTrDzcZxu+94Op2akUpmvzEan5INm65wPI8YPP/2zz5yftvz2ecdd7dXvP/2kekc4bay3TrmZabvAvf3E/tt4OpK01J7J/z0J3f8v/7pr9juN1xNM/cfz1z/4A5XEttg+PEPbvjm25EwWK6ue6Y48Xf+4JZvju/5diyN12p0iWmeR/b14LoAOWad7Feq/rOI4uJb+5emhJeMLKEg5nKHqCqwQEyJkiO7bkMwUHPS57vp9sW0CBSvC7R5jpqe4S2DUSe4kitLiaQ8MB2F83JgvwuEXl26BoRihdyC91Z2i6uGYIQSLLUk4jkqpCbaeGW0SFrrtftMReNVvMcgjNOZvvdYL1TryIji3a6yv+0JInAcGeeZUGF30+O84G2gFBUi/baPf2dBfbnoaJdIVRMvOsp1FNW4Ec/DnBhLxhqHMS0JNGcFtNFiYdopaI20m6JtqY1gURL0SiFWIUCir4LZek45qQuMgpf0aHaQlMpgQXaeqSbGMTIvidlaPkyZ2JyJrrxha9W6TDJIKdSsmKx4rfIOCyWqo7i3GKrGYZuWQiqaq9V5TSIQpws6EcOu9/zw7Ya3r7d0naPcWZaYOB4Tu6srvn134jQmTTQohYLVB9Y8j9yl1uYzmp5hEK2SrHHW+hRpgNrLjzWSWOGCF1LF9r9qEiLP308056qzlauN5/UgvN1arnvhrnf8jddbPnsd+Gd/8cT/+V/es1QYk2lUrIaNirnEmaiXwMsS37Bas8IWzw+vEx1Vc1Fl3fr9DApPOKvwRM6JnC19CFjRiJlKRqpth5D+vFKMeoCKEoJqbXr1BguczxEpZ97e7nl3f6DrA7f7DhOPDNbx6mpD1xk4tUXHkgl2oNSFpzHy6lXHELVDTB5+/otH5nnHj39oef16y+PjI4/3D2w2Ox4ezmwHz8dT5vFx4tVtx+ef3iD1zFUVvvl8x1e/fuR3Ptvz8GHmw/0DP/zBLVYS++1AWVVF58h5nvm9t1f88RczDz97IhaDsZZUNY5ZVghtfa/rSvZ/cdi+3PDX35g92/ApTcPeDrb2vYxtcetLVvinmbkcz5GrkBgPE1Ms5GKQnPmkVJ4OkQe3cFwqx7lQsfS2YpwQbSJmoViHdx6Kvp9zhPg08YpBTbFLRpwW10Jh5YplKmcqsSSMlRawqZaNtQrLkjBVSFPS0E7bFFPGUFLzFfGCVMM0LRSx5JTpeo+kTIyJkhIpF5wPeKOwSD84XHCXJf1f9fHv3aG+/LgU1YZdkSs5asjJYclMnWHnBJMTGYjUhof8pqTQWViKorG2chmtFVelbeEKi7eMSQHosVTIQgDe9B3f3B/ZtyjfuBSmWAjZ8KbrsD4zdJVgPa44ejF4scyTJlk6bzgvWdkKc6QfHFQDy6iR1EVJ2k4UC+2zFiNfIYgunXK7bTsn3PSOT1716vjthW4YMAhdiGw2AXHNt6BWMLX5tq4dWcM0aYimPGOqF7ildSG0QnYpp5XGDKjPVLf1gZGVB6rb1owaKDupdLZyPRh+58bxxc7zJsDbXeD1lWMfLF+8vub6Bm5vBoq3/KM/e+TnD2pmk6t2vJGsE4M+lzw7ATw/sRfVlyjmLq3Lzawd6Sq7bDxUs/pDZFxLOOj71VRDLu+BdZZac9NtV5Zc6axRc+AcEecoCYw1TIvCPPt9Ydhv+PBwZru50lC36thuAwbNnUccSyOS4yyHObFJjt224zxOOFfwXc/D/cifx8yXP7hhf3PFEif62ivXdZ7Z7zueng4cDh3nq4lP7hw1LfzJH99wfoh4Kp99suUvvnnk649H/ujHW3ojvL7b8vH9E7FqQNwnd5afvO342TvLVx8jMxmNkmnKJ547TurK936xSK5cOKurOc4zZ7Ud0pQLy8LwHGFec4GiES25VGKGORuWaKFUprmyACUV7ooW0adUGZNKSTvnMdKMo3PlHOF9nHG1KBZ61VOtIYkuvJxxpCIaA59ht+0xaEx8qjDlDN5hikMqeLHE0gQenVIUc6k4H3DeYqxCQS54Ol/pe40vP44zS0r0g2e3CYg3CAZXLfugVn5xjlztBmqMLHFq3rh/9cdfT5tqDzA8Lz+epYLP3WtpD2uqSpDOpVCK8N24ULBENDE0l2YyUpVGpYtoDdqTstJ7pC0XKslAKJZqElYC5EzOhiRK0jTWEcKqjgIbBTt4vh0jZYlcG0tfMp92Dr8xhJSZsXy3ZMaU6VrxnmMkVYMJliKqwDJqDIk1lpgrzjucK7oRrPo5hRxtYwToGOCMYKqqMpz39L2h7y0Yx5t+w9M48nQcqaWw2fQYc9DTVzSe4lL44Fn9snYTjRGxFsraRuh1Mbh6Ma/uQg3pbq+1OeLX0mz4NAzxtoc/fLvhDz/d8moQvCx4BFeEMeqD9e3DgZgdr6+3/O/+R5/zv/7bb/nFd0f+2dcz/+jfPPLfvTvxmDLFWPU0eEESp/3/SqWIOhOY2ozGZX2Fa7etC7D19y41Xyz3ajVQmlKnE6ot5OIwki9CkTUxNC2JzcbhROEbTCUvymNdsv6sX3135A+/2FKt5/5w4NXVBtMsKDGK4XXOqpN8Luy8JZXM+Vi4uS283gdSiogplCDEKfPLr5+4+3TDzd4Tzyf2+w2PTyPBJ6L1fP2LBzaDoQ873twGXjPzH/wPXvH//VffcPum58PJ8/Nfnfjy7ZarW4uzjvFo6GzP+aQ0rE9uOn70uuPjMTEvWiA7J8x5zUF9AQFdsHPT5oTnJubC+X5xn5UmUV4FGOsE6axQUqEahXQyKsVeCjzMM7aq50UqGl+i2Lo0zqxOpTFHpToVzaErGWJrCE7V8H6K9FZw1fB4XlShVgGjnWeuLdbaWl0gVg3ZPI4L05LpncXZhPcQ/EDOGe+F7UY7yqVm5qiQRBD1Rk21sOkcobOM08LjdzP7q4C3OpEKla63OOfoOou1esCXJX3PgiovxrP2sOsmtlz+d10yVRHmWPBWX0pCOKTMgFOQ2WRWXpttGUFitdU2IjixFyrHyldUsq3yYFPRXOxSwDoLVog5QzV4Z8kVDnPl4/szv7o/cxUc+8FqAF/7ukkc9wXuTxNbrwYJReBpXJirpTqwncFhGJ8WSrakVDTeesktJVO18BU4xczT08jHc6Lb8HwjFyEtkGKFUKFmTqcRYzrmOTFOhSU2COGCfgFoN1xWxZSsWPLzVaj1N52fVs/LdeCj9RYr5Wh9iKzo4+ZE46e9qbzZOv7k857ff9Nxt/N0RpiTYk7TknkaI5WKfV8ZjGHfnfj8LvD2Vc8nfeV/8vt7/sO3G/7Jv/7IP/n1I3/2UDhNEI0a5LRg7AvGvp7PymRoZPPyDEdU2i9nVnWY/oK56uKkop6kyPN7oFv9osu2LJTcFo29uoHNc6IWjQg/PI1YUa/Uh0PkF+8nfvx5z/lsyHXih3cb5mlmGAamedIIZaNKwGAt+01POk/UQdhee8I2cDqPeh9nkAIP70fiaPn0bkdwhZt94MN3R/o+MI0L33470nc9m0F4de3xxnE6bHn3fuKHn93y7vCOP/vFiU/vrjBpZL/vmQt8+cUdT4cz+82GL15t+bN3C9/NM6kWutBh20F5WfrJc2ldMdO65lQhL9gV6z7keSmlCy59vtWFQ79Tbt1rFbl8DfIiWLDtRfR/W/ZU1Qj21K5naRNWLhHfe0ypxFw4x8qyZIJzxCyMy4S3z+nA4xiZ55ngLWJyo8PpDmXoPFtvsM1NSkQI3nBz3eEcpCSUc9aDQCrWBWJSToM1ehA4D53v2ew6fHDUktSzIrecuLhoA2Ud1n/PpdTlRq8vtskX+sW6XtIHWqogRU8e0045a62mglrRkLx2CfOah9Oup7QXkteL3HBZaacSRqjOkWpliTAYR8ya8DfOieNpwkQwCYy1fHa1IYiQY2RxjmyVV/o0R+Y5c9d1DINjjpmYhX6zU3vAkqhR6VdxUpX6XBOT6GloRQUJHu2QZdtzjk9kQUUEon6bdbFMUyacZ4IXfOfZX10BjqXOVA4qhDjPmkWFee4mLhs7LnEoz9v4FR9rN3Mb3f4SV+ry9asLkSasGlZD/o2tfLmz/P5nPT98Zbjb2yaIWDicR+aYMdWSi2GOhRQzHmHbVd6fC7/8MLMfHNdXwu3O87/42zf8nQ+Bf/Ox8M+/G/nZu8ivTwuPU2Ju5iqg5hrm5YFc1AP0Zcz1KqmttV7I5asZhzWK9a6dU6xV6XooxFBEu6wiwtzG0pwAsZwnDXTz3qm23xh+/bDw6e1AP1i++vrEm6uOtCRS80M9nidonqq1ZG43wt3dDbddYZzP+LDl9dWO0/kEXtMDUhUOh0RJmU8/2XF9vWWaCqfjyDB0PN3PvB+OGNkSXOD6qvLFZzueHt9zHkd+8vaG77478PDxzOtry7ZzmCUzLokyL2yGHW9uHG9f9fzF48SUwCwFazQK+TK412dvhEprfmo7tNZ7gyZlbo1MaYec8Myw0WWfTpPGWL3vJWEENVtHO/f11l1zuxBpuVnKgIlN+afOVIlSHTEmMGpr2HkhGI+p0sxuKtOSyWIpJeOcIQwD1tLi47UZC1bY9YKXgjhLCYaSM8PGUkskLUC1WCK2ZrpgCb6lGfiOaNRA2+dm0JQSxVSMUx75+XTW5dSwo9RCmmbi9+1QnYWY2oPeih/rhWpN05qmqPvA0oyMK4MTtr3DoumFsFJmtGiai8qm4TVmvdAAVSOkMiCVWjQmxXcCJEiCyZl+4xk2ltB50pjIMTFguHFCkgheOKXCOFWsF3pnuOssgnC/FD7OhSoZsZlxTvjeswkdh2OkZnUV3229Rg4bQ3AGZyvD7LEPQj9YsrXcbjyD07KRWmhbNQbfB3xQM4UUM857Yk6M06Sm1NZj2wJGJ/Pn4riOXC87uwqXzm3tXOtafFbqUbs4InKhSxWauq1WOoEfXAX+5HPPH/xg4HavdJ7jeeI4J7IIru8Z58JhWhjPhbSooeCQKr8+6I1ujWEbHrkePD94NfBqL/zJJzv+x3/8CV+/O/Hf/OKR//ZXZ/7NQ+F+jCylktpBadv7maiXe+DCqTWC0adGl4PWYGrDNGvFXuSzeueUKEgSas2NC5w0m6vCkhLGaSrtNEWVppIxNjNYy9Nc+cW7iT/5vQ0Yx59/M/H6uofxzHazQepMijPzJPjrHZ/tKm+2gqvgTMf5NHG133G933I8nBHRJZJgmRN88/7A61q1UzJwOMx0vefduwPOOaRGfvTDPa8+veLN/cKf/fwj1/tb7E3P4bsH7navsK7QOcuH0yPbbU88P3HT93x+5XmzD4yPMylHOuvbNX8BxV0Gm7UJep6Gns/gFdJ7+eQ39q+oj7HRrFRSzdRULud5Lpk5GXLzwlU4r9WFWpqJy8roaPxmo8tL2wxivFV/gFIqY1ywGbYbzxAcKWd8v0Fybhr9Cec9w3ZQqlgxbAelStacmeaCiVosoeBCrymtuRCk4oouMoMT+t5inTDseko1PD2eLveIFEjjTGpL0f3VVvX7YrBWIaHfWjN/62eA3uv2LTc3dsuaYvlMFDYrHtMKrRHVvDsRTMnMS24kZEfJ9TlobF2sQMtbquu1bMWlRYE0SV2ulW3v2ftCqDDPFcmJugiHxzNJPNVnrj1saqb4wGlOTCkzUnm1HXgVDHVOfJiFjzFxnBa6zuOMpZSEGEscI8SqIWoNxA/eN/qW6p1BvRfPxxHvoANy1BjpJUX6bEkxMU5CP3h835FTYR7PPNyfmBZ98DrvCU4gNk8paUsBNZ3U99a2Tq5c0Mb2TJgX1Kn1sXhBf1m5Z3AhRg8OPt8F/oc/GvjbP+nZ95Zpgam5e203HblW7k8zp3lhSoWIkKQZo9TCEjPHOSLieJgr708z3zyNbAN8uZ/46enEzXbgP/n9K/7+T+746rsT/+LXT/zLjwu/eEqcY6YYp11rM1NWfFWhANX2NwPtWrAI3goxZsDhLEgzP2mWmEoly1lrcBVV4tUCRVNNl2Vm0weWAjEvbPoA1fAwLrw7FubzwOevOj6eNbcIWQhi+exuQ3CF7Va4HSZ+8sUWGyPjnDicCjZmTo+P9LsNm2HgPI54gRIjMUcwHd99eOLudsv2KmB8YTwlcgrcf3zC+Rv+9c+e+PyTGz797IopJb59f+DTuysODw+8f3fkyy+vmaaFznuVRcbE/spyc+X47Npzngvvz5FSC85ZYlp1+c+N0EsJsPwlTH59CteCuD6CesdUjFSc0+jzNCeWpFNFobJQOMeF4NzFl0FWStGL+qBWlNoVp6Iua840Q6FSOJ5GYrB4B/vgtX4AIQSMVMQs7DYG4wZNhPCFlKuGb1adBqxo/M2SK+fTTEVTjYNbVXxCCJ6UlXJmKJATcSpYE+iMxW82hE6fOYvHWkvNhXGcWnosKiZy39MPNbQsINO6pHI5fXQpawS8QYPrRAje4kSaH+HKc1tHjufvqxPGChAWaKF9z9CCXJjHSglSNxxn9Wetuv80R5zpiAjnvOiixwdqUHOUcdYHdbdxOJPIOB5z4mMWYknqeeqa+qkaylJYqM9UpJIZx8I45bbIUcJ4nEeuc+HwNLPdRuWStoPFO0+QQOgCh+OJ8zTx5eefsukdj49PfPfdxDQrNrnEub1PrLuD9o/itNY28afVKF891KRVz5XzKZftfn3h5H8xnxZ9OIIRPt9b/t6Pt/y9n15zNaiNWlpUijfsAnkR7h+PzPPSrM8KnYVO5OJ+HosyNawTnAVnVRWTo+Pr4xn3Afy7M1T47NWGH991/N7bN/yDx8g///WJf/r1ka9PhTFFslTKyj1u6iaDTjjeGoyxlKK2b7bhqmKybuxF/46xlWoqJWfFUTGcxsRup/dIXCaGvleZa4rcbAcwjqfzhBPojeM4Ju6uDdMC7+8X9p97lnHCm8r11rHfFn7nU8cPrkMrlobxZHh8Ej48Fe7PJ7LvCH2HLAkhkyUT55GUPfcfT9y82rDZBoJzjMdILXA8nCDv+OWvH/nyi2t+93df88nrI6enjImeZZmYDoNGkXvDw8MZg8PmSlcq+8Hzeqicl8JUCtY5lpoupfTS/bdiuh5i+gfr06iyb1Anq2cgtGJroRfB5sgmWO3KY8Ka3Ng5miybW2RFreDFQV1DAEzzVGh1oEGtgzX0VgCdQopo+mywjlgKY4KUBSMZaxaGTthvlY1Ta2aaFupcQXTHIa5SJZPiTMZgncKNOt6rECDGBWMTQ+ex3mCD4JwDMeRYyQ8nljkBHrEZcdL40hbrLL5zeuhUXfB+r4LqjL6YLNqd0IxOrOjnglOOoMNgjWCc/qAlFnxwegrZhmvlhrOIvtCK4jSgoL9KTXPDY5XOM/QWbyxzSW0B5VQBA1TnWOZEmQt7b+idYYzw7XFW6osFs1eqRipwyJVRDDYEbkzhCagddM5ymjJTzexQl3WLENuJXxftyp1tlJ7c7OQQXMtm8s5QjAE0N1d0jc3V1TVxXhjHCe8GwrDFd4WuX+jPmfGscdmmwf5GqvLpRJVKpnXxtRVXams8W4eRWRdW2t3W9iw805eEYtSh/9Mh8B/9pOc/+ptXDAGmOXKaFfO7utoSU+bdxyPzknHWkwoMzgMasxwLzEtUOKdzqq83hb5zxAjWWrKrPCwZSZlvPoz84iHx9oPjyzcbhuD46eseK8L2u5Fvz4X358y5wJwLudiLasq1CUWdtxxGdHmgmHGj8xiotZCywVhPrgspqdHFu8cR2++p4lp30zGdznT7LXMqHMaJXCqd065oTpmUYD84Hp4m4qcb+p3n3f1HaoUvbyxf3DqCTdghkEsleEHjlPX6HU+Tpp32+ruKKcRYOM+F85Ni5a8/2bEdHOHaMI0LuQrjMoLzfPPdE5++3vHq9Y6724qtkcePwsN39+xub+nF0lvD46RZbq/6jjx/ZLvZcJcz7w7qdta2fhdIZK2dpd0rq2lNaUss05qE34hSEQ2qU7GO2iOamAkihM7RO0vvKtvOYo0aveRcadDni5q8Tk+VXMvKKseKwYtOvM6aRqtS+tM0F0puB2pN9MEQgmNaCqGZ4dQs5AxTmhk2gT54gjfIbPX7tpSQ4AUXBGcdJXvmKTKNC9U0b9iSteMUr6rH0gQBDnKOlFgwPuCdw1qvUBStln2fgmpFLsqVVR/srBCccLUJBGc4n0cka5qgEV0SJGuadZp2TwaDrUXx1PaeXww9EGLS5EHFUZVP1znHde+QKqQJDduqha4KtlROS2LCYObCfqg4r0DyYYrYXLDG8+H9E1fDhuvO8niaeVpm/uD1FVd5ZMrCEgTvLEUikUyvTzJj24ZaYyEv+pA7Q6wKRq/MT2l4X2kqDkF9Nk01xJKwbiAumdO0kIDzmDkcRjyOriuk08yc60Uxte7FvTENW32BTUF7P6Whog2WoG1vZR34a6s20gqy49Mt/IPfD/zdP3xF54TjcWScIqXA/nqg1srD04lcYOh7ZFrUY9S16F0sH58iRgxdUPs2I4ov9b1HyDhrMRamUhlCx7meOR4iH0+R46yMgmEY6EXYUNqSQ5CSW/Lti52zKKsjNr6fUsIEwTbj6MrqKZBywttnHirWK/57THQWhq7XnKPgydXweBxbnI3SjYKviDFMU+bN3YZ33515/90TP/mdPTf7QO8yP/40sPERJwbjHHXKlJgpc4YlEYB9X+h2HnGFKUW9BkNPv2ROS+a8FD6+P2Je79j0hr53lOrIOULM5CXwzXdPvLrZc3Plub3bUKbM6bjwp3/xjp/87luudoGn05FlOvB6f8PtLvCrr09c73rGmDlHCM6RcmkLprUzrb9RXFcrxWrW/Kjn4rt+VGhuS9IojcqVLlHTEDZW+GynXeOcMilCqYYaA9YIN5uAcWqTOedKzCrWqTUzBIs1toVUKmJba9ZuMDiCL2x6hxMPOXE6zZxOlc5bnCn44DHBMThhv+3YbqwmBy/NDrIlUYgBqQZrNPokO2WJuOAxTgVD8byA6D3kdz3GFMSC7TqM9aRUmI8H7OK1+BrB/OZb9d+/oNYCrj3UzqsayYqAFHqnRSy24mmN4I3o4qgAUpo8VCWJvbekUi9E8JXA3lZV+n2NIGZ9YNUiTJyOd1MuzNmSUtXgv6JggqOyLElVRrXSG0fnPN4bOmvJMYOpOCAulfNh4doLS048zhrLwmbDLhl8KcTmDdn3XWMXCGMSllRpAzfnoia9Yyps1811e89yqRzHyJvSkaN6Z+acOE2Jw1w5TpnH48g5Vuak3N0iNAf1FvqHqqRSRbvd51LJ6ip1WUa1YqOoUIUmV1UOoPDJRvi7Px74uz+9JZjI4+PCOCZyLdzc7qgVnh4mahF2m07VMF612KYWgvV8PCRSSW2JoER6asFbofdCnCudVwu0EoJaIG4Dj2PlFDPvT4nznLgastq9WdHlXXO+EqMG21akOdIr97H36q1pxaiU0VudBASsPi1KuqdgjCOljHOOoRdiLAze0HvHuGSqWI5j5HBc2O0GvClIyYrZeZhiZegswRUez5HzaeazT3Zc2TOvrlo4trXkJCxT5fgwsowRQ+XmtmdzuyN0Qk6ZZYFphjEvDEPgVjzjFHk8Zp4+npCbLbuNpwDOe3KMxDgzhI7HxyMxddxsr7h+ZZmj5S++uufrd/f83g/uuJkLx+OMKTN/9Nmef/X1zIfDyCfXW371cGZKysulajDKip7pefySjtjUdOjkqfeTUtCMUbclQ8aK4tC6cNPvVKvO9CZGjFhsLkjWw42iEhcnlc6gaaK1YKo+w95ookctaiGXctWsJmOY04IAnXd6gJYEVA1TNJXgHdZAXmYMFe+EQMVlhRdY0wZypRiFEiiiHsTGYGzBeq1f5vL76ISMaX6q5YXKUArWCeI8NgT9GQIvaYz/XgXVmspNrxK82rbEyiHTqyS12a+VFXxWOou36tZfCqC0R7xAbxQPTFaHEIPgjKXvNBY25cyctMDUClMs1CykYogV7sfCJle6mgldYFcLIQRq0C7NFyF7NWMJtbD3avVVRX0GdoPlGGcO3UC0hXfHkW8PiULmR9uemhPWWfadw3hLzpli4JAKOeaLY31Nqqo6N/qNpWigHTAuifEx8ergWyZRZXO14fGc+fDhgaeDGiw4YxQcl7EN9WoNWKpcjINFhJrXzmLt6HVnuxpSS23kbPJl/Jc2Tt1tHX/yxZa/9eM91mWejonpVEi1cH3TI1jOTyNWKrtNR81QYuLmusNag3fCeC7MMV9+T+sFa1Vf31noO2EZhWAtp/PCaCPXe81eWmpkrIVshWMqpPPM50Pg1XZg83jkHprY45m608h+Oqc0jF0szZ3dKr5tFGc16Nerokclp1YMwYCTrHiZ1cI7xswcNcqjc4bOaXc0dJozpKYxmaut4f5c+fqXE3/4+4HrrYcSNWK8CufTwng/sZwTRiq3bwJXrweMT6p6sxYxDmjuTbEynhLz4jhfeT48Rt5/+IDlmmHXAxXbB+2EpoXt0DNNhXfxwPVmw+sfOP7j1wP3j0dqmfnhl3v+4heFOmfe9sLf+HzP/+PPH5nnmU+ve1JeOCeDq0pzKu11rB2ohmxygdyem60VhtPpp1YueGsu6ikMlc4ZnBHmqqbO63MhGUiVkpJi+TmDJFVOJpib7d9uYxmsTqyCIVJ1qWQF5zw5JpaYiVan1gZsYa0QgtElbhggCykuiqeWhPeGoe8pCOfzRI1JefQpskwZ4x3GaYdZqnayJatc3nlHFlozJ8owsrYdoCp5VWl4phSFG75XQa21EKzFimHKi0YxsL7JFVc0GK3aQs6ZjKcslRQLWLU7K1kfvlwz1mo6qhFNp6yN8NsFw2ZjGadKmSrjUii5yRerav0XKimra1XtLA6DN8JDhPcPJ15vA3eDVarXsuCLo7OWmUoxOpZuQmVaZt7FhWLgylsyFpsqrztLRyGJpQaD7RzHU+sAG32sFlVO6UYejBc9OUWtvUDxKGssD48zu8HyyatANzhMrOQ48WrXkbPhVx9nyJGNMxrvInoAOScE55rHQUaMbeGH+cJ+SLlRYlb55ao8ayYWFuEmwB9+3vOHPxjYdtrVnMZKipmrqwFK5nQ+4oz6ggqG02lku7V0vaXzgVKE+4cTuU0Z4o3SR7JmBu16izXCGLT4Lrkwj4lXKXG9D8ScqAV874jnSTX708i1H/jxbc+HXx94yroUqECsGZqU9rJcax1ILomTE2pUyaGtFu9K4zoWjKtY9J+AwVSD946+1wSF8zLSByFnC7YwbB2ZjLeWzqrPaI6J623PcSx8uB85HzqmIORkqAHyUjk/ZcaTBtJd3XXcvBkQVxCr6Z4mGFxvMK4tWNNMOhfOD5UPHxbqYAl24BDPlGgw1lFyJHgPznE4j3QhMPSW7z68Z7fb89lnO37ykz3LeUZqYdPd8PXPnzinyN/4rOdfvz/y548Lb73ltg+Uc2mR1CsUpO/RylG5bPlXJktb9Jo2JaaVg2pFTeCLMlycMS1CR3FQU1Ub75u+fWXCgDItNsaBySTJeFHPU9v4f5V6SdCtFU1OcKaF/BlSVXsqqULOqfHbFT81WJz1FCNUaylWVKE5L/jQIyJ0vWsNi7pMpbxgggfnyW3qsU5aBqSQxpmYMsbq/WecxYVARU2brLONF12Vx/x9CuqcCqm90RqA2X57KaQCNT5fHDFCykWNCKy6J1mrSxtjpbXk8FI8uf5njgm7CFWsLmYo6tJNG3VFVVExF5YsSAabIkkqTynx3VSIEtl3ht5ZqKqiyFY4l8LTGLkdHFcbz82uU/xNDLvOc54Sg3TsrEA1+E2g9o4xZR3zRWk7pdTLQs634unQccZbR7WWSlIFhjgQzZ/Z9D2yJG53gZ/8zmvmsfDt+yNLXthvhD/a7dv3BO/Be0fwFt/06c4WrPPEVDWyOit1KRUtwAXRZYzoBjSlSl0yr7rKH33e8+baM09nzgc1197uApjK8biw7XtCEHwwTOeItcJu1+GdYK3lw/2J87zowyWCcxaxSnL3BoZOb9jNxqmFYa2MU+Y4Rl590nFHD7USbMV0sA2WsRqWxyM/2QfGNzuOvz5xMJBrk8Y2hHh1NNPRrDZdP0BFmnu6tO4U0LExazfhPHqPxojbBjwT10PQgyiphZ5BnatsW6oasVAsoQts3Eg3GOJ54rAbOC2VEITzw5nT40Itlf7as73tqC43I3HR7fHg0Ra53cDW4chs0sJ0yjw9zkxPGd/1mDQz9KoMSilS8kLfd8Rl4Xxa2Aw90/nE19ORu7s9d3cb+q6yuUrst5ZffXvP/hj4O49byl8cKCWy8Y7RzkwCSxFivYQU6HvXKHKsJuA8i0VqUTVjaRxxbw22jbi1YU6pBeHppKCHt6lF5c6XKUqXt4S2WwG8tQ3fzFg8BSHWjG2eJ3NMWHF4Y3DeIqaQUyWngvdOF95GhQLjOCMSsQ6st1DNBZKocwJRo51cpRk29ZRSmOJCWTLWe5LV72ecThSleTcYUWFAiuky2pvQYfseY4Q0a8zT9yqozmrcRm5nm6nKTltR2dx4mSJFJYG6m1VQ1wgijlVFCStdSr8mNu1xbUbSp0l5g1KhcypfzTTT3GooWZUxGE2ptKkSesOrreFUC7sQcGIJtjAY1WDjPA/HyP25kKtlnxK3g7rGLAV2u0CuZ8qceJgiWE9+Gulyz3GciYuBXKhZ4QljRFv/hkrlovr00gTRzsGPflqYHyfuboTPPovcXrdlkXdcf+757t2BvI/E64qx0HcavesbW8DajDEFa7NuYk2h7y1GlNUwT1Ex1JX9IhVrneaSZyEnKCXg68yPvjAEe+B8XqjXlWsrXF0J59Mjb/qe7RBxTpdqh+OJod8QfMI5w3kc2fQLb64yu6mSS1Y3LqO5TNuhst9pUbtLVeO7byunGe6uFj77Qg+oN49Q8kIphd4L82niNBbeXAn/2e2Wt7+Y+bdPC+cMtjbNdhvlhaa8qS0Izguf/Whmvy2kU2I/Z3V2N9pRLVPG2cyrWTta52C3Ez4L2gXPMXGYhZgTziz4YLF1YbexlFh5dQXbIbF9ndg6S/ALdhDStaFsPFUWrJ/oNo7tK4uECRMs1WbwYLoAQWj8Ph1tioFYsHeWm7eBsBjexqQKrgLzcqTimGImFihlZGOdHt5mYggeamXK3/JuNFz3O3Z3jlc3gv3ccn6c+Z//wPF7vwz88jHzuMBhNDzNmakozp+Kqs1WhoQ2iatARC73UaHo1NO435tQ+XRT6XPGIhdI7+o1fPeVloEiur0Xp8brFvW0VY/gtlisFVMUt+2dvRTpXNRYyAE5JU5paoun1aZSNNrGQDV6qDpncM4zzzPjuGjHFQ3BW4wLgDCNJ4zt9BklQ2kG7NZoQ+C8GgSVTJ2LehpX0yiMGt5prO5LpvPEcj4Rl3SZzu33JfZb71oGzbqBlTZQrvSLdiC17ZdemAZ+r+tDFBu7RJwgFFZbOf1cRbubVTFEk1x61PQiNgOQiDDGSM1q6nwnlisLf/AqYPHIHBGEwRWExDFVXFEp2bfHM/fGkHLgB53neDqpvWDJJFM5iqEzlukwU0W73OAN1xtPnIuGj6GnYD/ryT30Ft97deDJhne/svzv/w8TAli74N2h0ULWMELtMF/G0L4cb1sLT4OrGu/VIHJoKbNcRvxncvb6Pkujq+j77KzBmiOlVHXHEbU4q+URay3Wjpe/nFeWhRz1JmyQwtqVv2DU6H/V8sIuTT9ZisZyJJ3aGcKItYZ8MQ8XSpnJKTfVWcTbM39fDHOlmRy//Gh4sM6kOtnUiJV48dzMZc3Mermkm1ljdSBS6xljDU4t7KlrR1/UanHF/mvVJYfuC8BbHTFTPujUBQyl8NoK1mZEpnU9rvd7411fLujlmjYoCAhUwm+8j7oEylHpc2tWk6xc7mfc47JQqvWbFqkt3LR/Pk2Zv1mUHphru37ooVte3it/xcf//36ldZiXpdVyuSHXMEcR+Bf/RKhG30/vlaOyLOmSh5ZLuSx9QH9Pa2km4Tqx5pKJknWXYi3OeOYccamoK35RJzLJ2lQJCgn4oJOc90LXdxgyktReULzBd2qz55zyuKfz2Mxd2kTnKt625U7zbF7GGWcNyVUV09SKca757lqdYEQFJnH5nh3qaY6XjbJeQ6100roH1zT7lyLaFFB6D6wFdN1CP6uqLoqf37iO7YYRHRsEaeOefrpUWNrCxopiJnOGLldsEk7jmRQzufdstobrzlBT5Pd2A29SZTGGMVlSWqhV2HWeJVacCVjUtu80J7z39Juepcwgllc20BU0/8k5VX19dLiD8NndluvdTCwLIV3zf/o/ZiRrMkAfNNbB2IINHotiNrlCTFBLxlTdNHvroCoDAqSFHlbEWXb7DdTC8elIXgrOBDRfPKuktagG21hh2Dhymhmc8OXbPZIXDofIx8czN6+usFbzuD5/c8sQMvjKw8NEWgw3tz3eVk6nxONjZJ7hPEaWVJln9Wq1RjE2cuTVdeDNqx3TtBB84HiYeP9x5sPTQjbwhz/ec3fVcx6P6n2ZHO+/G0nLzJd3ez6/6nCA32z4QOaX55FsFBvDqjRVD2JNLcg1U5dIXwt1GSkJno4zU9bFpWCJc8JaxzgvWNdRpXIcZ8Q4Pn87sNs44pI5jsJ3DydirOw6w9VeqKnw+RtPzZW0OH7nVeUqLPzyvuA2PbfBUo8PvH17Q7/JWB8x3uL6gOutMiC8x3iHeAuuvVdGzYGqZJBMrW2as3odZRxZfnni4bvCmHref5jacjJpsWvOR13ncN5SamKZRkoxmvW0GyhFeDgvvD9mHs6Vp1PiaYZUhMOcObRO3hpD19ybnG9OWotGnxvDRUUlVdkKGS3QS0ykRYMoHQplxHPFm4JzButNO7S5dAcKqTik1OZdLKSald2RaK1yc/H3qtcvoofvlDJztPTOEQzkuGh6Rgn01iMGOlGlpPdKIKxV+acYZQ0YWzBWdzWh2yoOHJOS98Wgpoe62HRGKF7ouoBYfe9zjOSYFFd1zR5GWlLC911KdcFeTsaXRU+Mbe9d00FVWVVsrOYM61+6eGC2joKaqdJiUFg7X1rh1u5LT2I9Zecl6zgkhoSeuF6ErW6KqL65aCdhyYaxCAnLUoSHlPk2tXESC2Vm6HX0SbGwLIWUC9NZJZGh7+ivPGEwBDPw8TRjcmGwvo1LheDAbFzTsmskducNQ6cYnus6fMvESXMhlcK4RDpn2G87pf7kSon6PpjiSGIxpiLiVP6aWrqjccynSpwi81gxxXJOUZd1snYg+t72g6OMhjhVXr3eUSbD8VD59tsZ5zrqbJjzzKef3BCMV3XI4nn8OPLqzkHUr//mm0gulnEaSQlKsSyTTq/ZonzBYii9Jy2WZTRIssSzIZSOK+eItcBsSWNR/XS2PDxE0gjXmx2vNltMFT7eT+T7MwwdcYSnHOl6j5WID8pBjsusuLS1Oto7g1mUTD6fLTEbSsnYCsusM+3TU2EzGLpNxkvPh4cZQ+TLN56SIM2CTQOn85lzhB6jOFzskHmG2fAkM7udxSyW++PM5m7P0AL3akl0O0fnAqZYXNLNt1miyqGjgWZWXl1Bamj7H4uYoS0Hor6p1tAHRxdH8lTYFM/D01lljzRYrVayjfihKE7LlnhKHB4y4/3Idrfh5mpHKpH3HyamKXM6ZB4fF56mxOOiO4/gDZ1YvBW6zlFBye5xdZHSzpeqQJ13qjayxZCqY4pJ5byp0FtD59UQqIpgnMcVQ6k6DntrwRviknRCQ3cstVrmqLSmzinVKeeiG3U0ncNmIaZKSZMu61q9mGeNAPdeGPqgnW2hwUO6iW8NtmawlQpGDzBdKoFY8NY11yt1qCq14HuNTxEjlGrwQ6f3yuEMxrR02dJMz5+5Ef9eBXUb7GWEVFMLTSBcJxFrW/9fuYwnxuomsF7GNP0FTes4arkYbyuO007Ey0jTqBpF9GH2vjKlE0tO4A3DYDFloatCF9ZxOuOtcmGdryym8pjhY7H8cpyxJvDDjeVm17HfGLyHOWrb74Fu47gNW4bB40wlzpWP9wsfjguLNcpiEPChEpzgW/NhLVijo5nFIE7wtuIuGRKWzgSGfadOVVZ0vEu6vUcy1VgSFucEisoo3eCxXhc9ZIvvBB+C4k45ryjKZbUntWJqIc0Lu0643hlimvnwMDHlzOvbHfOycHuz4e52wNuMuJ5ffPWRYfB0nWVZ4KtfP1JKR8nKqDBGi781awejEIwR5SXraIgyPEpGjNAHYXCOrjMYW/Heq/CAzNAZeq90qKlURgyHw0hYKk4EYmQp50ucSfCeZYksS8R4h+9160rVG0RWas+F01zAOsYknI8zX+48N9sAUjjNC+el59XVjuN8aP6dOgmAsOSo6pqgUNOcM0u0dCLkRd3p+5ueuSx03YDrDC4YQidYk5RHbfWmkDVBoAAlQXFAaGkQte0gAiKWajJmX9i9ipQPiSvTkZMhLplqPLVU4qI+BbEWZHA4W9h2RgtcKiznE9ZEvrjbUkV4+tOosMrGsxVlvswVptKiYzKaxOoMXefBFMaYmGNtG369rkYim95xs++53VhSzpzOE3nWJNFFqhaaOauZTdtJ9UBxhmL1nySFuSaVGqPWjpoS0Uxy2sQrRhuLnBOpqGgmF/URsM4QvKPEyBwTtRacAWsr3oK04L5SG0NIDEUMZLngxWtOmbUCKVFrpsTUrA4bfCU6JYvVRaULuog1onVNlZq/DUD5dxTUmJX/1bZJFNOqPs/Y1dqJrrpx2/Aka7iQ9UHjDNYF1ap8eQYP6wUXbOGVpFQ4LZmHU2KMlViEcYls3u652QQ6QGrSTeAsdN5SMJhO/TgNsOkcPxgMtmReDcKmq4gs5GIJfWA79Gx6yzwtjCNMh8h4mBmnzFOEs0Dt5SKhdZJxRtVCCGx6g+0gxah8tSoqeXVO8dJm9tlt1WOxpMwyRfJS9MapahmIWELvyDkjtrLZBjX1rXozKFVKHbtyUfsy/b8Ga6HvHHU84mvm1X7Au8TjY+LpMbLZd+Qy473wyesdQ1cQU3n3fmScE29ve0ryfPWLjzwdLb6DPI/YYvDBkJe5qZIsFME6ZTmEzpBzpFY4T1HHOPT15pipOHzo8VaYl4Vu6Ch1IZbC8ZygCg+HyP3jzKaH3c2G3jmi1W4HsYxzIqO+udLYIys+V+sql1R77YLSfIxUihG+O4y8ToEgkdd3Pe6w8DSeudpYOm+4Px6ZptzGb6G0qIy+Kkvgw1g4zA4zR0wWDoeZ632HB2wwdL4SjMqUzYvGQblv7d5unhNc9PUNAhCnywZjqXWLDJZwl9iXGTcWbPXEKI2HDbOBeVUBxUKwypf0W0+uIM5S44yLnv/wjze8eTvwX//T7/jVt4ZA4VgXXC50Rk29TTP6lpJVQLF1hKVyGFXgkHLbcNTK+Typ7d31hl1nGWxg7gy5QMxtky7PmWBS1K7T9JUaivLMRchOn2lrVJiTU+KcK6Xzyg1NWRe7RiGvMUaM+IbTF+a44Mj0ztCFvmXULSxJHXK6ELDGUyvM0xlvhBRTU8eAsY5UanuuCkhVQ6aq4qPc8FQn4HuPGEtqOw+HUecshJz/iuDD/74FdVqend8VEo5rR628sQaSrsC3NW1DZ1pXI4rLrPrhdcsICjaXWi8GIK3NhaJeAIepcH9OHBcluVuEcSr86v0Zu7e4nWe76QgbNRNZ5WDGGNRaJPFJsDo6pcqSYZkjKQp5gWWJnB4LjwjTtDBPmRx1eTRTOdYCg6fvPX1nGJwQjGA7wTrPJFrEN0NPdJElpkbETzoKWcvpPFGNZv+QC2VacGi2jQIxVsF7ImmpYIUueIL3OuYtSkpeltRYEYYYk1JX0PF7NwQ6CZRU2PSW7daxLJmH+xkQnAvkGHn9Zsv13iNEzlPk618f+fzTgc55vv7VmY8PCRcc4zgTjGt82rZGtEYXOFUXOJ1XD4RliSzZcDpNkFb/AT3RO+/ULHiJxKjZPtOibIu8ZOaYeBozhxGO8wzeMVz1nJaF89KoL0lNT7rOsekDGSHlhK06PhYj5Ky831KKvjijhatkeDzM+G1PpnC73TClhaeHE7evr9gOE/eHievcqdIPkFhxxpJNpWZ4msAcE6lCMY7DGNlunGYMWaWRWSnQsqsEB6vPb9VuXrDUapHqqLnxVmxuX1LVY9QKst/SFbCPM0PwpOKJMRKnzDIZTqdETGrWTEXz6+tMyQum84RhQ9iCdZWf/v6OTz91/L//mw/8sz8tSDCUBGIcKSVy0ue6FFUFGidY29MFdTZLSXFGaayW1UFORAjBtYWOYU5q+p6a6EcxCnWsdTYQvMdKaXqUQkIVcVVEPQBqZVwSxhhsreRFr3WwCv/lAksqeC84H0AgLpllUTl4FwzOOqwohIYoZ7TrPD44VkgxxdQ+L1TbvI1t15Zjwvg4MY4jtjdka0jtawsWYxy+V4FIXCI5VeZp/n4F9bgUpauhWOiqYa/o2LIGLmjnqXzBhbZpluYN3r7BMxZbX3wfQLKyTGQ1baicpsTTVJgbmb43wuAtOwdLzLz/qGqkvoHOzgverwoafb3KRFDua8mFknShVaoqJJxRA9pcC0upzFlB/BlIFuzWsdk49oPQdbbROQQ/OJbYXn1cSOOsJ2DTebNuD6USgiYJmKaF7vsAnRpjzKeklmttvK0Fut6x6QekFsZxYRqjdiDisE436cZUjNdT21vDftNj84yYyv4qIAbGQ+VwmOj6gRQrm97y5m6gD5mYLV99dc/QFW62G97fL/z8V0eqOOIUce21d0MHKaq1YVUNtjUtl9wbvLc8PB5Z8sBxjHgs2ISxhdvbPbevbzg+PfH0eOZwXDiOiadTJs6Jkg1LKixRGKNS0lIZ+aHreThGfjkuiiG37q9zjv1UuL4KWK/dYMyZVITTuLDbu7Y1bcOUVKz1zLHy8Wmm33bMceb22mOdZ55GPr3pebifFPvPha0L1FpYlkTKjpotY61QMqbzVAzH40S+6allwZqANe0+b/Z0mEotCTEBsFpkSpuDLy2srkN0uZdQV/QCwSDXW02bTULvHCVW8rmSzpX92XE6V5as2GfwhW2vS7xsK/7G0l9vqJ1BQuX2057/5D99zSefnfkv/skjH0+LTg5ZD6mUtRNFKlIM07wAFh8UX42Lykj7YHFWD8/DWQuNM4K1jvOSOI4zSRtnYoaSPb+TC+8eZnJNbLxCIIK6iY0pM1W17nPWQDFI0gOtVJCkKiWDQjIi2kzVWqAtKMUYSizEpbDYQrdRIyZZdzhFoTFjlGLlXKDUwjROlKUt26Ia1OM81TrCRrC9ELwnpsw8Z5Y4IWLwQ0/NLYViXRZ9n4KaSm1y4GccdS1YtTwL2kwrmLQu9KXD/GXxJGsJvZBcWP0ZJbYwMJObA49eHFMr22C4GTw7bwi2ECQjVaGpuFQykThDbBSe0l7viues3DJjVCOO1eJVSyFjOafCXGERYRGonfD61Y6rzrLxenNhq/LfvG3O5422UzWPqrRFW82RKsrD897TDVtqLZRlJqWkoH+pxDEyLQkvtumUhdwkcKVW5vPENC5INaSqD4BU7aoFQ+gcWGG77Qheb46hd2y2gek8cXjU2A9jPXGZuP5kx+1NwFH4i68PPNxX/uAPbpiXyL/96p7HubAdDCmqe09w6qY0xYRoaD05J4LzWKmETq/vOGVirRzOiavOcbU1XF97hp1jjMLPf31AZynbusfIYU48HGdOSyHV9WEzxDyzH9VVXyo468ipUAucS2FOZyDTXalFZM6VnBUL7xFNQm0LBWlWj4hwipnzaUZZOIlXVxuc9Xip7AcN8U4xsb/qiWnCN/aEiLCkyNXNQPCO8bSQFv15pa2oV+rgSoNSfbpALdTUmhERILF6LOhmRKcTse1rqyDWQajI9ZYaC9iAqR2yPePOM/25sJmEeRFS8VgrbLaCGUCGjtpb/fuuvddA2Gz4k/8gcHPd85//39/xb355JmY1Y86LZYoLqSQyliVVxBR8FDY9rVPV6cgawzgmSpVWpCw+ZKoYnO9YpsQYM6eoeV5fFjjMhfPTwiYIV5uOrg8YIssMOVamWClLVujCGDyKqx5nfYauhl5NcnJEFm3CfKh0VtWEskKJFOZlodrn5yfXjMGSc2GJE6CDQ82Z0Dmcb/6t7RmONaltprNIHzCLGlLHdFJvhuOIgJrvXCre9yioFCE3NXBnDK7Bu6vXplZWhWhLo4PkokYnds3+afxCKXrnKb1iHdGFuo4XYpQkXHTc2jq42QRedXDdwdYLfTcQOssyzYxPE8uoWKkTsFVNZlXJAzW3DrXQQGcdqUHzrkDUYKDzmj01aBeIFTpn6YxGnthOeWg+OJWvlaJdHFBKwgahc516cpaq0dTOKhZMomYNDCyR5mCuXXvXBRxBzSdcxXcecYZpWYhLxIttx5EhSVbX8+BUZGAqfnAMG4tJEWvheuMhF6YpM84R17rjQuWzT6/oQ+W7dzN/9mePvPl0j3eOf/mvH/j1uwW/s6QU6Zwq1Tadx9SMsZbauhiVCxuCr3SdsCyRacksNTKnyubO8dnnA97Ct+9P/OxfPvDdxyNvP7nmzY2nMwU7FYwtYBxzjRxmhXOMqCrmVw8zYW+x3nKYom50qygP0JZmbqx8wJxrU+q1Ec40Z7TW1TjJiHFMCaZ54u52x/vHTEoj11eBUivBqlPSEguZwmmqDENPPJ/UNjJmUvFsvMP7pHShpZJzp8vVUnTkb8sMEaMYKSg9CqNiFWMVWxWn7yepUcJCY7aszUcBHxDbuiAxiBuoVpA+45eMTXo/GxzSqSSWfkBC+7nP9R1kxpiOH/34hv/Nf+b5L/+rb/mv//mR02jx1tNthBgdpxlyTaRaiAlStvTB451jnmbmpbRnqOGNCWrVqah3PX4bsHOklshsdOlojaFWx3FKzGlk23u2zmLFkl3zvJgiOWUGbzT5I+lheportS5cDZ7BepaYFJZxmh9nqprzGGfwXmXlFc2GMlbwzjWnNKsR8eiyWyOaNEHEhuZRsigHtdZCjoHxOJKzYKxXj+fN0J67TFnUy+F8/p4j/9vQtq1G4wKwQjZrMXl27gd1X6ooJmrX/CKaC3jDV3IVlf+1PChrBKkesYalFs6jbtNubx1vrzx3G8tNJwxBR1ypFbGV2Du22z3zUi8phHlWvW/JwlIy1eippjhJJTjPsPd0W4UIzGqRV9Xr1LZ7UNkJ6nlYjf7OrjOUspDjpGIAa1g1vbaq1KG0bbfzakFWS6EuMzVGVWpU7aBDb+g3gXGaIRWEjA0GnN4Qyzw3azPtdImZQmaz36m8d56pUthtOy0aNWJtZTv03D8+skSV3bmWQf7q1vP5257Taeaf/n/es1jh7vWGr746869+fqA42BiP1ETnA51XH8maEtY4hIypqMmxgdCWAvf3Z2ISzvPM1c7xgy+3bPrK+48Tv34/8rOvRjKGbpi42m1wRliKMKWWL2QsRgrnUkjoNDQ8nbmzG1LJSFHmhBVDSZVE5TwtpGz1ujTXMgpMp4VhH3BSyHoC4KziqnMRjkvhOoKI5TAuYBR2WWIzMCmi99KUuLvZ4evIYA1YyzjNdIMlBLWIEwO5tknlspCtSBODUDTxVkT030vRAtkgrbXWtae8+depyY52lpY1uQEqWK+Hc8nUWJVgXgWMBynKHLD678jq7bCGP+r7hE/sP+n5h/+zL7m6/o7/4v/5keMEEgvVWzqTEW9ZFqXg2KQTkbUG2+vhM7VMrtKoj7rUAskZC+y8YPees3OY73Qa0PfIEWPmVFJb5lmch8FaqtE4nSUXxqQ+DENwdBFiycRStJEygi8qmzUZpGvwG6t01pFr0knarOZAKucWrxr8ktV/gdpCRLOmB2Asm/22CWi0563VEFPhfJ7JtarUtAktwuDww/d07P+HzbTCVygWTlvhcdcx9YZcC0FMiwjWO8RYLVTqEZrJSTlmFaW4NIGLXuRacc1V2/eWmDNLXNh4x8220wtUi3aJVim4KSse6o2lc3C1dyohmyI1VjIap5tKbqGBVeMWvDD0umAyFmoupCVSlha/nFJ7MJQKUmnYjQ1Y55uaJarO2FqlXzTKTq5ohEnRDt0Zq0XIqDk1VRVKpQolVeoScd6x3Q3kkpRC4jypGpbDqGOuVOY803uvFoJU5nHCOEPMkc3Q4W2hQzeVwXuMNRxOmWo1w6dUy5wTP/6dNwQD/+1/98jHh4U//uktT6fIv/jZPbEaeqsUnc2g2+8uNBKzceoeVDKV3MjSEDpVud0/TZTSYevI7355w3awnI4zh9PMvDjFrtBE21iEcY6c56yxNLG2EVMFCeRCrMLjXOjGSHAVY4yaX+RKRtVlc1asW7ome2zqsXnK1J3oqJK1qw6+eTokDXY8jpF9r7JHDWSEWJL6h6bCcUzsOuE4Rj69uSHICe8dvzgr6yMMlsEK3tSmRsqNe21aMW3Fq6gKqLbrpq71yl6hqEWctmMK/4gRqjVUSYj1UHO7/1a8riLGIdZRTW7sAdGfJ0E7R1mlkKtzftWumKTF3QQEj985/sHf/5xu7/gv/6sHPnxIxJhw1SkFMCj9y1ZtNGpJVAwhBKTTpW6RqpJoDMY4CuraL66wGxzb0OOcYR8cixRiBazRopy0xdp0liHoAjtYw3FWK8eYs3aQVnDaxOszZirBgjOO86SJqZugMFyVCiZDSToFF7UCXB3ZjCltemjApbOIrEwjNTpS+Wym5cK3hW+liAUJkJQSaIxZyS7fr6B+noWo9wE2C28K3MeJ+9eB497gLfhGuDfWYu3KA9P8dGsNEhSvEFkXVLq99d4rJueUDmOtxxpdyNSsKZumqoJlmRYtktap0bTR0R4BlkW1wKZSi0Z29KY2up/BeaHfaOyB0jMKyzQjqegFqLVdFI0qts4hVmMPXLA6CqSCNU6lie1mN0bY7HtMSMTTTM6Vfr9FTMFSKEtE+SeZWrPSSUTHpfMhYWzGbhwmKB+xnBfyktsaBmqqxJJUBpdgzmof5oIuybwFn6DkSjc4zuNMFeV/LnMmpsrra+1O//yXT/zrnx/57JMt237gn/+rbxknZUTkmCnOqB+AddRaMMZTKJSyKJXFK4VKTKYPgfG8MI46odzddtzcBOZ55v5hBPHUHLkZLMZYtsGRk+H+48LhnFlaMUslU6pV+KRNC7Fpv7dDRymZMUasUVpNpjAvSZU9wV4MuCuQxTCljF+5saVgraeQG0RiiTGTnE46uURSSgRn1UEp6xQRXGA5z3Cl9DG3sdxsHI9TZu4dt9sBx6IFTIx2gpIuh+vKC6ytc1pxMam1RfrKRVWoC6kKBCSbhq1GeLHqFRoTXTKQ1YB9jQ5araHFto5Uuy+k7RBKBdHiQY369eJwg/D3/tan3G48/5f/2zvSB21GUlwIkglBqUqi1hoUgVQTPhhs6MiLvqc1V3JVj4dcCq4IUko7ROHNLtD5nocxclwqsaofqojBLAUnCoX0vf7eThJzssQMT0vWeGlvyBmiFwYEmTLFV3I11JIJ1tMFYQi9Gp8D8zyRlgjZQBTwquSqpVBibOeRKrvEqU0k1ZBehJGKQM6FkgrFaENhikb1qFT1txOn/vpMKWcIpRKNMMaMzYbbWUj3E6nbkjohi0GsFkzlR2ZKUixDrPpTCrowUAdwi9t4hQVkXXVBTVFP9aInoOhBoyYRfUBS1kLqNDDPWKNxIxYtlrVii3aEwQc1jaWqjVdnKWjyaMkFMDT6oY4J1pCFZuugXEvXG70Ja2lxuYayLJim1DIiDL1jSYmUE8N+i980Evs0k6eIM45aVqI3msrZTuucq3Y31pGXzHKaKC1uZc2GElG+HjrNYb1KUYMtbJxlOY5AwQXHx8cjwWkK5OnpRDDwBz96TYmVr/7tA7vO8MVnW959N/LhPiHSk0vCGOX+WaOm1sqOUOs3SsHUwm67peaFPigG9vEwUrKhGwqvXgUkzcwzzHNms9tgy5nBgO88Fjg9zRyeEsclqWt7W0qWori3phMbkNIYBXqIJ4RaE51rAgOM5hO1QDM1zFB/yuNpZhPUX1RQCtecsvKf27LUWUNe1CSFWvFGcLUweOF332zxduE0Ru4fKv5VINjC7U3g9G5hXma6fo9fJqUQGUdl0QNTchv/m3RzLbYVsDpGkiNSWidZ1j+viF0PXHPxkFDRR/tepX3eQHNcaWOeVu9aEqvIXotnw+QEal2L/jPbBuuRYeL3f3rF/7YL/F//8Tu+/nZhHgumqgIwNbMJQ111ExTRLbrxVX+EAc10aonGojr3OS24oszgfa9KKlMWYjEsorZC56QTRymZvjdsvGEIgTkmTlPmHA0pFZbG0hmXzI3omG9zJU2RmsGbDHWm5sjshC6o5aSzXg/aUpCoy7OMYPqOMHRY48hLopTMeYqMUwSx7do1zmrOBO/xwVNqE+TEwrIs1O+7lIpV5ZZVNPI358yVDdyOlcN5Yek7bNOWKrEdjNEOb3UJErGq97UV613bzqnSQR2cmqi3kY2NMxijkbiFJrWzghM9vaYlkmKlWLW401Om4uxqhK0ULNtUKcVUpjRDVWduay39dlA7MWmeVqLQxBwzxjiMU0u7kiK1dds1q3WfcaJ7B+HiUzpcbei3jlxnptOMzIWhV5egZW5uVaK8xYK+Jrtx2CbNOz+cSFPEEABVmVFa3IQXfKenbLcNdIMqtaTMxJRwTgnM41jYbBzbwdF3gU1Xefs68LOfPXKeCl+86fFUfvXrIwWvqZwottwbwRtNah32nloiFbVb2/SBYC1zTAzDhnGOHE8zxhT2ux5vDTkLh6dF83isxRnVtteamcbIkhTDWhkjToTOOEx9zoJX+qa59GcFXb6s475UR06Z83mmbLfQkni9MYrLL5madNxVGl7bFVk111ipOKXq9NJ7wZP55KZjXgrBLLy+DQiVx8cz3lRu9+qper1zHOYJHzQht9ZCsV4LWVm0zq15aOurr3pPU1rjsMogrW/dLawnx2ptKcXo96xKURErUJaGuVpqaU5nq9ACddVHrySlRG10JbR3MF+6ZO1yaVurhAk9P/hdz/8yvOU//8cf+OqXsEyVnKPG2XjNUMtFLQ9LSpei7IJvTJuCFHBe7SpP49yUjhVyxpTMVefpnVenr3FhzJUxo2yDUsmT0ugGb+hDR/CFfopMUZhi5Rh1Ua1euR5nNL3hnJSREALEWKhLc/I3leqd1oYGz+VFhTDiLSavXbUufAv6vmLU6cygrm0xF0LwFBFdhDp9ZqZUWnDo9yioD3MimEKphZj1YSi10MeKO0XKPrCYQomFnJJiRbRxG2lOPoJzgvEQBs+w6+h6j6SEzYU+OGyLCamlNJ/LtaBXsjRRgdjmiZlJc6QWzZAvaDdnbYtOMRZso484gxMLWf0ZTdXinUtULMsacio4q2mltsXhql9mbdpSi3TKnF3d3/NiG0Ad6G+vNcyv6kUzdPitpesdcZyQUsDYC+Btg27zq1dJWJ0z232gbjtScxAHpYXlXDBeu7C+Dwy9Izhh64V0nhWTNp7TaSFGXYAEB7fXHbshMMbIr74ZGXrDq1eBjx9n5kUJ3SVlhiHQWaOZTlIVx3Meimn5PInNcM00jux2W4wY7j8eyQWNmGm+BEt1nM8LN7c7kITrHSUteuBczCQqnRXlG4rQG0O2lbkIsUTEaBtuquLRNC22twbbClWuzed2Lb0t+cCaxCb4JsGFzjc/BMC3GtKZdhAZ6CzcbA0//Nzx2R388z8tzLkQXOHzzwyPj4XjU2bfQR8qr7aesDFkyQzbnlRii9jQomLkMu4866pXyZ9W2RdjetspGFT3XytiQ3NTakWv4UoryZDaCq5pxboli7J+fbPQpEkotfdNrTO1rfiKfr0sSLWN++r4/AeB/9U/fMs/+sdf86d/XjhOFSkJ72iCB4cvjrQset9gEAPeeUopnA6RZV7ACME7gu3azJwoc8TYQu8VivIEPoxRyfwt1saJUIpwnhIlFfousN0EupQJc+JstUOdcuV+Sog4rnrLBjgvkc4bdt4x9D3Wa8ptSYklJazT57Y2vwDvA6bdQ0tSPnpc9HoUsqIppbIsmkpctKMkp0osmSXpbsb99nr67zBHaV2AlZYqmlUK6FvLPB0Uj+JiOZa1O5FGnbAaqGfRDaf3zdAgZ4KpdC5QS6ZMk25Es5pgGNMMi+dIQhDrqTUynxdNLcw0E0FV81QqSQreOTKF7At28DjnsSUhKVFjw3ZXxoHR360UxVIIgnQqcaSCrWp0jYCxQe3QSgEpLRyPVmTBSIvR84FOwFRR15vtgN/u2pJInYYuPo/WwhxxFopri4msvE8xDeivgKuXpV6wlX0Hu+A4zBZjJiiOJSqFpKDQ3Ou7wO2rLf/qZx8Zl8jdq/D/4+zPf23bsvs+7DNms9bazTnndq9v6lWRRRZFFltBnUVRsi1EMR3YgREksWMgQZCfAuRPyD8S5LfASGBBURBYiiQbVhyGgiJSIkWKnYpksV7V6+67955u773WmnOOkR/G3OcWfyCFqgc88tW595yz99prjTnGd3wbUgi8ulmwBOVYfckkRpDGOG1odeXp1a5z+BLzfGS324BUpmzsNxPPX11zmhspe3emTcEG5pPzLGMMrE2R6AbhWHDyu8FFihQTNHZHLdws51Bg1YjFPvqHCiEymJGnxP5yS62Fw7GQDFJUd+hHoDX3wA3Gbjt2Qpx7Oqyd3xlCJKgyRneWOtbAdop89G7jZ7+RqUUJCrezcLhb+OirO/Rryre+deL5i8ibV4GYjVEy87xw8WiLrTNiGRMPwwvatxyoj+fyfZv/Zt11yvnPbhNyroOKWHBstRlilQd1StNeDINLVLVgzZedjtM6buqwWV+knuGBmBFJ/nfOCywTL6zmG2uC5yyRlMtnwt/+998j/spn/O6fGHUNbMeMttYN0wOy3XJ2iZMglLUS4sgwZo6HmbIWzqmqVYTNdkAnz44yq86W2TudcEjBmR7aDxfBX5sq1ipDTmw2iW2Gi2osJXnQX2m8mBcWizzt8dC1CUsx7mVhGxzeG8ZE7pv9UhrazV4sJGcaqUNLpSnzuhJj9ktk2tMwXOG2FvcsVoyi5hE6IXYZ6g9RUPfR45FrM1aEWRuHIuxTpqzKfL8i0d3Mc06UVRE88jd3XXWIxrgdGDb+EErpQVhBWKhoaz4eYkh23XZTv3iSB8Qip1Ph/v4IRQnqJi1Kx3XMRyOJgYoSBMbNRIqBUBpJFGrBqnc8Hqngvy9i5NzND2IgENClOm46ZHKIbpNXVmpr3r2mhKwFU6UcTmgsBISyuLmEVicUh9BISfpuIDrxvDMIQnI8qN2fCKURvSpDU6o1N3XoD1aIdAgFLAMpciwzOTsPdZk9ZmQaJgRjXVYeP94xDCM3tyckJaZx4yqZ4g+CmTEEmJK4D4CuTKMbzWdJnI4Lm2Fgu82UeuDq8Z7rmwNfXi/kzs8LwaV4st1wnA++zAu+RByHxJD6subs5xD8ITJzbGttShVfYk6pd+3mSQBXFyN5HJGYkKgUFZ48HomiZDFyD3IEQ6IyihdaVyb1RSPue+oJusJmikxb43BobHLmx38k8+TRwPFuZTcGrq8XnoeBi93Ce88GUjW+/XHl8xdHpkmYpkbMRjtNjHmglRXNkaAJs5lzLth5L/DQkToA6d2jGNZWsICQnBsZFVgBUA0IIyKdU6qrd6Vt6D+yF8bWKVvysAXznxGyL7jUIBZ3gTv7qop0OKFxzkU4460hZrZPIn/7l97k0b+651//4UyUfu8upePavYnQjjNKQBGGMCDZfSisKVIm7qKwu9rB5UVn+Zy9gJ19sdu55WdUQ5t1ubh2SfXrxF9NybnteKDlcTVeHFau55X5BGUbPCtujESF1GAYE8PgJiuBiOXEfCrMx4VYK1Lw6SW4THjcb183OWq0pTEfF8AIrfpjGANigSlPfQ/4Q478oIgJQ4gPQGzFaSzWw7yGnJiygdbOQRXGHEidUjdsE9M2Ow2p69PnY0FzRsQduAGPBg4BM6fZrCZYcWON+TATejEN6i7xigfjtW4xaCgxR8bNRM5GKCuhh8uZiWMh4h+sRzZ4UQ/DQNy4d6athWyN/X7rsdHr4ouZbldIg5wC1QEY554uynymbZnzMudVSUnYbUfm00xpynbMDCmQxkSojdxWcjMygZwS1ZRGQMxNUda1UqwxjYmKG0BMl1vubgtaVt5485L9fqSubvVWtDktrDc4tSoRYcq+ECu1YQFKaSQJbHJkSh7v0urK7mJPloiuM9vBvTZFKtvdFXe3K59+dkcjusOX9c01sNbKvBR22xG/KI0hed6XNOk0Mmi4wXBTpVRfyjWBcQDJwVUuTRljYLRKLMCyoroyBWHaJCTgXgYxMgSfAsy8gEsn0mtfDJ0jY6LAkCK7oTHmAWuRr35Fee/JBUmMIRlPrhL3c+HVzcp3yBxvA5eXgXffypyObky8GxpTKByvb9i8+YiyLuiwQ9nSaERTrJsl29kUOAAYZh3LtHMUkEHrmGTvUENPqTCt3TG/C2+tIm4R00UDBn8qkLEn93YIwMMi5aHz9VfSF0eqTsGSMwugswQkICqMF5m//Jcu2Azw6799z7xWDseFddbOPw09mNN/ajUlRDdoidnzxaw1/9nzShhXECf5CwY5YUNkNsVCwIoizZNjVY3V/H0ZbooeDY8xPwvtzLAciHXkbl745N69bjfDBAGG5ou4Wt2FSk0pa2BZjFp6AOZaaWa9X1FEfGmnBmqB+7mxVnfNi/7xeYaU9ZRUMaL9kAV1bs6hjF1zvs8ZzJVTAdfcjmLOF42hu86YdxJDYtwOhAFSMEYJiCotwLDb9swWGIZMiB5BYCoUc0noWTuNKkMQGu57KRJ64+PDU1Hxvz0IcYyd41yJpTo9rblpSU6BGuyBkG3iXa0EtyVry8JGlXEMRK3oDOvRSfZ5m91wtzXq2lwDHYQqgXVunE7ndNbK0mqnmkVkcRWOmYf8pdHxp1AKA0IKiboWZ94E8diOcfTueRASbiqcqt90dJOR0N3ON5vEITTPwZJIaydiuCAPkSCFxxcbcvTRt5iwlhVRZUqBHBy/DlbZbwZ2YwZb2W0zuXd/KY88f7nw8edH5gUe7SKDT9qYNSwGTqeCVeldhheNCEwSvFHykuL6+s6RNOtLpxDIky/cxJxKl8Ujv61U1sWhg5RhxDuv9bTCMBBGN+0utTJtt5SyME1b5h5t3FrlHK69SfDGxUhpje1G+fGvXpInJdbKlDJvvVH44nnh8nLAtHJzXbA1s9kJT56OpNicppYNyRUpB5BIOTYsVDQUJDuzZVkKMSXPuNLWcaHQ6XAdxLXWNf8diA0B2opIQugwAY5LnuEukeSFioaFXixFvCsMvTD3pd5D54q6Bj5It8iM/WvGgzOHGaYnRLzBGbbCz/3cE4YIv/ovv2ROIyGtLPcrtfpCB7XuzC8YLtuMObGZRmJMIEIUt9fDgOo0K5MFicImuaJSkxDzCOYpDpvR3bWaVpqJexmUgmKkwRuSYYjsdnDcukjj1bHyh1Z5sjHe3reOtTt3fF2UeWncHRtzVXKuaCs+MEU/xJzP7Plbp9I4LYsvpoPLYkMA+lIZHP9vfzZr6s8vqGvHEpMYwRpjdA/GIBmxRg6B7RSYxsiyrgTckWmTYPdkhwos90c/iZMrXFpzodg4TiynlfW4umbXlDSNhDG749PasdlWSQHSlLg7Ga1Cip6oKVEIagybTNo4N63dKxsCyRJq7l9IdepMDpl5cSA/DAHdZFr2jKQIxJQgCMtaiSkTh9Hjog8zrVVSjhRrlL7hvzssrHVBLbHUiprnwru3aSecNyHFTiEaIknUDyF8BGvRt78xOYWq0Fg6X26cBqoZwwCX00BZ3XFoGDacjhVGYTdtIQXCMNBaZJ0bx8PKZtqznUK3SXSsqplRgRR8DA8CWYzdmImhstvmjqlGxCKff3HHdz85cKoT2jvPEAKKL5tiCixH70IwAY0E1Du/IFQa5wgHoWON6g95Q52jHKM7RNWV7Th1kwz35fTgPyWFxKrNOcrSHcW0Lyuty5jV48OplajnHXgjU3m23/Nkn/j2xwe+8nbgrX3s23ElDca7byQ+v4zcHipXTycuN8oovkzZ7oztThgGf0hzFnJe3aJRV384VbHqh0mKySWwOCGfs95fnXzeReNOoTobpphgLSJpgBjRs6LK8AJr+hoHDaFPB/Zwbb3J8f9/zqf3ouqEdNNzkaX/rN5UdJqhxPOSyxeocTJ+6ucumTYD//RXvsdNnjASN6+OrA0Ej432jg2G6ItOdCaHjd8KQyQMvlW3Ym4bGP1zs+J0RksuxFERDwNNboK9zn3JnP2eqhgaAhaFbSdMrGuiLBFdjdOsvJxXrr+sfHq/klKhKhzmwqkqOUU2QyA2OJ2UefFpaZOEnHwXM3fXrCFGd/5X7YbYICoeK2jGai6L/qEKKqGDt/Y6NEsQBotsNfEHdydOOmJj47SemBDe22zYxcgyF6rAfCoMJpTU+Zel0UIgjo793dfCqqASSUVJ1XW7t7czbTE241nqCfdmHNWduVttDBLZZthvBvIYuHlxy/1dpcXMmAULtcc7i3PX1Ls+FUOmTBpG56PdnphC5CgFkcYQAjG79FTVWJfKdrshT5ljWTmpL1RubhZObaapW53F6GYpZoFxnGhtISdhs8lUXbm0xONpJA4Za43apWwhCFbc2Wmu1Qs7wmk1SlPefDbx9L0r5uOR6dSw2Zg2E4inZAZzAnJT2O0GtruRNCb2lwm5PZHi6FEh0WgaqUtjTJFSKrscGXJgv58QKrvdRCuBTz694cX1QmmRui5c7hybErrJbid2r9UPVhFnTwRgTELLESseAe7+k94JlNpYWnWWQxdaqDrnEX199KfBifmlVYqqW+vlhOQBOh6dUmQKZwaGx2hUVVoVsggtNqY08NYbk9u+zYUP3967AEWNmBIhwqMnkR/9WuPzz1YSC4+vtjy+jOSsDJvgxuJD6gcHRJTG8hBM+WBAqZ0f3Q+ss1LqgT4V6FQpl5U+YKidS+q8UaNnuvr4LKUXv/pAGHD+asZ6wUQ7i8SUs27R9yahMwYc33Uslz91yDnftfrvk4BYclZKDvzoj0Nb3+Cf/rPnhMuJIV/x5Ze3tNUFCIj6LsMcYmityzQNmghLiMRWXcosHk8dg/OMW2kkMwaDGoLXhBzRVomD3/+1VlL2g0wI5BiJ2bHd7TiiO7BakFrARuBs++cH8kUbvAMXQXIijwnTLVadtF+LerR1PnN4oRTnq8cxIim4sARnxcxHp1++IZsfrqCe0wsNVxFZp0OpVp6GRJtnvicBXZ2UPIhwvx45xomLAdI4sHYPz9H59L7jbCCLL2OqBe5KpQKtnBhLJuXA9f3KskA4VXLsZP2YIUOUSK39QQuJw/HEJmxYNTA3Y5gEdZ44rcGq8PxgnGohxcqUAlODx2NiQ/D8puBUq0QgmLj5cA7sNxPalOPpyMIGnTYsJWDlCvv0f0/Qo8uxO5PB3aFgiS4ekDGhU3Jj3Slz3yprqay1E6SlL/SWhdJPcs43evLTspYrXp0mrDY3abDKfO9+AbfzQj3TUMT168+j2+uVsjIfj2hLKEJ7fk+4dv16uxs4hQajYF9O3GRjHDJqcPNq5nBwEcLSA8nWKfLyCz+cTIW1VEJyBVJOgdPofGHUM8/XU+cLq2+8tfs4rK15t5mSY3+YR6aEwH12vbwLN2LfuHoHugZIKSMYBYegSnWqy9K36gdxGe2yGDRhEGMcArffHjjeL9Sl8snvbTh9EghSicGXfWJCmZXlpJRaefWpJ0PE1LXhwXoU+lkr3ysbZ2ZUz7Q/Z66Fc0U7f5beHdrDft+FMJyhp06RIgTnVYu8JuKfC8Jr44DOr3X+teev+Sj/uuL2z6kzArx4nl9Op3NJj/R4KMEdMniAC5wNo0Xg8xvunrsr1VhcudhUSRJQfT0OEwOVCSuPWE+KbZQ4JmqrLhpUp+JJcGvD0PnPc4cEEDfCkRSppWE1MhdlrW6Ebdaoxa9pbUrITk+UZEhz5WPqRuTr4ku50JWHDad/Wkpo6iwF/N6NfYIQcXiy4n63EoNDDtGX18PVliT4JPRDFVSRB9d5RWjmo2Oxyt4iuyp8b/VRK2KUFDmGiG0mFs62dnRaleNnjUiTwFILg/jpNO123M4rp7Ugmnw7FwKajPvFlQlDjYwZSi3sx5EhReZ1oSRfZoTUcNPZyLTJpFLQVYHEXIxDdTedFBMVuJ0Lu3Vkkx3rimJsdxt/navRDm6WoeXoUlJz1/Z1NWr6Njz7r5H1HXKj5814MQXP06lrozQfa2V2zubtrUvqVoWlb1/PuuI+tPl9KYFSjcWMZ0/3sFwwX2dEXHmCVdbFRQJlXaAaZXWifkhd+9+zZpw64r/ramgwrcyzIjOMm8QkEV0i8wm+PKwsq3UjYu/oRZRpTIQaWOZGyIG69qUeDtRLitjSLdXMxRl1rbRiqPrNX6tTT6qZZ6lX71xjDG4nFwLzyTvN2txC73xNRF6blOcAm+zKNhc2RO/UEJoGSvHuIwZhiLCLmbsvOvVrhHYfeHHyLtBpY74nCN0tXcSTKsptV6vF84b+XEx7h3gmS/dOk/N2n04KE/rS59wVWq++5wrXv973SQ+eqrwuvucO+HzAnovcw8b+zC+19vrnnX823/ffD8WYP/31vn0/Cwv+9J+d/1y42r7D7m3h008P3CyKFHUhxfntBA/JK519EJ/839Hwx7RTRactC8bptLLJka1El3yLK9cUdX5wM9qpYmNENiPzfeF4cyKac7BjVOZjYW1eQ0pz8YxWZZgytVTuXh1J4kkdo7i1wznIcu1eGxYCJQZqCBwPC1IrU4o9Hwuc8R9Z7gsWqosGBvcuOdwfGASu9ts/s2b+uQVVxHl8a+t+nyacWiOqkSd4Km5rVVrC4tmj0lhNiCokc2s2Najnm8RccmbSeYmiLGXpxHuXI6q5IiSGQCWwVENFqT32IixO69EQOBaPlA7FCAo04e5mJou5U5FBSpG3L6PLYs2oEji01reqirRGaJEhJIaN0AYvFuvicR4xDcQYON3foykS8gv44P9IK3hURTPuK5yK01GeXe2gFd/+77eMjwKb/cgbbSXNhQORg3hXOyYwGqeqXYIpJBl5cbNwfyr8/F/7On/jb37E1f7CN5O2YjSaut3hfHvP9R/ecvOdA9NV4s2vXxI2zjVc5hUkYGvl1efXrCXw7e98yauXhVqUp0/3XF0OUJWXX57Ql4VRMseluApGApvRePvtLZsxcbo/cnG54frVkbUI2txBf7cd2GwGP4xEmTYj969mDjeNeYXDuhKbsxdezcbVkw3MTggfx0CrBVLyaThmru+XnlBA78LO9SlyOQbeemtLCsbLuyNPr3YEOxFEWNfMi+sZUyEE442nI48mZdTK08vAdgvvvT8R7NRVVUqWSBoC2/3AZnL/gTOt6MEoPXrH/NDBmfl22MxdoJSHAnRuVB8iK7BOyD87pvlo7hnxbqLiy/4ASXo2UnA3KbzIavPlVujjsoQBtQKSHB6Q7++YXUptsZu2BPrriC6D7tjxmc7mtCqfduRcr+PZgtDflwBWIs8/n/ln/+Il//L3Tjx/deBwXLCQUQLr6hSoFNxJf0RIVTjcLL6EMyVXV8Zl6cY4lrEMoo2wFMyEpVOnWjFigVAbcXQDakdSIqX6VvN0nCmaCM2oRTnNldwasTQI4qKgFN0wSQLaPRtaHjiJsipEFdbVsNWNYWpzrNtCRJPvAaoGyAO1CtTGy3b64QqqqgPGEqObWWCsGC/UmNR4FgMbrdw1p4ak6Bfs+n5m2KcuI3aJ6Fk10tmDXrDNjVLWtaIWEaJHAofoctcmVPXT4VzU3fUodImbsELPoXGJp5owF+9YzkmOzSoJYRCXpWoQ9y21Pr7FSMqDc05rIOKjZ60rTQQZYRhH7HQihMAwJHTthhsIJQROVrlefBsf7w5cdQpTqYWlZIoV5uRLohyNi5yYl7UHjBnkboXYFLr7VTVlTMJuysSgQEVtYWVmLTPb8YJBB7789pecXlS2j9/k0dNn6LBgq5LWIzcvX7ly5O6IDImPPnzElF5SlkiM8OKLe46HxnIy6hJY6spSK/NaGafEcDWQR5Dg/MUoAtp8k27VO+bgxUetPhSU09p4eV85rYFmlWcXG6ac+PT+hpe3jVSMLEA3nrmfK2vzn+HpCUpM0VVIdKPh4JHh96UgVpmbTwHR1AP9mmOu2ymx2wqPdon5eMPmYkBk4erRhs0mIWFLs4IYZAt9TARrlUhz2CXIQyFHG0r1OhdjLzKxd6aeooB2jnEvjB5x8X00qTOeqp2QH/rfJ3Q6E30L3zX8dDy0nYdWdYWTGWKz7+i7wg3pjIHeBLsewMDW/iSf6Vd4Ee5MGcQ50Wa4YECb+2MY3Si9czTMCDny1rsX/J2/nbi4fM4//n+faC2jMvDJqxO3y4qEwMWYaRVKCxznA8Muc7FJXORE7lJYNSORWA8rOkiXsLqv7WyVJhFZlezCd+7mxrxUDwk9O8UFX2StpZFz7p4C1qeN/tnEwLCdiAHm08rhvrCEyCqNAhyXlUHBGeluUp5z7HJTf51NfHHWdPVUY4NWzwq1H7CgQs+YEXOZnRktwFKNU6vsBS5NOFAZYmSTA6vaA5bkeJtRS3OrP3y0IziRnvOmsY8l1voytMGpmj9k1Sk/IrgpiQmnZcFyRqu7L9HVSiJuqLHiMFJp5t6oIiQJ7AU2yb0Og7h5CzigvpZCrEYqLsXUxYH36WJi2GbWOkP0791tQENkxR5O/3mZIQi73cSQhBhhrbhOvHqVWXF+6pSdlzt0jC7mgdi6H0LzJMaAuOZ9Gh1PtEo06wdPZr/bMo6X3N+dGK827J9Fnn31kjgFzDJaT5RPrpk/eYk8nhgvL1Aq47Th3QCff/c5a1lf6+VDoGhlWd1kWDXSTJlG2IyZ5biQRLqxMr0r8+4oiE8eoh5Jg8JpVV7cLUgYuZgSlzmSx4F9jlyvLsHcdNZAlcjdsrJ28ngzpZghxZV3OQQMDzm0IXG3drMKCy4/DA51nJYZEWEahUf7RCvKdz9XvvW9Ez//jYGvPdohQ0MWL2RRKqBUFWdj5AxjdmlhK2grrvO2fs9mp4t553j2uuL1WB0CGvr1eDAxORPpvXCeDaftvKTS2qXJ56YjOBxK7zRpWKsOSagiCGrFO2nt3aR4lpkX7R4CeN7yI96QBuuNzOvibp1JcS765wQNxF+XWfHHuC/ZCDDtMn/jr7/DG48yf//vP+c3P7nlsNAn2MhaYAxKipmlKRForRIlMAZhE88m4I7vRvOHdS2FWZU2DCyeRMl2jLQUeDlXro8z23HDbkhECX1aE1dVnqXK4stNM/c+NQ3YsjrrQI2YnUFkEqgKVRKtrQwputVCN8hv6su2tTZWUxg8CVgkIDkQ/5yq+edjqEHIxO7b6ZLBYI49WfAHfqr4qaPBfUardSWSj2vVlIhB6JyvXuza2XvQxNVOVgkhd4K7g9Xb7GOtFY989S54QPLgHMcQGczJ8BIiGhonCrP6TW44TQvxKNsiSgyZpXkueKmVFnz0itlPtbYqKl4YxuzGwruNG2jcJmM+NVpxS7+GMStcn2ZCEN6+2jgVw5RtFMQSr5bqgXJqkJSYgagkQLJ7E5RSOZ5mTBKDRUqXxuUoPHm8Qe9PHF8d2VVlfPsR4eoxw+Ay3v1T+Mbf+AbreqItR+5uXpCGAW2Vewo8GkmPJ3bPLtBaOd3cMAyBx29ecFoz715MlLnwxRcHnn++8MXzldPBY5Q3w8A0+YRxW7z7PJsMexfVfUut45nikb++K3H54sVu4iKt7KPnfj293HD78p4hGpuckCjczAv3tXLqZ+uQEkt3phqHTgPCD+q5Ks/vPZxuyoFpDaQh9fvOhRCbKYEF7u4X7hbjWI3d1UQUY10KeTEkqz/N4nlFdVEOtVF7QmvKkTxGLyxN0UVpx+pqvmHwxSrnDpGuA8cpSckLLg/eup1bavQFVHvAKa1/zalt7QEqBfeH6Izz3n16SutZACPBX7tVn/bEg9+wuvb35vHViEMTvvXvG3/wZZaFjhu6o5U9YK7iX8+pQ7Ctw7uZtKv81M+8Q1P4/f/zHboYTb3paabMVVjL4skCIud1FznAdsy0Zsy1+YSThTBkpPSDJAttbVSFtZYu5BgokvjyuLI04/EUGHxGQLQbnxBpCESPZW/qgZhlrg84cbEzgd+9V02d7ZLFEz2aqYc9+rvth0RPeK3enap65/xDFVT/YK0bSbufoeI5R2utbHNmwnEHDY2svh5YS8dlelE2/EJHkYeNryRXx5TuUDPGyJQzt2VxuZr5oiiLkfqo5TbT/m9KAbQyDMlPI3OTBVXvTNUaiGflGMpmENIwUFU59JFBEU8hkIYFNyauwDAl4ujLBRNFrBEwaj3HfoyUmLg35fPbhYrwaJt4vBEoFVH112JGrdXldeIyumW+Zze552RbjFJcPzyMWx8b61mlrVzsR548npivD+g//4TyeMPFh2+g0V2y1AImHlz28XeuKcvK+195zN3dge/+wefMLwsXj0a+8dWvILvIcjjyrW8953vfuuEnv3FBCJntfmB64222T655/OyO3cd3/Ntv3TIXl6Ve7l1L39R9CErT7mvQbQZxRZRZJQ7+GUkzj7SeIrtsDA+0Hdf/iKpn+wQ4aeDVsbDgROspBt58tOfucOT5zYlSm3N0O8B3WJrzFpvzpHejsC3ORw4ijEMixshxrRxWd0qbJuPRRaKVlVp8iTRM+IOs7r9rFrAmLMeG1pVhymg2UnT6XJhGMiOtzOh8gpiQ6PdWCBFs8PTSPsrLA/bbzadjQKS596p3Ev7nQUC8C/aE1PMiqo8B2rxQd36pF93kT0HvLH38j73bPLtZuaDFja+kd8GCSeqTYcU7WOh0Ey+yTjdwvFeky1jdYQvrf58Kg/HNn3nEf/Ifvsv/7b/7nFeLuKjFDFsjQdzVP4ZIsErAs6QKFQWPCsengmkKPLoauZ0rn53cu1V78bMYGDeRSxm5uV8opqwaeYCpk0+6quomSAImnmwg/f2YCkZAza0uawq0tbIslTEniD5HhOQ2ncFcZWiti1AsdGiHvtv5vqXfD1JQq+oDjSfghieifpM2cynYOyHyRwazeEZ3RLvxbEC1Ov4T/FQIBs08rZLVui+ug/DWsSPVsxLHT0p/cx2YD9F/h8HFlBl3I2X1DtDHKdei56hUVarC0mlTx1rBEs+GAJLcBCTwEO1iIqQxIkEpWlmXAmMi5ERDfFPeaRnFjOtD4eZQyGNmH43Hu0wKwqFWWoUpRUKKqJlvJq0iFggM0Nx3oC7VO9cgWC1oNWIUcopsRfnKV5/x5NEFw2mhXk1cvPMMKf4AmSq1Ftal8kd/+B3+67/323z4lSe885U3ECY+/uLIb/zGNX/hm0/56d3ekzW2kR/9Cx+wyQFpM1dPMmlKBCnsdnvyuGWzGZnGwHe+d0CisN1uuL29JYq7yat2/BnrMKPjWtLNk13CWxlahzOKevoqwvObmS/vjuQY2A0eVf3qvnC7KpIyuUMKp9PK5X7LvFSOM5TinhAhOL+RJmjz8ew4GJfBDyBLCQvCafGlXbHIMBhBClRjPpx8DNzDFDMpWmdaZJqYc97J7jUwr5gKmj2GxaTCAGGTiQxYaZTFlXtNGkZFcL9Yl8w3N0gPnfJUDbRLQlHv9pxG4MXwbO/XuvGGeeqoWOJcovuD5H9mA4jfr6bqMSj0omcNM+8+De1sKl9wiXYjluC+wn+KonXe+iO+FOvuVt4Zp/6sNsQiIithSvzy33mHP/7kjn/469dkIsM0IsVD+0K07t6G05DMjaatwwqtgZwWigamx1uiBeTQ6VU4tl6LC3w2m0wrjbUYh1KRCNsxundsMFqbferTvowK4otw85Y/SqfZaezdfehYslFRhhAInV/eTDvc0scFcbXmECFaQH9YDLVfTqdLdSaIc7aMxRoaI1dNGAzumhvUShDmUqhtogUjVkWacZ4uTDvVxte2PZ46oBYopXl3Wv1ULtXDwfZj7hOIZ4bHTruLwV9fCmfYXlmbx+GiSg4BT+xQigY+u59hGxEZCLgJSmuNPAp5iMTRYztaj5p1ExGhnmZ0XhmAsWfUz6XR1NhkuNoksinz0SleTZX7eWXYbGkd82otYWnAmosdQuyJAdrd69W5lajw1rOJb3zjPX7kx95j82hEtpB+5AnxW8+Ry4l2NVFq49WrG6btnkMpxDHxC3/5Q4Zs/OHHn/Cbv/sFw9XET/zUV3zhIq7ievbGE8J64ObzLxg2E8M0UEvB1kSskd3lJe98BfLkWOiQGqfjSggj57yk0l3wU3A4n44SOrgUqHNlVP8Miq68NPjssPLJceFkxuMxMkXhpMareWU1wYOXAQncnSoqgXEYqWV9UOQ0c0w/BGcALGrcLH4C5yERRdg1ozbn+UrMnmsVIk0D80lppTLuBqKA7/MdB41RvF7IuTN06KqZESl++Kqrd2IMSBrIMWKrZ41pLZ5OKy7UIDo3W4IXOaNv93FTdB/tOwYWO/xgcDaLxiJo8J8ZwIP9vJM0q2CLQwc59mLnkJwVL/JCA82IuWzMaifvh9ohiL5ck3NxPz/z59fRC751OhGhb7u+n18Fu/3A/+p/+hW+/LLy+5/MZJRV4IQ3LLmb/kSEoXvrrq1SasGKN19LUxpHTsHrQK2eUgFKU+F0WEibTMjR9wi1dEzavVTNHC7IMXTRhHfj55iacx1bVYlR+vPWiBGGGJhSJpj1JIkuyS1G6+/BenaTdA/mdDbx/kEL6tkgQLWhAbfX6zSQgzUuVckiZKBVYQ1KTD52r6USWiNIYBqcAKzmJO5qvhRq8PB1t//znKkYhZicQG4KwyBssm8jV3UHmFoLp9WpVRIdl8pDJK0+OvnhojzZOuB/X4xXx5VjNabc6S1q6FoYRu++yqIeOSKRm+ORIQYeBaGVipZKK54VHsxVQkMO7MaAaGM1MEk+DgxQW2PtI0OUfmg08wyfNDKGgKRODTKorRDjyEc/9phf+Itf4+rxnpxdvWT9+o3bTHk2srbG7c0NQ8p+M4XA3/pbP8rbb0wQGl/7kY/4z/+XV1zudrwVN4SPD7CN6OOpL5XcM2C7f4M4Qri75pNvfc7hZeTJ+xfki8zF1egHZye7Dym5aXDygy6mQAhulBM7nud56dBOjusFgWG34dPjysf3MyW6pHA3RkSM+waH4pi19OdVRCjNuD/NTDkyjcKyqDMAxE9lLW700sRzqA6nyjgJicYuJVTgtFSGIZJjYFkqt4eVKTeUnorZJyJ6kfai4X60QXraK50jLDiHtjSiuiUhUsl5QIZAyhs4VtZ1pqjCNJI2ffJ6oKR6hIybf3jpkmCdSto7Lc5LL4OOn4qdW7nz1O/FXpI7rVlVLPYHXvDuUs4ZZ/hh15VGZsF/X/QDPETvOs2Cx4X3tsR3Y8HLQy9WXvB98cVZ/SWGpca7713yv/mfvcf/6b/6I17eGbNEzBK7i8wmRrL5+F3WiiXtmWxexWsrfWQX1uYghBFozWG2IOICkbUy5AmTQmtKRUlTdjxZIZiQs2Orrq5tbuFn7itcDWeNDIm2rKAwhcyAgMHauurLLYoeKGQxuJNWKQ1J7udw3v/8wAW19G8M4m5G1g8AEfEs+2pYErI1VDLFGolM6KDtUl23rcEpMmJdKdX14Est3bijSw/7A0XwdNXSGobHLBRTxnHgatpgbSX2hyCOmVpXIo3tMLGEwn01TtU5cTn4gzNkYYqQholmK7ucmFJmMJelbnZujKvdUCLmyLqstLolx8yxnVCi03lqIwU3pqD5R7BU4Vgqq/UtoRrj6D6qEhMhJpoVtvvRTXU7N0/FT+w4BH7iZ9/iF/7ij7HbTgRJqDRUGpmIvnfF/M4j6jZyONwzjiNlbZwO17z/7puEqESpTJuJ7fYRzx4/QT++If/658jdQvtgB7/wLktZEYVhHEhDIo0jWhe2j4988tu3LK8az35myzgmTw29X9hvR5bFUxYQN3qBnsWVXVI8DYmEoKdGOxVqVSQHhu2IVE9eMNxkesyRRRtLPet7lJx6ymuP2m7N5YMpC4lIWVxa63sfd0+SHlXhemujiQfimSqlGW2t2JipmrhbhDdydL146tS8/nP8n065oR+2+ayhr67bbz7NtO5JGlN0/LQ5FBE3iWnYeELBWgi5x/4EPzG1rYTzcq0ZEnqEdFVEilPlgnuYPoyjtSGtelk3v04SQ/eVrpyTOJHi1C6BkPqGXw0L/fd1+bVDEbF34jh80N/+2Z7PwH9/zEDhoX09O/H3MfjsVSAiWIJv/OQT/rf/c/i//v3v8PufVza7kZiC1wscKospUNV540ut1OKUxxgjZa5oSF3/oAwxkdV3IVOILFUoVgjBDY1yigw5sfbJNYp3mwlYW1cOIt0IJaJ9uaRqlNqo1UNGs7h7HaEL680XfJ3e4GwAg1WVTQikMfb8qR+ioLqap9NDuiHFOT/7rAwaqvE4RBJGwdDSKEmogssIiyePqrjJCgZxOPsnWu/OnJ+WxolSVlJz9CKY8+RCAo3GXJxLN4Tu1J59TJ+ijy4hZ1bgfjVuj4Xh7CGo7n66VmPWlSkDRJZSuBwT2Yy2FOLoru/WGle7wQkvxwJEchy7t4E740RxI91VnV5RGp5hFCJzx4Ctk+NLE4o2yL5MK3OhHld3PbfC0ycjP/3TH/FjX3+DcRowXF4aLRLEKR3pcsfclPVwQi3yR996wf39LX/hZz+kiudajdPEZnOJWMSWE/b5K7g5UadIeLqhpMp6PGJNmIbHhOCD9pAf8ejtI2//WOHuVSRPfhCoFlJStB0eFlKmrrwKCC34JNGiElKmzEa5XyhLoxIgDBzmxsu7GcMhgn3yR/TFoXDskR9BhDENRO00NxxOWVZFM6RhJGmhrC7MsHQeuXxB0M3haEBRTzrVM5TSm8DPrgsfbI1HndPbqtvkWTcK8R5Avk9wdO5CUl8wdkNyM4etFiWIww8pgUYhpMy4najzEVvcFcpSJ9/jNC+zTt4P8lqQpM7jlt4Wm4a+oe+G0Oda1p2eOEdN2+oqJfUiI+Aj/wMWG9y3Iicv3h4x4PVWi8t/+67CxGlaQRwLZ60OWaTcu1XDQvB0gTPMq51WJRmJwk9+8x3+D28N/MN/8jn/3W+84v4UCGNkk9Q/z1oguPryWLswISb/XJvCGKnBR+1xGGFeETFPYGjuAxCiGx9ZqawFyKMjE+rsgHP97+pbDKhmPfTA+abWobYY3UQ+YAyhx0urTwG1KYXGam5bjsAwBvZXG37okL65uWLpPA6crRdCNw2pBC5i4KrHyI5xZEiK4tZrqV+8IMI2OxgvmKcmqhE7mCTqTk/TEFhKo2lFeu9yWlcakcmEzZihY6sMmVd3M4eXK/v9wG5L34C63+kQ/OZwk31fPEUHgMnR+WiHU2VqyqSNaZpc+hY8JycqJDNWm1kDrM0hjxhgGAZynllWd8pPuIfq8bQw986qqWdYJRGW08K8ZLYop/nkOwWNSG3stiO/+Fd+grfyyPR5gXcbpzyTUySEve8ZpFFLZj7egWQ++fQVv/Krv8XP/tzXOAPu47hlt9kTJFHL4suWd68oORMvt6xvbVBm2jzD84X12Bjf2KEfNUIaGXZXvPOTA8+KoALH2xXRwLibaDTvbKpjXK05N5meMpqyd7PrfaUelNrgJIEvj4UXXxy5oytorDLmDfOqFAvOrOjl5jA7Wd1MXBHX5YnaN8NNHDuJCilAUVfUrGYEM6ZuUNPMR0oLvuix1c1uvrhZaB8ktjtQEWIVVNyFDDM/3AlodiNoX7pF59y2hljo3i2uEFMz98lVpz6pQtRGSEbeDLTTiSoLcTpzO63f/13C2t8r5nCTS1l7ppYTJ0E7I6Y62d4jqbuy6vtt+5o7HNDz3c52LWZuQWfr6kXTXPdvtXo9zhHaOROL7q/ghu2eVNncfCQO3uXG1zQwc/Y/kJDW/HeK8eYbj/kv/rMLvvmTL/i7//gTPn3Rune6ezh42kMiZT9Illq9BRRvVtSSi2kCrpgMiZQSsRVqbWSUq91EOAl0hzZZVzCjWKChjGMCM09KlohF9wtoqYfMmBC7R4QE8fvJlKL+OnOHOsXPJFTd6vLRpSv0Hs7aH7Sgquvk6HE1fc/Y1TLiI7nmyCYIA+o2tsG12neHwn7vy6S5qvuKThFrpfcTRsVHegWsNmpxPKvhmGqKkVWN2/sFHRK7FBmGzP1x5bgEXp2UpQoWlHEIZODpfmA3BF7eV+6W4mNQDERCH9Mbm2FkolIND986FtpxIY8Zkw58aw/jCs6VpKlbGfbiIALWjGl0FZktM5uc3VbQPCohCWxzZgwNLYamyLJ0U4ngURhjHNinjNwekZcNuwzc7l+RNPM4TDAr88vG2g4MlxM3NydevnjJ1aOR995/7CYTRMbBnfRLWRwbTAF7c0t4Y9f9ExqclPXzmeOv3lM+WzldHXjrP/4Q+/pIzCNxWLGgtNYfeRNCEFLKHE/VmR0EN6rQStOMtkAgcToU5ruClsSqgS9X5Y8PB1ZJlGDsBS6mQJHAq07iX5pfA+1qlBTcWMUNQYx2Lg3VyCk55UacoF0WLxLVDFEld6eqIB2T7pJQZ5YYz19VDjqRRyE2we0yXm+3Qw9pO5vc+HLG4YfQm4DQlKqVpu6U1dTNk6H/txpJjTRm0vaCVmaohsbUYza6qxSd+H9mADQ4A8kPxirmO4VzTIj1Qutepl0+6nso5DwqN//JHp/jNCG1/vdwiKpJdKm1BZAVobooJkQ0dRpYEojNG+EUvMmYJqxWQjHIYy/e8lpRZoC6G1MaAj/304/56vsD/+i//Yx/860jo4zcn049rNEL+lp90Za7iUsVl17nznqoEmgm3B9nd8IiUGflbj4yibEZJno0HHk7UdeVUhtTGMDUpefdXT/Q+ad1dVvJ4OeBZe/io0RsKYg4BCEhYn3xXMrKGCO6FMfxf1ge6tn7svXlEtB9bbyormZUg70EBmmczh++OQF7rI6dluYjcZPIdjeQRTjcnSgNCJ7b4jQgRS0h6lnbtaxUbTBk8jBQNJBNqAZ3p5mqkKMxDWevTSOosB4XVF3VNIgrG0y1L7wcq1OFcQgMg8EauDvB8VgIofHm5YZtFDYXIxgcX97RTkomdmcpJQch9+wnLFLUWBfn0O7GhIXAujh/c26VtSoxbSiHIzEl8pDdPSqBpgAXA+vpHknNu/coiDa+/OMbvvdbz/nK19/iixev0DHyI197kw8+fMzucmDWhaL3yPgIKdFn3ibY+FrCqxhSlTKvaAE7GlON1LsT630hYRATMQ7MywHRSKhCO60QBrSBFb9BYxQ2m4Hl5JANJqxL43RaWVvkdq28XBZuWuNobrMn1tx8Ysh8fn/ielVqc+jEzPHCKDCGwCYIM/p995KzLcpS6FRDjotb2UWDscfJjMHzxXwr7im5aym+aAjQSibE7OIS65r2pp1i4/d0EGjND5FzVpNLmAMe/+0eCWagrS/irCe6AqnvkkSMcZdIw4hWfa2TD4A4Rmu19PHUq6J45jqmBawiZKhOj/MN+9lYHcdOXRHg01zPQTODWhtt9c+rLR44Jz3GoakRs3q6gQkq1bFrKYjULnAJWIA4+oJGRDGphFWRAZfV9vh2DxYYOhTQvyDODiGMPHp2wX/6n4y88yuf8N//yitqU8YhMwTPezvMK1qtb/v74doa0lxYMlc4tUJRv05ikJEuH+/iIG0EGs+eXHJ/axxmZTFlk6JHGOEGPOOUWFdjXRq19iaRSKs9cbezLtz8qfPazU/oaE6pmg8rtPjgdfADF1Rr6hLRmGja3WUAcG6o23QYoxlDM16ZUhZYi3dyO7UeYOddgvNF/eKMKZFS4LQ6uJ66kUQIgtZGiX6CCcIYE2txWeS6+lILEy43iU2G/T7QFtdgNwncr8Z9c0tAKl16quxzYEwepXFSp/7EmCihcn04cjLh8aMNa6lMYhxOJ9cYV3MHqardQNodrb68WVlWqMU75Mf7iSEF5la5Wwt0P2FtRo6uoZbYx8gul5suInE3IRvBHg/IxcglAzkMCBnJxlsfbAntnjffviI/2/uRJpVFK4f1MyQFpD3l9k8K3/tXnxBj5mu/+Ab56Qbf8jrdyXLm4qM32fyPR+onR2oS5IMNmG98JY80OWL3J+zLI+W4suyhNoHo2GNMynaTPNdLXTU1HytzE2608XxdOBSlEhiCe89OObDfjNyXxqu5MasPt9agmjhTQzx9dozeWTULfeML51V5iqEb8EAMgqBMMbOfBqgVV+YJoj5Wp5wpzZ30Ly/g2WXGWqGyYNV6ZHgPvbOGBuk+DkYIXiQtRrStD0ubEALB3NbND2YvbCLR3cZS8EXTsrCZoo/erWKxR6bH0CWe7lvh3WPFpHnlD329bMUbVUJfnmnHMgM0J/W36tMEuKfscl9pB9wlrXbvhRghnLFFw4p3YNbt6taV7l7v1632XyOtOvcXnM1RZ2LFJ6scPe8tDlhwVZb1ZZjkEUJvu1pkGhJ/7W98hMQN/+SfftdZGtpoq0MVcylo7SotXLDhablwN1dmhCFnqjVqrWyjMMXYJ5Hvo7ZN8Chv4NqYD4W1KZtt6nQ6P7hXq960NeupCH4QaTPWtlLUXKWonmmXxql7eneYBT9I63r2SPhBC6r0iAUJRPFE0ao+FvimWjr/C96ukS8786IYDF2ZUauiVViDP5hFxeNRNh5zEnscguciWV9oADhVSHBAPcaMojRxHCqGwCZlLrJv2s3kAWCvFpnXkz8MZu59GBrSu0IV/521uarLauNyiFxG4cN3LoihshzvHctKGZkGdK7kMSLqNoGFwIvT6vaGBlNrjEPg0S5RF2EaEmPOrGtlEUG6c42qd7RWG1kiMbj/YxsDDG7dt0lbJA60U/NiW5Tdsx1cbplvjty/OPLk/R2WCiEN7DdP0XniD//5t7n73SNyNfDezz9meHzWkTvNi5x9cfJzO7Y/2TgsK7oYtXnn3Vojhm6SvTT0fkVzZrffMYeVVk5IDIw5MGS32VuWympQzBeRYUgEa4jzy8haucxbTmvl5bGQUyZ1hsapejEydbpaVeMo7kdpnZ6nfasegoC5ZNd6jEhAaE05tcU5iIIvgvDO2Qw3okZ5/63AxQYwZ6uEPGBW3VTHXvudnk0JpLvpGzi9SCIJdYzRZxTn2CsP3x/FnHWAIatTAHMcaOsRYiDk/lSagHUtfe/M6OpBB/n6Jr3LvX0mEmiOZ2iXJq+zYjMsdzP11LpG3hVnMQdiVFKK/SA3CK50kiBYz6fS5gtGq/3gbS6+iTnTOiHf+ogsrTegqxKGLt+1zs7JA5biw2cpaecKq1bIMfOX/+ozrq8P/OZvHihauJ+dAukjX0TFKOaWlfSDVKR7PTRXE+aYSMHFB8UCQ0q405VyOq48udqz2RS0T2lpGElT5P4wU5bK9c1MCyOqq4tIzHPgDC+iOWVC7JlXlH4fdnjPhFp6zPmDdeMPWFCDuazRcGA24guLZo6mNm00VXYp8oYIYoViySlNSR62bGoe9Xp3WNEasE0iB38galXm1bGKYchehNWXSgR/YNa1UVSJWUgp+RgpsM4rxypYaIScvbNpxry6c3yOkYiyyYldHljqwrE/xGpKtUK1TDDl0T47RUcXtruR3e6S07wAmRknbS8V5tLY1ILSeiKs38Qnazw/LKQx8/J+Za2NTUpkcYjhvNGVFEDds7TUlflYuL85Mh+Vp88ufHQTf9/3t/fc1zsePbug7hKsynd/4wUSA8/ev2CTt1wOjxHJzMfqB92l8OZPX7J5tuVspmUqNBLa/RdEXAGUFY6ne+pd4Xd++0t+6w9v+ca7G95/I9H2GdFKSsbTiz2Haeb+5eqdVe9aNmMmJKEcC1ICQxWuutrsdl2JBMbgKqTnhwW1yIaOA8aAiReBnBKbKVPL2hehwtkA2YMfgydSNpe/ShC0+vto5hi9SfDQRtPeBVXoB28aI+8+TQx4MbUWUemcyl6/zhHIcnYt6xQh6fehy/caAaUZJBlZa6Wsa6dkChoBccigFmUNFRkg54QuBRt8Yy7WJah21tI7z9Qq3k1C30QHmnVZrH+x3+OR5V5Zbio6u8NZyoEwBVIWjwaSgEj1jDWfb10iG84qIety0gTqXZ6d2VGafdmG9c7dEBKtuHetc8QrLIVuvuBNTciQFJkyMCOxdgWXMOTC3/pbH3D98tv8wXcaJfCA9QZTV0GK+4bEjmWn4GZLxbzREHG5MWrEcwpALYgKty+OyKmRspBxNWOtDSu+JC5FcR2nN4KCm7Hb4NBYiqEnBRix+mLOgk9EQuj7Po9LfIie+UELqnOz8MWU2IPBNJ2U30SYtbJTz5ISM5o576w9GLq6a1FT9wefVweaxuzmD7Upy9pIIRHUx7zWjSRiytwvCwU8wiCKLy6Sn9i1OGE3yFmn3C3vsi97Uk6UUqitESbY5cRQ4f5Y+yEhzqVLkWmakARzUertkasnW3YXG8paWQ4zacwsa+NU4VKd37pJAX0IWVPWqrw6rCxVKWsjWUSCG4pYd5PyraNvkqcp8e5HT/j1X/u3WBv427/8TWI2osLt9UukNJ4Oe0YZ3MEcfy2PP3iGBshxyxBG1toIg/LRX3sLWmD/zpaw8a10A+5b4VAKSQKXOZPNQAIhOZ3kT779gv/Lf/MHfHKt/MGzzH/+H/0ou/2ONGX/7IFpiNg0+CY4SDdPiWw3o2ObCi0DVdlJoCCMEqkqfHJzz6LCJsvDQtJv0jOdyKhrBTNPSRU580EI0aNa1qqsRcmj05DU3M+2e1n0Gx0vUBJ9uabd7T01nl6MvvGunk1V+/JUxL/vwSC/y0Fb8xRfwTsi320oTStBBlpbMXUalxA7LuuMhXZOmI1Gyh7n4gAhiPpm2qw426O55DX0FNDzksc3tfbAvXUcWzjerpR7cx7lEBm2gZQ8zSAE8wIa3cNVQvJFcm+M6TBJCNppV+bjuQViv44OR4UuA9eO8eL+FHSbxhZ8yrLO2W0FiRCyE/CtVaQ1GDNEh1QkZvYXjV/6pacc/mHhjz6ZWRG0uWJNuz1nqA2pHQsX/2CWXhQD5pixNd7JG3JTqrqbP0EoTZmXSlk8YPI0N+pcWBvMxwUlOXRgnSUyRGcddLFRw6djFV+KSgAtbr5iUbAMLUD8c8rmnx/S15QUXm+0TMKZid3Jen0zKa4esjPpT6QTaLUvA/0mSTmBNVZ1TNI459b7j2zVTyptTn3JOYHNiAWXr3b02In+boYSUTZDIoh3ixbc5SiP0ZcIFphnZVkDKSjBhG0eOdTS233vIOviem8JghK5uVvYbhKbzcjwRub6+kQ5wWJ+SgVx9dba45CF4LZvpgwBximxH0dqqZz61jnHkdbcOEU2kb/41z/ivY/e4Hf+7nf5ylc+QKNxbCvLzcp2FuS7R9bbO3Q7Mn3tknYVeOsnn7K72qFhddcvVWqdUZTtG1tyHojf5z9bzLgulYMaqVXGEImdDywhMm52jNMtcRCqNL48wLys7KZM6xESWj3BMiWQnq+0nirz0rjcDWw3E4eyeOyvCLkZV2kgVuN7rxYOzXX7zehwTt/Gi49PCqxVGaLfS81eZ7m31miCm4s0er5YLwgP05L0PCTtibDq42vfkWw2gcd7+kMSujox9c2vdoJ86EFy2u/pAD32R0Lq2LffLaqJkDwKw6KLEMz80JQkPc2ge0msShgCeTdhJ7cXbPjvtFqhx7xYc9sf70o7FGKK1oA2Yb4rLK8WWF0htN0NxAwhebaWNxreEErCH9K+rBJ/TN10GodGSMEt/0L0rlldIi7V3CuBTjZo1lVk9Owv9zluTvkAlKCFoGfIDbDqeVitICNYNOez5pEPv5b4X/wXI//Dr37Br/3akRevZue2h/jAukjBl0U+1QVO6+LwA0JtHgk/V7eyVG0PbqEhnMMy/fUva/UmrihrBZLzwa373UrySVWbYjG5R+q8dlWbu2KV6pN0SC5gKLUh51P8By2otd9wznnuMi7pKTS9SDZVVyVJZkeg9HHaxMmxkUBTxyNEYDNlrLp8VLW7CEWj1UZRcRtAnD4R8YcsRg/qUmusxTeBc1UkZsYhUZsRU6HpyPX9LWuDS4VIo9XKXIW5rL7RrCtX2w1j9Ahit6KDmCN5EzkuJ04nOL448OaTHeuucrHfsbnKTFUY55UoSpBItMYQYFG/Phac/J7EKRpVGzEYCXFnIinkATZp4MnTkfe+8gb7ix1/89//BleP9khqLMcFM0hjIrw1kb++hQSVQJiVp2+NyBhoJMwq8+LKrNYKIXtwm480PjZLp6D58yWInh10HKmKMfHu+4/4D37xTf7pP3vJR28PPH3zgtBW6PEuTd0BPYxO4N5MkcPcebwTDGNm2lSOpSJpIIWVHBKWFImLF3hRjy3pdDR/Rn3kFryRiaF3d2fDGPz+EnPXosXKQ+fkrmXOQRVz/X4I/r5rl3c6mV3ZD5EUhdZWBEMlESpoDgQJhHDuPL2AeDLpuWX1RkHVxScBZy1EiTQrnjJhybu3ICy1EUOCJtTVus9AZdgMnmfU/J4LkvzBV3twJvPlkpPFmvqmeTkop9sTLMKQInFr5K1bIcYEeXRz7JDcZi8kl6WeYQwhOU4Z7XUxDT5VOeEfpwX2iildHRFNPN689O07nnJLrwe11Y4tdqhADGm1Y44RuuILWbEB3IR8hZh4/GTDL/+dd/nm12f+4T/5hG99+46TRZbmTA5Vd5paauNQGyk6h71VL2RjwEMTQ6CtAIHSVmY6MT/AqboHhFnf85n/WVNjENgkiKaUuRFzZJhyT/RwgYCZCzMUIYgSRyEMPSrnz/ZG+XdIT4M714d+EZ2y33Up3cc0pAjRmBrsBF7SaCa0boLS47F8dKnK/moDLVJqYS7GWlyPr2aeeJgT0tzJvzXPIZpbobXKJkf2Y+5dpF/sYsImJKxVV1yF0BcPkePamKtQCS4SUWWbfFSJUfrI5h9SrZUxZEJ2ocEYB07XC1IGcoL9kz1vAJ9/eQCEeS7spg0qhSyR41JYa/NPL7siZm3K0LHT2LHUq8c7pnFwaWdzzPiDr7yFECgYooGQMjUIKY606B21Xlfy7zyHMdF+KhGmiVKM25sXXO2fcLy7Z3iUunS2Eb2MOzc3Jna4yba7oCtqK0UXmhbiNPCLf/XH+MZXv2CTRoaw4XRQSllZaqHezax3K2njcsIxJ1JQ7k+VNBjPpkAeBpZy8ugIBC0LKY+8+9Ylw93CF9fH3sGEBz27muvZpUNLAe9OFPelNAA1mnZrNXGnHxNfF/mZrj4lC31E9AidZgYkppjYpQ4n1dZdhKyHAbqAQKVDW1EI4ombGN4Jc8Yaw8PvjO7KQ4yuORcMVCmrZ5ah3Q9AnQWzLitjFdJ+pN4bUgpFl27iHSiloOoOUGqCVaEclPvrI9ogJ4e50iYwbCIp0/0uAmkMIJUgq9N5UgJRN1AJLt30JNbOWjgv3uDBf8HdhrzoSDKnbzUj5ATZN+JyngSasxFcG3le2ynaql8Hx7i6TPncxVr/bKK30AECI1/9MeF//e77/Nq/uuYf/A9fcPdKmFenvRVgUWGuK1EyyYS5L6QvhsyY3Da0qtGiQ4uruppLzfruJ1BrpapSxCg45XI7JDYBktjrnQCQBUJPRJWQqOpNl2BYSGhMnOYFKT/kUmqutcssQ4d2+pbR6J4JytKMfcxIaMy4zOusWqg4UT91jfThuPJZ880rYv3vOt9t6KdkbeVh81rVWJpRmjHGxCbFh1HOgmMcpVTC1iV0qsaYnHpyOi6eJWM9uz1GtlnYJ2GIHePF+WVSG2trxDEx7bZsBrCpUY4LVpXrz2+Z58TdGnwMUuNwWjlVp1kszdz1vTvr5y75O64FS7lLa50S8urVgYuxopNw9/LIfg8imZaNZT2hYuQU3W7srqJS2TzbEl7dwsc3yMWO+OMR3SYXQKQMoqz3M3W3YwxAMlQajjbCNiYmt4fviw1jaTNfHD7hVE+MacuT8S2ePX5CXXoy5VmPKZ7RVT5duXg3MExbH7sFlgqfXh/ZXsRuiuOKpzyMhJA4Lis5Dzy72ECr3B5LVyT5Jrmpu6er9f/d6UQ5OU1O1M18q+/xvGsV75bohdEffE+YTdH5rFGDb2iDMhDYTh1EFGd9SPIljLYKIm4mHc5b3+rQQup2ld1848y5tS5XxaTzcruoCTquGHu3ylnqghaox4U4RpddLwVtgbWE3qEmTx9ZAsuxcrg5YNXNuodJGCc3DBpGIQ7ijIIozk6KrlUXfGlGc1zcC9kZI3ZmCSH1wmY9cbbjzlifZhzuIKfuklUhZWf6WMef6dekdUs/dWWimZsXyXmCJb7+nv6rkB1oxaK7chESu8s9f/2vDbz3wcT/8x+/4J//1g3LWjnhpjkX08Z3I6sxm2O5A0JZCnWApVRycKepWq03Zr7sjCljkvqovjC34qF7MUMKrLX6or0ZzKvzwzv7oklgrs2XoYonHUdPQ9YftqCGGLq79Vly5suYBxwrOI98QSj02AqTznk7U0Okb0X94Vy7HjwKrtOPAj1fvJnTsjREVmssZXF+qgWywHZwO7JSChFhSl4kqqv0yM3YbwZknWlVXCG0LGAw5sSUhSTu5jOOidKUWgpTCs51qw29bQ4DB4iDE/cjUNeFZe4nNEoeIrenxQnKy0xtEIYBE9cm5+yppO6t6MmfnnGvJF2p1fj8k5c8ehTZnFbs2QTJ2E1bchqJ2pjN82sygfLGluVRduJ3rYTqXfhmc+EGyHlgLY0wJMbSgOqCAcGxOaOXV4BK1cKpzhzXmXVtbOOOzeYRZT3QZuP4orCcGjL5d5xuFi7e3PuoVyCs7oL+4lh4dr/y6DKzmTbczPfMx4XdfoQg3B8XILLJI3GbKKsyl9U5n8GXem6e4Wqf6s82Gs7STHm4b1rf/J7x+hA8PQLAusVkqY2yCBYhjv4k11ppLVCXLoOswpDFMeiOffZ1AIgT9j0e5CwsOC8O6JQnpVQ3azFCZyTYQ0OQQnRmR/CDp5ZGOSoxrwwXO+pQ4dCoi1L6JLYejdN1YVkXRGDcJqZtJiUlDcaQo2fa0xk0/jg5JIEzHszOQEcvlKHnUGl/zOXsbdorXDibTePdbdC+aDo/7x6vQqleGFMHalSdOWCLC34c+HYfgL7rOrMlJMbuGqeIdkqZJFdfScOkEaPxo195wv/uv9zxs/+/l/zj/+8LvjjBbME9mfGF0+OWGILwaIqMOfRgUNhP7ufhO3qHtFLMSErUvu1vuqF2ssZmTAyxkVL3uu0nYowB+2BLrUqNiUUbYkakMQyZnDPzvKW0H7Kg7vDQvRp9+16hj6U+Rkins8zN1QnnUDzp1AKPPQ7uuh78hkghkxPuH6n+wQ0psN16sujtYWXp9LZpGGg0DrN3qs9vZzYpMqXEIMqQI0trnEojheBy1eR3SLPEYS5UC65oKpWQhk6wN3IeOdWFY2kMQVhXl6nmIbCdMq3UnjDQQ9zMPF8NIQS42I4sJ1eA7XJiSB4mmCNMKdFMWXohEAI3twv7YcsmJU96rcLHHz/ng3cfkb48ErfP2D7bY9po5Q6t8OrFHW0x3norkq42yF99n/U0U1KjnY7EnD12GSHsMrUWwl0kfOsl3K7Y1x4j72+xVNAQ3NOgc4dTjGynkdoqiclvBYFxDNx/OvN7v/oJSOTDn33K9mpL/GokPRmQGAkLbArsU+JzrRzmxnYKbgSdMuVUOd7PTJsJs8DxtJDy4OT9BLl5V7WI05NSdJ6pNk83OJPlm3nBcP21Y9EJpxK97q6cMB+DMydi9viU0gxWI6TUOaMR53P3AzJ6MmrAD2xwOpZ7KvcxH3o358tQEaNJN3jrcKQQ4Dw1dR6oNt8kRwskBMuBxYxcoS2VNI2UwwldhdNN4frFkba6qCWNPtZPm8iQhZgTIRWXhcUIJCR4OqunGDRMa99rhIcIkwe3fVEvXqGntlrsz6a4qUqKTnlqzR22+vv3xYJDAZKya/trweNbEqLRl7gWUa0uFnhwTfHCrs0IzeEGkwZpRmRwtkPXXLq73IAB+82WX/r3Ej/7U5c8vwkc66Zv3PPDkkykR71YdFd9Okh6/iweqHDf14GbOc1RwEg9nLN1Sb71g8gnEeeNOeRgpg+euee9gz28vx+ioOa+iIoGpdMsSsdk3dwk9LHGSGlgq54SGk0oqtydTuyy0HqXGwww7wi1KQ9UEhFPL+zgsarnveQYyTSSKDXHvln0pVdT86C2aJhEh05NsTBiMbrErDWGnNgNiVIXVJXTrKScub1dONRKGuDxlGFewCIt+lYXqazVc41EpRuqOFBdihOlWytoy6zFdc+blB3vEnVnbyqluZzttFSsmW8Vc0Jb4+XtyovbwrSLbKaR43KHtcZ+2tMQHj+99EwdW5nyiD2+YHw0EoB5KZSy0porv3IOrKcT9fMjw28/h9WZEfbmhOWAET0I0NyTcwg73ti+w2VckBYRRjfekMRuv+HZ5cQff+eaetoz7TeMH2bn3DZoWtjsN2zCymZeSQGOp8L1rVKTMYTEvK6UthCCx/6e1pk8JHIOjNPgmVNDpJVGwsjiNIIi55gNv3Gtu0mF1ZdPm03mWHzheYYKRL3YqTbGIZE2wjwbWGJtjeu74P6ok0crj1Qspgd+5pnW52NrcCFBJ/Z7lLSrlbR3LIHXHEXVitC6F4Av0mIIvlsw69JtI2mkrkZKjTRFJI88/+5n3DyvvlzdZsatMIzmDIIspMHJ/kFi754bMUWE0pdB0UPqDExey8PRc4fpqcUh5v517xDpB6vQMVBJeCx25WwibVpBpTvi985Yuj+qnb/uWLdY7Asp69Q0BZEeQ5IwxyXopGhnT2gEydBK92gVoBJi4PGjDRcXgftj4PoeliqoRtzloy8tDZxvzINnwXkf4nuE86LNO2rFZfKqhrhJ6oOs2XCowOFgp+JZ901wkYgvQtsZovxh/VDVnDyr4sVwECF1eWfDt9gIrOYGFYMZud9U1WBeG2N0SVrtQmzV0PO+HGiX4PSs64PfmI59C636z53X6mC5Ly7dAixFpjGRBWaDon6jrdpYS+l2cMKT/UQT9xPIeMzwzVqIzVkETZ3km6cRmytSjEhgOZzIQyCOI0vpmmvrfpkhspTqElRxo5A1B05VOcyeeZPFw8hCDE4E1+4SbkJdG9LNl+e68tlnX/LeL3zELCv3xxueXr7vxtIy8+TplW8e+7FFdEPe2hopu4S11pVWXCIsBMrhgK0F1UgaBAvRu4i68uL5gbXAxdXEMMEYdqQ4oqLdIcuQV5Xxtz7jq4sSHk/kyeN6rQh39zNJhHwxujFvWwg0tpuRj7+4465Y9/cMLCpYLeRohJjQVjguFanCmAJTjH4Nla6Prw/+CHRxh2/bozNJ1An1oaz+wIj2Jio4X1SctaAKeRhI6rSu2YTPbldeHif2sRJypGkkVUNyX7Ceu54udlVPvet4oHRypkAawHxCk6Z9GeUdjXQeqq8ZtFO46NLNvqBqSmsKtaA1Mh+FabtFsjFuIuNGyWPo95kv7EJwt7YQhBD7axTD833VBQCAhYFmbkUn3YrOPTcStNQ7F32QbWO4pFUNcuyvuT9kAUJ3D7Eul0YrSEYGny5drOYsAjdGCQ+/Q86/q7vJaXWPDAmjR66E5BBU6w+Cvb4HRByuSZK52lWkGv/6tz7h+rZ2BoDvAM71z3qXeu4ZHZ+3hwWiF0M/fM62fWLa2R6uRLM+6UiHKc8qO+cgNzRoL77SG74/u2b+uQW1qaHip6yIdwrRjBzEN/n9Zm9AVOXdnLhpjS87H62Yv+HQ2+oz3nKOF9Hmjv/Su94HvhznOILGsRhLgxR9TBWJFFxJNY2JIMLxcPKfV71rbaX1SJ5KGpLjtVGQlLkvynEp7KeJMXVHq1IZYsATqZ1oPR8Wthc7cvIIiO4eR06B03zi8uKCnBNWhF0UNimiRObamNdKa8aQvAuarRBCf0AtcDzOXGwmxjGzLitLhNQqg4xs4uDws3SbQ1O0BeZ1hgjH+oKlzkSZCDJRLbKuRi0zcRgYH20pbx1hM5F/7BF1FEJp/Na//JS/9w/+DYdZ+Q9+8ev8zb/+AXlyLWFKmVA92Xb54gQfn9guwntXI4cYOJ2MV5/eM2thd5lhm7BgxFh589kFL24XvvNqBgYGo+NjzqcstTEN6SEvShGOa+NkHt/sbrPKdkjul4tv10MQlz2aOc5oRsieiqtm3bruNSWst2TU2tgMmcvtSK0HltK4K5FPXinv7gJJBU19EVark/ujTzi+F+1YoGrncQaCORtEq77uaszvU0mR2pV+VMcnQ/T3bmbEISOhy9vFIARqhZuX90Bg3Ebi0Jg2iTwZMXUGCuqLIRE3J+8O+wLeYQZ/PkP0ZVDom3iV7tAlDrmpqiuWOhMB+75l27kMaenFUCBY5976s2rtvFTqPFlK542bMwoMN88eghdXNRi7kUsPHHz4jIofXA9x1Q+jsy/T5Dyxnn9PCFxeNX7mm5f88Sczrw5gFl1pVhsWUxct9QoXzovt/l7M/0zNqWk+dPjvPSfKyvelKoAvtqVjwWr68BqdtdTH/4cR6gcsqEHOhZSHDqAf6D0LqgO55ty/Zw1+QRLfDcr3WuUW3/wmhNrxMjHHpFxr7K5VZ9EGeHtOCB0/A6NjX7WiFogxslTlbjmRY2A7uv2XwyjOQKjNz+9ixrpUtxTsTjopedxsTgId+3EBgJOtPY7Kcc86e+6UW8bFfk82JLn13Iv7hUgiBWFITgdaVs+8ss5USCEwRSFmIY/Kj371MW893XP1eCSPA+OYybcH0MawHbFdJVkgx8yr62s++f0XXDy54tF7ERuceXFod8SwsMtCznvWeeWzlzfMB9h//QPiv7chTpm68Rv67rDw3/5/fpd//fEdYoGL3/gj/vLPv8M4DpzziEwjKoV4GdALQasybAZOMdHCjKXK4+0G6XAaEhnHLbcvj/zJF0cOq3+eKn6zRwJKIHcTDyS42UZ3cW+145HmxsGe1egHXFNjLW4OHVKk0gg5MQw90rh1/NIAbZ0jmfqYGaA2NlPk6cXEq7sDKQkv7hurZvfujH6wmThlRoI9jP9BcjdO8RA7sfq6E+qdqvSONAZneQSxXuT7PYx3eNIlxOcmIWUP1ju8bDz/oyM5uC9o3gTyaKQhPNDIQl8cnRfAoTNTQjynm9LNWc5OVu5TG6J1c2lfqoXYDdbNzWCsN0genim9E/dn2Dq3HHFoKkiXoKrjqQ9iB+3Y55kVMAwOQ5t/Bmbl4ZDTUnhgHYt1p6s+tndLPrRxjnr2AnPupL0du7yc+FrM/MG3bzis0SmYURydoD04b7l0Vh6kYWdk1RvQ89KuL97wiHp6MfX3ZK9TX/1v+ve2blZ+LqQ/bIcaO/KuHW2hd5wuSQsP9JHWwWVVY8D4Roh8PQ38Tlu5Ns/xsSbUYK9NgDV4uJ549HTrlmQpuikK+IZ8UeNYO1dVzDEX61iNGBOOWYl5qmltoATnT0p0BkAxaEbsgoDzCRb72JlToLXSsSVv7QV70AnL4A95a+YZPmZodsMIRdAg3C+usIhDZFAfxVqtWPJN7y4nfv4n3+dnvvkmOQ2E2sibiN42xn/xGUEb6WceUcRYy0wpkevv3vKdX32FXBz5hV/+Gru308MoZ80YGBnyQNgn9vcn7l/eUgNwMSAy0iygWmh2ombXJ4+x8cFHl2z2rixRFVqbqbPn5rTLifYTT1gOM3oxEaeB2FauLvYOu0xnzTw8v134oy8O3JcuRw/Gos3H+ODc0dalfq0XJbezs/67PQ6j4UYpGozcR7q1uaQz4o7tnKlu1kc28eWgTyLaO2YIEliqUg8nhpx499EFEhP38+KmPdK7E/NpJwZzVgcg4rEmZ1asCP1e7MbPfRxutXZM1Z+JID0NtxPJrUcDkVybHprRkqCSgMzzz6/RJoxTdSORGLBYvTgFcavAjkF6++kF4SyycUVo/zogEkiiXUgo3SYwQPN7Vrrqilbc4zY6V/RP/WPmS8smkLPDAWfuKN6leDigP/OOY/aTLeXOoe1LLm24oIDenfYjKRpmxRVZ4cxjld41h9eFWuLrqiUCIbPfGW892/PH3509xEDMUwfkXDnjgyjA385r1oOpHxDn5ZKd8eNOgZTz++8drDWfkFsXGHgKq7N7/LIH/qx//p2pp+A6Zq/Y/h/ea/rF9EHgjLXC2hqhCXsxPpLE7xscu1qIzomLIbl8rPmTZmKdz+a/zy+uuLmO+Bb4vFONQTvs4qdHqUYVHwmTQVLX5VZgVbdzq+oXbHz4KcZhLoxR+mLBvyda8pOcQEJdhXOOqgjeSU/DwFErQ4pUiUhR9lPmlSbcjNhleUmEIWbXH8fKT//4m3zz628TM7RXMy/+9XPGd54QtoW8LuyfXKJvbv0F5kxoxrOvPeGnf3lktcLFmxPDJjHYwE47fSkIaCTQuMhb7qYbWl0xy1govv0HNtsd/+n/6Cf56vsfc7Hf8PM//SHDNDo2uxbm6xPHu9lddhTaRaRtNxADtRT+6I9fsv0s8M47ifTogntWjs3AEsfSKBYx6eoaPW/KxR/CgBth0F2SOuXE6bOG9Bt30YrFQOn4tt/bTokLMVJacXZCclf2SCB2il4kELSzTFBqc1xNaB7EaIXb1qiS3IYxVlJXDcXggockPrGcPQRUe+x38EfEzh6kBF/OJOinNzE69OVr4tCNXwzVFQmNWr34u+y4crh2BR1ZCMl6M9bd+fGoFde3V6SP+2LefZelIVG60rCPizRojrt6SmcvqAJqK1H9sFD8GbPmHV0w8eWMdgigx6049TE+BPOJjF5Mm1MQ3auj+OKuBV9oERBbOhXrNXRAHkCjn3wCTt1yeABaRxqMB66sRLDi3xPOHSsgxpPLkU/iPafyms3gI4bX33aO4O6Tut9D59Rml2mbgvb0hYdC2uucwxf+ryk9iv71VOI7JTsjCz94QT1X1e9PagQeyLshdB2/Ou6YgtBipNTGqpGnIfJWbXynZ0wl8U7zvFml46oPFV/wbZu4Vl47ANxvcVrzNj31Tb8ILEDAAe1QIzlnijZUugqk+WraPfvPB14kS2Qtq8vRJFAIvQD7Emufs2/OtZCyMAyR0BSRkbu7SsVpPRF3S4pBWcoM0S353n5vy9fe2/HG1ZbHV1s+ePqEsBrzPQQtLGPl2//qD9n9yCXTh3vafM/ldz7l8dMrNtuBPCR2+4nNT+yArhbsstZmgduXlesXJ6ZhhFRdQJEDh/mOCyY362j0sS/xIz/6Hl/76rsOcyB4+GGlfu/AZ7/ybU5LZfcTz2hXHqTmz2rj25/M/Pf/+kt+Yrvj7fEZMUTKGrEcyKM8hKGZ9PFTFQtuAnzekNf6mvvoz4F/vuHhZvZRX3p3QfAljPXJp1qjtcqQBqeJWY9EVu+qJDju6YXYC4MTvJ2/CML+UWDKmRDWXiwiQUJfhPSuReQ1rCUBsdB9Q0PnVsaOTdrDc2zqoo61wkPkcv8eXyyN7hxjBW2V9RhYT92EXBxiijn2aacRmoEG2nkyjHJ+MPzvnyPT1bpCNnaWkyvLnHF2vs6OR7ZuwQmuKgqdzmidNCrRnznf6p+n0fO/ihXnxlqtnczfCKZIHPz57OP92ZbQFz3959CHbgGtFYkOmfnVG7xj7fi6QBcmRIToS68HpVVkmpQnjxLXn1Q3wwnnuuT/17vJzoXtbICznLiZd51mbsjS1A8Lo8M/Z4hCe4OlSi0OTUn3ZnDl3Fla+0MV1G4I3UHu7r3zUFitl3nr200xI5qP3EWNrRjPTHgVhFOAKUUeXQygjbU2dpNTgF7erR009lG+UmlRaG7VjWNJ7ru6T8KzRxccTiuvbhcfxc0eZImt+JuNYsTgWvuQ88PrrR1YHoMyt8ZaGqcSeXUy5uLmyVszBgNtzU+36s7zVZVTnSm1kq0RAyDGqjNZDFLk/ff2/OJffo+vf+0RF/uBIV4gItw9v6WdKtvLHflyIF5mdn/hCRVjmi4IdcfN/cx3P/+ccRy52LtEtaVEzondODAGqFb58ot7/qu/+5v8y997wXbY8Hf+2of81Def0rRS6uIfnYnbrpmRdADJWKiey96J1m2defm7z7n7jVtOQ2H/tbfYXuzQtjIvlXld+ezVDS9L43oSwpMtx2VliIltjOhl5tF+5Iv72sfkXkTpcBGOS4c+IjrdpOfaP0wCjm/n6BBS6qo87fhcEB/jNzk+xJrEM8YlgRT9garN+gY5gDVnQJg7GAnKW1eR/Wg+GpOcWyxCwE1PJPWaoN65+WLGJZRC6JSkPr5a6xBAVxd1KpDKuXWx19BRrSQiMNAsQxDW2tiOmTRkJBQ3AooJWB8ebAverWlde3OBY46x1z4fGf157Nr811JZ76isqfOye0EQ6e/D3Inr7KaFemAekjzSJp0tBivndqaHWnnB6ybMQXhQ6p2lnnJ+nYSH2uC11SEnZMKqIKF4JpQ5hTKk5OXInDXgP8bl4dJrdZDGW8/2/Mmnz5nXfiH6rCqcqU2vabTgqAT9a0X7FVI/PMxSV0K532qtRqtQ9ETEGKhkIpsAuTbEInU7cPfDuk2dodMOw/sniHcfQLcuk+7K7zdiFGdQrKYctXHZAj8+JObHG1KCNy5HYjKqwtCXN19cR5pFRCu77cixFBaUtVXqHqYYeXK1YZOVZ9vENA383ie3qCnrurLfeA7NJkeGJBxmyBiPNgPb7cj98UjKgZQTpsLx4A5WKSaGoty/nLlelBcF0lp4moU6NfabwKOLC1hWVM09XY/Fr03zk2utjaqBKUXeeTTwy//hj/P0DWU7Tj3qwQ2kn37w1BcNMQGBqT3mUgtmDasrlMCjiy0NYa3GaSncHmdmcUNuUUjjCBKZNo2f/uYz2lC4flVZ2onDcWE5GUEmv2EkYCuUdUGmiCVwzbdhwSitsJSVtXgx3ITENAbiBFLcuSvOwo9/+Db/sTYutolTu2PMGy53mXqqPJ4ibz2ZeHW476C/d+3nwuK0E98sn81MpG/o3SbTi3Bt6pAPUFGHAoJr1Q1X2e1TouJ0Ku0Fxvo2M4aEdDWTdktI79Dc5m9MxlffmBiCG6AkiYRoJKHjlEpTj/UIwQnvvteoHVf163nu2AL2kI4ZAh61oefgQHDJZ6/Qfrx4h2Pa41VgmRu7y0yM1tkMjdxTSN2gXwnRkROXsvZUU1OfCK2LFqxv9zGiKFCx3iWewzTPGKr17pXYO9ZuwXceq43m3apVCDgvGfGOO+Cf47mIi8NwZupF1bQbcft1MvX3K90hTohY8KUc6vCBSMKixwxZn7PFTWH70uts91j7z4HdNvL2myN/8O07mo7+urVjpTSqCc1evwbf6Luiqjb/fGp3+NLuMtaacFob98eF06pcTIGffFMYP1uoXwhLjEhKtEmQ3Njvf0geqrfa0m9U7XQeedh+BengbjfAtXPp7VvPoxkXJlweK1fPAjwaGALkYFh2xyaxwEdvTYScmaI5LWkJxMFv6hgSZV2ZhsiYc48wUX7mww3f/PAR96eFzWbkdDwy5uxZTOmS423hyZORcRf47IuRd97ccbHzG+hPvnvNx5+fePuNpzy6iHzy2T2nT5RNjlzsRuwwM4iw2Y9MuwgJNpsJmQs2RS4eXXAsC8vqPNXH24mLwfhLf+kj3npnYFnv4Nicv7f1B95ZKJEmwqkpd1oYTLgYt+RxB6q0WmhlJcTCOCWQPRKEOq+0+wNlLuT9lu32Cb/4l0b+ys+/w5fPT3z+yS2qD8gi1EZdEh//i8+5/uyGr/z0uzz98T2Wu4doJ8PTAtuv7jmt9+wv92w+3LOIoWuglUirK1f7xF/9qQ9Z5pnjaWba7YjauP9igSpsuyrMziMkDhGF6NEVpVRa9fskiL+2lLwwSMSdlppPNtapVs16yqd5XLeKg1s5Z0rvSELHa8/8xRS6BLTTlrQvXUKMbCd4etWFidFxxkTf7Jv0NADxaHO8A1b1yO/zWKzmD7WTAJNzVb2reE2v0XMBpWdbOdXvPBGf5/CYItaUda2MW8+5Cj3S2ARidhhDzWN6/KJ6gdHO/T4/h5gbv4eHR1nO24aHadLMDwq33OcBN/R9gfTiiKsBq8dLn6FLOuPlPA6ffwe9G+4j7GtY5Pw+cVn5eQFohmOweeqigAm6+ZI1V1z5+8K7/uCNh6HeZjZBJBOC8sFbe/7ksxOf37tHhp1VTmdGUD/Izjzz1x2rH45FFA1+UDUzCsYB5RCEOxM+uVu5PjW++Whi925lfvyMknZUnRlvFf7t9Q9XUIV4Rm4fPhjOI925fcXHsqr6gHkl8zGt4GFb21mQz+8Y0h7ZRzRGhujxuSm5JVbOkHuMxLBJpJycbC2GbEZiSA4naAVNXGy2hGA83u985Lm8oNXGEBMpipPSBx8Ldu9lLvYbAsbpcOTHP3jMj3xwRRQvwG892vJjH65c7veECMuyYGo+ctdKDFeEGJjXhWHcoBa4ub3lJz7aklJgGid224F33n6ERshzIt3fkp6+TwoDr63goFjjpiycEIzErj/YIkLIPt57LnijrCttPiG/8wr9g1eUdy4Jf/EdNBtJJnIeWb+4RW+Vq7dGttvM0MP9Xn5yz/d+8xOWY2TYvuDxj2yRQZAeF6PNiCkyvb/nnbe+0ou9YWvhZMr1aaaWlUmMFDJikTEPIFBr4fjxDfVGCI+V7T5yOjZo8Phyh7TZMbEej9x55OQQyCmQB5ApIE0ZVWgp+YPRlBCcalV79zek6OYbFeLgMdKtGhRIwZkiHWh3tZJ06owKFhTTxH7TGKdAi43MmQpOf8q8c/ME0tRH4l4ymvM86ZPYeSusqgQ8+0pCcOzx3Eio9aRQ8Xib4BOcU3KE+b5wOihXlyNVK8saCTmQs3MJzMwd8YEU0rlevS5k1gu0+YQUgu8QaB6yZ93ondawFPwgOy9YBRBXSwVRHGJwNsWZVwv0pdVrLPSs76eLdiRH33uouAWglg5HjB3DbSClFzF5ff2C+r1RnRPukBCdV+vvVVLEpPpmSPprDgEN4i50MXCxKbz/wSX/4Lc/5dCgxUBrUIILjOqZGCZnPrBPSypKNf8Z1heaak67Y3Jq57qurHcrv3sLf3K98Es/esVb1zcMf/wFpeJsjOHPrpn/joLqJ4x8///mDE74RXeqg/+gIMHPrXC2DBSKGovB5raxrjeM714hz5wY7i5UsMmRlD2oTYyeHYN/LfgWNfQTsFRlmWe22w0xObZ5Hv2G7fSgx661EKMvO4YoxOBUoxwDOScPr1ZhHEZSSlzst+QUsVZ5tNuxlpXdZttdg4x5XRmbGzhjwjBs8RV1JefAZsrElAhzZbhRNh++Q9rue5iYc1lFOgkbYWPKJoDUFQ3erROSdwxA05mT3dKikX9sYvPR+2wvHtGmgeV4QNd7Wshsnkx89e3HFC3MNweKVdpGqdojcQNM++hZS61T3qoRS/MOYZwgDay10KpLCYs2DmWlVY/MDjKzlpW1Roag2Fpo4pHKH771Bu99/Q0+f3nH9c09bz275GpzRWteqA5L4E8+fYU2+PC9Sy43gxfq/QYDSi3O7ZXI/f3RjWHSyKubazbjwDSNCMbxcOTi4opTKTSMu7s7ppwZQmAtrZs5n+WPg2NnIXDz8p533xiI+IY7SuimKo1zNI119yXtC9azYkgw3+TTC0zHBx2JS72A0jHF4lW4E9dNw0OkyJCFnNxLYT0Yt9cuOLl4lKh1prSR2EJfxvm4G3sSsPNQebAshG7Q08UzMTqtScx8waaddQPdcs+705icieEQRsWseEihZEyFs1GJ9hJ8ZhsAtFq6xWLozJJ+4iRxmhTWr4n6grhVPH69d5lR+3Jv7OZCOH+4Fee6EqGn1VrrKisA846dGLupsyLqE+rX3tny3ve2/KOXN9TOMlGLDzaDDzSpXtP7BemfjfZjxnHhB4esUdDLxKzCaSn8znXj/neP/K2PEl/9CefWntaB9cEm/QcuqNqLaqcKSFc0nV9kx1TPLjN9DeFttlgfx+CgrorZzonTp7ds8gXx2eCJmCl7N1Bb9yhVf9D7zQVOVwnirtkhRIaUmMaRZjNDDKxrIeWBzXbjeJPANozEYA90nZgiKUdKj5gYckZwocAmZubZhQJu0myMYXBLulYJKTBNQwfiA7VU72ZzYD7eEySSshsYj5LY/Nh7DLuR47ww3xzY7Cem3QCiRIWLjrLn2kirEoaEjP573WB4ZW03LPaC1TzOYZMvCdKQuRAsUpaZ03xit9lyc5z59Luv+PL5De998ITLy5Hd1cCzb1whCO//5FMsNu8E5hX7kwN8fkSvttgHIwwuAXalZ2QYhP1+YDnNiHoExcefrPza7z3nr37zXd64EOLFSEuJ/eMNebfh0W5E379CghJzcMMN9dHt7TcGaJWrR5eU48LhUBk2vjFPw4BaZcyJsg7UpbLZb2nvTmitpDQ4V7XtiCFxuDu6ZPXdi2767F0NAuuy+oEsmTQkmijLYccu3BNl8UJpPfjPlJbcouP88EpPn5B+CPqSS9COHYr1TbUAUh84jWesUKz1JAJ6Y+a/TyRAct+B41J4ftMoSdlfKU8ejayr06MYhSFG/HF3EUSQHkXSO9IYIho7nvtAxPessDOW+EDllLMn8WuakdnaGRTegaqUvkkPYLk7eVUazQ/4nlZgpoQhOz0x9PcsjlsK38d1DeJpqGoQk3e7Kq9d6DokQYUQkxdQCg/5VjK4mqpDGy57xW34xA8JE2EXlf/sq2/w3fXI79zBofNJz7WY3llr3yM+OHBx9uN1eEdacOpWFF+EDoF4FRktsAK/f9t4+buVX/pq5MevIFlD8rnF/AEL6msM9fUPCA9EXf/A6Bc39BPNFwGKqHcDFv1GrM0oCsNszJ8eCJuMXEZ/W7V5d9hhmdYaOeWeeOnk5NBji4NAHJJ/iOr+lTFGxiFjZXU2wDj0EasxTSPLukIw1qObQ4+7Ea2VnEearohEcjTGHqI2LwvTbucnpDbHqHobXmrt+nGhlpXNOBIiSGiM08Dm4hEhw8ffu+G/+X/8Lp9/fsv/5D/6aX7qF97GsegDSSfyTSX/4Q35usCbW9pPvUHbugYbEtPwmJQ2rLqyHhttFu7rgRgG8nYLeY++uGZ+ecvtaebLT17RBnf6NxPyNvCVv/6+d+ndxi80sM9PrL/+OXI9o1NG8xPkvScgyd2XUmBEeWN/wZozy2Lclsbvffeaf/PZia9/uLDbBOQyEy4ynkcU2KYdRVeMlSl4x2ABJAXGIVJLQeuK0Hh0deGJtGtjM0wuu9NGHgLFhIDbI1YxhsGlobUorbpvQIrucDQvC9O08Y4zCpP4gqOVxpCAnHl6OaL3lRAW5372biCYi00UQxqeVeUIFiE5oKXdfwF10re2LmCxs0OR9i4xgESsnsdieWAOxDMH10DXyrrAYYb5i5nddmK7GxknWJYGIWNSydnx9tzjmD2JwPx6SiAHHmwkrRcJtD5o+I2uPqPQ3S99ARPwPKu+FyFEPzTszKHVjowGzt5FZpUHlpCcw/KkZ0VJL9YdJjkzIRSoq9v3BU/bQASr5oY9XZjhh2HGNHH2Wu6WUh0SeL0MdA6w4Gahznv9yqPEf/n+e/y9j1/x6zc3HO11pp1r8r+P+aA9jsnOrIDvr3I8fMaCG9NwOSAhU9ORT68L//j3Cy/fGfgr7wu7P0cq9edLT7s939n04YyFvD59X/9d6cXV6Ql+KibxtMDVlDXAnRYeyUC8Vu7+6Ib9RxfIZWSaIqHbtuUccBdt6R9sl6nRQDoNBDdmcXDbL7SPVolmlbWcGPNIkEwtLtdTMyrKOE2YiG/fcQwXhDxN3cCleGxJglKUkAZKWdFWGYdMDpHjaXEStdDjPeDi6pLtxRUxRub7lf/XP/o3/NYfvOKrH13y7K0Ntc5AJsWBbEL8w+dMf3jEirvbhx+5oGy2fjMRGcKOIWzZqHKMJ3TjN2+rlbUdYVmpf3IDv39LSMIuGU+/8hbT4FrxkJUQ+2Is+jUSQI8rMjdX96lr0S27M1VZPYFVm5Hj4B2kKNEi923lnScTl/tAvVXavRI3gfW0kg6VdKxsNiPprQ3NaseBlVYhDpkwud3buihlWZAQGGLCjid3RQ+RNHhRq8uKtIyoR22o1m4a7LhaTCP///be/Ney7Lrv+6w9nHOn914NXdVzk2wOIilKYZSYsZ1AQBwI+clO4AT5H/NrECBIgASKB0SEYikyZVuWRbHZzaqeanzv3eGcvfda/mHtc6sRREzcIRAgqPMLG92s927dc87aa33XdwAYh5F5mlintRO7a2U9jpgrCxjyiLUZtBKSnCEmxCllMfoCSqxRq0fkLOOiU6qCb9DNOs/TR39dusHg3U3AI0vQSG29swze5VYVD+NDkBa5nQqTgkyBjx9V8urIB9/cIAiHQ6URsBB616dI9OKTU/a/R3Q8U4bsQiV13HAxjV4amsXkWrTjrRJ6wZBeYDqUgXd8ToBvnZg/IOZCFeva/RQSZv5uSpQuXGgEa2duLOKHTshDP2z6QYN1XHqRtzpX2PmdkcVFxrqsQptPpwa9C14MV1z+bfiYnnPkw3sr/n59wFoSf/T8GTfmJvfnkqde6I0ub+0Tt5gfqs16AT/jKo4vxyzIRWCTRk7RuH5q/KNHlScH4/ff3Hy9gvpqq/fqC3FWH+d/H8RpMgmYDPbm1JShj03BSyEhwNyEW6tchkC8UW4+uiZ9+w4tQMjmOnH8xp6KMlj005+GoV3P792kamMY0nnEVzwTPQR/IEs15joRU+pGOG4xqNqI6gopiz6OhBCQ6gXblRSVEk6AMJ1KZzU4XSUAVivlBHntL/Z6u2W9vcDEs+r3hxMffLDjt37wgO98eJ/dOCA1ktaRlLbEciLsBurlhORE+fACuxy6p0DHvcS/61qUkEYoFZ0qIQxEhMkKx6cHxqczl+9fcVgV5tOe8jTzJ3/yOf/R3/4Wd4bQnxEfXVsA7m8Jb4+k5xH94Ap78w5zMKabI//kjz/nT//sEQ/u7/jP/s4HXFw6rl1K4YfvbVl9eMHGjPbJnnQ09L1LLAkvH7+gXe/ZbNek6xVcZC4ebAljQ9vA6eXBaUphZMxDN3z2iI5aXZGEKqfTtS9j8giqvqm3wO3Nnhgjw5gRMY7TLSFmpwc1Q0+dMtUaU5sRrYRRKHMkB6dHJRFCcCewEJZwNkElu5dqVDQk9xnFOr62LF/9njjp37tp66N8tEDTTkvSngKQXS3kqxsBjeistFIoc3PXrBg4NeFXjyfCILzzzgatDd27VJKV9N1EIiXxQhWD5zqJ505JcpxVGHqB6kutEJ2VIO4h0dpifuAdm5l3qmd9fnRaVxDtNKtICE5J8nX4fHaBW5opMw9EXPBUiQOiPX9ukX0u8lRdiq7i1oOLw5O7cpn6Mm3REgfT7tcg3iGH5EX3PM8Hx0ylcJGNqxH+4O03uZsy/8Mnn/NcQEMjhsBa8GUYzmV3xkKXEkMXGvu9copV3xkZWAu0nGljolw12nXl2aHyR8++9sj/athfvs+0nIDL1NBpJ9WMUw9KyxLOemvtiphsYKEvqYBNMOS6cHx8y/jtO7RRGMSIRB+rg9G6JC+nJfgPYkyoKtPhwJh2hOwnUK0NE3U4vXkRiSGiGqjFlTN5CJhWx3xbI1jsJ7ePcK1VYh4x4DgVRHzEt04z2c8Hck6kcaC1Sm2FvN6x2u0cOy5HUGO93vB3/9b3mJ8WnvzpNc9u9rz72w+5+/3coSqh/eA+7cO7vrjIkdOxMJ9ObDaJPIxoWHHqRPYkwXN+5kL7xVPyg3vYGyt2P3iD25vPufzePe5dGr968gW3zxp/+q8+58NvPeT+m7uF++wdHIo8WCP/yXswNWwdmaPRJuWLJzf8jz/9Kz7+Ah68PPC3/9Zb3F/vuDkUTlPl/naHNCFcVzhWVpa4LcKTz19SDtdcbAZsEKw0pieFL7+45t43H5AvKnM5kWokr3oCQnfqTzkzVx9LS6m+zW0ep1Fb99PsU0CrhbkowzgSY/boCnPVzTyf/L6kyOFwIBisJKOtuN+DCZa7NJPmW2/xfHt1fhamDTe5Exq1R5XjRXuhGS1Yo0TcOm9ZcHQFVRSXqsqyXe8djzpTwKrQJnOZaDI3V6nw2adHYkjcvZeo7chsEcWlwavBFysupnIbyJhwBZVpbwL6CB69cZAUiXHlnzMIWfp6RoRzhtLi5MKr7tBqI2pFtIEeO6SnLNM46t4DiwrKO9/kXaA1L9AhYLWcx2cv+F1okKJLzRGfnNoEOCvBU1Ktq58WRVLHn9ULqOCdmah5tw7IAJsxUg+Fn1xeEB8Y/93Hj3iRIz95b8t//sG7DP0dd0exAOZpEY7+LKWVDlv0ydv8s/s/ho69O4wo9rWXUl9ZQC2gdmu9LV6kep6OemyVJkLu6hdXToB2nly/n0wSODRPvhwkMX85c7iYWb87+uiiMyQ5W6n5jaOPVobWGYBxM5yleiENtNYYxoGQoLVKzoPjVrVwOBzIq8wYN0xTI4WE008Mq5W5f0Gxj4om6gmZElhMJlSNmLNLGYPLOcfVmvXlHY61kKwyhNhHypnT88K//sNf0h4lNBgX7+y4YgfWiGrUENDRePLlgZ/+0Sf8xV9+SZ0a//C//G1+9Dvvc2jKbZ0QhWFutKDkcUQUTv/HR6x/5330u+8xbxP7a4VxZHfnDf75v/4FXxwas6nHr5jnYHUAyU/r7Yq6gmYFqwXESEPkapsYYmWzyqxW7rr0/OmB/cHtCNcpsXpjJKzuIDXw7Nktty8K25ydhaGV0GATI9enys/++cc8/PCKt+9uSa0gkohDoEwFq0psrnCbZ3eWGoYRpH9uDOsKpTysGMc1pc4gnq+k8+RLB01+mJqy3e6IKTPvnZRGhDkYMoQ+Jrr7l/thRkKSnsduRInejfXFnC3hdgZG7/i+OrpCxyw5KwYlmCeFNtBgtNilk6buFRBgrtYLu7/gKUVaE778cqLUwO5u6JE+hdqg1cSQhCEHcnZup5xOhOQFPEYfxVlGeIE602GHioVuKhMjMWco5UxYOLdipfTOO7hpiXXtvda+d05IzL44ks4+SKOza4IXK+9iffEqVjHJHYKgG8k4A4DQI7qXxV13lWLhm0pELJ1/l/eQCz7rHh+CdHMXJYpxfweHmxl05HcfXJEl8sdPHvMPvvEu717uSHXE04A7lrsUTHtF6YN+GOEx7w4LObZsXZarwamOJn8zb+rXFtRE7JQSPyXNDA04/oCPkWbGvjWqBHcqP2/7pU8D0g/BxUIMTij75hLEqMLNpy+Q3R3aZYIQSc29XVpwuVqtnj2+qF+cJO+DWRgGf9BK58ESncvZm3k1z3dab9eeEqA+AkkArYaeio952b1Va1XXVsdEKQVTx5LLXMkhQWi0quTNmnRxwXVxE+Uhj36zgi/QyuFInRRbGxfvDrzx4ZU7us+F1o40U04n+O//pz/ln/z0BUWVN68y43blhhqzkSRQpxPXLw8c9icudxes7q2QX71k+sWnxDvvMbYVX/ziEfr2FRqNm9mFAzlHytzVR9Gc9C8R6XJaJ74LIQyEQXn7rbv813/wXX728y/51gcPefvNLfuXt7x4dsPhdmK1c1f5PA6kq4HjAcqTa8bgG+awvBPqncSKwHwqPH50w93NmtVm1ReO3b5xGNEQIGaKHgjBHb1U3Uk9xugOTqX6giRF3wqr9ZHZzZ4zgZAjIo1SKjEObC6HHrBhzOXIPCm2cSg59C5TVAgpEDtToJsXuCRXAjFk76bMUFwd5z4BcpZj2xL5I3hcjhhDiJ7BJoqHdESXRBvMpXDYN3KGmFy0kFJ0l/4Ah9uKpMS49ufNqiv7avKFXCmeChwlEjqe7DHYHim+UK6aNmQxEepuyCEEYpo7z9Xf69BFDt6BO8ZqtXgXG4AQ3TA89Lapc1FZuvMg3YovYiSCLDEoPRwwgpbZxQnSObL+oXxbb92ZqquuvPp2s+vFZMWse6ji6sQemihhAFOijKxy4XInXH/WOB0q74nw7Q/e5yEZezlTtPbPvKjpuitYl62DIwralVTWcXNZGjp17xHrIoFXc/u/Y0GV7uJkeMcofeyWLg1TxDtTgyEGumK+bw6761AvbQudJBuoZE5WSFa4CpF4MA6PD+yGHS37iNtoFAytUEths1k7l6+fJyLKOAw+LrZCXiVyip2F0PqUa97thoyWRp0LQ3bbuGEcaLPLL1fbLaFvoptOhOZbY1XHbWMcunWaY7khDqx3FzQt5JzZjTsSrZtIZ0LOXH7jkh/+/W8RJLK5GFxuO92cl2gS1gyj8sPffsjNQUlxze//nff51od33aAlJGyqTizfrHj29DltqlzsLhh/5wEhRI9aWQfalycOJ+XpaDy/LeToGVrHwwGTRNHEqVSSCNvkOUfh3Bm4moQofO97b/HtDx8gEv3w6R1ifVpIrEmXnmFP8DA6a5VsgWRGaK46UpzAHc0YLFFOQjkpsnPXr1orRnMhAwEJmTisqGXCegzzVAsikVq96MfYY3GaulPQktPeZnLHUj3ZoPizGYwxryAEclrTysGLoTg+HnsKL9Idqqxb59iybF0wPvreZsFN6HZ+XlRVQG3xFu14qeE5SNAX54GUXJNeT4lpOjIMkGK3GwxGzpCSmw0d9zO1Jmc5tNlRx6i0URhXmWgdtkjL4eBLFNME0pPfhDMWjy3mm0rKyYvo7NSvmGPvxispRO9OnY5wJtQj2cfi5cctX0VzqanHXwe3tazeIRsGOiFphQwrzhb3C2No4bIG8YlURrCuxgzdNlCr/56umOooAKB98Vb77ZkJIXP3TuDLL6755PHE/nDg3ffuMK97V7sUadwlz3C/PNPeXfdzwl29Yj9frX9uv5e+T5KzZePXKqjaT99ue3DGhCS4beKxFGYzxhiIZ0eDfpMd/vW/ghnNG1UCLvurIpyasgmwJjA9P3G7jhDX2FDIqWebY+Sc/YFX862/CSl799lqJz9XL56qzfmvnYIlZy5hI9A8MXXwDaZqY9ityasVpsqhVtQa27x2ypYEmhZO0y3rleORMmR2V3fIQ2YYIzEnCgduyoF13LCR0fHbAe7eH5kBXk6cPt6TgPxb92AbkeCd9E9+7/v8+EffJgRhPa6JwUMNVWewSlys8dKa/TTD/oherBnXKx9P7wzc+f499N9cY5uKtooRmI6FOlVOKbKXwp7AEB0LuogRUb+/aurEevUKEcges1ImkMhb77zB808nDjcnLg6ehUSD6VA5HQqrGh2j6oco5j4A2jx8kQDz6cTty4mQ194VJqfAHY+TY47Rc48Eo/UIFUUIFtjvj4Tk0t0YveuV1ogpIDFRTGnNWQB5cFPo0+FIiY2Ynd0RFaaCd3H0yGWpWFueavGX+Dz5dIhn4Xmavyaece94q4XOGFgc6COg+iqeXGK3163UFqi1cJyUUpwKFqIrolJ20UnOfoiLOM3Imo+ax2NFx+BuiNbIyYiJHsGyHHqdKhW94mn1PUGM2Tvq/mKWChLmZWdGqgMxqb9Looj6tEbwMZ3W/QBa9e8bgR55olaQ0jf5tD7ad0vJXjzPdKngNeRsKwh9jK44tyy6VFd8KUVKmGVMOw1LXlnsWT8gJESaNlQ97HNImW9+cMUnv/yIJ08mQu7rJnPlIFb7eO8HqDd+ylfPTTpMaYtW9UyRoxfTPs3Y1zRH+Qr5AMydfAJCC4FjKRRVxpzIXTHlX6R/utYzfwTfWC+qBFOng7ghr7FvRo5CKsbp2ZF6NVBDJIpvT3Pycd9b7rBUbFJOHl8R5OwA76RvOW8rtXm+EOrnTDDIeXBJWnHs1XoapJZCmWbyesN8rFiZGS5WpNVA5UCtJ0RWrFc7VpsVeRgIOVJsz7PDZ6g1Vqu3QNbehHaFF49uOP7jx/CZIheZ1TfeINzpqjACZom8cs6dc20dJkkLDe2vnnGicv2y8OjxMy6v1rz1DlyJMUompsD6u29wtQpEKjoV/vDPv+RXj254750LbAUaHCecRWk2YD2psllFi6trSB6kpq1RizLNTpSvbebumyvHwYP2kzrw8mbi5c2JNG5chmi+9Kmt0sQ9aofRYDQXQMwNKUdWmxEskJKr5WqdSJK8Y5XA3K0Dm3omfOsUF+mb+TT0GJAglOI4t3QZqCcDGCaZ0gLH6QCm5JCY5m5vp+5iH0JEeeXPSrcL7LqP88LCMJr5grItzlMh9RfOiH0EdWcVHxfdJo4uhmloE7csnWGeXZ8fg5Ei5/8dkiAZME8TDqFjsilQSsOqoFmYg7oEN4nLsfvvN6mv8F2NjnMyO/WxHzoOhRnDIKQEKSm5f9iQjOTbLp9OgjtqqQoiilbrBSq4CYoJrvzrI7h4vXBfVW/5xAHpzhCwXkbUpzQLvlFq9dyISUh9OVW8cKs5VNB6kdWe4xW6N0C3Bg/9pq3X8MMfPeTly0cMaQAN/hwZQOcU9ynCyf3aa783Gg6RnzdSr/jDKue/o56VVl+joHo/2S1PxL+gKsZtKZRmrFJipaC9eDoM4hU1nJdliyOVENQYQidBmdGCd6lTDFxIoB6V+bpgybx4Bpeczt2lJ4TOnhDHKWkgMZFyoMwzdS5oFMbVqkv2GqU1RBuqlTFmapmJYUCLMZ0m0rgiroRpOjKuso8ezSB5HLGpshq3lFl49PiWHzx8m2HISGwoE7enp6g0Ljdvsh3uEmQgmi9OogjHLz4lHJRwEUnfvWC8v0VS6PuP5dX1UaKV6qNrs/OiRecDzz5+yX/7v3/Mv/xy4v07mf/m730XicKWwCYPSDLGty+QUnh4nPnBF4UcoE6V3CpZhBFllMhaAK00lHQEfnGApwfk/pr6wR00Cm1S5psTh5d75mJc3R9JQ6Y1mE8zFeF6PzFVwcblvgitiTtCiTM5LATn7uZMKQqhe9bWyOn2wGo9Om49F0IIVBFMQsdRtRdSqPVEziNqSp39OxpWI2bGNM00hXmeGccBbZVajECi1cqQhKIn9iePXpEEqoUYInS8V62g1TrMEzttqNvO4aQaVfWEnOCjoRGIPZJcgqLWXm20vXE8CwCqKlqhTO5YJsFTJqz7HEgMSHJa16IQ7H0yMTrfthxnTkfPWUoFcurPWDBPBZDBX/RedLRb7CGNJSNpEk/nnYqyXmWG5hNnytm7a1niVpyF4V638dyhWR+a3UDKUJsRyThpcoEFYi8bM1A73rkID7pev4/g2OwqLK19yvQlqi1OXX0JLiE5A8v7yw49RhbSvl+u3Hr41sB3v3f//P4TmsuQuzzdqVhwHmFZMHHvTI2Fc+z+JM0UK4Y1P5hQKPPfPPP/+g7VrMPqHewW46YosxqblFgv7ehXlVP9lFy60tA/dkKW+Gxm8YhqC1BNOWljSJFUhfnzAxK3TAJhMFJL7gQfOl8sBlKOzLWhBGJpaKwgPflTgmNiMfuXUpxWYhZ9MxmcF9tmfxFTjOwPkwP90YnLRfERMggWE0WFn//qhuMx8Hu7wZc7IdMsMOQNm3iHZDu++OQWnYV7D7asLiNJM8M3L6jayGHF5jv3KWki1gENfvqKdCFDEOaQiURqcbJ7QJCHFzx89Jx/8JOH/HCqfPTJLc9entg86QfKNpCyITkxkrj/xo77915wuR39gZhhIDIO4hhz36gGE+TTG/jzL5EjtE8PtBSZ7mVurw8cnu8hBNZXa+II6yFz/eSWZ89n0sXI9b5SLfpCKAZM3Qin+h6DopH9qTGgnA4nJPrY1vriiuoBfixadThvdiX4gqCVLvMMwjRVaqlM88khoOoHj1ZlVmWqjYC7+k+lUctMDkIKLi+em8NA1pQw9C5KXM9tYclq94UJyV/e1o3TMaFVgRCcWhdc9LDo0tUUoiHinpkxuwNXaAFrXVTSjJcvles5MSR69IlRq6IaEJzX6c3C4sTl8MKwXTGOkePtEa1GLf73XpZKIkawSps7nacXjEU6C+pFW3yh3DAm8d+X1CilMkiHK4neOYrfqyAQUva8qpAQ8+bEcKaLc1j7bsP880ryPDFC71IRhwMWgr4mxPx30NSpV92bwLT2otYIVK8t6irKkBJNOxunFf+LSp8WqIgJOWS+9e07fPrpLbXinwkvhI7T+j/6tLwYS9uCVJ6x77PctynWmjfS6vdrOn3Ngprklct9EbipjWKwi9kJsx0p9vFmcdwRznnWLEJK/Mta1A+mnkQqxoFG0cDcApfBWM0wfXogrS9gI0x4jPMmD6QhknNERKltJsTM0CkuEnA8szTKcWKzjJ+1Ya0Rx8xcG+M4MLeK5MzQNbl1nrybiJlWJ+9WxKGAIJF9EZ48K/ynf+9HrAZ8fGtKC0aOW5gG/pd/9Bf8059+SZTIf/Vf/Hv8+//BQ1qAeO+K3dWlb0yH1M2TfZPdtOvBJdJMeHlbSNfKnbtbVndHpEXKuObeJvLjWvhRWjHNM19+9pLPHj+nTBV7YOwuR4bRPUTXq8S779/l8ScvOU5bUmkMSYgWiC06/y4KzA19fkCOldZ8rJxOB043kfm2wOOZ8Lyx+cbA8N0LbC1Y8sOoNHh6fWCTM5toBO0LiRCoasxqnKpBFC4u14xjdJ5pg1Z96bBejzR5JTMutXQqHrTihUFp1G6aoc0znJy5odRSUVPyGGmTdkqL9YLsG+hTbcQIm7QsGvAiah69obV0atRy7AcsdpVeXUQW4p3UsgVH0Ta9WtYEhxBEjEp1D1P1RZUBtQk0oZTIzz878dlt5O66/8og5Chdlu2LmtjXrhIFlYA1OB5PXO1WjJdrDjd75tLpSH2aEfHncWGkAN0kqJsL5dCDFX1ydBhTaWqU5tFBJhEL3ShFu22geCSQ1tkpT1b7guorjvVmYO7v5JCzsFj+iSV6ADohrgHH980iQQy04HEoAYr6iyUGkr3I4jQ/6womh2d6GOEiU7WAkHv3qgQq6zEyDh0q+QqGbBo6jOO5ZV5DX6mndOFT9Q2cmOPQxEjO0Jr7aIT8NTHUIP7LGsJtKZzM2OXAiOvo9bx88xsczh+xGxXQTx1zaVfoIL+ob1RVhDF6hENtFQuJbUqEqXD8dE8at5S1kIfo5tXBgW6lEcS9U2MM1Np9HfH8q9Xl1rudZsQUCNm1yYfTCSMwjJmmPj75A+nbzxgyOp8Y4tAVM74JXQXhP/797zDOjad/vWf39ohIplTl9tB49Olj/vB/+5jPXwqjGP/yX33Odoxst4GQlN165RDlkMmrTAyNmDIEz2pXayCZm8cHPv5ff8n3f/wh7/7d+0h0KpZeXZGbEquRUmbzfuL+m3d49NFnPL25oSa4GwZynw7uv3HFR49e8MmTp3xj+wZxvSKqm7vE5rxhM5DdiraOyLFR3x5pFxmJxjZknv31nvlJo35ZeHB/hSVhOsHPH99w+cC4PcK9IbvowKxPet1fAaPlxsWdNXmj5y4g4NSlFAPzPHuBi8JU3C6xiRcCA0rxDqSWSs7ZcTKab8zBI3U6rhajd2GtQYqJGF37XWtjqo2VRY+9wKit+mFOdJFIXz4Y3h0vzBRCQKt2OG159rrc+cyj9Bex9UgWJWMaXZnUu9uqRpsrL6+Nn39e+WxfOZ4SD8273BggzoJM/Y3JwZkDFn00Lo1TLQRtXF6subizpZwm5tOMmq94a+vtS3DmwWJ0I3jSc8zKMMZuYuQGQ05BdHigleghgtkhFgme1hqj9SInWBjQ/vN8geQLSK3FWTDaukmN+UTYqqunekIAViA4nOJzf1sWLB1wdmmx4f/dK0joG/2Cibu+mjZfVofUIZXQub6cx/gYhd3FyOE4oS3imv5wrk2+51p+Rz8XlmZQF5+GzmSypcCay25NSMOrP/fvVlAVpijc1kprylXKPuabnQ2Fjd6t98N8wZ/lTOw3Fv6ad2egIfiY1tyfcDEfKa1RVRiCcPtyZv8ssXv/0k1JDOLcnLeXE8MY+ovmX1CIkel4YuGCam3knNBayCkyayWl0a3ILNKmE2G1IcZIXHfKVFGEJY3Rb/bu8pKchc//cuJP/tnnvPnNHbvpkkdfPOeLz/fc3CjPb/YEueTu1hdkP/3ZE/74Z08YonvFbnYDKbqr/93LzBv3M2++s+H99+6z3bo+OcXG/OWe/eMjf1k/5s731qzv+HgcYiSLLwPt6ZHyi5dcvL/jh7/7PvvjxM2Nq5kwIRRjCMJbb9/j408/Y3wSGMYHpJjRphR8u40p8taGkh+iRQlXEaKntsrpAD2ddNbCaZ4Jp8iz6yOPn56wnJEmbEcvCIbfu6C4fZyAdZNkVJhP/vBWGlrM+4gyI/iIpr34+L7LQxlTN8fxJNPYCfl9UacgLXQWCIDzbjF3AqvNE0TzauS4PzJkxWKiSfO9uHo0yqKs7vwVwHz5ItLtQEOn97ivpnSeJyFg0pBu7WxWfPkhPt5rr25NobbGaVJ+8avKfnKZ9MujMuuJWQcqiRYaGnxxxWDEUWiteNSLQTNh3xStJ3a7FbvNhs16RSmF49FtAxueskvwkThGRz3TEEmjd6guOvHONQYBa6+6ygY6FdIQfIy3Bft06qB0wYOYE/bdxNsIkjArRAZUJ4ReRMXclT+MvSudocvEhcVqUBCqE+VTxuoJistqCd2xjgQ2uiquu9e0LvIRQl8+uwjDJHS6n7o/iChV+9LuTIHqln69k3bW1CuanPsO9Cm70+eaLo5f/v9R/T8lxv4/LahzgNtSmZqxSwPrsEDldHOExfG735ReO8Xfo/PYb/gJH0KgaqOa+rIIYRcTg7j2JGJMtbHOA6kqp5eV4UEhJhhidmKwmN/g9YBSsOYvXK1GzKNv4urky4aufyYIOXgoXYxQ1cHw2nxpIRjRvFs0fIsZonBx54KQE/Ms/NW/eMTnLyufflzIn5+4vj1izYP9WlWuxhWXo4cIltkXGE2dTPzi5cHzkCzxy0+FEBrjELkYP+H+1ZrLi4HLqw2HJ0farEhrHG8mZPS+p5mX+WaV9vk1+58+Iv+LDXf/4EPyNy7YXcB0nDg9v/YN/QTXz4+kvOXZ9YFxfE68H9EhEZPLA3FFH3Z3QCxQgheWaEodA/r2miq3XHznEr2InI7Kfn/k/Te3ZFHubxLbZEQVijaKugKtBTwjaT16GmitmDXS4NCMq12c4pdj7g+79Ukh0KoyNyXn5NtuVVJwkxwVPyhj8m1wrd2WMXpXGKJjdiEkpClqQguZ47ESTahTxVYZYqJhnIMDOYsPe5HWbpahRAmU3qWxaM4NJDhTQq05xCBuByniJtnNvDuts/HiFj76zIgZ1l2oMjf44vnk3+sucGcyylpoG2NoRsqLyXYAi7RifhjVSp0qm7XzrofBKPsTSiSPuVPshCBGiq0HTHrjIdbVRR3bXA6OkEGCP6vaMlbchNr9FgLkwbvSqgjVCfvQGTaN0AKE6jW0zqRh6/e6NVCn38kCrUjqopLshasvlqS436jJ4N1982W3JxfYWQlmONuorxOXLY5/Vug/093rcg7Mp+YPOh6Ps2Dn/F+Q9F051bFgwxkMZgjt3AyaLmF9X6Og3lTjtjU2MXoxFV1gZfyjWLcWO2sKzq3qMjmpf1InxluXqVYlEbgaMlHcnaqp5+bUZpTYFVWnSt438iqi2Wit0WIgju5QU4vLTV2B5Q96q/5ixxxpoeNExV/KYbWjaaNNlXZSNEJMiZwDeUy044zVxoxxd7UjpMz+GPjso5dci5Lvr0gjIMadix1zrcRpZrfZsB4G1qtEzEotymE/M9XGaW4UFeaqlKrMpVGq0QpuN9YKT18U9ONrXuyP6DHw7Y0w/uxT3n//Lhd3R/IAqxQoKOFOZo4z9svCy3/2GfcffgCjwjAw3r9H2R75+NNr/ujPPueD9+/yzW/dQWthf32D7nak7LidR6IuALxLP6sVWilQG7sf3yPwBnE3MpfK8frI5Xrg8lvOf12VmXztXMDahBYztbmxzZR9cz6GRB5SL3iOFy8jlE9R6osD+kMqHlanptQ6E2PuWU19QSPdRamzALKkM4Ulx+TRKdHNQVYh0MyVYKfjLdttN7Q2RYsxdpUTYq4r7+uTMx8e7ZSkQIxjl5d6EV4oXD7FdIORhrtsdSP0VhtthuPJ+PnncF2VMUGShIpPYohwmJX5WeH6WrjZCW9cZa42sFrh75ao442tcTJjujGOo7AfHbrIYyaOa6JNSHTT8JQSMVTHTjtHPGJY9CKi5souMTkT2xEvsM1vEh5Z7T9DOi9VpENkdJ5wyriXqado0IP/tBY8tiQiQWlWiIw0PRGicc77MOfzavOsMwBrMypKkI3DYe1EjFuHAuhFtRV3uYqeUaV9m7TgokJAYmC7GzneHvxPyVex8qVW0fFT6c9g71a/gqOCf0+uwnRGvunXHPlvWmWTItvYhXwifTFAl566wqSHCDhOJh5LURd9Ln6OmIEFxyQSxkXKjIu0DMFipiIcKcylQIoUU6ainsQogZgTq82KGFyXO4yDY6jaaK0RY/JO3rpvI1DKRGtGdetvf1W0b8VT4rg/UqL/7FocDxoPjbjKfPHZxC9+cc3+Wmk2Qojsb2dKmQgxElNmzCtPj+zUlDYbkcDVdk0z4zQXWnV99qk0jnPhNEfmQqdGza6RzwMPLwam7cTTw5H/+R9/xBv3vuRyOyDRePfuBaurxJ2LEfn+O8zDU/QyoE+OTGXmyfMDnzy6Bal844NLfvIfvs9Hf/0Zo12xGweKBTgo7bjn+GxmuxnZvbUijN4N+HfY46jHzOpig4px2k+crvfYVNitByyCxcCDh1tuj7eUg3A0mLSh1RU8OmSI3q3EceW+uIvT0Tm8retWmjoWao5hlTp3Wk5XNSUnjs9lJo+ZFL0z89a2E72Dk9tz8q72NBWmU/VJRgK7XWYVmr+s5pJWx159adK0kpMvZGjNOzgLXX64LHu8qJj5skX6lkBVaXSVWJN+T70LnWbjyT7wmSXGd7bUL56ywh3SNLj7EUCdfTH1Yq9Mk3HYJO7sYDWY862Da9ZFjGKNuSing099aZzZ3YmstwPKTMxCSs4ASCl4Bpu1MxyXJPZwut5ZY1gTmgBD7Fi1z6FKL66I46JRzs5yOk8wn7yAhm6KYgGtvn9dDERa8wNL+/vXVHsDFP306tHcrUtnLQVohWZHX6Ba7nhn6e8uHZN3JyqCL3hDWKBIp94FGheXws21cHu9jPD+XNl5zu5d6vLfvkq755VENZgf6M5BDV0t9zUK6ioKuxAZbFE79eRKOetLaHjQVQAyLlE1gbkps0FliQeGZMpgypA9xnc/F0z8zwwSiDTGFCim7LXSUiQOTpMaVolxjAiFIUf/8k2Zi6FNGceRMhfncGrxh2fI5Dx0M+iJaT521kIkpbHbKwovr/dc7nbkYSQWsMfKR59e81gnplvYz+5DWktjCfpqrWC1+qnX71SOgTEFovjixfOyIjJEpBkhBoYU2OTIaW4cSuE0F45TRSySR7gc14zJM4bEAk+fHngxN/7837zg6c2tix6i6/yHX92S/+knaCnMZeb2BJsBfvytPb/3uw/YfvdN6vHASTaEVaTsZ37xZ084fNFYj5n3f3yPN7+388VANsZh5YekBIIZh/0tx5sDqkrejj62IcytMQwjGg7ctsLJlGEzooPQgtOSJCg5Rco8A+7qZB1Hn6cJCIzjiJlv62tzI++UnObUWiMN7v8UAwzjijSMX1Ht+LOY8tiLb/NRNwXinBjGgWkqnI6NdfTC0Jp4mmtwFyPpCydZircA0RsFp/51Lqh0maJkPzilvZKg9s7ImtAqYJnalKk0pgm+vBWeHJXVZoAUkdpIA+y2ie1K2I6Bwas1p2bcvCzMs/JiXxlPymbVGAdhlYWUXBQTkxu7xGSsd4ntRd8lxEQeHVdOKfaFoXaH/i6z1T7NSXe7iKF3/bj9X/Vuz4L6PYgDxIRI8rSCAJYSxAHtizyriqSudacv3y0QYnY5Kd3eEKHMJ1LuUI9F7zZ7+W7i1Cjp30etHtmi1rostS9TyWdrR5e3+WcP4oov8MVSTsqduxv2+wN1btRaumDCqV6ufPJ7KLKETC6NVzsX2qrehGmnU/0aCPXXF9Rtd+Zp/aHRzuGMLNQoH33AR4gcI4owaeOo1X1QRcjC2ZOySaIC16Vyao0kwkUvQjHE7lyllJ3wxjfW3H2QuVhH1uvIakzkaKSAjydOanRIocvtJPVOU7UD2f4VteaO+mouP03D6H8elzQqRh4H2hT4i8+eULZ3qSLUigeEBQf2z8Tg5sp+6wdKa0ZtgblHC8XonXrsPEgzJabAkBM5JcYsjFXYz8LxVDnWE0UFHQzM9enjyq0MX77cozFzdbkGUSc5myJSfZnWfFyrplyXmU+e7rnz15nvfeeK59fPeH4zsTNBTsrhMHGqjVrgxZd7Hnwzkzbj2XauCBzmgs2VdiqEFBnXI0UL+8OeaBlmZX5RuDkZp+rbfEmRF8eJMA5YFFZDdl4v3qHEADEsD7vH1pymyQ1FcvJ7U4tLYmnkMSPBuoNYOC8+3fvTFW+1LCoYxy5Ox5ndxZYcA3ETKdYYa+BybARVjkcvgphRRbGk5OSkdBOnwQUc/3QptS84ZIEoEkiXLMdgdMD43LkYbmPZOuS3nwL7OjDNEze3z7lqyjoZecwEXGW1WUcut8ZmDOTBaLpimo06Ka34sx1xq78YnYedR2O1jlxdrcgJSjkhQzyzIdyv2WOlpU8BbrLuKaP+3tAPSCMOkWqGFXeRA1AJXswQp01Fp18ZfQlk/v04jupuZE3dalFLpakSUoWY3S8WN4RJyQUIZ+ZVn66DuSKt9VQEkL4T7O+15P7MB0zd5Jp+SGjfaDqn2KlRfiAGdpvIZmNcF3NueoctzVyw4Y/Dsp3shtPmkI9q6xz2ZZHlBf3XNKj/N0opE5p4eJlnoSu1j1iOFbn/6FI4Go6Hnkolx8gm+dbNR3mYqgPDMXjI7TZHj2uWQE6Jm1p4RoU7Ax/81gX3312xGQaSGnlQhiwMY6KUmRQStbjhQ51m2txI3a1dFkbygvEK5NCJ7XnN/vYWbcVd+McBCUIcBhhW/OXnX/Jyu8ZaY57V89et9ZiNHo1RPewv9hvc1NAMpTZqp11IbbiU3U1VQgCKkWplGDI5ZS7GzG674ngqngl+KsxVUatMs1JbYbsaePtO4GK7ZTMYQ/aXynsB/25v58Knz/cYE6X6ZvnR8xveuB2Iw8j1iwNZEjnCnXeveB6ekwSu3t75fZ29IzlZ5enp5JhzU65yYBhdDXRzPPF8f8sqjGzmTHl54tl8YrfZYBg3xyOWE2FMWGjeLXTXq4VaVc3J8l683ZdhTJ5mS3f6KdpYb7YgxnS6JcYVebU7d0OC0KbCaT4gMRD7/Qvq97cq7vg+xO6beuSDNzI7Zp5cN/Ynd4vfjtBCQrRTgXAYS6XnONHcOU26ZVwAbbPT8IKPwCqBFAWrxV9q863zXL3LfHZKvGiBvF1zqrcecd13Dm2ulABlJdhGiGNgtV4OHuGcNrqQznmVlhEDrMbAdpM5TSdCTkinEPomX10Giy/xRJwZ4ehnT/yEs4qp1V5Igyy/xeNegvmSNkUISlU7O0pJcGdZ56/61Geh2xNGZ4RGDOs2mK50Sp2vHvvP92WYYh1TSV7kMFRrp2aZ08Os2xB27q/i0tvWvKtstadPdAcRF0qMEIX7d1fsb17QWnr1vdJNxL3lPRdMp9D15WZ0hkUHV6ml0qoneXytgjrjioUl0bRgbsgRIBnnmFx/WZS5NaqBRrda29fCdW0cFHIMbELgbspcBi8IVaCKn2w3c2F/oYwPNlzeX/POe5esVpBTQkthu1nTmDkej6+WYCpo7SRd0S5DVMwaMQ3EmClzobZKHlf+5+YTq+z4lYaImTGOCXLiV4/33N74Nv44ORNh2QK7NjrT1Eg9M0kMJ/pGv0lxSGc7sNa8qCxyxKaONFdgasYQEgMQkzDmxHiROOaZ09yoyyltkRRHVkOizjPHqsgwUqU5LhkgruAiJC7fuc97VxOfP33JvjRupsJnL058+HDDcVJO2TfcFw8zd956g2FIbLaBubprk4ry4njk+amgBHYpoGmgFWVqhefXe+apsdsmz+3aJr7/8A0+fbzn6fMTUwpsNpk8RFLOhOZHeQwJmnTqTUDrjEUhSUair/sV5/vWUsijY5yIMF7dA5RyKt555YSkTMhwnG+x2tgNQg4rx+iGyDRPzgM9NlbDwNX9xv21QTHECqdDw4qAeJ7SSpWAd+du7tE8U0qWfCZ/2s64b9NuSxnpvt8EBubo3hFR3dBnmguPn1ee2MDVxQVP91OXiU1cv2jkIVLmSsyR9S6xaQCRHNU7clzD7rHWDU8o9WKRIgyjEFMllEoeRt+Mm5uyu7RdXcKZHC/2A86nxkW7rqZIZ0qYWhduNMeY3SCAVis5ZJokpL9biMcJITC32Z/VhPNm1aPSm1ayRUwiMQW0zV5uonbPCiEGj0HRhX7ZF3WSA2IDtU7+bsnotDTca0JRYh5oFl6Z0cTOAViYA53PTID1NrPaZqYXbt3pslxXxKU8dJqA08Na30yaOcZuCIfJ+etalPk4UTXyN11ybndfX6+v19fr6/X1/+oK/19/gNfX6+v19fr6/8v1uqC+vl5fr6/X12/oel1QX1+vr9fX6+s3dL0uqK+v19fr6/X1G7peF9TX1+vr9fX6+g1drwvq6+v19fp6ff2Grn8LJULoTmPqfNkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Yellow boxes are the true annotation, blue are the detected objects with confidence score above 0.3\n",
    "show_detected_objects(test_df.iloc[217])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at a few of the test images, a little less than 50% are classified correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained model no. 3: YOLOv3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to try YOLOv3 due to speed (100x faster than Fast R-CNN). YOLO (You Only Look Once) looks at the whole image at test time so it's predictions are informed by global context in the image. It makes predictions with a single network evaluation unlike systems like R-CNN which require thousands for a single image [Source](https://pjreddie.com/darknet/yolo/). YOLO3 uses Darknet-53 as its backbone feature extractor, which has 53 convolutional layers trained on ImageNet. For the task of detection, 53 more layers are stacked onto it, giving us a 106 layer fully convolutional underlying architecture for YOLO v3 [Source](https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b). YOLO3 is better (in terms of Average Precision) than Faster R-CNN for small objects, but behind RetinaNet. It is not as accurate as other models for medium and large objects.\n",
    "\n",
    "YOLOv3 makes detections on 3 different scales.  YOLO is a fully convolutional network and its eventual output is generated by applying a 1 x 1 kernel on a feature map. In YOLO v3, the detection is done by applying 1 x 1 detection kernels on feature maps of three different sizes at three different places in the network [Source](https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://miro.medium.com/max/1000/1*d4Eg17IVJ0L41e7CTWLLSg.png>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained model no. 3: VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VGG-16 model is a 16-layer (convolution and fully connected) network built on the ImageNet database, which is built for the purpose of image recognition and classification [Source](https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a). The model achieves 92.7% top-5 test accuracy in ImageNet, which is a dataset of over 14 million images belonging to 1000 classes. It makes use of multiple 3x3 kernel-dized filters one after another. VGG-16 was trained for weeks using NVIDIA Titan Black GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://miro.medium.com/max/605/1*E7zhhan7Sp7hats4jkKdeA.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of 13 convolution layers using 3 x 3 convolution filters along with max pooling layers for downsampling and a total of two fully connected hidden layers of 4096 units in each layer followed by a dense layer of 1000 units, where each unit represents one of the image categories in the ImageNet database. We do not need the last three layers since we will be using our own fully connected dense layers to predict whether images will belong to a certain room type. We are more concerned with the first five blocks, so that we can leverage the VGG model as an effective feature extractor. We will use it as a simple feature extractor by freezing all the five convolution blocks to make sure their weights dont get updated after each epoch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Images into Numpy Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for Images to go into a TensorFlow model, they to be read and decoded into integer tensors, then converted to floating point and normalized to small values (between 0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below function converts all of the images into numpy arrays\n",
    "from PIL import Image\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "# We need to get all of the images into arrays, which will be used to combine with the bounding boxes & labels\n",
    "for image_path in TEST_IMAGE_PATHS:\n",
    "  image = Image.open(image_path)\n",
    "  # the array based representation of the image will be used later in order to prepare the\n",
    "  # result image with boxes and labels on it.\n",
    "  image_np = load_image_into_numpy_array(image)\n",
    "plt.figure(figsize=IMAGE_SIZE)\n",
    "plt.imshow(image_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to Normalize pixel values (integers between 0-255) to values between 0 and 1 by dividing all pixel values by 255. \n",
    "Why? Because large integer values can slow down the ML process. Neural networks process inputs using small weight values )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
